<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0-rc2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jiaxiyang.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="深度学习&#x2F;自动驾驶&#x2F;C++&#x2F;性能优化">
<meta property="og:type" content="website">
<meta property="og:title" content="Xiyang">
<meta property="og:url" content="https://jiaxiyang.github.io/page/28/index.html">
<meta property="og:site_name" content="Xiyang">
<meta property="og:description" content="深度学习&#x2F;自动驾驶&#x2F;C++&#x2F;性能优化">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="贾夕阳">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://jiaxiyang.github.io/page/28/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Xiyang</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WGS6S6YFJ6"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-WGS6S6YFJ6');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Xiyang</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Think twice, code once!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">195</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">44</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">55</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/jiaxiyang" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2021/05/10/3d_detection/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/10/3d_detection/" class="post-title-link" itemprop="url">3d Detection</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-10 12:48:48" itemprop="dateCreated datePublished" datetime="2021-05-10T12:48:48+08:00">2021-05-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-20 15:37:37" itemprop="dateModified" datetime="2023-06-20T15:37:37+08:00">2023-06-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Algorithm/" itemprop="url" rel="index"><span itemprop="name">Algorithm</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Algorithm/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/05/10/3d_detection/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/05/10/3d_detection/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>551</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="lidar-data"><a href="#lidar-data" class="headerlink" title="lidar data"></a>lidar data</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.shuangyi-tech.com/news_161.html">format</a></li>
<li>pts、LAS、PCD、.xyz 和.pcap</li>
</ol>
<h2 id="PCL"><a href="#PCL" class="headerlink" title="PCL"></a>PCL</h2><ol>
<li>install pcl and pcl-dev or install from source</li>
<li><a target="_blank" rel="noopener" href="https://pcl.readthedocs.io/projects/tutorials/en/latest/compiling_pcl_posix.html">dependency</a></li>
<li>pcl version &gt; 1.10 否则匹配不上 vtk</li>
<li>notes: update yum source when dependency error</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/a464057216/article/details/54864591">pcd</a></li>
<li>yum search boost; yum infoe boost;</li>
<li>yum install boost169 boost169-devel libusbx-devel</li>
<li><code>cmake -DBOOST_INCLUDEDIR=/usr/include/boost169 -DBOOST_LIBRARYDIR=/usr/lib64/boost169 ..</code></li>
<li>rm find_package(openni) and rm with_openni</li>
<li><a target="_blank" rel="noopener" href="https://pointclouds.org/documentation/tutorials/passthrough.html">passthrough 例子</a></li>
</ol>
<h2 id="ouster-lidar"><a href="#ouster-lidar" class="headerlink" title="ouster lidar"></a>ouster lidar</h2><ol>
<li><a target="_blank" rel="noopener" href="https://forum.ouster.at/">forum</a></li>
<li>不支持 pcd 格式输出，支持 pcap 格式</li>
</ol>
<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9127813">survey</a></li>
<li><a href="A_Survey_on_3D_Object_Detection_Methods_for_Autonomous_Driving_Applications">A_Survey_on_3D_Object_Detection_Methods_for_Autonomous_Driving_Applications</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/traveller59/second.pytorch">SECOND</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/W1995S/article/details/114498082?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0.no_search_link&spm=1001.2101.3001.4242">pointpillars</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/75103748">点云目标检测之 VoxelNet、SECOND、PointPillars、PV-RCNN</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2021/04/13/TensorRT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/04/13/TensorRT/" class="post-title-link" itemprop="url">TensorRT</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-04-13 21:35:32" itemprop="dateCreated datePublished" datetime="2021-04-13T21:35:32+08:00">2021-04-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-09-26 16:16:16" itemprop="dateModified" datetime="2024-09-26T16:16:16+08:00">2024-09-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Program/" itemprop="url" rel="index"><span itemprop="name">Program</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Program/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/04/13/TensorRT/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/04/13/TensorRT/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>22k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>20 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="NOTE"><a href="#NOTE" class="headerlink" title="NOTE"></a>NOTE</h2><ol>
<li>多看开发文档 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work-with-loops">developer guide</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#data-layout">data layout</a><ul>
<li>While optimizing the network, TensorRT performs transformations internally (including to HWC, but also more complex formats) to use the fastest possible CUDA kernels. In general, formats are chosen to optimize performance, and applications have no control over the choices. However, the underlying data formats are exposed at I&#x2F;O boundaries (network input and output, and passing data to and from plugins) to allow applications to minimize unnecessary format transformations.</li>
<li>模型内部只支持 hwc layout（cuda kernel 效率更高), 如果输入是 chw, tensort 会内部做转换成 hwc</li>
<li>输入输出都是 hwc，会更高效一些</li>
</ul>
</li>
<li>tensorrt log 设置，打印出 warning 和 error: <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#build-phase-c">link</a>, trt 内部代码会使用 ILogger 的 log 函数，内部有各种级别的 log</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Logger</span> : <span class="keyword">public</span> ILogger</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">log</span><span class="params">(Severity severity, <span class="type">const</span> <span class="type">char</span>* msg)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">// suppress info-level messages</span></span><br><span class="line">        <span class="keyword">if</span> (severity &lt;= Severity::kWARNING)</span><br><span class="line">            std::cout &lt;&lt; msg &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; logger;</span><br></pre></td></tr></table></figure>

<h2 id="转-trt-engine-方法"><a href="#转-trt-engine-方法" class="headerlink" title="转 trt engine 方法"></a>转 trt engine 方法</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA-AI-IOT/torch2trt">torch2trt</a></li>
<li>TensorRT creates an optimized engine for each profile, choosing CUDA kernels that work for all shapes within the [minimum, maximum] range and are fastest for the optimization point - typically different kernels for each profile. You can then select among profiles at runtime.<ul>
<li>为了优化 dynamic shape 需要提高最大最小 shape</li>
</ul>
</li>
<li>python 用 trt 部署验证模型更有效率</li>
<li>pytorch2.0 之后有了原生的 IR, 可以修改和运行 IR, 也可以直接支持转换成 trt 模型，简化了 torch -&gt; onnx model -&gt; trt engine -&gt; load to torch 的流程</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/TensorRT/">Torch-TensorRT</a> <a target="_blank" rel="noopener" href="https://github.com/pytorch/TensorRT">github</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/onnx/onnx-tensorrt?tab=readme-ov-file#executable-usage">官方两种转 trt 工具</a><ul>
<li>trtexec <code>trtexec --onnx=model.onnx</code></li>
<li>polygraph <code>polygraphy run model.onnx --trt</code></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/onnx/onnx-tensorrt?tab=readme-ov-file#executable-usage">onnx-tensorrt</a> python 接口，直接导出 trt</li>
<li>onnx 转 trt 两种方式：<ul>
<li>onnx –&gt; engine –&gt; c++ inference</li>
<li>onnx –&gt; c++ convert and inference</li>
</ul>
</li>
<li>使用 trtexec 先将 ONNX 转换成 engine<ul>
<li>优点：<ul>
<li>简单性：trtexec 是一个命令行工具，可以很容易地用于转换模型，并且不需要编写任何额外的代码。</li>
<li>灵活性：trtexec 提供了许多命令行选项，用于优化和调试转换过程。</li>
<li>性能测试：您可以使用 trtexec 来测试模型的性能，这在评估优化策略时很有用。</li>
<li>重用：一旦创建了 engine 文件，您可以在不同的应用程序或设备上多次重用它，而不必再次进行优化。</li>
</ul>
</li>
<li>缺点： -两步流程：首先需要使用 trtexec 创建 engine，然后在 C++程序中加载它。这使得整体流程分为两个步骤，可能不如单步流程简洁。</li>
</ul>
</li>
<li>直接使用 C++ 接口转换 ONNX 然后进行推理<ul>
<li>优点：<ul>
<li>集成：整个流程，从加载 ONNX 模型到执行推理，都在一个 C++程序中完成。这对于集成到大型系统或自动化流程中很有用。</li>
<li>动态性：可以根据需要进行动态优化或调整，例如基于不同的输入尺寸或条件。</li>
</ul>
</li>
<li>缺点：<ul>
<li>复杂性：与使用命令行工具相比，需要编写和维护更多的代码。</li>
<li>优化时间：每次运行程序时，如果不保存和重用 engine，都可能需要重新进行模型的优化步骤。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="base"><a href="#base" class="headerlink" title="base"></a>base</h2><ol>
<li>使用 cuda graph 加速 trt 模型 trtexec –useCudaGraph</li>
<li>onnx 转 trt 时假如输入部分和输出部分有些不好支持，可以提到模型外面去做</li>
<li>模型转换后输出 tensor 顺序和 onnx 顺序不一定对应 <a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/onnx-model-and-tensorrt-engine-gives-different-output/193061">onnx-model-and-tensorrt-engine-gives-different-output</a></li>
<li>推理是使用训练好的模型进行预测的过程，而部署是将这个模型集成到最终应用环境的过程。</li>
<li><a target="_blank" rel="noopener" href="https://microsoft.github.io/AI-System/SystemforAI-8-Inference.pdf">推理系统优化目标</a><ul>
<li>延迟(Latency): 满足服务等级协议的延迟</li>
<li>吞吐量(Throughputs): 暴增负载的吞吐量需求</li>
<li>效率(Efficiency): 高效率，低功耗使用 GPU，CPU</li>
<li>灵活性(Flexibility): 支持多种框架，提供构建不同应用的灵活</li>
<li>扩展性(Scalability): 扩展支持不断增长的用户或设备</li>
</ul>
</li>
<li>推理考虑延迟、吞吐量和效率，部署考虑灵活性和可扩展性</li>
<li>提高 batch size 用于提高吞吐</li>
<li>等待浪费 GPU, 造成低效率</li>
<li><code>PyTorch-Quantization</code> is a toolkit for training and evaluating PyTorch models with simulated quantization. Quantization can be added to the model automatically, or manually, allowing the model to be tuned for accuracy and performance. The quantized model can be exported to ONNX and imported to an upcoming version of TensorRT. PyTorch-Quantization 是一个用于通过模拟量化来训练和评估 PyTorch 模型的工具包。量化可以自动或手动添加到模型中，从而可以调整模型的准确性和性能。量化模型可以导出到 ONNX 并导入到即将推出的 TensorRT 版本。</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work-with-loops">work with loop</a></li>
<li>rnn 在 9.0 被移除，使用 loop op 替代</li>
<li>NVIDIA TensorRT is an SDK for optimizing trained deep learning models to enable high-performance inference. TensorRT contains <code>a deep learning inference optimizer for trained deep learning models</code>, and <code>a runtime for execution</code>.</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-861/quick-start-guide/index.html#ecosystem">转换模型步骤</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-861/quick-start-guide/index.html#conv-deploy-opt">多种转换和部署方式</a></li>
<li>all operations in your model must be supported by TensorRT (or you must provide custom plug-ins for unsupported operations)</li>
<li>at inference, we pick a small batch size when we want to prioritize latency and a larger batch size when we want to prioritize throughput.</li>
<li>TensorRT supports TF32, FP32, FP16, and INT8 precisions.</li>
<li>One of the most performant and customizable options for <code>both model conversion and deployment</code> are to use the TensorRT API, which has both C++ and Python bindings.</li>
<li>TensorRT C++ API 中的接口类以前缀 I(interface 的意思) 开头，例如 ILogger 、 IBuilder 等。</li>
<li>同步推理: <code>executeV2</code>方法; 异步推理: <code>enqueueV2</code></li>
<li><code>enqueue</code>异步接口没有很快返回的原因：<ul>
<li>有插件会强制 stream 同步？</li>
<li>nsight system 上看 tensorrt node 调用， 对应 cuda api 里有 cudaStreamSync()函数， 会阻塞 cpu 导致 enqueue 不返回</li>
<li>有多个 stream sync, 每个 stream sync 执行之后之前通过 cuda api 调用的 kernel 都已执行完</li>
<li>为什么要多个 stream？node 不相关可以并行加速, 可以看到 kernel 执行时间有并行， 为什么要 sync? 后面的节点需要前面的节点都执行完，有关联</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/issues/2828#issuecomment-1488371967">控制单模型 stream 个数 –maxAuxStreams</a> Set maximum number of auxiliary(辅助) streams per inference stream.</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#within-inference-multi-streaming">within-inference-multi-streaming</a><ul>
<li>In the context of TensorRT and inference, each layer of the optimized final network will require work on the GPU. However, not all layers will be able to fully use the computation capabilities of the hardware. Scheduling requests in separate streams allows work to be scheduled immediately as the hardware becomes available without unnecessary synchronization. Even if only some layers can be overlapped, overall performance will improve.</li>
<li>如果放在单独流中调度，不能充分利用硬件调度能力，将 request 放在多个流调度时，硬件准备好就能使用，不用额外的 sync, 即使只有少数的 layer 并行，总体结果也会提高</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#trtexec-flags">trtexec-flags 搜–maxAuxStreams</a></li>
<li>enqueue 可能不是真异步，内部有同步</li>
<li>想并行运行多个模型需要使用多线程，net1.enqueueV2(stream1); net2.enqueueV2(stream2);</li>
</ul>
</li>
<li>注意 event sync 和 stream sync 的区别; event sync 在 enqueue 之后执行，enqueue 执行时可能会多次调用 stream sync</li>
<li><img src="https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/nvidia-inference-stack.png" alt="NVIDIA 推理堆栈"></li>
<li>在 TensorRT 8.5 及更高版本中， cuDNN 和 cuBLAS 内核是可选的添加，以减少 TensorRT 库的部署大小。</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#tensor-layout">Tensor Layouts In Memory: NCHW vs NHWC</a></li>
<li>因为为 Tensor Core 实现的卷积需要 NHWC 布局，并且当输入张量以 NHWC 布局时速度最快</li>
<li>NHWC 格式的输入非常适合 NVIDIA 上的 Tensor Core GPU 。由于 ONNX 仅支持 NCHW 格式(?)，因此必须使用技巧启用 NHWC 作为输入张量。将输入维度设置为 NHWC ，并在 CUDA 或 TensorRT EP 删除的输入之后插入 Transpose 操作</li>
<li>tensor core 用 Implicit GEMM 实现的卷积不用生成中间的矩阵，直接原位计算， 不需要 im2col <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#tc-usage">link</a></li>
<li>The process of identifying the sequences of operations that can be fused is called <code>pattern-matching</code>. TensorRT has a powerful pattern-matching algorithm that can identify a lot of possible fusions.</li>
<li>可以使用 python API 来调用转换后的 engine 来和 torch 结果作比较</li>
<li>workspace<ul>
<li>在 TensorRT 中，工作空间（Workspace）是用于存储临时数据（如中间层输出、优化过程中的临时数据）的内存区域。它主要在模型优化和执行过程中被使用。</li>
<li>较大的工作空间可以允许 TensorRT 执行更复杂的优化，可能带来更高的运行效率。然而，过大的工作空间可能导致内存资源的浪费或者在内存受限的设备上无法运行。</li>
<li>在使用 TensorRT API 时，可以通过编程方式配置工作空间的大小。这通常在<code>构建优化引擎（Optimization Engine）时设置</code></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-722/developer-guide/index.html#troubleshooting">Q: How do I choose the optimal workspace size?</a>。</li>
</ul>
</li>
</ol>
<h2 id="Using-the-TensorRT-Runtime-API"><a href="#Using-the-TensorRT-Runtime-API" class="headerlink" title="Using the TensorRT Runtime API"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html#runtime">Using the TensorRT Runtime API</a></h2><h3 id="runtime"><a href="#runtime" class="headerlink" title="runtime"></a>runtime</h3><ol>
<li><code>std::unique_ptr&lt;nvinfer1::IRuntime&gt; runtime&#123;nvinfer1::createInferRuntime(sample::gLogger.getTRTLogger())&#125;;</code></li>
</ol>
<h3 id="engine"><a href="#engine" class="headerlink" title="engine"></a>engine</h3><ol>
<li><code>std::unique_ptr&lt;nvinfer1::ICudaEngine&gt; mEngine(runtime-&gt;deserializeCudaEngine(engineData.data(), fsize, nullptr));</code> runtime deserialize 到 engine</li>
<li><code>auto input_idx = mEngine-&gt;getBindingIndex(&quot;input&quot;);</code> 得到 input idx</li>
<li><code>assert(mEngine-&gt;getBindingDataType(input_idx) == nvinfer1::DataType::kFLOAT);</code> 判断输入类型</li>
<li>通过 engine 拿到输入输出信息，申请 host 和 device 内存，host 上处理完输入之后，copy to device， 执行模型，将结果从 device cp to host</li>
</ol>
<h3 id="context"><a href="#context" class="headerlink" title="context"></a>context</h3><ol>
<li><code>IExecutionContext *context = engine-&gt;createExecutionContext();</code></li>
<li>you can have multiple contexts associated with a single engine, and run them in parallel. 多个 context 可以与一个 engine 结合，并行的运行<ul>
<li>A current exception to this is when using dynamic shapes, when each optimization profile can only have one execution context, unless the preview feature, kPROFILE_SHARING_0806, is specified. 动态 shape engine 只能有一个 context</li>
</ul>
</li>
<li>A TensorRT execution context encapsulates execution state such as persistent device memory for holding intermediate activation tensors during inferenc</li>
<li>Inference execution is kicked off using the context’s executeV2 or enqueueV2 methods. After the execution is complete, we copy the results back to a host buffer and release all device memory allocations<ul>
<li>executeV3</li>
<li>enqueueV3 <code>bool status = context-&gt;enqueueV3(bindings, stream, nullptr);</code></li>
</ul>
</li>
<li><code>void* bindings[] = &#123;input_mem, output_mem&#125;;</code> input_mem and output_mem 在 gpu device 上</li>
<li><code>auto output_buffer = std::unique_ptr&lt;int&gt;&#123;new int[output_size]&#125;;</code> on cpu</li>
<li><code>cudaMemcpyAsync(output_buffer.get(), output_mem, output_size, cudaMemcpyDeviceToHost, stream);</code> device to host</li>
<li><code>cudaStreamSynchronize(stream);</code></li>
</ol>
<h2 id="NVIDIA-Multi-Instance-GPU-User-Guide"><a href="#NVIDIA-Multi-Instance-GPU-User-Guide" class="headerlink" title="NVIDIA Multi-Instance GPU User Guide"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html">NVIDIA Multi-Instance GPU User Guide</a></h2><ol>
<li>多实例 GPU (MIG) 是采用 NVIDIA Ampere 架构或更高版本架构的 NVIDIA GPU 的一项功能，可实现用户控制的将单个 GPU 划分为多个较小的 GPU。物理分区提供专用计算和内存切片，具有 QoS 以及在部分 GPU 上独立执行并行工作负载。对于 GPU 利用率较低的 TensorRT 应用程序，MIG 可以在对延迟影响很小或没有影响的情况下产生更高的吞吐量。最佳分区方案是特定于应用程序的。</li>
</ol>
<h2 id="ecosystem"><a href="#ecosystem" class="headerlink" title="ecosystem"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html#ecosystem">ecosystem</a></h2><ol>
<li>workflow<ul>
<li>导出 onnx 模型</li>
<li>选择 batch size</li>
<li>选择量化精度</li>
<li>转换模型</li>
<li>发布模型</li>
</ul>
</li>
<li>有多种方式可以转换模型</li>
<li>有多种方式可以部署模型</li>
</ol>
<h2 id="fusion"><a href="#fusion" class="headerlink" title="fusion"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#enable-fusion">fusion</a></h2><h2 id="dla"><a href="#dla" class="headerlink" title="dla"></a><a target="_blank" rel="noopener" href="https://developer.nvidia.com/deep-learning-accelerator">dla</a></h2><ol>
<li>TensorRT allows you to execute part of the network on the DLA and the rest on GPU; engine 可以一部分在 dla 上执行，另外的在 cuda 上</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA-AI-IOT/jetson_dla_tutorial">jetson_dla_tutorial</a></li>
<li>只有 edge 平台才有</li>
<li>jtop -&gt; ENG 上方显示有几个 dla core</li>
</ol>
<h2 id="quantization"><a href="#quantization" class="headerlink" title="quantization"></a>quantization</h2><ol>
<li>FP32, FP16, BF16: unquantized floating point types</li>
<li>fp16 不算量化</li>
<li>low precision quantized types (INT8, FP8, INT4)</li>
<li>TensorRT supports quantized floating point, where floating-point values are <code>linearly</code> compressed and rounded to low precision quantized types (INT8, FP8, INT4). 线性量化</li>
<li>TensorRT must know its dynamic range - that is, what range of values is important to represent - values outside this range are clamped when quantizing. 量化时需要知道数据的动态范围，会做截断</li>
<li>Dynamic range information can be calculated by the builder (this is called calibration) based on representative input data (this is currently supported only for INT8). activation 量化时， 校准时会收集 activation 的动态范围</li>
<li>TensorRT supports the use of low precision types to represent quantized floating point values. The quantization scheme is symmetric quantization - quantized values are represented in signed INT8, FP8E4M3 (FP8 for short), or signed INT4, and the transformation from quantized to unquantized values is simply a multiplication. In the reverse direction, dequantization uses the reciprocal scale, followed by rounding and clamping.<ul>
<li>对称量化， 反量化时只是简单相乘，量化时还需要钳位</li>
</ul>
</li>
<li>TensorRT quantizes activations as well as weights to INT8 and FP8. For INT4, weight-only-quantization is supported.<ul>
<li>int4 只支持 weight 量化, 不支持 activation</li>
</ul>
</li>
<li>对称线性量化： Q &#x3D; F&#x2F;s(需要钳位); s &#x3D; (Fmax - Fmin) &#x2F; (Qmax - Qmin); 反量化：F &#x3D; Q x s<ul>
<li>s: scale 即最小刻度，Q 表示 F 有多少个最小刻度</li>
</ul>
</li>
<li>scale 是核心，ptq 和 qat 都是为了得到 scale</li>
<li>Post-training quantization (PTQ) derives scale factors after the network has been trained. TensorRT provides a workflow for PTQ, called calibration, where it measures the <code>distribution of activations</code> within each activation tensor as the network executes on representative input data, and then uses that distribution to estimate scale values for each tensor.</li>
<li>calibration 只是为了得到 activation 的分布来估计量化时 tensor 的 scale，weight 不需要</li>
<li>Quantization-aware training (QAT) computes the scale factors during training, using a technique called fake-quantization which simulates the quantization and quantization process. This allows the training process to compensate for the effects of the quantization and dequantization operations.<ul>
<li>量化感知训练 (QAT) 使用一种称为假量化的技术来模拟量化和量化过程，从而在训练期间计算比例因子。这允许训练过程补偿量化和反量化操作的影响。</li>
</ul>
</li>
<li>Implicit quantization is only supported when quantizing for INT8 只有 int8 支持隐式量化</li>
</ol>
<h2 id="streaming-weights"><a href="#streaming-weights" class="headerlink" title="streaming weights"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#streaming-weights">streaming weights</a></h2><ol>
<li>TensorRT can be configured to stream the network’s weights from host memory to device memory during network execution instead of placing them in device memory at engine load time. This enables models with weights larger than free GPU memory to run, but potentially with significantly increased latency.<ul>
<li>需要 host to device, 这使得权重大于可用 GPU 内存的模型可以运行，但延迟可能会显着增加。</li>
</ul>
</li>
</ol>
<h2 id="plugins-custom-op"><a href="#plugins-custom-op" class="headerlink" title="plugins(custom op)"></a>plugins(custom op)</h2><ol>
<li>注意 shape 区别：固定输出 shape, shape 有输入决定(axie), 动态 shape</li>
<li>nonzero 才是动态 shape，输出 shape 不能由输入决定, nonzero 可以由 padding 来解决</li>
<li>构造函数一般设置为三个。<ul>
<li>第一个用于在 parse 阶段，PluginCreator 用于创建该插件时调用的构造函数，需要传递权重信息以及参数。</li>
<li>第二个用于在 clone 阶段，复制这个 plugin 时会用到的构造函数。</li>
<li>第三个用于在 deserialize 阶段，用于将序列化好的权重和参数传入该 plugin 并创建。</li>
</ul>
</li>
<li>TensorRT 在 fp16 运行模式下，运行到不支持 fp16 的插件 op 时，会自动切换到 fp32 模式，等插件 op 运行完再切换回来。</li>
<li>getWorkspaceSize 这个函数需要返回这个插件 op 需要中间显存变量的实际数据大小(bytesize), 在实际运行的时候就可以直接使用 TensorRT 开辟好的空间而不是自己去申请显存空间。</li>
<li>serialize 将 plugin 实例序列化到文件（包含参数，属性等信息）（在 plugin 里 TRTBEVPoolV2::serialize）， deserialize 通过文件发序列化到 plugin 实例(构造插件，在 creator 里做 TRTBEVPoolV2Creator::deserializePlugin;)</li>
<li>createPlugin 用于 build 阶段，生成 plugin 实例(注意不是序列化，序列化是存文件)， deserializePlugin 用于 runtime，用于从 engine 反序列化 plugin 对象</li>
<li>onnx 转换成 engine 存成文件，需要序列化，加载 engine 需要反序列化</li>
<li>supportsFormatCombination 用于检测格式和类型是否支持</li>
<li>tensorrt 支持的 op， 提供的标准 plugins, 自定义的 plugin</li>
<li>分为 build 期和 runtime 期，两个时期需要插件的功能不一样，build 是构建网络用的，runtime 是运行插件时用的</li>
<li>IPluginV3OneCore (build 和 runtime 都需要的接口), IPluginV3OneBuild (build 需要的接口), and IPluginV3OneRuntime(runtime 需要的接口)</li>
<li>core capability 是 build 和 runtime 都需要的接口</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#add_custom_layer">实现流程</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#register-plugin-create">注册方法</a><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/9db1508f7479b24857d4fb81e33b6b10cede6bd0/plugin/api/inferPlugin.cpp#L180">手动 initLibNvInferPlugins</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/9db1508f7479b24857d4fb81e33b6b10cede6bd0/plugin/geluPlugin/geluPlugin.cpp#L43">自动 REGISTER_TENSORRT_PLUGIN</a> static 变量</li>
<li>trtexec 通过 dlopen 方式加载注册，python 通过 ctypes.CDLL 方式加载注册，pgpredict 通过 link 方式启动时加载注册</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#using-plugin-library">using-plugin-library</a> runtime 可以手动加载， 不用 link<ul>
<li><code>runtime-&gt;getPluginRegistry().loadLibrary(pluginLib.c_str())</code> runtime 加载</li>
<li><code>builder-&gt;getPluginRegistry().loadLibrary(pluginLibs[i]);</code> build 时加载</li>
</ul>
</li>
</ul>
</li>
<li>onnx 和 trt plugin 可以通过属性传递参数，封装自定义算子 symbolic g.op()写入属性，trtplugin 代码可以获取属性</li>
<li>要在应用程序中使用 TensorRT 官方插件，必须加载 libnvinfer_plugin.so （Windows 上为 nvinfer_plugin.dll ）库，并且必须通过在应用程序中调用 initLibNvInferPlugins 来注册所有插件代码。</li>
<li>几个概念：<ul>
<li>plugin: 具体 plugin 实现， 需要继承 IPluginV2DynamicExt</li>
<li>plugin creator: plugin 对象创建方法，相当于抽象工场函数的具体工厂, 改工厂的实例会被注册到全局工厂(注册表)中，用来创建 creator</li>
<li>register: 将 plugin creator 工厂函数注册到, tensorrt sample 中注册位置<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/main/plugin/api/inferPlugin.cpp">register tensorrt plugin</a>; 单例模式实现<ul>
<li>需要调用 getPluginRegistry()-&gt;registerCreator(*pluginCreator, libNamespace)来注册</li>
<li>REGISTER_TENSORRT_PLUGIN 宏也可以来注册：#define REGISTER_TENSORRT_PLUGIN(name) static nvinfer1::PluginRegistrar<name> pluginRegistrar##name {}; getPluginRegistry()-&gt;registerCreator(instance, “”);</li>
</ul>
</li>
</ul>
</li>
<li>设计模式插件系统; so 加载时 static 变量初始化注册工厂实例到全局工厂；反射 由字符串来创建对象</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/samples/sampleOnnxMnistCoordConvAC">官方 sample</a><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/plugin/coordConvACPlugin">plugin 代码位置</a></li>
<li>enqueue 在 cu 文件中实现</li>
<li>自己写的插件可能用不到 workspace, 见上述代码 enqueue 函数</li>
<li>onnx shape_inference 在模型上显示 shape 信息</li>
<li>修改模型时要照着 net forward 来改 op input(tensor) output name</li>
<li>自定义 op 内可变参数作为输入, 不变数值直接写死, y&#x3D;kx + 2; k 是参数，2 是数值, y &#x3D; x + 2</li>
<li>netron 上不变参数(weights bias div mul 需要的参数)使用 constant tensor 类型，输入是 variable tensor, attributes 是 op <code>初始化</code>用到的数值， 计算不用</li>
<li>参数传递与转换：在插件中，你需要将原始模型中算子的参数传递到你的插件实现中。这通常涉及到参数的序列化和反序列化。确保所有必要的参数都被正确传递，并且在需要时进行适当的数据类型转换。</li>
<li>attention 输入为 q k v 输出 q 维度类似的结果; 参数可能为各种 shape 信息 地址 head num, scale, mask 等。如果是 paged attention 需要处理 q k v 地址不连续的情况, <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/638468472">link</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/master/plugin">官方 plugin</a></li>
<li>ONNX 解析器会自动尝试将无法识别的节点作为插件导入。如果在插件注册表中找到与该节点具有相同 op_type 的插件</li>
<li>python 自定义 op 用了多种内置 op 实现，在图上表现为多个内置 op，需要把 onnx 模型内置 op 转换为自定义 op 再转换模型</li>
<li>适用场景<ul>
<li>python 中算子 trt 不支持可以用 plugin 方式实现</li>
<li>trt 算子性能有问题</li>
<li>手动算子融合(将融合算子的所有输入作为融合算子的输入， 输出同理)</li>
</ul>
</li>
<li>合并之后的 中间 op 的参数也不用管，因为算子等价，中间参数无用</li>
<li>op 没有权重，只是计算方法，处理输入得到输出，不用关心权重问题, 权重在输入接口,<ul>
<li>attention 插件会在接口中传递输入输出地址，kvcache 地址， 长度等信息，插件负责计算</li>
<li>load 模型时权重会从模型 load 到 op，作为参数传递给 op；序列化与反序列化</li>
</ul>
</li>
<li>onnx 模型添加了 plugin 之后 neoton 可能显示不出 shape 信息，原因是 plugin 的存在，妨碍了自动推断;<ul>
<li>如果 plugin 不改变 shape，直接删除掉之后，在图上就能看到 shape 信息</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#extending">Extending TensorRT with Custom Layers</a></li>
<li>即使 TensorRT 拥有强大的模式匹配算法并支持许多可能的融合，但始终存在无法识别不常见和&#x2F;或非常高级模式的风险。为了克服这一不可避免的限制，TensorRT 提供了一种称为插件的强大机制。</li>
<li>The plugins are nodes inserted in the network graph definition that map to user-defined GPU kernels.</li>
</ol>
<h3 id="python-plugin"><a href="#python-plugin" class="headerlink" title="python plugin"></a>python plugin</h3><ol>
<li>python load plugin.so <a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/loading-of-tensorrt-custom-plugin-shared-library/237504/6">links</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/ba696521eed5d8b49a3c6f21ef243ca4b3474d64/samples/python/onnx_custom_plugin/test_custom_hardmax_plugin.py#L40">tensorrt python 使用 trt 内部结构示例</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/ba696521eed5d8b49a3c6f21ef243ca4b3474d64/python/src/infer/pyPlugin.cpp#L2612">python plugin 相关 bindings</a></li>
<li>ctypes.CDLL 是 Python ctypes 库中的一个函数，用于加载动态链接库（.so 文件，DLL 文件等）。它允许你在 Python 中调用这些库中定义的函数和使用其中的资源。</li>
</ol>
<h2 id="修改模型"><a href="#修改模型" class="headerlink" title="修改模型"></a>修改模型</h2><h3 id="拆分"><a href="#拆分" class="headerlink" title="拆分"></a>拆分</h3><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/master/tools/onnx-graphsurgeon">TensorRT&#x2F;tools&#x2F;onnx-graphsurgeon</a></li>
<li><code>polygraph surgeon extract</code>也可以提取子图</li>
<li>一个一个算子累加形成 grpah 可以找出出现问题的 op; 加入 polygrpah 不能用，trtexec 能用；所有 graph 设置同一个输入，和 onnx 比输出</li>
<li><a target="_blank" rel="noopener" href="https://github.com/leimao/TensorRT-Custom-Plugin-Example/blob/main/scripts/create_identity_neural_network.py">sample</a></li>
</ol>
<h3 id="op-替换"><a href="#op-替换" class="headerlink" title="op 替换"></a>op 替换</h3><h2 id="build"><a href="#build" class="headerlink" title="build"></a>build</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main?tab=readme-ov-file#optional---if-not-using-tensorrt-container-specify-the-tensorrt-ga-release-build-path">源码编译</a><ul>
<li><code>export TRT_LIBPATH=</code>pwd<code>/TensorRT-8.6.1.6/lib</code>必须设置</li>
</ul>
</li>
</ol>
<h2 id="查看版本"><a href="#查看版本" class="headerlink" title="查看版本"></a>查看版本</h2><ol>
<li>jtop -&gt; INFO</li>
<li>jetson_release</li>
<li>nvidia-smi</li>
<li><code>dpkg-query -W tensorrt</code> tensorrt 版本</li>
<li><code>cat /proc/driver/nvidia/gpus/0000\:02\:00.0/information</code> 查看硬件型号</li>
</ol>
<h2 id="查看模型信息-对数"><a href="#查看模型信息-对数" class="headerlink" title="查看模型信息, 对数"></a>查看模型信息, 对数</h2><ol>
<li>参考 python_sandbox&#x2F;model_convert<ul>
<li>get_onnx_detail_info.py 能够获取算子 doc_string, 可以看堆栈，找到算子所在位置</li>
</ul>
</li>
<li>余弦相似度来衡量两个 tensor 是否对齐</li>
<li>pytorch 模型，onnx 模型，trt 模型都要一致<ul>
<li>pytorch 和 onnxruntime 先对数，然后 onnxruntime 和 trt 对数</li>
<li>有 plugin 时可以直接对 trt 和 pytorch 结果, pytorch 添加 hook 导出所有 tensor 输出，onnx 对应 pytorch 输出，onnx 切分子图，分别转成 tensorrt 模型，每一个都有输入输出，根据 onnx 模型找 pytorch 输出，看哪个算子对不上<ul>
<li>pytorch 模型 dump 数据时需要处理 sequential 和 submodule 问题，重复 layer 使用问题，onnx 模型导出时需要处理将中间层输出，名字要和 pytorch 导出的名字对上; 如果是直接 pytorch 和 trt 对数，不用对 onnx 中间结果</li>
<li>pytorch 中间结果和 trt 中间结果做对比时， 注意 onnx 和 trt 算子可能融合，需要比较 pytorch 前后一些算子</li>
</ul>
</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_intermediate_outputs_pytorch</span>(<span class="params">model, input_tensor</span>):</span><br><span class="line">     intermediate_outputs = &#123;&#125;</span><br><span class="line">     hooks = []</span><br><span class="line"></span><br><span class="line">     <span class="comment"># 处理重复使用情况</span></span><br><span class="line">     <span class="keyword">def</span> <span class="title function_">hook_fn</span>(<span class="params">module, <span class="built_in">input</span>, output, name</span>):</span><br><span class="line">        <span class="keyword">if</span> name <span class="keyword">in</span> intermediate_outputs:</span><br><span class="line">            count = <span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> name + <span class="string">&quot;_&quot;</span> + conut.to_str() <span class="keyword">in</span> intermediate_outputs：</span><br><span class="line">                count = count + <span class="number">1</span></span><br><span class="line">            intermediate_outputs[name + <span class="string">&quot;_&quot;</span> + conut.to_str()] = [output]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            intermediate_outputs[name] = [output]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 递归注册钩子</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">register_hooks</span>(<span class="params">module, prefix=<span class="string">&#x27;&#x27;</span></span>):</span><br><span class="line">        <span class="keyword">for</span> name, submodule <span class="keyword">in</span> module.named_children():</span><br><span class="line">            submodule_name = prefix + (<span class="string">&#x27;.&#x27;</span> <span class="keyword">if</span> prefix <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span>) + name</span><br><span class="line">            hook_handle = submodule.register_forward_hook(<span class="keyword">lambda</span> m, i, o, name=submodule_name: hook_fn(m, i, o, name))</span><br><span class="line">            hooks.append(hook_handle)</span><br><span class="line">            register_hooks(submodule, submodule_name)</span><br><span class="line"></span><br><span class="line">    register_hooks(model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        model(input_tensor)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 移除钩子</span></span><br><span class="line">    <span class="keyword">for</span> hook_handle <span class="keyword">in</span> hooks:</span><br><span class="line">        hook_handle.remove()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> intermediate_outputs</span><br></pre></td></tr></table></figure>

<ol>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy/polygraphy/tools/">polygraphy</a><ul>
<li>Use Polygraphy to dump layer outputs and verify there are no NaNs or Infs. The –validate option can check for NaNs and Infs. Also, we can compare layer outputs with golden values from, for example, ONNX runtime.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/c5b9de37f7ef9034e2efc621c664145c7c12436e/tools/polygraphy-extension-trtexec">polygraphy-extension-trtexec</a><ul>
<li><a target="_blank" rel="noopener" href="https://www.wheelodex.org/projects/polygraphy-trtexec/">位置</a></li>
</ul>
</li>
<li><code>--mark all</code> is not a good practice, it will break all layer fusions. <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/issues/3592#issuecomment-1888624281">link</a> <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/issues/3596#issuecomment-1896408532">link</a><ul>
<li>I think mark all is becoming increasingly unreliable. A better approach is to manually mark a subset of the intermediate tensors as outputs.</li>
</ul>
</li>
<li><code>polygraphy run   sim_sparsedrive.onnx --fp16   --trt --trt-outputs /deformable_aggregation_output_0   --save-engine test.trt --plugins=/workspace/mmdeploy/mmdeploy/lib/libmmdeploy_tensorrt_ops.so   --save-outputs ./dump.json --save-timing-cache=fp16cache --load-timing-cache=fp16cache</code> 可以 mark 一些 output</li>
<li><code>polygraphy run  model.onnx --trt  --trt-outputs mark all    --save-outputs ./model3.json</code> 转完跑一遍 trt, mark all；<ul>
<li><a target="_blank" rel="noopener" href="https://www.stubbornhuang.com/2432/"> 转换 onnx 模型出现 requires bool or uint8 I&#x2F;O but node can not be handled by Myelin</a></li>
</ul>
</li>
<li><code>polygraphy run  sparsedrive_fp16_3090.trt --model-type=engine  --plugins=/workspace/mmdeploy/mmdeploy/lib/libmmdeploy_tensorrt_ops.so   --trt-outputs mark all --trt  --save-outputs ./dump.json</code> 运行 trt 模型</li>
<li><code>polygraphy run model.onnx --trt</code> 转 trt 模型</li>
<li><code>polygraphy inspect model model.onnx --shape-inference --show layers</code> 查看 onnx 模型信息</li>
<li><code>polygraphy inspect model op16_iter7_refine_filter_fb.trt --model-type=engine --show layers</code> 查看 trt 模型信息</li>
<li><code>polygraphy inspect capability model.onnx -v</code> 查看 onnx 是否被 TensorRT 支持<ul>
<li><code>LD_PRELOAD=/workspace/mmdeploy/mmdeploy/lib/libmmdeploy_tensorrt_ops.so polygraphy inspect  capability sparsedrive_lantu_op13_fd.onnx -v</code> 有插件用 LD_PRELOAD； 还可以看用了什么插件</li>
</ul>
</li>
<li><code>polygraphy run dynamic_identity.onnx --trt --onnxrt --save-engine test.trt</code> Comparing TensorRT And ONNX-Runtime Outputs</li>
<li><code>polygraphy run your.onnx --trt --onnxrt --onnx-outputs mark all --trt-outputs mark all</code> To mark all nodes in onnx as output and compare between onnxruntime and trt<ul>
<li>只有 onnx 才能 run</li>
</ul>
</li>
<li><code> --trt --trt</code> trt 结果与 trt 结果比较, 跑两次比较，查看输出是否异常</li>
<li><code>polygraphy run dynamic_identity.onnx --trt --fp16 --onnxrt --input-shapes X:[1,2,4,4]</code> Comparing TensorRT Precisions</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy/examples/cli/run/05_comparing_with_custom_input_data">(good) run&#x2F;05_comparing_with_custom_input_data</a></li>
<li><code>polygraphy surgeon sanitize model.onnx --fold-constants -o folded.onnx</code> 可以 fold constant, 作为 op 参数, 不用作为 input<ul>
<li>可以看出来有多少个 node</li>
</ul>
</li>
<li><code>polygraphy debug precision net_bs8.onnx --fp16 --tactic-sources cublas --check polygraphy run polygraphy_debug.engine --trt --load-outputs onnx_res.json --abs 1e-1</code></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy/examples/api/00_inference_with_tensorrt">polygraphy python 接口</a><ul>
<li>比 tensorrt python 接口好用</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/tools/experimental/trt-engine-explorer">trt-engine-explorer</a></li>
</ol>
<h2 id="op-支持"><a href="#op-支持" class="headerlink" title="op 支持"></a>op 支持</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/onnx/onnx-tensorrt/blob/main/docs/operators.md">onnx operators.md</a></li>
<li>tensorrt 支持 rnn, lstm, gru;可能算子内部循环，外部还是 dag</li>
</ol>
<h2 id="TensorRT-CUDA-cuDNN"><a href="#TensorRT-CUDA-cuDNN" class="headerlink" title="TensorRT, CUDA, cuDNN"></a>TensorRT, CUDA, cuDNN</h2><ol>
<li>CUDA 是<code>并行计算框架</code>，提供 API，可以运行在 Nvdia GPU 上</li>
<li>cuDNN 是深度神经网络加速库，cuDNN 可大幅优化标准例程（例如用于前向传播和反向传播的卷积层、池化层、归一化层和激活层）的实施。世界各地的深度学习研究人员和框架开发者都依赖 cuDNN 实现高性能 GPU 加速。借助 cuDNN，研究人员和开发者可以<code>专注于训练神经网络及开发软件应用</code>，而不必花时间进行低层级的 GPU 性能调整。cuDNN 可加速广泛应用的深度学习框架，包括 Caffe2、Chainer、Keras、MATLAB、MxNet、PyTorch 和 TensorFlow。</li>
<li>TensorRT 是用于高性能深度学习<code>推理</code>的 SDK。此 SDK 包含深度学习推理优化器和<code>运行时环境</code>，可为深度学习推理应用提供低延迟和高吞吐量。借助 TensorRT，开发者可专注于创建新颖的 AI 支持应用，无需费力调节性能来部署推理工作。</li>
</ol>
<h2 id="trt-engine-explorer"><a href="#trt-engine-explorer" class="headerlink" title="trt-engine-explorer"></a><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/tools/experimental/trt-engine-explorer">trt-engine-explorer</a></h2><h2 id="trtexec"><a href="#trtexec" class="headerlink" title="trtexec"></a>trtexec</h2><h3 id="转模型"><a href="#转模型" class="headerlink" title="转模型"></a>转模型</h3><ol>
<li>NOTE: <code>--noTF32 Disable tf32 precision (default is to enable tf32, in addition to fp32)</code>转出的模型默认是 tf32<ul>
<li>A100、H100 tf32 算力比 fp32 算力高 8 倍左右</li>
<li>3090 上 tf32 算力跟 fp32 算力相同</li>
</ul>
</li>
<li><code>trtexec --onnx=model.onnx --saveEngine=model.trt</code></li>
<li><code>trtexec --onnx=model.onnx --saveEngine=model_fp16.trt --fp16</code></li>
<li><code>trtexec --onnx=model.onnx --saveEngine=model_int8.trt --int8</code></li>
<li>trtexec –onnx&#x3D;.&#x2F;bisenetv2_310102_1280x1920.onnx –shapes&#x3D;input:1x3x540x960 –saveEngine&#x3D;bisenetv2-xavier-fp16-540x960.engine –allowGPUFallback –fp16 –workspace&#x3D;16024</li>
<li>onnx 模型转化为 engine or trt 后运行</li>
<li>trtexec 在转换模型时做了一些优化，以提高模型的推理性能。这些优化包括：<ul>
<li><code>算子融合：</code>trtexec 会尝试将多个算子融合为一个算子，以减少推理过程中的开销。</li>
<li><code>内存优化：</code>trtexec 会尝试将模型的权重和激活值存储在更高效的内存中，以提高推理速度。</li>
<li><code>模型裁剪：</code>trtexec 会尝试将模型的参数量减少，以减少推理过程中的计算量。</li>
<li><code>图优化?</code> <a target="_blank" rel="noopener" href="https://microsoft.github.io/AI-System/SystemforAI-9-Compilation%20and%20Optimization.pdf">link</a><ul>
<li>等价变化简化图计算; 算术表达式等价替换：a*0 -&gt; 0; 公共子表达式消除, 常量折叠</li>
<li>子图替换，如 flash attention</li>
<li>包括算子融合</li>
<li>剪枝</li>
<li>调度优化，异步， 可以并行的并行做</li>
<li>内存优化</li>
<li>混合精度</li>
</ul>
</li>
</ul>
</li>
<li>trtexec 执行步骤：<ul>
<li>trtexec 会将模型转换为 TensorRT 的内部表示。</li>
<li>trtexec 会对模型进行分析，以确定可以进行优化的算子。</li>
<li>trtexec 会对模型进行优化，以提高推理性能。</li>
</ul>
</li>
</ol>
<h4 id="性能参数"><a href="#性能参数" class="headerlink" title="性能参数"></a>性能参数</h4><pre><code>- `--best               Enable all precisions to achieve the best performance (default = disabled)`
- `--useCudaGraph       Use CUDA graph to capture engine execution and then launch inference (default = disabled).`
- `--useDLACore=N       Select DLA core N for layers that support DLA (default = none)`
- `--useManagedMemory   Use managed memory instead of separate host and device allocations (default = disabled).`
</code></pre>
<h3 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h3><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy/examples/cli/run/01_comparing_frameworks#comparing-tensorrt-and-onnx-runtime-outputs">Comparing TensorRT And ONNX-Runtime Outputs</a></li>
<li>推理时可以看各种信息：<ul>
<li>Build Options：<ul>
<li>输入输出 name</li>
<li>input output 格式(fp32, fp16…)</li>
<li>batch</li>
<li>layout(chw or hwc)</li>
</ul>
</li>
<li>Inference Options<ul>
<li>推理次数</li>
</ul>
</li>
<li>Device Information</li>
<li>Performance summary</li>
</ul>
</li>
<li><code>trtexec --loadEngine=test.trt</code></li>
<li><code>trtexec --loadEngine=model.trt --exportOutput=output.txt</code> dump model output</li>
<li><code>trtexec --loadEngine=model.trt --exportLayerInfo=LayerInfo.txt</code> dump layer info； 如果出现<code>ForeignNode[onnx::MatMul_311</code>表示不支持; 使用<a target="_blank" rel="noopener" href="https://github.com/daquexian/onnx-simplifier">onnx-simplifier</a>试试</li>
<li><code>trtexec --loadEngine=test.trt --plugins=lib/libtrtplugins.so --duration=10</code> 带 plugin</li>
<li><code>trtexec --loadEngine=test.trt --plugins=lib/libtrtplugins.so --iterations=N --verbose=1</code></li>
<li>动态输入 <code>trtexec --onnx=fcn-resnet101.onnx --fp16 --workspace=64 --minShapes=input:1x3x256x256 --optShapes=input:1x3x1026x1282 --maxShapes=input:1x3x1440x2560 --buildOnly --saveEngine=fcn-resnet101.engine &amp;&amp; trtexec --shapes=input:1x3x1026x1282 --loadEngine=fcn-resnet101.engine</code> Where –shapes sets the input sizes for the dynamic shaped inputs to be used for inference.</li>
<li>(good) <code>trtexec --loadEngine=my_model.trt --dumpProfile=1</code>推理时显示每层耗时</li>
<li>(good) <code>trtexec --loadEngine=my_model.trt --dumpProfile=1 --exportProfile=profile.json</code>推理时显示每层耗时</li>
<li>对 profile.json 进行排序</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取 JSON 文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;profile.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    data = json.load(file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查文件结构</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(data))  <span class="comment"># 确保数据是一个列表</span></span><br><span class="line"><span class="built_in">print</span>(data[<span class="number">0</span>])     <span class="comment"># 查看第一个元素的结构</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据 &#x27;percentage&#x27; 字段进行排序</span></span><br><span class="line">sorted_data = <span class="built_in">sorted</span>(data, key=<span class="keyword">lambda</span> x: x.get(<span class="string">&#x27;percentage&#x27;</span>, <span class="number">0</span>), reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印排序后的数据</span></span><br><span class="line"><span class="comment"># for entry in sorted_data:</span></span><br><span class="line">    <span class="comment"># print(f&quot;Name: &#123;entry.get(&#x27;name&#x27;)&#125;, Percentage: &#123;entry.get(&#x27;percentage&#x27;)&#125;&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可选：将排序后的数据写入新的 JSON 文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;sorted_profile.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    json.dump(sorted_data, file, indent=<span class="number">4</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="python-推理"><a href="#python-推理" class="headerlink" title="python 推理"></a>python 推理</h4><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/onnx/onnx-tensorrt?tab=readme-ov-file#executable-usage">onnx-tensorrt 来推理 onnx 模型</a><ul>
<li>需要本地先 build 再 install</li>
<li>onnx-tensorrt 与 polygraphy python 接口类似，不需要使用管理显存，但 onnx-tensorrt 需要依赖 pycuda</li>
</ul>
</li>
<li>需要 pycuda</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=$&#123;LD_LIBRARY_PATH&#125;:/usr/local/cuda/lib64</span><br><span class="line">export CPATH=$CPATH:/usr/local/cuda/include/</span><br><span class="line">export PATH=$&#123;PATH&#125;:/usr/local/cuda/bin</span><br><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pycuda</span><br></pre></td></tr></table></figure>

<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/container-release-notes/index.html#pullcontainer">官方 docker</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#troubleshooting">troubleshooting</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/index.html">tensorrt docs</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html">各种版本 doc</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html">release notes</a> Ensure you are familiar with the NVIDIA TensorRT Release Notes for the latest new features and known issues.</li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/zh-cn/blog/tensorrt-c-interface-cn/">官方 c++ sample 讲解</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/zh-cn/blog/search-posts/?q=tensorrt">官方博客</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#c_topics">c++ api</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/samples">官方 sample</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/wang-xinyu/tensorrtx">samples</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/release/7.1/samples/opensource/trtexec">trtexec</a></li>
<li><a target="_blank" rel="noopener" href="http://www.hpc.iitkgp.ac.in/pdfs/Nvidia-2.pdf">nvidia tensorrt pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/plugin">plugins</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/main/quickstart/IntroNotebooks/5.%20Understanding%20TensorRT%20Runtimes.ipynb">Understanding TensorRT Runtimes</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/cyrusbehr/tensorrt-cpp-api">c++ api 教程</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/zh-cn/blog/end-to-end-ai-for-nvidia-based-pcs-cuda-and-tensorrt-execution-providers-in-onnx-runtime/">基于 NVIDIA 的 PC 的端到端 AI ： ONNX Runtime 中的 CUDA 和 TensorRT 执行提供程序</a></li>
</ol>
<h2 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h2><ol>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/container-release-notes/index.html#pullcontainer">官方 docker</a><ul>
<li>docker 里有 tensorrt sample</li>
<li><code>docker pull nvcr.io/nvidia/tensorrt:24.02-py3</code></li>
<li><code>docker run -itd -v /home/xiyang.jia:/home/xiyang.jia -p 10086:22 --user root --gpus all --name=xiyang_cuda --shm-size 2g --cap-add=SYS_ADMIN nvcr.io/nvidia/tensorrt:22.07-py3</code> 注意加–cap-add&#x3D;SYS_ADMIN，ncu 收集 gpu metrics</li>
<li><code>docker exec -it xiyang_cuda  bash</code></li>
</ul>
</li>
</ol>
<h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><ol>
<li><p>bevfusion 遇见的问题</p>
</li>
<li><p>onnx 转换使用心得</p>
</li>
</ol>
<h2 id="常见错误"><a href="#常见错误" class="headerlink" title="常见错误"></a>常见错误</h2><ol>
<li>转换出现类型问题，可能需要对模型进行常量折叠 <code>polygraphy surgeon sanitize model.onnx --fold-constants -o folded.onnx</code></li>
<li>出现 trt fuse 问题先找到 python 代码，调试一下; 更改逻辑，去除 fuse 问题</li>
<li>nvrtc_compile.cpp:940: CHECK(false) failed. NVRTC Compilation failure [09&#x2F;07&#x2F;2024-16:07:22] [E] Error[10]: Could not find any implementation for node {ForeignNode[onnx::Where_4341…&#x2F;Div_8]}. Timing Runner: {ForeignNode[onnx::Where_3922…&#x2F;Transpose_9 + &#x2F;Reshape_49]} (Myelin[0x80000023])<ul>
<li>解决方法： 把 where 去掉</li>
<li><a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/conversion-pytorch-to-tensorrt-fails-when-using-fp16-works-with-fp32-and-int8/285141">conversion-pytorch-to-tensorrt-fails-when-using-fp16-works-with-fp32-and-int8</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/issues/3154">Could not find any implementation for node Error while converting the instructor-large model from ONNX to TensorRT engine</a></li>
<li>并通过增加工作空间大小来解决 <a target="_blank" rel="noopener" href="https://github.com/onnx/onnx-tensorrt/issues/758#issuecomment-1095330781">link</a></li>
<li>更换 opset 版本</li>
<li>更换 trt 版本</li>
</ul>
</li>
<li><code>--workspace flag has been deprecated by --memPoolSize flag.</code></li>
<li>fp32 无问题，fp16 输出出现 inf; fp16 范围：±65504，判断 fp32 模型的中间结果是否会溢出<ul>
<li>可能是 inverse_sigmoid 问题， <code>1.0 / 1.0014e-05 = 99860.19572598363</code>越界</li>
<li>解决办法，pytorch 用 fp16 finetune 一下，pytorch 会处理？</li>
<li>为了避免 FP16 模型出现溢出问题，可以使用 PyTorch 的 AMP（自动混合精度）工具包，它能够自动选择 FP32 和 FP16 混合精度运算，减少数值溢出风险。</li>
<li>查找问题，pytorch 中 model.half()还需处理输入</li>
<li>使用 torch.cuda.amp.autocast 来进行 fp16 推理，vscode debug 来查找第一次出现 inf 时的地方</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">   <span class="comment"># 使用 autocast 进行 FP16 推理</span></span><br><span class="line"><span class="keyword">with</span> torch.cuda.amp.autocast():</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 禁用梯度计算</span></span><br><span class="line">        output = model(input_data)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2021/03/22/Actix/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/22/Actix/" class="post-title-link" itemprop="url">Actix</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-03-22 09:53:35" itemprop="dateCreated datePublished" datetime="2021-03-22T09:53:35+08:00">2021-03-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-20 15:37:37" itemprop="dateModified" datetime="2023-06-20T15:37:37+08:00">2023-06-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Program/" itemprop="url" rel="index"><span itemprop="name">Program</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Program/Rust/" itemprop="url" rel="index"><span itemprop="name">Rust</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/03/22/Actix/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/03/22/Actix/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>931</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="basic"><a href="#basic" class="headerlink" title="basic"></a>basic</h2><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> actix_web::&#123;get, post, web, App, HttpResponse, HttpServer, Responder&#125;;</span><br><span class="line"></span><br><span class="line"><span class="meta">#[get(<span class="string">&quot;/&quot;</span>)]</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">fn</span> <span class="title function_">hello</span>() <span class="punctuation">-&gt;</span> <span class="keyword">impl</span> <span class="title class_">Responder</span> &#123;</span><br><span class="line">    HttpResponse::<span class="title function_ invoke__">Ok</span>().<span class="title function_ invoke__">body</span>(<span class="string">&quot;Hello world!&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#[post(<span class="string">&quot;/echo&quot;</span>)]</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">fn</span> <span class="title function_">echo</span>(req_body: <span class="type">String</span>) <span class="punctuation">-&gt;</span> <span class="keyword">impl</span> <span class="title class_">Responder</span> &#123;</span><br><span class="line">    HttpResponse::<span class="title function_ invoke__">Ok</span>().<span class="title function_ invoke__">body</span>(req_body)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">fn</span> <span class="title function_">manual_hello</span>() <span class="punctuation">-&gt;</span> <span class="keyword">impl</span> <span class="title class_">Responder</span> &#123;</span><br><span class="line">    HttpResponse::<span class="title function_ invoke__">Ok</span>().<span class="title function_ invoke__">body</span>(<span class="string">&quot;Hey there!&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#[actix_web::main]</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">fn</span> <span class="title function_">main</span>() <span class="punctuation">-&gt;</span> std::io::<span class="type">Result</span>&lt;()&gt; &#123;</span><br><span class="line">    HttpServer::<span class="title function_ invoke__">new</span>(|| &#123;</span><br><span class="line">        App::<span class="title function_ invoke__">new</span>()</span><br><span class="line">            .<span class="title function_ invoke__">service</span>(hello)</span><br><span class="line">            .<span class="title function_ invoke__">service</span>(echo)</span><br><span class="line">            .<span class="title function_ invoke__">route</span>(<span class="string">&quot;/hey&quot;</span>, web::<span class="title function_ invoke__">get</span>().<span class="title function_ invoke__">to</span>(manual_hello))</span><br><span class="line">    &#125;)</span><br><span class="line">    .<span class="title function_ invoke__">bind</span>(<span class="string">&quot;127.0.0.1:8080&quot;</span>)?</span><br><span class="line">    .<span class="title function_ invoke__">run</span>()</span><br><span class="line">    .<span class="keyword">await</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>Use App::service for the handlersusing routing <code>macros</code> and App::route for <code>manually</code> routed handlers, declaring the <code>path</code> and <code>method</code>.</li>
<li><code>path</code>: http URL,例如：<a target="_blank" rel="noopener" href="http://github.com/xxx/xx/xx">http://github.com/xxx/xx/xx</a></li>
<li><code>method</code>: path的响应</li>
<li>All actix-web servers are built around the <code>App</code> instance.</li>
</ol>
<h2 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://actix.rs/docs/">official docs</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2021/03/16/Web/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/16/Web/" class="post-title-link" itemprop="url">Web</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-03-16 22:27:02" itemprop="dateCreated datePublished" datetime="2021-03-16T22:27:02+08:00">2021-03-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-20 15:37:37" itemprop="dateModified" datetime="2023-06-20T15:37:37+08:00">2023-06-20</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/03/16/Web/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/03/16/Web/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="前端开发"><a href="#前端开发" class="headerlink" title="前端开发"></a>前端开发</h2><ol>
<li>HTML: Markup Language 构建网页，例如：建筑骨架</li>
<li>CSS: Styling Language 网页样式: 例如： 建筑外观，墙的颜色</li>
<li>JavaScript: Programming Language 网页功能 例如：建筑功能，电梯</li>
<li>常用库： REACT, ANGULAE, VUE</li>
</ol>
<h2 id="MVC"><a href="#MVC" class="headerlink" title="MVC"></a>MVC</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/MVC">维基百科</a></li>
<li>MVC 模式在概念上强调 Model, View, Controller 的分离，各个模块也遵循着由 Controller 来处理消息，Model 掌管数据源，View 负责资料显示的职责分离原则，因此在实现上，MVC 模式的 Framework 通常会将 MVC 三个部分分离实现</li>
<li>View 负责格式化数据并把它们呈现给用户，业务逻辑和表示层分离，同一个 Model 可以被不同的 View 重用，所以大大提高了代码的可重用性。</li>
</ol>
<h2 id="Controller-Service-and-Repository"><a href="#Controller-Service-and-Repository" class="headerlink" title="Controller, Service, and Repository"></a>Controller, Service, and Repository</h2><h2 id="增删改查（CRUD"><a href="#增删改查（CRUD" class="headerlink" title="增删改查（CRUD)"></a>增删改查（CRUD)</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%A2%9E%E5%88%AA%E6%9F%A5%E6%94%B9">维基百科</a></li>
<li>增删查改（英语：CRUD[注 1]），全称增加（Create，意为“创建”）、删除（Delete）、查询（Read，意为“读取”）、改正（Update，意为“更新”），在计算机程序语言中是一连串常见的动作行为，而其行为通常是为了针对某个特定资源所作出的举动（例如：创建资料、读取资料等）。这四个行为最常见的用途是在使用 SQL 数据库与网站的 API 端口口的时候。</li>
</ol>
<h2 id="HTTP"><a href="#HTTP" class="headerlink" title="HTTP"></a>HTTP</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/80e25cb1d81a">简介</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Methods">http methods</a></li>
</ol>
<h2 id="URI"><a href="#URI" class="headerlink" title="URI"></a>URI</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%BB%9F%E4%B8%80%E8%B5%84%E6%BA%90%E6%A0%87%E5%BF%97%E7%AC%A6">维基百科</a></li>
<li>统一资源标识符（英语：Uniform Resource Identifier，缩写：URI）在电脑术语中是一个用于标识某一互联网资源名称的字符串。</li>
<li>URL 是一种 URI</li>
<li><code>URI = scheme “://” authority “/” path [ “?” query ][ “#” fragment ]</code></li>
<li>scheme: 指底层用的协议，如 http、https、ftp<br>host: 服务器的 IP 地址或者域名<br>port: 端口，http 中默认 80, https 中默认 443<br>path: 访问资源的路径，就是咱们各种 web 框架中定义的 route 路由<br>query: 为发送给服务器的参数<br>fragment: 锚点，定位到页面的资源，锚点为资源 id</li>
<li></li>
</ol>
<h2 id="CORS-跨域资源共享"><a href="#CORS-跨域资源共享" class="headerlink" title="CORS 跨域资源共享"></a>CORS 跨域资源共享</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/53996160">所有人都应该知道的跨域及 CORS</a></li>
</ol>
<h2 id="CGI-通用网关接口"><a href="#CGI-通用网关接口" class="headerlink" title="CGI 通用网关接口"></a>CGI 通用网关接口</h2><ol>
<li>Restful 风格 API 的出现，让 CGI 获得了续命。CGI 解析前端请求，再转发给对应后端；然后从后端取回数据，给前端返回 XML 或 JSON。然后前端 JS 利用 XML&#x2F;JSON 中的数据来进行填充。可以绘制出丰富的界面或用作他用。JS 可以使用 Ajax 技术来向后台 CGI 发起数据请求。Ajax 完成的是不需要刷新整个页面就可以加载后端数据（比如从数据库中取出）。</li>
<li>请注意区分 Web Server 和后台 Server。</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/25013398">图解</a></li>
<li><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/CGI/607810">百科</a></li>
</ol>
<h2 id="REST-API"><a href="#REST-API" class="headerlink" title="REST API"></a>REST API</h2><ol>
<li><a target="_blank" rel="noopener" href="https://learning.postman.com/docs/getting-started/installation-and-updates/#installing-postman-on-windows">postman</a></li>
<li><a target="_blank" rel="noopener" href="https://reqbin.com/">online test</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=7YcW25PHnAA">youtube samples</a></li>
<li><a target="_blank" rel="noopener" href="https://i6448038.github.io/2017/06/28/rest-%E6%8E%A5%E5%8F%A3%E8%A7%84%E8%8C%83/">接口规范</a></li>
<li><a target="_blank" rel="noopener" href="https://api.github.com/">github API</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.github.com/en/rest">github REST API</a></li>
</ol>
<h2 id="OpenAPI"><a href="#OpenAPI" class="headerlink" title="OpenAPI"></a>OpenAPI</h2><ol>
<li><a target="_blank" rel="noopener" href="https://swagger.io/docs/specification/about/">openapi</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2021/03/15/Pybind11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/15/Pybind11/" class="post-title-link" itemprop="url">Pybind11</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-03-15 11:33:25" itemprop="dateCreated datePublished" datetime="2021-03-15T11:33:25+08:00">2021-03-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-05-14 14:15:45" itemprop="dateModified" datetime="2024-05-14T14:15:45+08:00">2024-05-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Program/" itemprop="url" rel="index"><span itemprop="name">Program</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Program/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/03/15/Pybind11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/03/15/Pybind11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>118</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Pybind11"><a href="#Pybind11" class="headerlink" title="Pybind11"></a>Pybind11</h2><ol>
<li>python 与 C&#x2F;C++相互调用</li>
<li>轻量级，只含有头文件</li>
</ol>
<h2 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/pybind/pybind11">github</a><ul>
<li>包含测试程序</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://pybind11.readthedocs.io/en/stable/">doc</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/pybind/cmake_example">cmake example</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/pybind/python_example">Setuptools example</a></li>
<li><a target="_blank" rel="noopener" href="https://pybind11.readthedocs.io/en/latest/">tutorial</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/fitzzhang/article/details/78988682">python 调用 C++之 pybind11 入门</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2021/03/12/First-Emacs-Plugin/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/12/First-Emacs-Plugin/" class="post-title-link" itemprop="url">First-Emacs-Plugin</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-03-12 08:33:44" itemprop="dateCreated datePublished" datetime="2021-03-12T08:33:44+08:00">2021-03-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-20 15:37:37" itemprop="dateModified" datetime="2023-06-20T15:37:37+08:00">2023-06-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tools/" itemprop="url" rel="index"><span itemprop="name">Tools</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tools/Emacs/" itemprop="url" rel="index"><span itemprop="name">Emacs</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/03/12/First-Emacs-Plugin/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/03/12/First-Emacs-Plugin/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>111</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h2><p>保存位置，高亮所在行，单个buffer跳转</p>
<h2 id="参考插件"><a href="#参考插件" class="headerlink" title="参考插件"></a>参考插件</h2><ol>
<li>hightlight</li>
<li>symbol-overlay</li>
<li>dogears</li>
<li>set-mark-command</li>
</ol>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><ol>
<li>dogears加高亮</li>
<li>symbol-overlay改造</li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/14454219/how-to-highlight-a-particular-line-in-emacs">函数修改</a></li>
</ol>
<h2 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h2>
      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/27/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><span class="page-number current">28</span><a class="page-number" href="/page/29/">29</a><span class="space">&hellip;</span><a class="page-number" href="/page/33/">33</a><a class="extend next" rel="next" href="/page/29/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="贾夕阳"
      src="/images/coder2.jpg">
  <p class="site-author-name" itemprop="name">贾夕阳</p>
  <div class="site-description" itemprop="description">深度学习/自动驾驶/C++/性能优化</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">195</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jiaxiyang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jiaxiyang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2025/08/20/AI-coding/" title="2025&#x2F;08&#x2F;20&#x2F;AI-coding&#x2F;">AI coding</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2025/04/28/Architecture/" title="2025&#x2F;04&#x2F;28&#x2F;Architecture&#x2F;">Computer Architecture</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2025/04/18/pytest/" title="2025&#x2F;04&#x2F;18&#x2F;pytest&#x2F;">pytest</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2025/01/18/cursor/" title="2025&#x2F;01&#x2F;18&#x2F;cursor&#x2F;">cursor</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/12/29/LLVM/" title="2024&#x2F;12&#x2F;29&#x2F;LLVM&#x2F;">LLVM</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">贾夕阳</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">624k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">9:27</span>
</div>

<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("06/26/2020 14:52:10");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : '[object Object]',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://jiaxiyang.github.io/page/28/',]
      });
      });
  </script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'g32ipLmEye1u5l6wBGRJt03S-gzGzoHsz',
      appKey     : 'zHgLkAICsZUl9Mf8LfdoVigP',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

  

  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>





 
</body>
</html>

