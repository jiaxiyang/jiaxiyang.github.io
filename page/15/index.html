<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0-rc2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jiaxiyang.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="深度学习&#x2F;自动驾驶&#x2F;C++&#x2F;性能优化">
<meta property="og:type" content="website">
<meta property="og:title" content="Xiyang">
<meta property="og:url" content="https://jiaxiyang.github.io/page/15/index.html">
<meta property="og:site_name" content="Xiyang">
<meta property="og:description" content="深度学习&#x2F;自动驾驶&#x2F;C++&#x2F;性能优化">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="贾夕阳">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://jiaxiyang.github.io/page/15/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Xiyang</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WGS6S6YFJ6"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-WGS6S6YFJ6');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Xiyang</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Think twice, code once!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">192</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">44</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">55</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/jiaxiyang" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2022/04/13/AI-models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/13/AI-models/" class="post-title-link" itemprop="url">AI-models</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-04-13 16:45:08" itemprop="dateCreated datePublished" datetime="2022-04-13T16:45:08+08:00">2022-04-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-01-14 22:29:54" itemprop="dateModified" datetime="2024-01-14T22:29:54+08:00">2024-01-14</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/04/13/AI-models/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/04/13/AI-models/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>588</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="base"><a href="#base" class="headerlink" title="base"></a>base</h2><ol>
<li>发布的模型包括模型的结构，模型的参数(onnx)</li>
<li>LLM 获取参数很关键</li>
</ol>
<h2 id="模型参数计算量数据量趋势"><a href="#模型参数计算量数据量趋势" class="headerlink" title="模型参数计算量数据量趋势"></a>模型参数计算量数据量趋势</h2><ol>
<li><a target="_blank" rel="noopener" href="https://epochai.org/mlinputs/visualization?yAxis=Parameters">Model Size of Notable Machine Learning Systems Over Time</a> 可交互, 右上角 option 可搜索，可直接到论文</li>
<li><a target="_blank" rel="noopener" href="https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/edit#gid=0">Parameter, Compute and Data Trends in Machine Learning</a> good：包含参数，计算量, 训练数据量，论文引用</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2207.02852">Machine Learning Model Sizes and the Parameter Gap</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/parameter-counts-in-machine-learning-a312dc4753d0">parameter-counts-in-machine-learning</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.05924.pdf">COMPUTE TRENDS ACROSS THREE ERAS OF MACHINE LEARNING</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Lyken17/pytorch-OpCounter">常用模型参数量</a><br><img src="https://i.ibb.co/hHLnMp7/LAmu-Jv4-DId.png" alt="常用模型参数量"></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ThanatosShinji/onnx-tool/blob/main/README_CN.md#results-of-onnx-model-zoo-and-sota-models">results-of-onnx-model-zoo-and-sota-models</a></li>
<li>可以通过模型文件大小和数据类型估计模型参数量</li>
<li><a target="_blank" rel="noopener" href="https://i.ibb.co/RCVGL1j/2s-Zv-Zl-E5-Ns.png">Pre-training costs of representative LLMs</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/optimum/llm-perf-leaderboard">llm-perf-leaderboard</a><ul>
<li>lantency vs memory vs score</li>
</ul>
</li>
</ol>
<h2 id="model-zoo"><a href="#model-zoo" class="headerlink" title="model zoo"></a>model zoo</h2><h3 id="caffe"><a href="#caffe" class="headerlink" title="caffe"></a>caffe</h3><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/SnailTyan/caffe-model-zoo">SnailTyan&#x2F;caffe-model-zoo</a></li>
</ol>
<h3 id="onnx"><a href="#onnx" class="headerlink" title="onnx"></a>onnx</h3><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/onnx/models">onnx&#x2F;models</a></li>
</ol>
<h3 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h3><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/Cadene/pretrained-models.pytorch">Cadene&#x2F;pretrained-models.pytorch</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/models.html">pytorch official</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2022/04/11/AI-performance/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/11/AI-performance/" class="post-title-link" itemprop="url">AI-chip-performance</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-04-11 16:30:54" itemprop="dateCreated datePublished" datetime="2022-04-11T16:30:54+08:00">2022-04-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-09-24 17:37:37" itemprop="dateModified" datetime="2024-09-24T17:37:37+08:00">2024-09-24</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/04/11/AI-performance/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/04/11/AI-performance/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="throughput-and-latency"><a href="#throughput-and-latency" class="headerlink" title="throughput and latency"></a>throughput and latency</h2><ol>
<li>throughput：吞吐量一般指相当一段时间内测量出来的系统单位时间处理的任务数或事务数。如：1s 处理 10 帧图片 (10FPS)</li>
<li>latency: 执行一次任务需要的时间。如：处理一帧图片耗时 10ms</li>
<li>Throughput – successful transactions per second</li>
<li>Latency – response time per transaction</li>
</ol>
<h2 id="base"><a href="#base" class="headerlink" title="base"></a>base</h2><ol>
<li><code>计算芯片的峰值算力</code> 理论计算得出的、理想状况下的峰值算力。具体通过计算核的主频、核数量、单核运算能力等来输出理论峰值，与实际场景中的真实有效处理能力有很大差距。</li>
<li><code>计算芯片的有效算力</code> 设备在实际运作过程中能真实输出的算力。比如 ResNet50 在浮点 16 位精度下，推理一次的计算量是 7.8Gflops 左右，芯片每秒钟处理的 ResNet50 的帧率乘以 7.8G 就是真实有效的输出算力。如果推理帧率是 400fps，真实算力是 400*7.G&#x3D;3Tflops。</li>
<li><code>计算芯片的有效利用率</code> 代表了芯片真实输出的运算能力。其计算方式为：真实有效的输出算力&#x2F;理论算力。计算结果值越高，表明该芯片工作越高效。实际应用场景中，多数传统架构计算芯片，有效利用率一般在 30%左右，而新型架构的计算芯片，芯片有效利用率可以到 50%以上。领启 KA200 异构众核、存算一体芯片有效利用率可达 60%以上。<ul>
<li>利用率： 模型计算量 * FPS &#x2F; 芯片性能</li>
<li>VCK190 133T, tf_resnetv1_50_imagenet_224_224_6.97G, 1421FPS, 使用率：6.97 * 1421 &#x2F; 1000 &#x2F; 133 &#x3D; 7.4%</li>
</ul>
</li>
<li><code>计算芯片对主流神经网络的支持程度</code> 下一代人工智能有两个不同的发展线路图，一个是以深度学习算法为代表的传统计算科学，一个是以生物神经网络为代表脑科学。多数计算芯片只能支持二者之一。是否能将二者兼并支持，也是衡量计算芯片的核心指标之一，异构融合是达到这一指标的重要途径之一。</li>
<li><code>芯片能效比</code> 计算方式为：典型网络（如 ResNet50）的推理能力&#x2F;芯片的功耗，单位：推理帧率（fps&#x2F;w）。</li>
<li><code>芯片的性价比</code> 每平方毫米推理的帧率数，计算方式为：典型网络（如 ResNet50）的推理能力&#x2F;芯片的面积。</li>
<li>多<code>batch</code>会增加模型计算量</li>
<li>浮点转定点会降低计算量，可能剪枝优化</li>
<li>NCHW 和 NHWC 会影响性能</li>
</ol>
<h2 id="模型计算量"><a href="#模型计算量" class="headerlink" title="模型计算量"></a>模型计算量</h2><h3 id="caffe"><a href="#caffe" class="headerlink" title="caffe"></a>caffe</h3><ol>
<li><a target="_blank" rel="noopener" href="https://dgschwend.github.io/netscope/#/editor">netscope</a></li>
<li><a target="_blank" rel="noopener" href="https://dgschwend.github.io/netscope/quickstart.html">quickstart</a></li>
<li>复制 prototxt 内容, <code>shift + enter</code>: 查看计算量</li>
</ol>
<h3 id="onnx"><a href="#onnx" class="headerlink" title="onnx"></a>onnx</h3><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/gmalivenko/onnx-opcounter">onnx-opcounter</a> 需要模型结构，只有 pt 文件不行</li>
</ol>
<h3 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h3><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/Lyken17/pytorch-OpCounter">pytorch-OpCounter</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmdetection/blob/master/README_zh-CN.md">mmdetection</a></li>
</ol>
<h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3><h2 id="芯片性能"><a href="#芯片性能" class="headerlink" title="芯片性能"></a>芯片性能</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.horizon.ai/journey2.html">地平线 J2</a><ul>
<li>4TOPS</li>
<li>2W</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.horizon.ai/journey3.html">J3</a><ul>
<li>5TOPS</li>
<li>2.5W</li>
<li>16nm</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.horizon.ai/journey5.html">J5</a><ul>
<li>128TOPS</li>
<li>30W</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.nvidia.cn/autonomous-machines/embedded-systems/jetson-orin/">Nvidia Xavier</a> 比较 Jetson Orin 和 Jetson Xavier 的规格<ul>
<li>21-32TOPS</li>
<li>10-40W</li>
<li>12nm</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.nvidia.cn/autonomous-machines/embedded-systems/jetson-orin/">Nvidia Orin</a> 比较 Jetson Orin 和 Jetson Xavier 的规格<ul>
<li>70-274TOPS</li>
<li>10-60W</li>
<li>8nm</li>
</ul>
</li>
<li><a href="">Xilinx Versal ACAP VC1902(VCK190)</a><ul>
<li>133TOPS</li>
<li>7ns</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.hisilicon.com/cn/products/Ascend/Ascend-910">昇腾（HUAWEI Ascend) 910</a><ul>
<li>FP16: 320TFLOPS</li>
<li>INT8: 640TOPS</li>
<li>310W</li>
<li>7nm</li>
</ul>
</li>
<li><a href="">特斯拉 FSD 计算平台</a><ul>
<li>144TOPS</li>
<li>单芯片 72TOPS</li>
<li>72W</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.mobileye.com/eyeq-chip/">Mobileye EQ4 High</a><ul>
<li>2TOPS</li>
<li>28nm</li>
</ul>
</li>
<li><a href="">Mobileye EQ5 High</a><ul>
<li>16TOPS</li>
<li>7nm</li>
</ul>
</li>
<li><a href="">Mobileye EQ6 High</a><ul>
<li>34TOPS</li>
<li>7nm</li>
</ul>
</li>
</ol>
<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.eefocus.com/article/1726287.html">佐思汽研《2024 年自动驾驶 SoC 研究报告》</a><ul>
<li>各种芯片算力</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-xavier-series/">jetson xavier params</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/embedded/jetson-modules">jetson parms</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/337618803">TOPS GOPS FLOPS</a></li>
<li><a target="_blank" rel="noopener" href="https://wuchenxu.com/2021/10/02/computing-power-metrices/">指标</a></li>
<li><a target="_blank" rel="noopener" href="https://www.horizon.ai/journey5.html">J5 官方介绍</a></li>
<li><a target="_blank" rel="noopener" href="https://auto.gasgoo.com/a/70266230.html">link1</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/embedded/jetson-benchmarks">jetson-benchmark</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/deep-learning-performance-training-inference">deep-learning-performance-training-inference</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/delivering-server-class-performance-at-the-edge-with-nvidia-jetson-orin/">orin performance</a></li>
<li><a target="_blank" rel="noopener" href="https://www.nvidia.cn/autonomous-machines/embedded-systems/jetson-orin/">orin 产品系列</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Xilinx/Vitis-AI/tree/master/models/AI-Model-Zoo#performance-on-vck190">VCK190 性能</a></li>
<li><a target="_blank" rel="noopener" href="https://new.qq.com/omn/20220127/20220127A06JD900.html">地平线 MAPS</a></li>
<li><a target="_blank" rel="noopener" href="https://img3.gelonghui.com/pdf/bc70f-43194a12-e915-4416-9ffa-2e241931ae61.pdf">中信车载芯片调研</a></li>
<li><a target="_blank" rel="noopener" href="https://pdf.dfcfw.com/pdf/H3_AP202112301537477959_1.pdf?1640871632000.pdf">亿欧网调研</a></li>
<li><a target="_blank" rel="noopener" href="https://pdf.dfcfw.com/pdf/H3_AP202112061533188037_1.pdf?1639041416000.pdf">艾瑞咨询-中国智能驾驶行业研究报告</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2022/04/08/ChangeLog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/08/ChangeLog/" class="post-title-link" itemprop="url">ChangeLog</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-04-08 17:08:27" itemprop="dateCreated datePublished" datetime="2022-04-08T17:08:27+08:00">2022-04-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-11-24 15:55:57" itemprop="dateModified" datetime="2023-11-24T15:55:57+08:00">2023-11-24</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/04/08/ChangeLog/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/04/08/ChangeLog/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>31</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://keepachangelog.com/zh-CN/1.0.0/">如何维护更新日志</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/changelog.md">mmsegmentation</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/google/glog/blob/master/ChangeLog">glog</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2022/04/07/clang-tidy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/07/clang-tidy/" class="post-title-link" itemprop="url">clang-tidy</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-04-07 12:14:50" itemprop="dateCreated datePublished" datetime="2022-04-07T12:14:50+08:00">2022-04-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-11-24 12:00:36" itemprop="dateModified" datetime="2023-11-24T12:00:36+08:00">2023-11-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tools/" itemprop="url" rel="index"><span itemprop="name">Tools</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tools/Clang/" itemprop="url" rel="index"><span itemprop="name">Clang</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/04/07/clang-tidy/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/04/07/clang-tidy/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>75</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="clang-tidy-AST-静态检查工具"><a href="#clang-tidy-AST-静态检查工具" class="headerlink" title="clang-tidy AST 静态检查工具"></a><a target="_blank" rel="noopener" href="https://clang.llvm.org/extra/clang-tidy/">clang-tidy AST 静态检查工具</a></h2><h2 id="basic"><a href="#basic" class="headerlink" title="basic"></a>basic</h2><ol>
<li>见 cpp-debug.md lint</li>
<li>需要 compile_commands.json</li>
</ol>
<h2 id="C-静态检查工具总结"><a href="#C-静态检查工具总结" class="headerlink" title="C++静态检查工具总结"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/u013377887/article/details/108651945">C++静态检查工具总结</a></h2>
      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2022/04/02/gtest/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/02/gtest/" class="post-title-link" itemprop="url">gtest</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-04-02 07:06:28" itemprop="dateCreated datePublished" datetime="2022-04-02T07:06:28+08:00">2022-04-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-12-01 15:30:03" itemprop="dateModified" datetime="2023-12-01T15:30:03+08:00">2023-12-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Program/" itemprop="url" rel="index"><span itemprop="name">Program</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Program/Cpp/" itemprop="url" rel="index"><span itemprop="name">Cpp</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/04/02/gtest/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/04/02/gtest/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="basic"><a href="#basic" class="headerlink" title="basic"></a>basic</h2><ol>
<li><a target="_blank" rel="noopener" href="https://google.github.io/googletest/primer.html">doc</a></li>
<li><a target="_blank" rel="noopener" href="https://google.github.io/googletest/reference/assertions.html">断言列表</a></li>
<li>opencv 源码 PERF_TEST</li>
<li><code>test_exe --gtest_filter=TestSuite.TestName</code> 单独运行某个测试，当测试失败时，GoogleTest 允许您单独运行它以进行快速调试。</li>
<li>断言是成对的，测试同一件事，但对当前函数有不同的影响。<ul>
<li>ASSERT_ *版本在失败时会生成致命错误，并中止当前函数。</li>
<li>EXPECT_ *版本生成非致命性故障，不会中止当前函数。</li>
<li>通常优先使用 EXPECT* *，因为它们允许在测试中报告多个故障。但是，如果失败时函数继续运行没有意义，则应使用 ASSERT* *。</li>
</ul>
</li>
<li>gtest_main.a 有什么用？可以不用写自己的 main 函数，链接 libgtest_main.a 就可以了。</li>
<li><code>add_test(NAME gtests COMMAND gtests --gtest_output=xml:report.xml)</code></li>
<li>CMakeLists.txt</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">find_package(GTest REQUIRED)</span><br><span class="line">include_directories(<span class="variable">$&#123;GTEST_INCLUDE_DIRS&#125;</span>)</span><br><span class="line">target_link_libraries(xxx <span class="variable">$&#123;GTEST_LIBRARIES&#125;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="NOTE"><a href="#NOTE" class="headerlink" title="NOTE"></a>NOTE</h2><ol>
<li><code>enable_testing()</code> 需要在第一级 CMakeLists.txt 里，否则<code>make test</code>不生效</li>
<li><code>ctest -N &amp;&amp; ctest -VV --test-dir build</code></li>
<li><code>make test</code>: gtest 想要被 make test 发现需要<a target="_blank" rel="noopener" href="https://google.github.io/googletest/quickstart-cmake.html">link</a></li>
<li><code>Testing/Temporary/LastTest.log</code> 为<code>make test</code>结果</li>
</ol>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">include</span>(GoogleTest)</span><br><span class="line">gtest_discover_tests(hello_test)</span><br></pre></td></tr></table></figure>

<h2 id="report-xml-json-html"><a href="#report-xml-json-html" class="headerlink" title="report (xml json html)"></a>report (xml json html)</h2><ol>
<li>gtest 结果 xml 可以显示到 ci pipeline 结果中去 <a target="_blank" rel="noopener" href="https://docs.gitlab.com/ee/ci/testing/unit_test_reports.html">link</a></li>
</ol>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">stages:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">test</span></span><br><span class="line"></span><br><span class="line"><span class="attr">cpp:</span></span><br><span class="line">  <span class="attr">stage:</span> <span class="string">test</span></span><br><span class="line">  <span class="attr">script:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">./gtxxx</span> <span class="string">--gtest_output=xml:report.xml</span></span><br><span class="line"><span class="string">unit_test_reports</span></span><br><span class="line">  <span class="attr">artifacts:</span></span><br><span class="line">    <span class="attr">when:</span> <span class="string">always</span></span><br><span class="line">    <span class="attr">reports:</span></span><br><span class="line">      <span class="attr">junit:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">report.xml</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">fail.xml</span></span><br></pre></td></tr></table></figure>

<ol>
<li><p>.&#x2F;gtxxx –gtest_output&#x3D;xml:filename</p>
</li>
<li><p><code>make test</code></p>
</li>
</ol>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">::testing::GTEST_FLAG(output)</span> = &quot;xml:test2.xml&quot;;</span><br><span class="line"><span class="meta">::testing::GTEST_FLAG(output)</span> = &quot;json:test2.json&quot;;</span><br><span class="line"><span class="meta">::testing::InitGoogleTest(&amp;argc,</span> argv);</span><br></pre></td></tr></table></figure>

<ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/Neil4/article/details/104484792">to html</a></li>
</ol>
<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://google.github.io/googletest/quickstart-cmake.html">cmake 集成</a></li>
<li>base</li>
</ol>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">target_link_libraries</span>(</span><br><span class="line">  hello_test</span><br><span class="line">  GTest::gtest_main</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h2 id="samples"><a href="#samples" class="headerlink" title="samples"></a>samples</h2><ol>
<li>通用框架: 添加 gtest 不用修改 CMakeLists.txt</li>
</ol>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enable_testing</span>() <span class="comment"># 需要在第一级 CMakeLists.txt 里，否则`make test`不生效</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(gtest-source-pattern <span class="string">&quot;gtest/*.cpp&quot;</span>)</span><br><span class="line"><span class="keyword">file</span>(GLOB gtest-sources <span class="variable">$&#123;gtest-source-pattern&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(gtests)</span><br><span class="line"><span class="keyword">target_sources</span>(gtests PRIVATE <span class="variable">$&#123;gtest-sources&#125;</span>)</span><br><span class="line"><span class="keyword">target_include_directories</span>(gtests</span><br><span class="line">                           PRIVATE <span class="variable">$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;</span>/gtest/)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(gtests PRIVATE gtest <span class="variable">$&#123;COMMON_LIB&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_test</span>(fcw::gtests gtests)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol>
<li>main, 测试可以分散在不同文件，只有一个 main</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;gtest/gtest.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span> </span>&#123;</span><br><span class="line">  ::testing::<span class="built_in">GTEST_FLAG</span>(output) = <span class="string">&quot;xml:report.xml&quot;</span>;</span><br><span class="line">  <span class="comment">// ::testing::GTEST_FLAG(output) = &quot;json:test2.json&quot;;</span></span><br><span class="line">  ::testing::<span class="built_in">InitGoogleTest</span>(&amp;argc, argv);</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">RUN_ALL_TESTS</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="yaml-gtest-CMakeLists-txt"><a href="#yaml-gtest-CMakeLists-txt" class="headerlink" title="yaml gtest CMakeLists.txt"></a>yaml gtest CMakeLists.txt</h3><ol>
<li>NOTE: use pattern</li>
</ol>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">find_package</span>(Threads REQUIRED)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(gtest_force_shared_crt <span class="keyword">ON</span> CACHE BOOL <span class="string">&quot;&quot;</span> FORCE)</span><br><span class="line"><span class="keyword">set</span>(BUILD_MOCK <span class="keyword">ON</span> CACHE BOOL <span class="string">&quot;&quot;</span> FORCE)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_POLICY_DEFAULT_CMP0048 NEW)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_subdirectory</span>(</span><br><span class="line">  <span class="string">&quot;$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/gtest-1.11.0&quot;</span></span><br><span class="line">  <span class="string">&quot;$&#123;CMAKE_CURRENT_BINARY_DIR&#125;/prefix&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">include_directories</span>(SYSTEM <span class="string">&quot;$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/gtest-1.11.0/googletest/include&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(<span class="keyword">test</span>-new-api-pattern <span class="string">&quot;new-api/*.cpp&quot;</span>)</span><br><span class="line"><span class="keyword">set</span>(<span class="keyword">test</span>-source-pattern <span class="string">&quot;*.cpp&quot;</span> <span class="string">&quot;integration/*.cpp&quot;</span> <span class="string">&quot;node/*.cpp&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> (CMAKE_VERSION <span class="keyword">VERSION_GREATER</span> <span class="number">3.11</span>)</span><br><span class="line">  <span class="keyword">list</span>(INSERT <span class="keyword">test</span>-new-api-pattern <span class="number">0</span> CONFIGURE_DEPENDS)</span><br><span class="line">  <span class="keyword">list</span>(INSERT <span class="keyword">test</span>-source-pattern <span class="number">0</span> CONFIGURE_DEPENDS)</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">file</span>(GLOB <span class="keyword">test</span>-new-api-sources <span class="variable">$&#123;test-new-api-pattern&#125;</span>)</span><br><span class="line"><span class="keyword">file</span>(GLOB <span class="keyword">test</span>-sources <span class="variable">$&#123;test-source-pattern&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(yaml-cpp-tests <span class="string">&quot;&quot;</span>)</span><br><span class="line"><span class="keyword">target_sources</span>(yaml-cpp-tests</span><br><span class="line">  PRIVATE</span><br><span class="line">    <span class="variable">$&#123;test-new-api-sources&#125;</span></span><br><span class="line">    <span class="variable">$&#123;test-sources&#125;</span>)</span><br><span class="line"><span class="keyword">target_include_directories</span>(yaml-cpp-tests</span><br><span class="line">  PRIVATE</span><br><span class="line">    <span class="variable">$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;</span>/integration</span><br><span class="line">    <span class="variable">$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;</span></span><br><span class="line">    <span class="variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/src)</span><br><span class="line"><span class="keyword">target_compile_options</span>(yaml-cpp-tests</span><br><span class="line">  PRIVATE</span><br><span class="line">    $&lt;$&lt;CXX_COMPILER_ID:Clang&gt;:-Wno-c99-extensions -Wno-variadic-macros -Wno-sign-compare&gt;</span><br><span class="line">    $&lt;$&lt;CXX_COMPILER_ID:GNU&gt;:-Wno-variadic-macros -Wno-sign-compare&gt;)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(yaml-cpp-tests</span><br><span class="line">  PRIVATE</span><br><span class="line">    Threads::Threads</span><br><span class="line">    yaml-cpp</span><br><span class="line">    gmock)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set_property</span>(<span class="keyword">TARGET</span> yaml-cpp-tests PROPERTY CXX_STANDARD_REQUIRED <span class="keyword">ON</span>)</span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">NOT</span> <span class="keyword">DEFINED</span> CMAKE_CXX_STANDARD)</span><br><span class="line">  <span class="keyword">set_target_properties</span>(yaml-cpp-tests PROPERTIES CXX_STANDARD <span class="number">11</span>)</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">add_test</span>(yaml-cpp::<span class="keyword">test</span> yaml-cpp-tests)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (build-windows-dll)</span><br><span class="line">  <span class="keyword">add_custom_command</span>(</span><br><span class="line">    <span class="keyword">TARGET</span> yaml-cpp-tests</span><br><span class="line">    POST_BUILD <span class="keyword">COMMAND</span> <span class="variable">$&#123;CMAKE_COMMAND&#125;</span> -E</span><br><span class="line">    copy_if_different <span class="string">&quot;$&lt;TARGET_FILE:yaml-cpp&gt;&quot;</span> <span class="string">&quot;$&lt;TARGET_FILE_DIR:yaml-cpp-tests&gt;&quot;</span>)</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="links-1"><a href="#links-1" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/doctest/doctest">doctest</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2022/03/31/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/31/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD/" class="post-title-link" itemprop="url">自动驾驶专业术语</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-31 02:24:23" itemprop="dateCreated datePublished" datetime="2022-03-31T02:24:23+08:00">2022-03-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-20 15:37:37" itemprop="dateModified" datetime="2023-06-20T15:37:37+08:00">2023-06-20</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/03/31/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/03/31/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>905</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="缩写"><a href="#缩写" class="headerlink" title="缩写"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/LEON1741/article/details/89510034">缩写</a></h2><ol>
<li><code>OTA</code>: over the air</li>
<li><code>FCW</code>: forward collision warning 前车碰撞预警</li>
<li><code>TTC</code>: time to collision 碰撞时间</li>
<li><code>NVS</code>: Night Vision System 夜视系统</li>
<li><code>NOA</code> “自动辅助导航驾驶（Navigate on Autopilot）”的中文翻译也有人叫“按导航辅助驾驶”或“领航辅助功能”，本质意思是，把“导航”和“辅助驾驶”结合。在原来 L2 辅助驾驶的基础上（如车道线保持、自动跟车），加上车机的导航信息（如百度地图），自动变道，实现从 A 点到 B 点的自动驾驶。</li>
<li><code>OEM</code> 又称主机厂，OEM 是英文 Original equipment manufacturer 的缩写。</li>
<li><code>DCU</code> (Domain Control Unit）域控制器</li>
<li><code>ECU</code>（Electronic Control Unit）电子控制单元</li>
<li><code>V2X</code>（Vehicle to everything）</li>
<li><code>TSN</code>（Time-Sensitive Network）</li>
<li><code>AUTOSAR</code>（AUTomotive Open System Architecture）</li>
<li><code>EEA</code> (Electrical&#x2F;Electronic Architecture) 电子电气架构</li>
<li><code>SOME/IP</code> Scalable service-Oriented Middleware over IP。即“运行于 IP 之上的可伸缩的面向服务的中间件”</li>
<li><code>AUTOSAR</code> (AUTomotive Open System ARchitecture)</li>
<li><code>DDS</code> (Data Distribution Service,数据分发服务)</li>
<li><code>RTPS</code> (Real-Time Publish Subscribe)</li>
<li><code>FOV</code> (field of view) 视场; 在光学仪器或传感器的情况下，视场是检测器对电磁辐射敏感的立体角 <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_33358099/article/details/112102463?spm=1001.2101.3001.6650.6&utm_medium=distribut">link</a></li>
</ol>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ol>
<li>标定：</li>
</ol>
<h2 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://pdf.dfcfw.com/pdf/H3_AP202202221548462770_1.pdf?1645518929000.pdf">汽车软件研报</a></li>
<li><a target="_blank" rel="noopener" href="https://www-file.huawei.com/-/media/corporate/pdf/news/intelligent-driving-computing-platform-whitepaper-new.pdf?la=zh">华为 MDC 智能驾驶计算平台白皮书</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/261291971">软件定义汽车（2）-软件中间件（Autosar 为例）</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/561699068">国内主机整车 EEA 架构汇总</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/460683077">深度分析汽车芯片的现状与发展（一）汽车电子电气架构的变革</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/461714692">深度分析汽车芯片的现状与发展（二）域控制器时代</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/462675111">深度分析汽车芯片的现状与发展（三）ADAS&#x2F;AD 域控制器及芯片平台分析</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/464930132">深度分析汽车芯片的现状与发展（四）整车控制域</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2022/03/26/eigen/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/26/eigen/" class="post-title-link" itemprop="url">eigen</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-26 07:08:50" itemprop="dateCreated datePublished" datetime="2022-03-26T07:08:50+08:00">2022-03-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-20 15:37:37" itemprop="dateModified" datetime="2023-06-20T15:37:37+08:00">2023-06-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Program/" itemprop="url" rel="index"><span itemprop="name">Program</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/03/26/eigen/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/03/26/eigen/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="basic"><a href="#basic" class="headerlink" title="basic"></a>basic</h2><ol>
<li>gdb 调试 <a target="_blank" rel="noopener" href="https://gitlab.com/libeigen/eigen/-/blob/master/debug/gdb/printers.py">printer</a></li>
<li>vector 转 tensor <a target="_blank" rel="noopener" href="https://stackoverflow.com/a/44664806">link</a> <a target="_blank" rel="noopener" href="https://blog.csdn.net/weareu/article/details/86486682">link1</a></li>
<li>Tensor 转 vector</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Tensor&gt; <span class="function"><span class="type">static</span> std::vector&lt;<span class="type">float</span>&gt; <span class="title">run</span><span class="params">(<span class="type">const</span> Tensor &amp;tensor)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">typedef</span> <span class="keyword">typename</span> Eigen::internal::remove_const&lt;<span class="keyword">typename</span> Tensor::Scalar&gt;::type</span><br><span class="line">      Scalar;</span><br><span class="line">  <span class="keyword">typedef</span> <span class="keyword">typename</span> Tensor::Index Index;</span><br><span class="line">  <span class="type">const</span> Index total_size = Eigen::internal::<span class="built_in">array_prod</span>(tensor.<span class="built_in">dimensions</span>());</span><br><span class="line">  std::vector&lt;<span class="type">float</span>&gt; result;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (total_size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    Eigen::Map&lt;<span class="type">const</span> Eigen::Array&lt;Scalar, Eigen::Dynamic, <span class="number">1</span>&gt;&gt; <span class="built_in">array</span>(</span><br><span class="line">        <span class="built_in">const_cast</span>&lt;Scalar *&gt;(tensor.<span class="built_in">data</span>()), total_size);</span><br><span class="line">    std::cout &lt;&lt; array[<span class="number">0</span>] &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> i = <span class="number">0u</span>; i &lt; total_size; ++i) &#123;</span><br><span class="line">      result.<span class="built_in">push_back</span>(array[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// os &lt;&lt; array;                                                                                                                                         &#125;</span></span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<ol>
<li>头文件和编译选项</li>
</ol>
<figure class="highlight stan"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#include &lt;<span class="string">eigen</span>3/E<span class="string">igen</span>/E<span class="string">igen</span>&gt;</span></span><br><span class="line">find_package(Eigen3 REQUIRED)</span><br></pre></td></tr></table></figure>

<ol>
<li>矩阵默认<a target="_blank" rel="noopener" href="https://eigen.tuxfamily.org/dox/group__TopicStorageOrders.html">按列存储</a>， 按行需要指定</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Eigen::Matrix&lt;<span class="type">double</span>, <span class="number">4</span>, <span class="number">4</span>, Eigen::RowMajor&gt; cam_intra;</span><br></pre></td></tr></table></figure>

<ol>
<li><p>常用数据结构：</p>
<ul>
<li>Vector3d: <code>auto col = Eigen::Vector3d(0, 0, 0);</code></li>
<li>RowVector4d: <code>auto row = Eigen::RowVector4d(0, 0, 0, 1);</code></li>
<li>MatrixXd: <code>Eigen::MatrixXd tmp(3, 4);</code></li>
<li>Matrix: <code>Eigen::Matrix&lt;double, 4, 4, Eigen::RowMajor&gt; cam_intra;</code></li>
<li>Map: <code>auto T_cab2cam = Eigen::Map&lt;Eigen::Vector3d&gt;(std_vector.data());</code></li>
</ul>
</li>
<li><p>矩阵相乘： <code>A * B</code></p>
</li>
<li><p>矩阵求逆： <code>A.inverse()</code></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://eigen.tuxfamily.org/dox/group__TutorialBlockOperations.html">矩阵取列：</a> <code>A.col()</code></p>
</li>
<li><p>矩阵取行： <code>A.row()</code></p>
</li>
</ol>
<h2 id="basci-module"><a href="#basci-module" class="headerlink" title="basci module"></a>basci module</h2><ol>
<li>Matrix init</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Eigen::Matrix3d R_e2cab;</span><br><span class="line">  R_e2cab &lt;&lt; <span class="number">0</span>, <span class="number">0</span>, <span class="number">-1</span>,  <span class="comment">//</span></span><br><span class="line">      <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>,          <span class="comment">//</span></span><br><span class="line">      <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>;          <span class="comment">//</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol>
<li>std::vecotr to Eigen::Vecotr or Eigen::Matrix</li>
</ol>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">auto R_cab2cam_tmp = <span class="title class_">Eigen</span><span class="symbol">:</span><span class="symbol">:Map&lt;Eigen</span><span class="symbol">:</span><span class="symbol">:RowVector3d&gt;</span>(v_R_cab2cam.data());</span><br><span class="line">auto cam_k = <span class="title class_">Eigen</span><span class="symbol">:</span><span class="symbol">:Map&lt;Eigen</span><span class="symbol">:</span><span class="symbol">:Matrix&lt;double</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="title class_">Eigen</span><span class="symbol">:</span><span class="symbol">:RowMajor&gt;&gt;</span>(v_cam_k.data());</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol>
<li>Eigen::Vecotr or Eigen::Matrix to std::vector</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;<span class="type">double</span>&gt; <span class="title">vec_cam_intra</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">     cam_intra.data(), cam_intra.data() + cam_intra.rows() * cam_intra.cols())</span></span>;</span><br></pre></td></tr></table></figure>

<ol>
<li>Matrix concat</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Eigen::MatrixXd <span class="title">cab2cam_tmp</span><span class="params">(<span class="number">3</span>, <span class="number">4</span>)</span></span>;</span><br><span class="line"><span class="keyword">auto</span> col_tmp = Eigen::<span class="built_in">RowVector4d</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line"><span class="function">Eigen::MatrixXd <span class="title">cab2cam</span><span class="params">(<span class="number">4</span>, <span class="number">4</span>)</span></span>;</span><br><span class="line">cab2cam &lt;&lt; cab2cam_tmp, col_tmp;</span><br><span class="line"></span><br><span class="line"><span class="function">Eigen::MatrixXd <span class="title">cab2cam_tmp</span><span class="params">(<span class="number">4</span>, <span class="number">3</span>)</span></span>;</span><br><span class="line"><span class="keyword">auto</span> col_tmp = Eigen::<span class="built_in">Vector3d</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line"><span class="function">Eigen::MatrixXd <span class="title">cab2cam</span><span class="params">(<span class="number">4</span>, <span class="number">4</span>)</span></span>;</span><br><span class="line">cab2cam &lt;&lt; cab2cam_tmp, col_tmp;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol>
<li>Matrix slice (<a target="_blank" rel="noopener" href="https://eigen.tuxfamily.org/dox/group__TutorialBlockOperations.html">block</a>)</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Eigen::Matrix&lt;<span class="type">double</span>, <span class="number">4</span>, <span class="number">4</span>, Eigen::RowMajor&gt; cam2ego;</span><br><span class="line">Eigen::Matrix&lt;<span class="type">double</span>, <span class="number">3</span>, <span class="number">3</span>, Eigen::RowMajor&gt; R_cam2ego = cam2ego.<span class="built_in">block</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>);</span><br><span class="line">Eigen::Vector3d T_cam2ego = cam2ego.<span class="built_in">block</span>(<span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://gitlab.com/libeigen/eigen.git">gitlab repo</a></li>
<li><a target="_blank" rel="noopener" href="https://eigen.tuxfamily.org/dox/GettingStarted.html">getting started</a></li>
</ol>
<h2 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;glog/logging.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;yaml-cpp/yaml.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;eigen3/Eigen/Eigen&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">GetPgCamParm</span><span class="params">(<span class="type">const</span> std::string camera_param, <span class="type">const</span> std::string cali_param,</span></span></span><br><span class="line"><span class="params"><span class="function">                  <span class="type">const</span> std::string result_param)</span> </span>&#123;</span><br><span class="line">  YAML::Node cam_info = YAML::<span class="built_in">LoadFile</span>(camera_param);</span><br><span class="line">  YAML::Node cb_xyz = YAML::<span class="built_in">LoadFile</span>(cali_param);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// cam_k</span></span><br><span class="line">  std::vector&lt;<span class="type">double</span>&gt; v_cam_k;</span><br><span class="line">  <span class="keyword">if</span> (cam_info[<span class="string">&quot;mtx&quot;</span>][<span class="string">&quot;data&quot;</span>]) &#123;</span><br><span class="line">    v_cam_k = cam_info[<span class="string">&quot;mtx&quot;</span>][<span class="string">&quot;data&quot;</span>].as&lt;std::vector&lt;<span class="type">double</span>&gt;&gt;();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Can&#x27;t get cam_info[\&quot;mtx\&quot;][\&quot;data\&quot;]&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">auto</span> cam_k =</span><br><span class="line">      Eigen::Map&lt;Eigen::Matrix&lt;<span class="type">double</span>, <span class="number">3</span>, <span class="number">3</span>, Eigen::RowMajor&gt;&gt;(v_cam_k.<span class="built_in">data</span>());</span><br><span class="line">  <span class="built_in">VLOG</span>(<span class="number">2</span>) &lt;&lt; <span class="string">&quot;\ncam_k:\n&quot;</span> &lt;&lt; cam_k;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// cam_intra</span></span><br><span class="line">  <span class="keyword">auto</span> col = Eigen::<span class="built_in">Vector3d</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">  <span class="function">Eigen::MatrixXd <span class="title">tmp</span><span class="params">(<span class="number">3</span>, <span class="number">4</span>)</span></span>;</span><br><span class="line">  tmp &lt;&lt; cam_k, col;</span><br><span class="line">  <span class="keyword">auto</span> row = Eigen::<span class="built_in">RowVector4d</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">  Eigen::Matrix&lt;<span class="type">double</span>, <span class="number">4</span>, <span class="number">4</span>, Eigen::RowMajor&gt; cam_intra;</span><br><span class="line">  cam_intra &lt;&lt; tmp, row;</span><br><span class="line">  <span class="built_in">VLOG</span>(<span class="number">2</span>) &lt;&lt; <span class="string">&quot;\ncam_intra:\n&quot;</span> &lt;&lt; cam_intra;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// R_e2cab</span></span><br><span class="line">  Eigen::Matrix3d R_e2cab;</span><br><span class="line">  R_e2cab &lt;&lt; <span class="number">0</span>, <span class="number">0</span>, <span class="number">-1</span>,  <span class="comment">//</span></span><br><span class="line">      <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>,          <span class="comment">//</span></span><br><span class="line">      <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>;          <span class="comment">//</span></span><br><span class="line">  <span class="built_in">VLOG</span>(<span class="number">2</span>) &lt;&lt; <span class="string">&quot;\nR_e2cab:\n&quot;</span> &lt;&lt; R_e2cab;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// T_e2cab</span></span><br><span class="line">  Eigen::Vector3d T_e2cab;</span><br><span class="line">  <span class="keyword">if</span> (cb_xyz[<span class="string">&quot;boarad2ego&quot;</span>]) &#123;</span><br><span class="line">    T_e2cab &lt;&lt; cb_xyz[<span class="string">&quot;boarad2ego&quot;</span>][<span class="string">&#x27;x&#x27;</span>].<span class="built_in">as</span>&lt;<span class="type">double</span>&gt;(),</span><br><span class="line">        cb_xyz[<span class="string">&quot;boarad2ego&quot;</span>][<span class="string">&#x27;y&#x27;</span>].<span class="built_in">as</span>&lt;<span class="type">double</span>&gt;(),</span><br><span class="line">        cb_xyz[<span class="string">&quot;boarad2ego&quot;</span>][<span class="string">&#x27;z&#x27;</span>].<span class="built_in">as</span>&lt;<span class="type">double</span>&gt;();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Can&#x27;t get cb_xyz[\&quot;boarad2ego\&quot;]&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">VLOG</span>(<span class="number">2</span>) &lt;&lt; <span class="string">&quot;\nT_e2cab:\n&quot;</span> &lt;&lt; T_e2cab;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ego2cab</span></span><br><span class="line">  <span class="function">Eigen::MatrixXd <span class="title">ego2cab_tmp</span><span class="params">(<span class="number">3</span>, <span class="number">4</span>)</span></span>;</span><br><span class="line">  ego2cab_tmp &lt;&lt; R_e2cab, T_e2cab;</span><br><span class="line">  <span class="built_in">VLOG</span>(<span class="number">2</span>) &lt;&lt; <span class="string">&quot;\nego2cab_tmp:\n&quot;</span> &lt;&lt; ego2cab_tmp;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// R_cab2cam</span></span><br><span class="line">  std::vector&lt;<span class="type">double</span>&gt; v_R_cab2cam;</span><br><span class="line">  <span class="keyword">if</span> (cam_info[<span class="string">&quot;rvec&quot;</span>][<span class="string">&quot;data&quot;</span>]) &#123;</span><br><span class="line">    v_R_cab2cam = cam_info[<span class="string">&quot;rvec&quot;</span>][<span class="string">&quot;data&quot;</span>].as&lt;std::vector&lt;<span class="type">double</span>&gt;&gt;();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Can&#x27;t get cam_info[\&quot;rvec\&quot;][\&quot;data\&quot;]&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">auto</span> R_cab2cam_tmp = Eigen::<span class="built_in">Map</span>&lt;Eigen::RowVector3d&gt;(v_R_cab2cam.<span class="built_in">data</span>());</span><br><span class="line">  <span class="type">double</span> n_norm = R_cab2cam_tmp.<span class="built_in">norm</span>();</span><br><span class="line">  <span class="function">Eigen::AngleAxisd <span class="title">rotation_vector</span><span class="params">(n_norm, R_cab2cam_tmp / n_norm)</span></span>;</span><br><span class="line">  Eigen::Matrix3d R_cab2cam = rotation_vector.<span class="built_in">toRotationMatrix</span>();</span><br><span class="line">  <span class="built_in">VLOG</span>(<span class="number">2</span>) &lt;&lt; <span class="string">&quot;\nR_cab2cam:\n&quot;</span> &lt;&lt; R_cab2cam;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// T_cab2cam</span></span><br><span class="line">  std::vector&lt;<span class="type">double</span>&gt; v_T_cab2cam;</span><br><span class="line">  <span class="keyword">if</span> (cam_info[<span class="string">&quot;tvec&quot;</span>][<span class="string">&quot;data&quot;</span>]) &#123;</span><br><span class="line">    v_T_cab2cam = cam_info[<span class="string">&quot;tvec&quot;</span>][<span class="string">&quot;data&quot;</span>].as&lt;std::vector&lt;<span class="type">double</span>&gt;&gt;();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Can&#x27;t get cam_info[\&quot;tvec\&quot;][\&quot;data\&quot;]&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">auto</span> T_cab2cam = Eigen::<span class="built_in">Map</span>&lt;Eigen::Vector3d&gt;(v_T_cab2cam.<span class="built_in">data</span>());</span><br><span class="line">  <span class="built_in">VLOG</span>(<span class="number">2</span>) &lt;&lt; <span class="string">&quot;\nT_cab2cam:\n&quot;</span> &lt;&lt; T_cab2cam;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// cab2cam</span></span><br><span class="line">  <span class="function">Eigen::MatrixXd <span class="title">cab2cam_tmp</span><span class="params">(<span class="number">3</span>, <span class="number">4</span>)</span></span>;</span><br><span class="line">  cab2cam_tmp &lt;&lt; R_cab2cam, T_cab2cam;</span><br><span class="line">  <span class="keyword">auto</span> col_tmp = Eigen::<span class="built_in">RowVector4d</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">  <span class="function">Eigen::MatrixXd <span class="title">cab2cam</span><span class="params">(<span class="number">4</span>, <span class="number">4</span>)</span></span>;</span><br><span class="line">  cab2cam &lt;&lt; cab2cam_tmp, col_tmp;</span><br><span class="line">  <span class="built_in">VLOG</span>(<span class="number">2</span>) &lt;&lt; <span class="string">&quot;\ncab2cam:\n&quot;</span> &lt;&lt; cab2cam;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ego2cab</span></span><br><span class="line">  <span class="function">Eigen::MatrixXd <span class="title">ego2cab</span><span class="params">(<span class="number">4</span>, <span class="number">4</span>)</span></span>;</span><br><span class="line">  ego2cab &lt;&lt; ego2cab_tmp, col_tmp;</span><br><span class="line">  <span class="built_in">VLOG</span>(<span class="number">2</span>) &lt;&lt; <span class="string">&quot;\nego2cab:\n&quot;</span> &lt;&lt; ego2cab;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// cam2ego</span></span><br><span class="line">  Eigen::MatrixXd cam2ego = (cab2cam * ego2cab).<span class="built_in">inverse</span>();</span><br><span class="line">  <span class="built_in">VLOG</span>(<span class="number">2</span>) &lt;&lt; <span class="string">&quot;\ncam2ego:\n&quot;</span> &lt;&lt; cam2ego;</span><br><span class="line">  <span class="built_in">LOG</span>(INFO) &lt;&lt; <span class="string">&quot;\ncam2ego row:\n&quot;</span> &lt;&lt; cam2ego.<span class="built_in">row</span>(<span class="number">0</span>);</span><br><span class="line">  <span class="built_in">LOG</span>(INFO) &lt;&lt; <span class="string">&quot;\ncam2ego row:\n&quot;</span> &lt;&lt; cam2ego.<span class="built_in">col</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// get results</span></span><br><span class="line">  <span class="built_in">LOG</span>(INFO) &lt;&lt; <span class="string">&quot;\nintrinsic: \n&quot;</span> &lt;&lt; cam_intra;</span><br><span class="line">  <span class="function">std::vector&lt;<span class="type">double</span>&gt; <span class="title">vec_cam_intra</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      cam_intra.data(), cam_intra.data() + cam_intra.rows() * cam_intra.cols())</span></span>;</span><br><span class="line"></span><br><span class="line">  Eigen::Matrix&lt;<span class="type">double</span>, <span class="number">3</span>, <span class="number">3</span>, Eigen::RowMajor&gt; R_cam2ego =</span><br><span class="line">      cam2ego.<span class="built_in">block</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>);</span><br><span class="line">  <span class="built_in">LOG</span>(INFO) &lt;&lt; <span class="string">&quot;\nR_cam2ego: \n&quot;</span> &lt;&lt; R_cam2ego;</span><br><span class="line">  <span class="function">std::vector&lt;<span class="type">double</span>&gt; <span class="title">vec_R_cam2ego</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      R_cam2ego.data(), R_cam2ego.data() + R_cam2ego.rows() * R_cam2ego.cols())</span></span>;</span><br><span class="line"></span><br><span class="line">  Eigen::Vector3d T_cam2ego = cam2ego.<span class="built_in">block</span>(<span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>);</span><br><span class="line">  <span class="built_in">LOG</span>(INFO) &lt;&lt; <span class="string">&quot;\nT_cam2ego: \n&quot;</span> &lt;&lt; T_cam2ego;</span><br><span class="line">  <span class="function">std::vector&lt;<span class="type">double</span>&gt; <span class="title">vec_T_cam2ego</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      T_cam2ego.data(), T_cam2ego.data() + T_cam2ego.rows() * T_cam2ego.cols())</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// save result to yaml</span></span><br><span class="line">  YAML::Node config;</span><br><span class="line">  <span class="comment">// test[&quot;camera&quot;][&quot;v_cam_k&quot;].push_back(v);</span></span><br><span class="line">  <span class="comment">// note: push_back and = are not same</span></span><br><span class="line">  config[<span class="string">&quot;camera&quot;</span>][<span class="string">&quot;intrinsic&quot;</span>] = vec_cam_intra;</span><br><span class="line">  config[<span class="string">&quot;camera&quot;</span>][<span class="string">&quot;cam2ego_R&quot;</span>] = vec_R_cam2ego;</span><br><span class="line">  config[<span class="string">&quot;camera&quot;</span>][<span class="string">&quot;cam2ego_t&quot;</span>] = vec_T_cam2ego;</span><br><span class="line">  <span class="function">std::ofstream <span class="title">fout</span><span class="params">(result_param)</span></span>;</span><br><span class="line">  fout &lt;&lt; config;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[])</span> </span>&#123;</span><br><span class="line">  <span class="built_in">GetPgCamParm</span>(<span class="string">&quot;camera_params.yaml&quot;</span>, <span class="string">&quot;cali_board2vehicle_xyz.yaml&quot;</span>,</span><br><span class="line">               <span class="string">&quot;pg_cam2_param.yaml&quot;</span>);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2022/03/26/yaml/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/26/yaml/" class="post-title-link" itemprop="url">yaml</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-26 04:00:17" itemprop="dateCreated datePublished" datetime="2022-03-26T04:00:17+08:00">2022-03-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-27 11:36:34" itemprop="dateModified" datetime="2023-06-27T11:36:34+08:00">2023-06-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Program/" itemprop="url" rel="index"><span itemprop="name">Program</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/03/26/yaml/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/03/26/yaml/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="opencv-对数"><a href="#opencv-对数" class="headerlink" title="opencv 对数"></a>opencv 对数</h2><ol>
<li>python</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">`cv2.FileStorage(<span class="string">&quot;output.yml&quot;</span>, cv2.FILE_STORAGE_WRITE).write(<span class="string">&quot;mat&quot;</span>, mat)`</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fs = cv2.FileStorage(<span class="string">&quot;predict_benz_trajectory_v2_crop.yml&quot;</span>, cv2.FILE_STORAGE_WRITE)</span><br><span class="line">fs.write(<span class="string">&quot;Q&quot;</span>, Q.astype(np.float32))</span><br><span class="line">fs.write(<span class="string">&quot;traj_out_wid&quot;</span>, traj_out_wid)</span><br><span class="line">fs.release()</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>C++</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cv::FileStorage <span class="title">ymlfile</span><span class="params">(<span class="string">&quot;predict_benz_trajectory_v2_crop.yml&quot;</span>,                                                                                                                                                                                                                          cv::FileStorage::READ)</span></span>;</span><br><span class="line"><span class="keyword">if</span>(!fs.<span class="built_in">isOpened</span>())&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Cannot open config file!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line">cv::Mat disparity;</span><br><span class="line">ymlfile[<span class="string">&quot;disparity&quot;</span>] &gt;&gt; disparity;</span><br><span class="line"><span class="type">float</span> traj_wid = (<span class="type">float</span>)ymlfile[<span class="string">&quot;ground_traj_wid&quot;</span>];</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  cv::Mat Q_mat;</span><br><span class="line">  cv::FileNode node = ymlfile[<span class="string">&quot;Q&quot;</span>];</span><br><span class="line">  <span class="keyword">if</span> (node.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">    std::cerr &lt;&lt; <span class="string">&quot;Key not found in yaml file&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (node.<span class="built_in">type</span>() == cv::FileNode::MAP &amp;&amp; node.<span class="built_in">size</span>() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    node &gt;&gt; Q_mat;</span><br><span class="line">    <span class="keyword">if</span> (Q_mat.<span class="built_in">type</span>() == CV_64F) &#123;</span><br><span class="line">      Q_mat.<span class="built_in">convertTo</span>(Q_mat, CV_32F);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    std::cerr &lt;&lt; <span class="string">&quot;Node is not an opencv mat  or is empty&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>numpy and vector</li>
</ol>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line">np_array = np.array(<span class="string">[[1, 2, 3], [4, 5, 6]]</span>, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">fs = cv2.FileStorage(<span class="string">&#x27;output.xml&#x27;</span>, cv2.FileStorage_WRITE)</span><br><span class="line">fs.<span class="built_in">write</span>(<span class="string">&#x27;my_array&#x27;</span>, np_array)</span><br><span class="line">fs.release()</span><br></pre></td></tr></table></figure>

<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">std::vector&lt;<span class="type">float</span>&gt; vec;</span><br><span class="line">vec.<span class="built_in">assign</span>((<span class="type">float</span>*)mat.datastart, (<span class="type">float</span>*)mat.dataend);</span><br></pre></td></tr></table></figure>

<h3 id="读写"><a href="#读写" class="headerlink" title="读写"></a>读写</h3><ol>
<li>yaml-cpp</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/learning_tortosie/article/details/97815514">opencv cv::FileStorage</a></li>
</ol>
<h3 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h3><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/jbeder/yaml-cpp">github</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/jbeder/yaml-cpp/wiki/Tutorial">tutorial</a></li>
</ol>
<h3 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;yaml-cpp/yaml.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[])</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  YAML::Node config = YAML::<span class="built_in">LoadFile</span>(<span class="string">&quot;config.yaml&quot;</span>);</span><br><span class="line"></span><br><span class="line">  std::vector&lt;<span class="type">double</span>&gt; intrinsic;</span><br><span class="line">  <span class="keyword">if</span> (config[<span class="string">&quot;camera&quot;</span>][<span class="string">&quot;intrinsic&quot;</span>]) &#123;</span><br><span class="line">    intrinsic = config[<span class="string">&quot;camera&quot;</span>][<span class="string">&quot;intrinsic&quot;</span>].as&lt;std::vector&lt;<span class="type">double</span>&gt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> i = <span class="number">0u</span>; i &lt; intrinsic.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">      std::cout &lt;&lt; intrinsic[i] &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  std::vector&lt;<span class="type">double</span>&gt; cam2ego_R;</span><br><span class="line">  <span class="keyword">if</span> (config[<span class="string">&quot;camera&quot;</span>][<span class="string">&quot;cam2ego_R&quot;</span>]) &#123;</span><br><span class="line">    cam2ego_R = config[<span class="string">&quot;camera&quot;</span>][<span class="string">&quot;cam2ego_R&quot;</span>].as&lt;std::vector&lt;<span class="type">double</span>&gt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> i = <span class="number">0u</span>; i &lt; cam2ego_R.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">      std::cout &lt;&lt; cam2ego_R[i] &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  std::vector&lt;<span class="type">double</span>&gt; <span class="type">cam2ego_t</span>;</span><br><span class="line">  <span class="keyword">if</span> (config[<span class="string">&quot;camera&quot;</span>][<span class="string">&quot;cam2ego_t&quot;</span>]) &#123;</span><br><span class="line">    <span class="type">cam2ego_t</span> = config[<span class="string">&quot;camera&quot;</span>][<span class="string">&quot;cam2ego_t&quot;</span>].as&lt;std::vector&lt;<span class="type">double</span>&gt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> i = <span class="number">0u</span>; i &lt; <span class="type">cam2ego_t</span>.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">      std::cout &lt;&lt; <span class="type">cam2ego_t</span>[i] &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">std::ofstream <span class="title">fout</span><span class="params">(<span class="string">&quot;./out.yaml&quot;</span>)</span></span>;</span><br><span class="line">  fout &lt;&lt; config;</span><br><span class="line"></span><br><span class="line">  YAML::Node test;</span><br><span class="line">  <span class="comment">// test[&quot;camera&quot;][&quot;intrinsic&quot;].push_back(v);</span></span><br><span class="line">  <span class="comment">// note: push_back and = are not same</span></span><br><span class="line">  test[<span class="string">&quot;camera&quot;</span>][<span class="string">&quot;intrinsic&quot;</span>] = intrinsic;</span><br><span class="line">  test[<span class="string">&quot;camera&quot;</span>][<span class="string">&quot;cam2ego_R&quot;</span>] = cam2ego_R;</span><br><span class="line">  test[<span class="string">&quot;camera&quot;</span>][<span class="string">&quot;cam2ego_t&quot;</span>] = <span class="type">cam2ego_t</span>;</span><br><span class="line">  <span class="function">std::ofstream <span class="title">fout1</span><span class="params">(<span class="string">&quot;./test.yaml&quot;</span>)</span></span>;</span><br><span class="line">  fout1 &lt;&lt; test;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">camera:</span></span><br><span class="line">  <span class="attr">intrinsic:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">1888.7618313608834</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">0</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">968.83548164132105</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">0</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">0</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">1896.5639945248038</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">547.4612962459845</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">0</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">0</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">0</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">1</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">0</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">0</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">0</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">0</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">cam2ego_R:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">0.034896920649101944</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">0.015135626613007833</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">0.99927629698862219</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">-0.99939075949347744</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">0.0010902662284327418</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">0.034884404167777945</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">-0.00056147988681672133</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">-0.99988485586850695</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">0.015164452300128389</span></span><br><span class="line">  <span class="attr">cam2ego_t:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">1.84679730821996</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">-0.089090448031248784</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">1.3980633087528846</span></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2022/03/18/samba/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/18/samba/" class="post-title-link" itemprop="url">samba</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-18 01:43:12" itemprop="dateCreated datePublished" datetime="2022-03-18T01:43:12+08:00">2022-03-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-09-28 13:13:37" itemprop="dateModified" datetime="2023-09-28T13:13:37+08:00">2023-09-28</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/03/18/samba/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/03/18/samba/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>481</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="install"><a href="#install" class="headerlink" title="install"></a>install</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install samba</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="config"><a href="#config" class="headerlink" title="config"></a>config</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/samba/smb.conf</span><br><span class="line"></span><br><span class="line">[share]</span><br><span class="line">     path = /home/jiaxiyang/share</span><br><span class="line">     available = <span class="built_in">yes</span></span><br><span class="line">     valid <span class="built_in">users</span> = jiaxiyang</span><br><span class="line">     <span class="built_in">read</span> only = no</span><br><span class="line">     browsable = <span class="built_in">yes</span></span><br><span class="line">     public = <span class="built_in">yes</span></span><br><span class="line">     writable = <span class="built_in">yes</span></span><br></pre></td></tr></table></figure>

<h2 id="add-user"><a href="#add-user" class="headerlink" title="add user"></a>add user</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo smbpasswd -<span class="selector-tag">a</span> jiaxiyang</span><br></pre></td></tr></table></figure>

<h2 id="restart"><a href="#restart" class="headerlink" title="restart"></a>restart</h2><h3 id="ubuntu"><a href="#ubuntu" class="headerlink" title="ubuntu"></a>ubuntu</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="regexp">/etc/i</span>nit.d/samba restart</span><br><span class="line">sudo <span class="regexp">/etc/i</span>nit.d/smbd restart</span><br></pre></td></tr></table></figure>

<h3 id="centos"><a href="#centos" class="headerlink" title="centos"></a>centos</h3><figure class="highlight nsis"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="params">system</span>ctl start smb.service</span><br><span class="line">sudo <span class="params">system</span>ctl restart smb</span><br></pre></td></tr></table></figure>

<h2 id="link"><a href="#link" class="headerlink" title="link"></a>link</h2><h3 id="ubuntu-1"><a href="#ubuntu-1" class="headerlink" title="ubuntu"></a>ubuntu</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount.cifs <span class="regexp">//</span><span class="number">10.10</span>.<span class="number">0.61</span><span class="regexp">/jiaxiyang/</span> samba_16 -o user=jiaxiyang</span><br></pre></td></tr></table></figure>

<h3 id="windows"><a href="#windows" class="headerlink" title="windows"></a>windows</h3><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">映射网络驱动器</span><br><span class="line">\\ipaddr\share</span><br><span class="line"><span class="symbol">NOTE: </span>share is [share] in smb.conf</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2022/03/10/Cuda/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/10/Cuda/" class="post-title-link" itemprop="url">Cuda</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-10 02:17:38" itemprop="dateCreated datePublished" datetime="2022-03-10T02:17:38+08:00">2022-03-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-11 11:58:57" itemprop="dateModified" datetime="2025-04-11T11:58:57+08:00">2025-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Program/" itemprop="url" rel="index"><span itemprop="name">Program</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Program/Cuda/" itemprop="url" rel="index"><span itemprop="name">Cuda</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/03/10/Cuda/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/03/10/Cuda/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>30k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>27 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><ol>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#execution-configuration">Execution Configuration</a><ul>
<li>&lt;&lt;&lt; &gt;&gt;&gt; 叫做 kernel launch configuration（核函数启动配置）, launch参数：&lt;&lt;&lt;gridDim, blockDim, sharedMemSize, stream&gt;&gt;&gt;  CUDA核函数运行参数</li>
<li>&lt;&lt;&lt; Dg, Db, Ns, S &gt;&gt;&gt;</li>
</ul>
</li>
<li>在 CUDA 编程中，CTA（Cooperative Thread Array） 是 线程块（Thread Block） 的另一种称呼。</li>
<li>Block and Tile<ul>
<li>Block (线程块) 是CUDA线程层次结构中的一部分，位于网格（grid） 之下。它是CUDA中的一个基本执行单元，由多个线程（thread） 组成，每个线程在一个SM（Streaming Multiprocessor）上执行。</li>
<li>Tile（数据分片） 通常是手动优化时的计算划分方式，用于块状处理数据（类似分块矩阵计算）。它不属于CUDA的核心执行模型，而是开发者用来优化CUDA程序时的概念。</li>
<li>Block是线程的概念，Tile是数据划分的概念; Tile数据并不等于Block处理的数据，如矩阵运算是，A矩阵划分为Tile: bm x bk, Block对应的C数据为bm x bn; Block也可能处理多个tile的数据</li>
</ul>
</li>
<li>block swizzle:<ul>
<li>runtime按idx线性调度：0, 1, 2….</li>
<li>用户自己将idx映射到要处理的block</li>
</ul>
</li>
<li><code>CUDA_LAUNCH_BLOCKING=1</code> 有错误立即停下来，效率会差一点，主要用于调试</li>
<li><a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/detail/KernelUtils.h">kernel utils</a> 简化写 kernel 流程</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/480719273">手动释放显存</a></li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">apt-get install psmisc</span><br><span class="line">fuser -v /dev/nvidia*</span><br><span class="line">kill -9 ***(PID)</span><br><span class="line">killall python</span><br></pre></td></tr></table></figure>

<ol>
<li>fp32, tf32, fp16, bf16, fp8<br><img src="https://i.ibb.co/vdTGHkJ/txy-EFVOs5-P.png" alt="data type"></li>
<li>NVIDIA GPU 的缓存行（cache line）大小一般是 128 字节。</li>
<li>cache line 又分为 4 个 sectors, 1 个 sector 32B</li>
<li>sector 是 global memory 访问的最小单位，32 个线程一起运行，最少访问 32B</li>
<li>gridDim 划分的是数据，blockDim 划分的是线程：例如: sgemm 分块矩阵乘</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">N=M=<span class="number">1024</span>;</span><br><span class="line">BN=BM=<span class="number">128</span>, TN=TB=<span class="number">8</span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">blockDim</span><span class="params">(BN / TN, BM / TM)</span></span>;  <span class="comment">// block 按线程分为 16x16 thread,threadDim=(16, 16) 每个线程处理8x8数据; x维度是列，y维度是行;</span></span><br><span class="line"><span class="function">dim3 <span class="title">gridDim</span><span class="params">((N + BN - <span class="number">1</span>) / BN, (M + BM - <span class="number">1</span>) / BM)</span></span>; <span class="comment">// grid 按数据分为 8x8 block, blockDim=(8, 8)</span></span><br></pre></td></tr></table></figure>

<ol>
<li><code>GPU通过计算而不是深度缓存层次结构来隐藏访存延迟</code><ul>
<li>当数据访存的时候，就让 warp stall，而后再选一个 warp 进行计算，通过这种方式交错开计算和访存，让访存单元一直忙碌，带宽打满。计算延迟的时候也会让 warp stall</li>
</ul>
</li>
<li>并发的 warp 切换没有开销，因为每个 warp 都分配了硬件资源<ul>
<li>不需要上下文切换</li>
<li>以空间换时间</li>
</ul>
</li>
<li>假如 GPU 有 108 个 SM。每个 SM 可以并行处理多个线程块，具体取决于所使用的内核；为了获得最佳并行化，隐式 GEMM 应包含 108 个图块的整数倍。</li>
<li>gpu sram 带宽为什么比主存大很多：<ul>
<li>并行，多个 SM 访问 shared memory， 如有 80 个 sm，每个 sm4 个 partition， 每个 shared memory 有 32 个 bank， 等效位宽为：<code>80*4*32*8 = 81920</code>, hbm 位宽为 8192， 带宽为 1.5TB&#x2F;s, sram 为 19TB&#x2F;s</li>
<li>shared memory 带宽：<code>2(?) * freq * 32(banks) * 8/8 *4(sm partion) * 80(sm num)</code></li>
</ul>
</li>
<li><code>kernel融合主要是使用sram来减少对显存的访问，注意不是device和host数据搬移</code> 也会减少调度开销</li>
<li>kernel 在编译的时候需要明确 block grid size 吗？不需要， 这些参数通常在运行时通过 CUDA 内核启动语法指定，这提供了更高的灵活性和动态调整的可能性。</li>
<li>nvcc 在翻译单元的顶部隐式包含了 cuda_runtime.h 。</li>
<li>核函数 K（kernel function）就是指 K(x, y) &#x3D; ，其中 x 和 y 是 n 维的输入值，f(·) 是从 n 维到 m 维的映射（通常而言，m&gt;&gt;n）。是 x 和 y 的内积（inner product），严格来说 应该叫欧式空间的标准内积，也就是很多人常说的点积（dot product）。</li>
<li><code>sudo update-alternatives --display cuda</code>显示系统 cuda 版本</li>
<li><code>export PATH=/usr/local/cuda/bin/:$&#123;PATH&#125;</code>找不到 nvcc 可能需要 export PATH</li>
<li><code>sudo apt install -y nvidia-cuda-toolkit</code> 安装 cuda 工具链, 不能乱用，需要和系统 cuda 匹配</li>
<li>在 CUDA 中，你会以类似于 C&#x2F;C++函数的形式来表达想要在 GPU 上运行的计算，这个函数被称为 kernel。</li>
<li>GPU 函数耗时统计不能只记录一次的，GPU 可能做一些准备工作，教训： nppiResize_8u_C3R 不管大小第一次运行耗时都很大 The cuda context is lazily initialized</li>
<li><code>autotuning</code> 搜索 kernel grid 划分参数(结果不变)，找性能最优</li>
<li>L1 和 shared memory 共享一块存储 可动态分配比例; 可用比例见<a target="_blank" rel="noopener" href="https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf">link</a><ul>
<li>多用 shared memory 就多分点给 shared memory 多用寄存器就多分点给 L1 cache</li>
</ul>
</li>
<li>warp 调度:延时隐藏 只执行 warp 一部分 当 warp 需要等待时 先执行其他 warp</li>
<li>shared memory 和 register 是 SM 中的稀缺资源.</li>
<li>warp 执行类似 simd</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/2023.3/ProfilingGuide/index.html#metrics-hw-model">Hardware Model</a><ul>
<li>对 sm 介绍比较好</li>
<li>Each SM is partitioned into four processing blocks, called SM sub partitions.</li>
<li>一个 SM 又可以由若干个 SMP（SM Partition）组成</li>
<li>A warp is allocated to a sub partition and resides on the sub partition from launch to completion. warp 被分配给子分区，并且从启动到完成都驻留在子分区上。</li>
</ul>
</li>
<li><code>FMA (Fused Multiply Add)</code>: z&#x3D;a*x+y …z,x,y are vectors or scalars</li>
<li><code>4FMA (Quad FMA)</code>: z&#x3D;A*x+z …A is a FP32 matrix; x,z are vectors</li>
<li><code>WMMA: Warp-level Matrix Mulitply and Accumulate (Tensor Core)</code>: Z&#x3D;AB+C …A,B are FP16 matrices; Z,C are FP32</li>
<li>global memory -&gt; shared memory 时，计算每个线程需要搬移的数据量。假如每个线程要搬移 A，B 矩阵为 4 个 single float point, 4x4 &#x3D; 16B<ul>
<li>A_tile（128,8): 256 个线程， 每个线程搬移 4 个数，每行由 2 个线程处理， 假如线程一维索引为 tid； 行 m &#x3D; tid &#x2F; 2 &#x3D; tid &gt;&gt; 1; 列 k &#x3D; (tid % 2) x 4 &#x3D; (tid &amp; 1) &lt;&lt; 2</li>
<li>B_tile（8,128): 256 个线程， 每个线程搬移 4 个数，每行由 32 个线程处理， 假如线程一维索引为 tid； 行 k &#x3D; tid &#x2F; 32 &#x3D; tid &gt;&gt; 5; 列 n &#x3D; (tid % 32) x 4 &#x3D; (tid &amp; 31) &lt;&lt; 2</li>
</ul>
</li>
<li>load&#x2F;store 关键点在行列索引(分别计算各 tile 维度的 m, n, k)， 通过行列可以找到起始地址。<code>#define OFFSET(row, col, ld) ((row) * (ld) + (col))</code></li>
<li>load&#x2F;store memory 的时候先通过行列坐标找到 global 起始点，然后再算偏移，用宏定义来访问矩阵，先找最外层的 tile，再找里层的 tile。</li>
<li>sgemm 分块 A 矩阵 global 到 shared load 时，shared memory 需要列优先排列，一次 load 四个数都寄存器，然后再分别赋值给列。</li>
<li>shred memory double buffering: 在 for 循环里只用 sync 一次<ul>
<li>load tile[0] 到 buffer[0]; sync</li>
<li>for: load tile[i]; compute tile[i-1]; sync</li>
<li>comute tile[-1]; sync</li>
</ul>
</li>
<li>wave 的概念：wave 表示 GPU 上同时执行的 thread block。例如一个 kernel 中 thread block 为 256 线程，每个线程使用了 128 个寄存器，那么在 GV100 上每个 SM 可同时执行 2 个 thread block，GV100 共 80 个 SM，一个 wave 就是 160 个 thread block。</li>
<li>cuda 遵循 IEEE 754 standard for binary floating-point representation； 由于融合和乘加， 其结果与分别乘加结果略有不同 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#ieee-754-compliance">ieee-754-compliance</a><ul>
<li>One of the key differences is the fused multiply-add (FMA) instruction, which combines multiply-add operations into a single instruction execution. <code>Its result will often differ slightly from results obtained by doing the two operations separately.</code></li>
</ul>
</li>
<li>每个浮点算术运算都涉及一定量的舍入。因此，算术运算的执行顺序很重要。如果 A、B 和 C 是浮点值，则 (A+B)+C 不能保证等于 A+(B+C)</li>
<li>我们可以将 CUDA 内核编写为许多短 <code>__device__</code> 函数的集合，而不是一个大型的整体 <code>__global__</code> 函数；每个设备功能可以在将它们连接在一起之前进行独立测试。</li>
<li>如果大多数函数都定义为 <code>__host__ __device__</code> 而不仅仅是 <code>__device__</code> 函数，那么这些函数就可以在 CPU 和 GPU 上进行测试，</li>
<li><code>local memory</code>之所以如此命名，是因为它的作用域是线程本地的，而不是因为它的物理位置。事实上，本地存储器位于片外。因此，访问本地内存与访问全局内存一样昂贵。换句话说，名称中的“本地”一词并不意味着访问速度更快。本地内存仅用于保存自动变量。当 nvcc 编译器确定没有足够的寄存器空间来保存变量时，就会执行此操作。可能放置在本地内存中的自动变量是大型结构或数组，它们会消耗太多寄存器空间，并且编译器确定可以动态索引的数组。</li>
<li><code>constent memory</code></li>
<li><code>The NVIDIA Management Library (NVML)</code> is a C-based interface that provides direct access to the queries and commands exposed via nvidia-smi intended as a platform for building 3rd-party system management applications.</li>
<li>可以通过 CUDA_VISIBLE_DEVICES 环境变量重新排列已安装的 CUDA 设备的集合; <code>CUDA_VISIBLE_DEVICES=0,2,1,3</code></li>
<li>Starting with CUDA 11.0, devices of compute capability 8.0 and above have the capability to influence persistence of data in the L2 cache, potentially providing higher bandwidth and lower latency accesses to global memory.</li>
<li>从 Hopper 开始，CUTLASS 3.0 将 Warp Specialization 的概念纳入了内核设计的一部分。线程块被划分为两组 warp，生产者 warp 组和消费者 warp 组。生产者 warp 组使用新的张量内存加速器（TMA）将数据从全局内存加载到共享内存缓冲区中。<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#warp-specialization">link</a></li>
<li>TMA 可以异步一次 load 大块数据到 shared memory， ampere 一次最多只能 load 128 bit 数据(指令限制)</li>
<li>想象 shared memory 和 register 是二维的, bank 不冲突<ul>
<li>shared memory: <code>smem[n][32]</code></li>
<li>register: <code>re[n][4]</code></li>
</ul>
</li>
<li>虽然 block 可以划分为二维 thread, 但调度时按一维调度，每 32 个一个 warp</li>
<li>laneID 是 warp 内第几个线程 threadIdx.x %32</li>
<li>WarpID &#x3D; threadIdx.x &#x2F;32</li>
<li>常量内存位于显存中 片上有缓存 类似于 shared memeory warp 内多个线程读同一个地址最快，不同地址要串行</li>
<li>常量内存对于 kernel 是只读的 对于主机可读写</li>
<li>constant 由主机代码准备 kernel 中直接用</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">__constant__ <span class="type">float</span> coef[<span class="number">100</span>];</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">setup_coef_constant</span><span class="params">(<span class="type">void</span>)</span></span>&#123;</span><br><span class="line">    <span class="built_in">cudaMemorycpyToSymbol</span>(coef, h_coef, size);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li>kernel 第三个参数是动态共享内存的大小</li>
</ol>
<h2 id="TMA"><a href="#TMA" class="headerlink" title="TMA"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-data-copies-using-the-tensor-memory-accelerator-tma">TMA</a></h2><ol>
<li>The primary goal of TMA is to provide an efficient data transfer mechanism from global memory to shared memory for multi-dimensional arrays. TMA 的主要目标是为多维数组提供一种从全局内存到共享内存的高效数据传输机制。</li>
<li>TMA 根据tensor_map和coord等信息产生ptr，用于搬数</li>
<li>批量异步复制操作的源和目标地址可以位于共享或全局内存中。这些操作可以将数据从全局内存读取到共享内存，将数据从共享内存写入全局内存，还可以从共享内存复制到同一集群中另一个块的分布式共享内存 。此外，在cluster中时，可以将批量异步操作指定为多播(multicast)。在这种情况下，数据可以从全局内存传输到cluster内多个块的共享内存。</li>
<li>mbarrier（内存屏障）是 CUDA 12 引入的新机制，用于 跨线程块（CTA） 进行同步，主要用于 生产者-消费者（producer-consumer）模式的通信。<ul>
<li>可以多个SM</li>
<li>支持多个线程块（CTA）之间的同步，比 __syncthreads() 更强大。</li>
<li>允许 生产者线程（Producer） 和 消费者线程（Consumer） 在不同的 CTA 中进行数据交换和同步。</li>
</ul>
</li>
<li>Bulk Async Group 是 CUDA 12.2 引入的 批量任务管理机制，允许在 SM（流式多处理器） 内进行任务分配，同时异步调度多个 warp 或 CTA 执行任务.</li>
</ol>
<ul>
<li>单个SM内部</li>
</ul>
<ol>
<li>tensor_map创建完在host memory上 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-data-copies-using-the-tensor-memory-accelerator-tma:~:text=Host%2Dto%2Ddevice%20transfer.">link</a><ul>
<li>tensor_map中的tensor_ptr需要指向device global memory上</li>
</ul>
</li>
</ol>
<h2 id="driver"><a href="#driver" class="headerlink" title="driver"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-driver-api/index.html">driver</a></h2><h2 id="runtime"><a href="#runtime" class="headerlink" title="runtime"></a>runtime</h2><h2 id="cuda-graph"><a href="#cuda-graph" class="headerlink" title="cuda graph"></a>cuda graph</h2><ol>
<li>减少 Launch 开销</li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/cuda-graphs/">cuda-graphs blog</a></li>
<li>trtexec 有参数指定</li>
</ol>
<h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><ol>
<li>想办法喂饱硬件, 好好调教它，让它努力打工, 工作饱和，不能偷懒</li>
<li>分析计算部件停顿的原因。</li>
<li>主要由下列因素决定：<ul>
<li>算力(peak)</li>
<li>带宽(peak)</li>
<li>指令和访存延迟</li>
</ul>
</li>
<li>性能 roofline bound 是满流水线时分析，一直忙; 指令和访存延迟会使得处理器空闲</li>
<li>一直忙条件：<code>warp 数量 = 延迟 x 吞吐</code>, 如：延迟 20 cycle; SM 吞吐为 64 fma 每 cycle；SM warp 数量为： 20 x 64 &#x2F; 32 &#x3D; 40</li>
<li>Use peak performance metrics to guide optimization</li>
<li>Optimize your algorithm, then unroll loops</li>
<li>Use template parameters to generate optimal code</li>
<li>bandwidth(带宽)和 throughput(吞吐)区别：<ul>
<li>bandwitdh: 理论最大吞吐</li>
<li>throughput: 实际吞吐</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://developer.download.nvidia.com/GTC/PDF/1083_Wang.pdf">Fundamental Optimizations in CUDA</a></li>
<li>kernels are too small -&gt; kernel launch bound; gpu is idle in many time, reason:kernels are too small <a target="_blank" rel="noopener" href="https://github.com/jiaxiyang/CUDA-PPT/blob/main/GTC2020/s21417-faster-transformer.pdf">link</a><ul>
<li>a simple solution: using tensorflow XLA to fuse kernel automaticlly</li>
</ul>
</li>
<li>在 CUDA 编程中，<code>谓词替换</code>通常指的是一种编程技巧，它通过使用谓词（即条件表达式）来代替显式的分支语句（如 if-else 或 switch 语句）。这种技巧可以帮助<code>减少线程发散</code>，从而提高在 GPU 上的并行执行效率。使用条件运算符（如? :）或逻辑运算符（如&amp;&amp;和||）来替换 if-else 语句。这样可以保证所有线程执行相同数量的指令，尽管这些指令的实际作用可能因条件而异。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">__global__ void traditionalBranch(int *data, int value, int threshold) &#123;</span><br><span class="line">    int index = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    if (data[index] &gt; threshold) &#123;</span><br><span class="line">        data[index] = value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__global__ void predicateReplacement(int *data, int value, int threshold) &#123;</span><br><span class="line">    int index = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    data[index] = (data[index] &gt; threshold) * value + !(data[index] &gt; threshold) * data[index];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li>cache locality 表示程序对缓存中数据的访问 locality 和重用情况。如果同一个 warp 中的线程迭代访问同一缓存线(cache line)上的数据(比如遍历一个数组),那么可以最大化利用 cache,称之为良好的 cache locality。</li>
</ol>
<h3 id="papers"><a href="#papers" class="headerlink" title="papers"></a>papers</h3><ol>
<li><a target="_blank" rel="noopener" href="http://impact.crhc.illinois.edu/shared/papers/optimization2008.pdf">Optimization Principles and Application Performance Evaluation</a></li>
</ol>
<h3 id="cuda-performance-guidelines"><a href="#cuda-performance-guidelines" class="headerlink" title="cuda performance-guidelines"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#performance-guidelines">cuda performance-guidelines</a></h3><ol>
<li><code>整体优化策略</code> Overall Performance Optimization Strategies；Performance optimization revolves around four basic strategies:<ul>
<li>Maximize parallel execution to achieve maximum utilization; 最大化并行执行以达到最大利用率；</li>
<li>Optimize memory usage to achieve maximum memory throughput; 优化存储使用，实现最大存储吞吐量；</li>
<li>Optimize instruction usage to achieve maximum instruction throughput; 优化指令使用，实现最大指令吞吐量；</li>
<li>Minimize memory thrashing. 最大限度地减少内存抖动。(减少内存申请释放频率)</li>
</ul>
</li>
<li>Which strategies will yield the best performance gain for a particular portion of an application depends on the performance limiters for that portion;根据情况使用哪种策略，比如 reduction 时受 memory bound，因此我们应该争取峰值带宽</li>
<li>compute-bound 需要争取达到最大算力，memory bound 需要争取达到最大带宽</li>
</ol>
<h3 id="优化技术"><a href="#优化技术" class="headerlink" title="优化技术"></a>优化技术</h3><ol>
<li>memory coalescing<ul>
<li>让连续的线程访问连续的内存地址。这样就有合并机会，多个线程间会合并访存，可能并不是 32 个线程都合并, 比如一个线程访问 4 个 32bit float 数据，8 个线程可合并，8 x 4 x 32 &#x2F; 8 &#x3D; 128Bx</li>
<li>不连续的时候每个数据都需要load一次，合并了之后能减少load次数，完全合并后需要n&#x2F;32次，32个线程的数据一次load进来(不考虑cache的情况)</li>
</ul>
</li>
<li>swizzle</li>
<li>bank conflict</li>
<li>分支优化<ul>
<li>谓词替换</li>
<li>分支预测</li>
<li>循环展开</li>
</ul>
</li>
</ol>
<h3 id="访存优化"><a href="#访存优化" class="headerlink" title="访存优化"></a>访存优化</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/600489819">漫谈高性能计算与性能优化：访存</a></li>
<li>访存优化是第一性原理</li>
<li>当我们在说访存优化的时候，我们具体需要做些什么。总的来说，就是三板斧。<ul>
<li>减少数据搬运<ul>
<li>如何减少数据搬运，最主要的手段就是分块，或者说 tiling。</li>
<li>要尽可能地保证数据连续访问，其中最主要的一个原因就是提高 cache 命中率，从而避免不必要的数据搬运</li>
</ul>
</li>
<li>减少数据访存延时(数据搬运确定)<ul>
<li>减少 bank 冲突</li>
<li>pipeline (double buffer, 预取)</li>
<li>就是切分更多的块，启动更多的 warp 来掩盖访存延时。</li>
</ul>
</li>
<li>保证负载均衡。<ul>
<li>关于负载均衡的话题，主要是在 sparse 里面谈的比较多</li>
</ul>
</li>
</ul>
</li>
<li>如果发现实际带宽比较差，数据搬运效率比较低，这个时候就要去思考，是不是可以有办法，通过分块的一些技巧来减少数据搬运。如果数据搬运不能够再减少了的话，是否可以通过一些方式来提高数据的搬运效率，比如向量化访存、合并访问来提高对 DRAM 的访存性能、避免 bank 冲突来提高对 shared memory 的访存性能、调整分块大小来让更多的 warp 跑起来从而减少访存的延时，如果不是 SIMT 架构，就需要精细地设计各级访存的 pipeline，让访存操作尽可能地 pingpong 起来，从而让访存流水尽可能地连续起来不要被打断。理论大概是这样，但是每一个问题都有着不同的处理方式，每一个问题可能都是不同的瓶颈。总之就是万变不离其宗，准确地评估每一级存储的访存效率然后尽可能地提高每一级的访存效率，尽可能地把访存流水打满，不要有空跑。</li>
<li>其实所谓“加速”或者“性能优化”的本质就是让软件充分利用计算硬件，提升利用率，从而逼近理论性能上限。从这个角度，“通用方法”就是：<code>分析计算部件停顿的原因-选择合理的计算模型减少数据依赖和对流水线的破坏(能兼顾缓解访存墙更好)-通过专用硬件或者结构优化消除剩下的瓶颈，然后不断迭代上述过程，直至各方面因素达到平衡</code>。</li>
<li>cuda gemm 为什么是三级分块，不是四级或者两级。因为 NV 的 GPU 内存结构是三级的，global mem-&gt;shared mem，shared mem-&gt;register。</li>
</ol>
<h3 id="reduction-优化"><a href="#reduction-优化" class="headerlink" title="reduction 优化"></a>reduction 优化</h3><ol>
<li>(great)<a target="_blank" rel="noopener" href="https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf">Optimizing Parallel Reduction in CUDA</a><ul>
<li>Memory coalescing</li>
<li>Divergent branching</li>
<li>Bank conflicts</li>
<li>Latency hiding</li>
</ul>
</li>
<li><code>What is Our Optimization Goal?</code> 先确定最大目标 We should strive to reach GPU peak performance Choose the right metric:<ul>
<li>GFLOP&#x2F;s: for compute-bound kernels</li>
<li>Bandwidth: for memory-bound kernels</li>
</ul>
</li>
<li>通过不断迭代优化，从而达到硬件最优性能。</li>
<li>Reductions have very low arithmetic intensity; Therefore we should strive for <code>peak bandwidth</code></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/426978026">中文解析</a></li>
</ol>
<h3 id="elementwise-优化"><a href="#elementwise-优化" class="headerlink" title="elementwise 优化"></a>elementwise 优化</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/488601925">elementwise 优化</a></li>
</ol>
<h2 id="NVCC"><a href="#NVCC" class="headerlink" title="NVCC"></a>NVCC</h2><ol>
<li>交叉编译时用本地 nvcc 就行，不存在 x86 和 aarch64 区别</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html">NVIDIA CUDA Compiler Driver NVCC</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#supported-input-file-suffixes">编译过程中各类型文件作用说明</a></li>
<li><a target="_blank" rel="noopener" href="https://www.linmao.dev/joy/1165/">CUDA 编译过程</a></li>
</ol>
<h2 id="PTX-Parallel-Thread-Execution"><a href="#PTX-Parallel-Thread-Execution" class="headerlink" title="PTX(Parallel Thread Execution)"></a>PTX(Parallel Thread Execution)</h2><ol>
<li>a low-level parallel thread execution virtual machine and instruction set architecture (ISA)</li>
<li>PTX 是上承 GPU 编程语言 CUDA C++，下启 GPU 硬件 SASS 指令，可以借助 NVRTC 实现运行时优化，某些层面上来说可以称之为 GPU 设备无关代码，因此 PTX 可以理解为<code>CUDA IR</code></li>
<li>PTX 独立于特定 GPU 架构,可以重用相同的代码适用于不同的 GPU 架构,相当于前端</li>
<li>使用虚拟架构生成 PTX 中间文件，虚拟框架由<code>compute_</code>开头。虚拟架构通常是从大的 GPU 代上控制的，真实框架必须大于等于虚拟框架，真实框架对应真正运行的 GPU，即编译阶段就确定要运行的 GPU 是什么。真实框架由<code>sm_</code>开头。</li>
<li><code>nvcc -ptx program.cu -o _program.ptx -arch=sm_86</code></li>
<li><code>cat program.ptx | cu++filt &gt; program_demangle.ptx</code> demangle ptx</li>
</ol>
<h2 id="SASS-Shader-Assembly"><a href="#SASS-Shader-Assembly" class="headerlink" title="SASS(Shader-Assembly)"></a>SASS(Shader-Assembly)</h2><ol>
<li>真正的机器汇编，由 cubin 文件经过 cuobjdump 工具转换而来。目前没有官方的 sass to cubin 的工具。</li>
<li>cuobjdump 可以用来分析 cubin 文件和 host 文件。而 nvdisasm 只能用来分析 cubin 文件，但是可以得到更多的输出信息。我用的比较多的是 nvdisasm。用来看代码的控制流图。</li>
<li>只有官方反汇编器，没有官方汇编器</li>
<li>generate</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ptxas -arch=sm_86 mma_gemm_demangle.ptx -o mma_gemm.cubin</span><br><span class="line">cuobjdump -sass mma_gemm.cubin &gt; mma_gemm.sass</span><br></pre></td></tr></table></figure>

<h2 id="sync"><a href="#sync" class="headerlink" title="sync"></a>sync</h2><ol>
<li><code>block level</code>: <code>__syncthreads()</code></li>
<li><code>grid level</code>: <code>__threadfence()</code></li>
<li><code>stream level</code>: <code>cudaStreamSynchronize(cudaStream_t stream)</code><ul>
<li>这个函数会阻塞 CPU 线程，直到特定的 CUDA stream 中的所有操作完成。</li>
</ul>
</li>
<li><code>device level</code>: <code>cudaDeviceSynchronize()</code></li>
</ol>
<h3 id="event-sync"><a href="#event-sync" class="headerlink" title="event sync"></a>event sync</h3><ol>
<li>CUDA 事件（CUDA Event）是 CUDA 编程中用于时间测量和流同步的一种机制。</li>
<li>Event 是 stream 相关的一个重要概念，其用来标记 strean 执行过程的某个特定的点。</li>
<li>Cuda api 提供了相关函数来插入 event 到 stream 中和查询该 event 是否完成（或者叫满足条件？）。只有当该 event 标记的 stream 位置的所有操作都被执行完毕，该 event 才算完成。关联到默认 stream 上的 event 则对所有的 stream 有效。</li>
<li>Events 标记了 stream 执行过程中的一个点，我们就可以检查正在执行的 stream 中的操作是否到达该点，</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_24990189/article/details/89602618">使用 event 测量性能</a><ul>
<li>cudaEventSynchronize</li>
</ul>
</li>
<li>CUDA Events 是 NVIDIA 的 CUDA（Compute Unified Device Architecture）编程模型中的一个特性，用于在 GPU 上进行高精度的计时操作。<code>它们允许开发者在异步任务和内核（kernel）执行中精确地测量时间，帮助分析和优化 CUDA 应用程序的性能</code></li>
<li>主要功能<ul>
<li><code>时间测量</code>：CUDA Events 可以用于测量 CUDA kernel 执行时间，或数据传输操作的持续时间。这对于性能分析和调优非常有用。</li>
<li><code>同步操作</code>：CUDA Events 可以作为同步点，确保在某些操作完成后再执行下一步。例如，可以等待一个 event 完成后再启动另一个 kernel。</li>
</ul>
</li>
</ol>
<h3 id="stream-sync"><a href="#stream-sync" class="headerlink" title="stream sync"></a>stream sync</h3><h2 id="bank-conflict"><a href="#bank-conflict" class="headerlink" title="bank conflict"></a>bank conflict</h2><ol>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/3842483/23011500">概念</a></li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Bank    |      1      |      2      |      3      |     ...     |      16     |</span><br><span class="line">Address |  0  1  2  3 |  4  5  6  7 |  8  9 10 11 |     ...     | 60 61 62 63 |</span><br><span class="line">Address | 64 65 66 67 | 68 69 70 71 | 72 73 74 75 |     ...     |     ...     |</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<ol>
<li>Each bank has a bandwidth of 32 bits per clock cycle</li>
<li>shared memory 一次不冲突只能读 32x4B， 看一次读取的数据量，多于 32x4B 需要读取多次 n&#x2F;(32x4)，只要在 n&#x2F;(32x4) 次内读完就是最高效的。</li>
<li>使用 float4 类型访存，用向量化的 LDG.128 和 STG.128 指令一次读 4 个元素，以减少访存指令数， 提高计算访存比</li>
<li>thread tile 时如果 TM 是 8，一个 warp 需要读 32*8 个 TM 的数(shared memory 上)，至少需要 8 次(如果有 32 个 bank)<ul>
<li>如果一个线程一次处理 8 个连续的数，一个 warp 一次只有 4 个线程不 bank 冲突，<ul>
<li>如果一个线程 1 次读 1 个数，一个 warp 一次只读 4 个数， 需要 64 次读完，</li>
<li>如果一个线程一次读 4 个数，一个 warp 一次读 16 个数， 需要读 16 次，</li>
<li>如果一个线程一次读 8 个数，一个 warp 一次读 32 个数，需要读 8 次（最优），但不存在一次读 8 个数的指令。</li>
</ul>
</li>
<li>如果一个线程一次操作 4 个连续的数(处理两个在空间上属于同一 bank 的数)， 那么一个 warp 一次有 8 个线程 bank 不冲突， 一次操作 4*8 个数， 需要 8 次能读完。与 bank 不冲突等效。</li>
</ul>
</li>
<li>这种情况也可以看作是：shared memory 基本单元为 16byte，总 bank 数为 8，冲突与否的分析不在是 32 线程，而变成 4 个 phase 中的不同线程。如果采用 64bit 的访问形式，则相应的基本单元可以看作是 8byte，总 bank 数目为 16，冲突与否的条件变成两个 phase 内的线程是否冲突。</li>
<li>4x32 or 8x16 or 16x8 (16B, 8bank)</li>
</ol>
<h2 id="关键字"><a href="#关键字" class="headerlink" title="关键字"></a>关键字</h2><ol>
<li><code>__restrict__</code> 用于限定指针,表示该指针是唯一访问目标内存的途径。可以避免出现不同指针引用同一内存区域的情况,编译器可以更自由地进行优化。这意味着编译器可以假设这个指针没有别名（alias），即没有其他指针指向相同的内存位置。</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">myKernel</span><span class="params">(<span class="type">float</span>* __restrict__ ptrA, <span class="type">float</span>* __restrict__ ptrB, <span class="type">int</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 内核代码，假设 ptrA 和 ptrB 指向不重叠的内存区域</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="occupancy-设计"><a href="#occupancy-设计" class="headerlink" title="occupancy 设计"></a>occupancy 设计</h2><ol>
<li>(重要) <code>最理想的情况下为 sm max threads 个 thread 都分配资源，占用率 100%， 但受限于 Max warps or max blocks per SM, registers per SM, shared memory per SM, register, 占用率可能不到 100%</code></li>
<li>想得到令人满意的 GPU 性能<code>关键是找到合适的group size与资源的平衡点</code></li>
<li><code>--ptxas-options=-v</code> or <code>-Xptxas -v</code> or <code>--resource-usage</code> 加上编译选项， 显示 register， shared memory 使用<ul>
<li><code>nvcc  -Xptxas=&quot;-v&quot; --ptxas-options=-v -O3 -o my_sgemm my_sgemm.cu -lcublas  2&gt;&amp;1 | c++filt</code> demangle</li>
</ul>
</li>
<li>nishgt compute 里 occupancy 有详细显示</li>
<li><a target="_blank" rel="noopener" href="https://xmartlabs.github.io/cuda-calculator/">cuda-calculator</a> 填入数值，计算 occupancy</li>
<li>注意 shared memory 是动态配置的，可以尝试改变 shared memory 大小来提升性能</li>
</ol>
<h2 id="stream"><a href="#stream" class="headerlink" title="stream"></a>stream</h2><ol>
<li>cudaStream_t 和 cudaEvent_t 都是数字类型， stream 默认为 0，如果使用默认 stream，直接用 0，如果使用其他 stream，需要 <code>cudaStream_t stream; cudaStreamCreate(&amp;stream);doing_something(); cudaStreamDestroy(stream);</code></li>
</ol>
<figure class="highlight c++"><figcaption><span>cuda_runtime.h</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="type">int</span> cudaEvent_t;</span><br><span class="line"><span class="keyword">typedef</span> <span class="type">int</span> cudaStream_t;</span><br></pre></td></tr></table></figure>

<ol>
<li>stream 主要为了隐藏 host device 之间数据搬移的延迟，不是内存和计算</li>
<li>CUDA 流表示一个 GPU 操作队列，该队列中的操作将以添加到流中的先后顺序而依次执行。</li>
<li>stream 作用：在 Stream 的帮助下，CUDA 程序可以有效地将内存读取和数值运算并行，从而提升数据的吞吐量。 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/51402722">link</a><br>由于 GPU 和 CPU 不能直接读取对方的内存，CUDA 程序一般会有一下三个步骤：1）将数据从 CPU 内存转移到 GPU 内存，2）GPU 进行运算并将结果保存在 GPU 内存，3）将结果从 GPU 内存拷贝到 CPU 内存。</li>
<li>cuda7 可以开启每个线程有一个默认 stream, 之前每个设备有一个 stream <a target="_blank" rel="noopener" href="https://developer.nvidia.com/zh-cn/blog/gpu-pro-tip-cuda-7-streams-simplify-concurrency/">gpu-pro-tip-cuda-7-streams-simplify-concurrency&#x2F;</a><ul>
<li><code>nvcc --default-stream per-thread ./pthread_test.cu -o pthreads_per_thread</code>需要加编译选项</li>
</ul>
</li>
</ol>
<h2 id="cuda-grammer"><a href="#cuda-grammer" class="headerlink" title="cuda grammer"></a>cuda grammer</h2><ol>
<li><p>在 CUDA 编程中，高效的并行算法往往需要线程协作(threads cooperate)以及共享数据(share data)来完成集体计算(collective computations)。要共享数据，线程间必然会涉及同步，而共享的粒度因算法而异，因此线程间的同步应尽量足够灵活，比如开发者可以显示地指定线程间同步，这样就可以确保程序的安全性、可维护性和模块化设计。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#cooperative-groups:~:text=do%20not%20match.-,8.%20Cooperative%20Groups,%EF%83%81,-8.1.%20Introduction">Cooperative Groups</a></p>
<ul>
<li>CUDA Cooperative Groups 提供了一种更灵活、更高效的线程协作方式，适用于高性能并行计算。它增强了传统 __syncthreads() 的能力，并支持 warp 级、block 级、grid 级的同步，在某些场景下可以显著提升计算效率。</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#function-execution-space-specifiers">Function Execution Space Specifiers</a></p>
<ul>
<li><code>__global__</code> 在设备上执行，在主机上调用</li>
<li><code>__device__</code> 在设备上执行，在设备上调用</li>
<li><code>__host__</code> 在主机上执行，在主机上调用</li>
<li>不填默认在<code>__host__</code></li>
<li>__global__和__host__执行空间说明符不能一起使用。</li>
<li>__device__和__host__执行空间说明符可以一起使用，在这种情况下，函数将针对主机和设备分别进行编译。</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#shared:~:text=7.2.3.-,__shared__,-%EF%83%81">shared</a></p>
<ul>
<li>要共享数据，线程间必然会涉及同步   </li>
<li>__shared__： <code>__shared__ unsigned int u1 = 1;</code></li>
<li>extern:<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> __shared__ <span class="type">float</span> array[];</span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">func</span><span class="params">()</span>      <span class="comment">// __device__ or __global__ function</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">short</span>* array0 = (<span class="type">short</span>*)array;</span><br><span class="line">    <span class="type">float</span>* array1 = (<span class="type">float</span>*)&amp;array0[<span class="number">128</span>];</span><br><span class="line">    <span class="type">int</span>*   array2 =   (<span class="type">int</span>*)&amp;array1[<span class="number">64</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>算index的时候需要用到built-in variables, 由Index确定要处理的数据</p>
<ul>
<li>blockIdx</li>
<li>blockDim</li>
<li>threadIdx</li>
<li>总体的原则是 global_index &#x3D; block首地址的index + local_index <code>int tid = blockIdx.x * blockDim.x + threadIdx.x;</code></li>
</ul>
</li>
<li><p><code>int tid = blockIdx.x * blockDim.x + threadIdx.x; =&gt; int index = h * W + w;</code></p>
</li>
<li><p>所有的 kernel 函数返回类型都是 void</p>
</li>
<li><p><code>&lt;&lt;&lt; M , T &gt;&gt;&gt;</code> Which indicate that a kernel launches with a grid of M thread blocks. Each thread block has T parallel threads.</p>
</li>
<li><p>dim3 threadsPerBlock(16, 16); dim3 numBlocks(N &#x2F; threadsPerBlock.x, N &#x2F; threadsPerBlock.y); MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C);</p>
</li>
<li><p>NOTE: dim3是按(x, y, z)设置的，矩阵输入数据是按(y1, x1)索引的</p>
<ul>
<li>dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE); name: blockDim,</li>
<li>dim3 dimGrid(B.width &#x2F; dimBlock.x, A.height &#x2F; dimBlock.y); name: gridDim</li>
</ul>
</li>
<li><p>首先确定thread和block处理的数据shape, 根据input shape， 推出gridDim(blocksPerGrid)</p>
<ul>
<li>注意cuda的block处理的数据shape和threadsPerBlock的区别，threadsPerBlock和一个thread处理的数据shape共同确定一个block处理的数据shape，等效triton的BLOCK_SIZE</li>
<li>triton tuning 一般是tune 一个block要处理的数据大小, block处理的数据大小确定也能算出一个thread处理的数据大小</li>
</ul>
</li>
<li><p><code>vectorAdd&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;</code></p>
</li>
<li><p>可以认为 M, T 对应图片的 H, W; 一个 thread 对应一个像素点；一个 block 对应一行，一个 grid 对应一张图片</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cuda-samples/blob/master/Common/helper_cuda.h#L595">checkCudaErrors</a> helper_cuda.h</p>
</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">check</span><span class="params">(T result, <span class="type">char</span> <span class="type">const</span> *<span class="type">const</span> func, <span class="type">const</span> <span class="type">char</span> *<span class="type">const</span> file,</span></span></span><br><span class="line"><span class="params"><span class="function">           <span class="type">int</span> <span class="type">const</span> line)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (result) &#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(stderr, <span class="string">&quot;CUDA error at %s:%d code=%d(%s) \&quot;%s\&quot; \n&quot;</span>, file, line,</span><br><span class="line">            <span class="built_in">static_cast</span>&lt;<span class="type">unsigned</span> <span class="type">int</span>&gt;(result), _cudaGetErrorEnum(result), func);</span><br><span class="line">    <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// This will output the proper CUDA error strings in the event</span></span><br><span class="line"><span class="comment">// that a CUDA host call returns an error</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> checkCudaErrors(val) check((val), #val, __FILE__, __LINE__)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol>
<li>cutil.h NVIDIA 公司在 CUDA5 之后便不再使用 cutil.h <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cuda-samples/blob/master/Samples/2_Concepts_and_Techniques/EGLSync_CUDAEvent_Interop/EGLSync_CUDAEvent_Interop.cu#L83">CUDA_SAFE_CALL </a></li>
<li><a target="_blank" rel="noopener" href="https://bohipat.wordpress.com/2014/07/11/replacing-cutil-in-cuda-5-0/">replacing-cutil-in-cuda-5-0</a></li>
</ol>
<h2 id="tensor-core"><a href="#tensor-core" class="headerlink" title="tensor core"></a>tensor core</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/620185229">4 种 tensor core 调用方法</a><ul>
<li>WMMA (Warp-level Matrix Multiply Accumulate) API</li>
<li>WMMA PTX (Parallel Thread Execution)</li>
<li>MMA (Matrix Multiply Accumulate) PTX</li>
<li>SASS</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/data-center/tensor-cores/">tensor-cores</a></li>
<li><code>--set roofline</code>能看到 tensor core roofline; 例如运行<code>cuda-samples/Samples/3_CUDA_Features/cudaTensorCoreGemm</code></li>
<li>cuda-samples&#x2F;Samples&#x2F;3_CUDA_Features 包含多个 tensor core 实例</li>
<li><code>Samples/3_CUDA_Features/cudaTensorCoreGemm</code>比<code>Samples/0_Introduction/matrixMul</code> 计算性能高很多<ul>
<li>cudaTensorCoreGemm 测试的是 fp16 性能</li>
</ul>
</li>
<li>cublas 满足特定条件才会使用 tensor core <a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cublas/index.html#tensor-core-usage">link</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc">Tensor Core Requirements</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1893665">FP32 vs TF32</a><ul>
<li>tf32 整数部分精度与 fp32 相同，小数部分与 fp16 相同， 只用了 19 位</li>
<li>包含 Amphere 架构性能</li>
<li>A100、H100 tf32 算力比 fp32 算力高 8 倍左右</li>
<li>3090 上 tf32 算力跟 fp32 算力相同</li>
<li>最新 trtexec 默认是 tf32，注意和 python fp32 计算比较时结果误差可能较大</li>
</ul>
</li>
</ol>
<h2 id="wmma-vs-mma"><a href="#wmma-vs-mma" class="headerlink" title="wmma vs mma"></a>wmma vs mma</h2><ol>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-multiply-accumulate-instructions">warp-level-matrix-multiply-accumulate-instructions</a></li>
</ol>
<h3 id="tenosr-core-和-cuda-core"><a href="#tenosr-core-和-cuda-core" class="headerlink" title="tenosr core 和 cuda core"></a>tenosr core 和 cuda core</h3><ol>
<li>计算层级：CUDA Core 是线程级别，Tensor Core 是 warp 级别</li>
<li>计算维度：CUDA Core 是一维逐点计算，Tensor Core 是二维逐 tile 计算</li>
<li>CUDA Core 是为通用计算设计，而 Tensor Core 是为特定类型的计算（主要是深度学习中的矩阵运算）优化。</li>
<li>在 NVIDIA 的某些 GPU 架构中，例如 Volta、Turing 和 Ampere，CUDA Core 和 Tensor Core 共同存在。它们可以根据计算任务的性质协同工作，提高整体的计算效率。</li>
<li>在执行深度学习任务时，Tensor Core 可以显著加速计算过程，相较于仅使用 CUDA Core，能实现更快的训练和推理速度。</li>
<li>RT core 用于光线追踪</li>
<li>芯片手册中有 cuda core 算力和 tensor core 算力</li>
<li>利用 tensor core 才能达到最大算力</li>
</ol>
<h2 id="build"><a href="#build" class="headerlink" title="build"></a>build</h2><ol>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#application-compatibility">application-compatibility</a></li>
<li><code>-arch=sm_70</code> is a shorthand for <code>-arch=compute_70 -code=compute_70,sm_70</code> (which is the same as <code>-gencode arch=compute_70,code=\&quot;compute_70,sm_70\&quot;</code>)</li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-gpus">Your GPU Compute Capability</a> 包含各种 gpu arch</li>
</ol>
<h2 id="cuda-samples"><a href="#cuda-samples" class="headerlink" title="cuda samples"></a><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cuda-samples">cuda samples</a></h2><ol>
<li>可以全部 sample 一起编译</li>
<li><code>cuobjdump -all  ./matrixMulDrv</code>可以看可执行程序 arch 等信息</li>
<li><code>make SMS=&quot;86&quot;</code>选择 arch</li>
<li><code>make dbg=1</code> debug 编译</li>
<li><code>Samples/1_Utilities/bandwidthTest</code> 可以查看 host &lt;-&gt; memory 之间传输速度</li>
<li>(good)<code>Samples/1_Utilities/deviceQuery</code> 可以查看设备信息, 包含 arch 信息, 多少 sm，每个 sm 多少 cuda core</li>
<li>11.6 之后代码放在 github 上</li>
<li><a target="_blank" rel="noopener" href="https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial01/">hello world</a></li>
<li><code>/usr/local/cuda-11.4/samples</code> tree -L 2</li>
<li><code>/usr/local/cuda-10.2/samples/0_Simple/vectorAdd</code></li>
<li><code>nvprof ./vectorAdd</code> 查看 kenerl 耗时</li>
</ol>
<h2 id="CUDALibrarySamples"><a href="#CUDALibrarySamples" class="headerlink" title="CUDALibrarySamples"></a><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/CUDALibrarySamples">CUDALibrarySamples</a></h2><ol>
<li>cublas</li>
<li>cutlass</li>
<li>npp</li>
</ol>
<h3 id="cuda-info"><a href="#cuda-info" class="headerlink" title="cuda info"></a>cuda info</h3><ol>
<li><a target="_blank" rel="noopener" href="https://linuxconfig.org/how-to-get-cuda-cores-count-on-linux">查看 cuda core</a><br><code>cd /usr/local/cuda-11.4/samples/1_Utilities/deviceQuery &amp;&amp; make &amp;&amp; ./deviceQuery</code></li>
<li>bandwith test<br><code>cd /usr/local/cuda-11.4/samples/1_Utilities/bandwidthTest &amp;&amp; make &amp;&amp; ./bandwidthTest</code></li>
</ol>
<h2 id="NVIDIA-Developer-Tools"><a href="#NVIDIA-Developer-Tools" class="headerlink" title="NVIDIA Developer Tools"></a><a target="_blank" rel="noopener" href="https://developer.nvidia.com/tools-overview">NVIDIA Developer Tools</a></h2><ol>
<li>各工具关系<br><img src="https://i.ibb.co/2qN87rv/QVJcq2r-QQ3.png" alt="tools"></li>
</ol>
<h3 id="nsight-system"><a href="#nsight-system" class="headerlink" title="nsight system"></a>nsight system</h3><ol>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/nsight-systems/get-started">cuda toolkit 不自带 需要下载安装</a><ul>
<li>wget 下载 linux CLI Only deb, dpkg 安装</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-systems/InstallationGuide/index.html#package-manager-installation">package-manager-installation</a><ul>
<li>apt install</li>
</ul>
</li>
</ul>
</li>
<li>viztracer + nsys + ncu + nvtx 分析性能瓶颈</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html#python-profiling">nsight system python-profiling</a></li>
<li>分析系统性能， help you to pinpoint performance issues and identify optimization opportunities; 帮助分析性能问题和识别优化机会<ul>
<li>gpu kernel 运行时间太长</li>
<li>某些 process cpu 处理时间太长</li>
<li>cpu gpu pcie copy 耗时过大</li>
</ul>
</li>
<li>GPU timeline 方便分析</li>
<li>profiling 可以简单地分为粗粒度和细粒度。粗粒度主要是判断瓶颈是不是在 GPU 上，具体又是哪个 kernel，典型代表就是 nsight system 工具，会显示出整个程序的 timeline。可以从 timeline 上直接清晰明了地看到瓶颈是在 CPU 还是 GPU，如果是 GPU，那又是在 GPU 的哪个 kernel 上。</li>
<li>如果是 timeline 中 GPU kernel 的占比很小，CPU 占比很大，那说明瓶颈在 CPU 侧，需要注意是不是数据读取花了太多时间。如果 GPU kernel 的占比很大，说明瓶颈在 GPU 侧，需要重点花精力去优化 GPU kernel 实现。还有一种情况是，如果数据一直放在 GPU 上，但是 kernel 的时间占比不是特别多，那可能是因为 kernel 本身不太耗时，可能只运行了 4us。但 kernel lauch 就花了 6us。这个时间就要想着采用 kernel fusion 的方式，尽可能地在一个 kernel 里面多干点活。</li>
<li>点击时注意竖线上的小三角号, 表示关联</li>
<li>从结果分析看多个 stream 下 kernel 可以同时运行<ul>
<li>all stream 或者 kernels 显示不出所有的 kernel 运行，起始位置被其他 kernel 覆盖的检测不出来</li>
<li>all stream 显示不出的 kernel 可能在被隐藏的 stream 里，鼠标放到 kernel 上能显示出在第几个 stream</li>
</ul>
</li>
<li><code>enqueue</code>异步接口没有很快返回的原因：<ul>
<li>nsight system 上看 tensorrt node 调用， 对应 cuda api 里有 cudaStreamSync()函数， 会阻塞 cpu 导致 enqueue 不返回</li>
<li>有多个 stream sync, 每个 stream sync 执行之后之前通过 cuda api 调用的 kernel 都已执行完</li>
<li>为什么要多个 stream？node 不相关可以并行加速, 可以看到 kernel 执行时间有并行， 为什么要 sync? 后面的节点需要前面的节点都执行完，有关联</li>
<li>点击 tensorrt 下的 node 可以看到 node 执行信息</li>
</ul>
</li>
<li>打开文件注意生成 log 时的错误</li>
<li>gui 可以远程 profiling， 将 nsys 安装到 target 机器, 类似 compute<ul>
<li><code>~/.local/share/nsight_systems/nsys</code>安装路径</li>
</ul>
</li>
<li><code>nsys profile</code> 类似 perf record 来记录信息<ul>
<li><code>nsys profile --trace=cuda --gpu-metrics-device all</code></li>
</ul>
</li>
<li><code>--trace</code> 可以看看参数：cuda,nvtx, cudnn, python-gil …</li>
<li><code>nsys status --all</code>打印 nsys 支持的状态，比如是否支持采集 cpu 信息</li>
<li>nsys stats 类似 perf stats 来查看统计信息<ul>
<li><code>nsys stats report1.nsys-rep</code></li>
<li><code>nsys stats --report cuda_gpu_trace report1.nsys-rep</code></li>
</ul>
</li>
<li>gui summary 里可以看 log 具体执行命令</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html#python-profiling">python-profiling</a><ul>
<li>Python Sampling requires Python version 3.9 or later. Python Sampling is therefore disabled.</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html#cuda-trace">跟踪 cuda</a><ul>
<li><code>--trace=cuda</code></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html#gpu-metrics">可以收集 gpu-metrics</a><ul>
<li>Is my GPU idle?</li>
<li>Is my GPU full? Enough kernel grids size and streams? Are my SMs and warp slots full?</li>
<li>Am I using TensorCores?</li>
<li>Is my instruction rate high?</li>
<li>Am I possibly blocked on IO, or number of warps, etc</li>
</ul>
</li>
<li>分析系统性能</li>
<li><code>nvprof</code> 旧版本</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html">user guide</a></li>
<li>NsightSystems-2023.2.1.122-3259852.msi 安装包</li>
<li>Nsight_Systems_User_Guide_2023.2.1.122-3259852.pdf 文档</li>
<li>鼠标放到 kernel 上有 kernel 详细信息，<ul>
<li>24.5.1 版本之后点击 kernel，可以在左侧(蓝边)找到关联的事项，如 cuda 调用对应的 kernel</li>
<li>包括执行时间</li>
<li>latency：launch latency, 与执行时间不一样，latency 是 api 调用到 kernel 开始执行时间<ul>
<li><a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/nsys-timeline-end-start-is-not-the-same-as-latency/238043">NSys Timeline: End - Start is not the same as latency</a></li>
<li>CUDA kernel launch latency could be defined as the time range from the beginning of the launch API call to the beginning of the kernel execution.</li>
<li>执行的是 <code>cudaStreamSynchronize</code>, 没执行这个函数之前，stream 中的 kernel 不会 launch</li>
</ul>
</li>
<li>gird, block 设置;(注意，同一个 stream 下相同函数调用可能设置不一样导致时间不同)</li>
<li>一个线程用多少 register</li>
<li>理论 occupancy, 打开 gpu metrics 可以看对应的实际 occupancy</li>
<li>注意各种颜色</li>
</ul>
</li>
<li>可以在 thread 上看到 <code>cpu call stack</code><ul>
<li>每个线程对应行分层的最后一层 <code>sampling point</code></li>
<li>gpu 空闲时看 CPU 采样点 可以知道 CPU 在干啥</li>
<li>是否可以生成火焰图？</li>
</ul>
</li>
<li>timeline 上点击 stream 下的 tensorrt node， 可以显示对应的 kernel 执行和 cuda api 调用时间点, 注意竖线上的小三角号</li>
<li>tensorrt 多个维度来看<ul>
<li>thread</li>
<li>stream</li>
<li>cuda</li>
</ul>
</li>
<li>左上角 timeline view 可以选择 analysis summary，有各种总结</li>
<li>左侧右键选<code>show in events view</code>， 可以看具体时间, 可以在 all 上操作，看所有 event 运行时间</li>
<li>右键 reset room 显示全部</li>
<li><code>shift + mouseleftdoubleclick</code> timeline 可以找到对应 event 在 event view 位置, 刚打开时可以按 name 排序，会看到相关算子集中到一起，再按其他指标排序会混乱</li>
<li><code>ctrl + mouseleftdoubleclick</code> timeline 可以 fit to screen</li>
<li><code>backspace</code> timeline 可以 undo room</li>
<li><code>ctrl + mouseleftdoubleclick</code> 可以找到 event view 对应的 timeline 位置</li>
<li><code>sudo nsys profile &lt;app&gt;</code></li>
<li><code>nsys stats report1.nsys-rep</code> 输出各种 report</li>
<li>可以看 cpu 执行情况， tensort 可以看详细算子耗时，也有对应 cuda 执行情况</li>
<li>analysis summary 中有各个线程的 cpu 利用率总结</li>
<li>可以关注 cpu 空闲的地方，为什么会空闲(同步数据？)</li>
<li>可以缩小看颜色占比，关注占比大的模块</li>
<li>cudaMemcpy 会阻塞 cpu 执行， 可以多注意 cudaMemcpy 影响</li>
<li>cudaStreamSynchronize 是 CUDA API 中的一个函数，用于等待指定的 CUDA 流上的所有 CUDA 核函数执行完毕。当 CUDA 核函数被执行时，它们会被添加到一个 CUDA 流中，这些核函数的执行可能是异步的，也可能是同步的，具体取决于如何在代码中调用它们。当我们调用 cudaStreamSynchronize 时，它将会阻塞当前 CPU 线程，直到指定的流上的所有核函数都执行完毕。</li>
</ol>
<h3 id="nsight-compute"><a href="#nsight-compute" class="headerlink" title="nsight compute"></a>nsight compute</h3><ol>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html#enable-docker-collection">enable-docker-collection</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/nsight-compute">cudatoolkit 自带</a></li>
<li><code>balance throughput</code> compute and memory throughput are near; 平衡比较重要，不平衡时说明使用率较高的是瓶颈</li>
<li>Both SM (Compute) and Memory SOL report the categories’ throughput as the achieved percentage of utilization with respect to the theoretical maximum, i.e. the “Speed Of Light”. Both metrics are composed of sub-metrics, with the respective highest contributor defining the resulting value. The Breakdown tables below the chart can be used to identify all such contributors and their values. SM（计算）和内存 SOL 都将类别的吞吐量报告为相对于理论最大值（即“speed of light”）所实现的利用率百分比</li>
<li>SOL(speed of light): 相对于理论最大值的比例</li>
<li>有开销 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/2023.3/ProfilingGuide/index.html#overhead">overhead</a></li>
<li><code>metrics</code> 是指性能指标，这些指标用于衡量 CUDA 应用程序的性能和行为。性能指标可以包括各种硬件级别的统计数据，如内存访问效率、计算操作的执行时间、流处理器（SM）的利用率、寄存器使用情况、分支效率等等。</li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-performance-counters">permission 问题</a><ul>
<li>可以增加临时权限</li>
</ul>
</li>
<li>分析 kernel 性能, 可以选 kernel</li>
<li>NVIDIA Nsight Compute is an interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool. In addition, its baseline feature allows users to compare results within the tool. NVIDIA Nsight Compute provides a customizable and data-driven user interface and metric collection and can be extended with analysis scripts for post-processing results.</li>
<li>需要 windows 客户端通过 connect 将 ncu 送到开发环境<ul>
<li>可以通过 activity 复制命令到板端执行</li>
<li>metrics 可以选择 Sets 和 rules</li>
</ul>
</li>
<li>profile -&gt; metrics details<ul>
<li>点击统计界面某个 metrics 可以看 metrics 详细信息</li>
</ul>
</li>
<li><code>--import-source yes --source-folders ./</code> 导入 source</li>
<li><code>ncu --replay-mode application --set full  ./preprocess</code> 详细信息显示到命令行<ul>
<li>Duration 为 kernel 执行时间</li>
</ul>
</li>
<li><code>ncu -k matrixMul --print-summary per-gpu ./test</code> 查看某个 kernel 信息</li>
<li><code>/tmp/var/target/linux-desktop-glibc_2_11_3-x64/ncu --config-file off --export &quot;/tmp/var/test&quot; --force-overwrite --section-folder /tmp/var/sections --set full ./test</code></li>
<li><code>ncu --set full -f --export nsight_compute ./test</code></li>
<li><code>docker run -itd -v /mnt:/mnt -p 30022:22 --user root --gpus all --name=Ubuntu20.04-CUDA-admin --shm-size 2g --cap-add=SYS_ADMIN nvidia/cuda:11.4.3-cudnn8-devel-ubuntu20.04</code> 尽量使用官方 docker<ul>
<li>需要 <code>--gpus all --cap-add=SYS_ADMIN</code></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/index.html">nsight-compute docs</a><ul>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html">ProfilingGuide</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/NsightCompute/index.html">user manual</a></li>
</ul>
</li>
<li>左上角可以显示：<ul>
<li>session: gpu cpu 等信息</li>
<li>detailed：overview, compute, memory 等</li>
<li>source: 汇编代码及耗时</li>
<li>summary: 耗时，compute and memory throughput; 底下有优化建议</li>
</ul>
</li>
<li>可以添加 baseline，对比多次结果, 分析结果变化</li>
<li><code>ncu --set roofline</code>可以测量详细 roofline</li>
<li><code>ncu --metrics smsp__inst_executed.sum ./matrixMul</code> 打印 metrics</li>
<li><code>--set roofline</code>能看到 tensor core roofline; 例如运行<code>cuda-samples/Samples/3_CUDA_Features/cudaTensorCoreGemm</code></li>
</ol>
<h4 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h4><ol>
<li>The application returned an error code (9).<ul>
<li><code>ncu --replay-mode &lt;application/kernel&gt;</code> 加参数； 见<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/nsight-profiling-crashes-with-error-code-9/230094/5">link</a></li>
<li>可以使用<code>–replay-mode application</code>切换到应用程序重播。 这避免了内存存储需要重放。</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/2023.3/ProfilingGuide/index.html#replay">replay</a><ul>
<li>kernels might need to be replayed one or more times, since not all metrics can be collected in a single pass</li>
<li>the number of metrics originating from hardware (HW) performance counters that the GPU can collect at the same time is limited.</li>
<li>kernal replay 只重跑 kernel, application replay 重跑应用</li>
</ul>
</li>
</ol>
<h4 id="metrics"><a href="#metrics" class="headerlink" title="metrics"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/2023.3/ProfilingGuide/index.html#metrics-guide">metrics</a></h4><ol>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/2023.3/ProfilingGuide/index.html#metrics-reference">metrics-reference</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/2019.5/NsightComputeCli/index.html#nvprof-metric-comparison">nvprof-metric-comparison</a></li>
<li><code>ncu --devices 0 --query-metrics &gt;my_metrics.txt</code> 查看 metrics</li>
</ol>
<h4 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h4><ol>
<li>每项下面会有建议</li>
<li>compute 分析延迟很关键 目标是计算要掩盖延迟; 分析 stall 原因，stall 就是 warp 闲着</li>
<li>避免 bank 冲突能解决指令的延迟</li>
<li>overview(GPU speed of light throughput)<ul>
<li><code>用于查看计算和内存吞吐对理论值的占比以及roofline</code></li>
<li>High-level overview of the <code>throughput for compute and memory</code> resources of the GPU 主要关注计算和内存</li>
<li>Achieved compute throughput and&#x2F;or memory bandwidth below 60.0% of peak typically indicate latency issues.</li>
<li>低于 60% 表明有延迟问题, 例如：指令有延迟，可以看 scheduler statistics 和 warp state statistics 进一步分析;延迟问题可能是由于 occupancy 较低， 也可能是指令执行延迟太大，无法隐藏;</li>
<li>roofline</li>
</ul>
</li>
<li>comupte workload<ul>
<li><code>用于分析 SM 计算资源使用情况</code></li>
<li>IPC</li>
<li>指令执行占比</li>
<li>LSU: load store unit</li>
</ul>
</li>
<li>memory workload<ul>
<li><code>用于分析 GPU memory 使用情况</code></li>
<li>可以选择看 transfer size 和 throughput</li>
<li>要提升 cache 命中率</li>
<li>多用 shared memory(可作为中间结果)</li>
<li>多种 memory<ul>
<li>global</li>
<li>local: 线程私有的。local memory 不是物理空间，而是 global memory 的一部分，所以延时较大。</li>
<li>texture: 只读?</li>
<li>surface</li>
<li>load global stroe shared</li>
<li>shared</li>
</ul>
</li>
</ul>
</li>
<li>scheduler statistics<ul>
<li><code>用于分析每个 scheduler active, eligible, issue warp 情况</code></li>
<li>Issued Warp Per Scheduler: 平均每个 cycle 发射的 warp 数</li>
<li>On cycles with no eligible warps, the issue slot is skipped and no instruction is issued. Having many skipped issue slots indicates poor latency hiding.</li>
<li>Out of the maximum of 12 warps per scheduler(Ampere 1 个 SM 有 4 个 scheduler), this kernel allocates an average of 2.00 active warps per scheduler, but only an average of 0.07 warps were eligible(合格的) per cycle.</li>
<li>可以看出平局每个 sm 有几个 warps; 参考可以看 Theoretical Occupancy 是多少，有可能受 regitster 和 shared memory 等限制</li>
<li>Eligible warps are the subset of active warps that are ready to issue their next instruction.</li>
</ul>
</li>
<li>warp state statistics<ul>
<li><code>用于分析 warp stall 原因</code></li>
<li>Check the Warp Stall Sampling (source counters 中) table for the top stall locations in your source based on sampling data. 查看 stalll source 位置</li>
<li>statll 解决办法： Try to increase the number of active warps to hide the existent latency or try changing the instruction mix to utilize all available pipelines in a more balanced way.</li>
<li>The warp cycles per instruction define the latency between two consecutive instructions. 每条指令的 warp 周期定义两个连续指令之间的延迟</li>
<li>可以查看两个 warp instruction 之间 cyles 组成： stall math pipe throttle, stall mio throttle;</li>
</ul>
</li>
<li>instructions statistics<ul>
<li><code>用于查看指令的类型和执行次数</code></li>
<li>Statistics of the executed low-level assembly instructions (SASS). SASS 指令统计</li>
<li>统计执行的指令数，可以看出哪些指令执行的较多</li>
</ul>
</li>
<li>launch statistics<ul>
<li><code>用于查看 grid, block 设置和 register, shared memory 使用情况</code></li>
</ul>
</li>
<li>occupancy<ul>
<li><code>用于查看occupancy情况及限制原因</code></li>
<li>Occupancy is the ratio of the number of active warps per multiprocessor to the maximum number of possible active warps.</li>
<li>可以看出理论占用率及不能到 100%的原因</li>
<li>一个 warp 中用太多 register 和 shared memory 会影响 Occupancy， Occupancy 会影响 scheduler， 进而会影响延迟隐藏</li>
<li>Occupancy 是 CUDA 编程中一个重要的性能指标,它表示 GPU 中 Streaming Multiprocessor (SM)上的处理单元被运用的比例。</li>
<li>右上角点开 table，可以可视化</li>
</ul>
</li>
<li>source counters<ul>
<li><code>用于分析分支指令是否有影响，是否合并访存</code></li>
<li>Source metrics, including branch efficiency and sampled warp stall reasons.</li>
</ul>
</li>
</ol>
<h3 id="NVTX"><a href="#NVTX" class="headerlink" title="NVTX"></a>NVTX</h3><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/karpathy/llm.c/blob/2ddf128365773670ceb4e76ab7b04ab671edda64/llmc/cuda_common.h#L108-L117">(good)llm.c nvtx 使用</a></li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NvtxRange</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">NvtxRange</span>(<span class="type">const</span> <span class="type">char</span>* s) &#123; <span class="built_in">nvtxRangePush</span>(s); &#125;</span><br><span class="line">    <span class="built_in">NvtxRange</span>(<span class="type">const</span> std::string&amp; base_str, <span class="type">int</span> number) &#123;</span><br><span class="line">        std::string range_string = base_str + <span class="string">&quot; &quot;</span> + std::<span class="built_in">to_string</span>(number);</span><br><span class="line">        <span class="built_in">nvtxRangePush</span>(range_string.<span class="built_in">c_str</span>());</span><br><span class="line">    &#125;</span><br><span class="line">    ~<span class="built_in">NvtxRange</span>() &#123; <span class="built_in">nvtxRangePop</span>(); &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NVTX_RANGE_FN() NvtxRange nvtx_range(__FUNCTION__)</span></span><br></pre></td></tr></table></figure>

<ol>
<li>和 nsight system, viztracer 一起非常好用</li>
<li><a target="_blank" rel="noopener" href="https://nvtx.readthedocs.io/en/latest/">doc</a><ul>
<li>Annotate code ranges and events in Python 在 Python 中注释代码范围和事件</li>
</ul>
</li>
<li>sample</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@nvtx.annotate()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_func</span>():</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> nvtx.annotate(<span class="string">&quot;for_loop&quot;</span>, color=<span class="string">&quot;green&quot;</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        sleep_for(i)</span><br><span class="line">        my_func()</span><br><span class="line"></span><br><span class="line">rng = nvtx.start_range(message=<span class="string">&quot;my_message&quot;</span>, color=<span class="string">&quot;blue&quot;</span>, domain=<span class="string">&quot;rng&quot;</span>)</span><br><span class="line"><span class="comment"># ... do something ... #</span></span><br><span class="line">nvtx.end_range(rng)</span><br></pre></td></tr></table></figure>

<ol>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/nvidia-tools-extension-api-nvtx-annotation-tool-for-profiling-code-in-python-and-c-c/">NVIDIA Tools Extension API: An Annotation Tool for Profiling Code in Python and C&#x2F;C++</a></li>
</ol>
<h3 id="cupti"><a href="#cupti" class="headerlink" title="cupti"></a><a target="_blank" rel="noopener" href="https://developer.nvidia.com/cupti">cupti</a></h3><h3 id="nsgiht-graphics"><a href="#nsgiht-graphics" class="headerlink" title="nsgiht graphics"></a>nsgiht graphics</h3><h2 id="cuda-debug"><a href="#cuda-debug" class="headerlink" title="cuda-debug"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-visual-studio-code-edition/cuda-debugger/index.html">cuda-debug</a></h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/tree/main/examples/02_dump_reg_shmem">02_dump_reg_shmem</a><ul>
<li>dump_shmem</li>
<li>dump_fragment</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#compiling-the-application">compiling 要求</a><ul>
<li><code>nvcc -g -G foo.cu -o foo</code></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://marketplace.visualstudio.com/items?itemName=NVIDIA.nsight-vscode-edition">vs code plugin: nsight-vscode-edition</a></li>
<li>shared memory(L1 cache?) dump<ul>
<li>申请 shared memory 变量，存中间结果， shared memory 对 block 全局可见。</li>
<li>可以申请多于 L1 cache 大小的存储，系统会自动调度</li>
</ul>
</li>
<li>传递主存指针到 kernel，用于 dump</li>
<li>print<ul>
<li><code>if(threadIdx.x == 0) printf...</code> 条件打印</li>
<li>cudaDeviceReset(); 不打印可以加</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/nsight-visual-studio-code-edition">nsight-visual-studio-code-edition</a></li>
<li>cuda-gdb<ul>
<li><a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/cant-use-cuda-gdb/235380/4">Can’t use Cuda-gdb</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/46676907/23011500">Error disabling address space randomization</a></li>
</ul>
</li>
<li>cuda-memcheck</li>
</ol>
<h2 id="link"><a href="#link" class="headerlink" title="link"></a>link</h2><ol>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-zone">cuda 相关开发</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/gpu-accelerated-libraries">gpu-accelerated-libraries</a></li>
</ol>
<h2 id="build-1"><a href="#build-1" class="headerlink" title="build"></a>build</h2><h3 id="cmake"><a href="#cmake" class="headerlink" title="cmake"></a>cmake</h3><ol>
<li><a target="_blank" rel="noopener" href="https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9444-build-systems-exploring-modern-cmake-cuda-v2.pdf">build-systems-exploring-modern-cmake-cuda-v2.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/building-cuda-applications-cmake/">building-cuda-applications-cmake</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/CUDALibrarySamples/blob/master/cuBLAS/Level-3/gemm/CMakeLists.txt">CUDALibrarySamples</a></li>
<li>basic sample</li>
</ol>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.8</span>)</span><br><span class="line"><span class="comment"># project(my_cuda_project LANGUAGES CXX CUDA)</span></span><br><span class="line"><span class="keyword">project</span>(my_cuda_project)</span><br><span class="line"><span class="keyword">enable_language</span>(CUDA) <span class="comment"># 需要enable language</span></span><br><span class="line"><span class="keyword">add_executable</span>(preprocess kernel.cu preprocess.cpp )</span><br><span class="line"><span class="keyword">find_package</span>(CUDA REQUIRED)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(preprocess <span class="variable">$&#123;CUDA_cudart_LIBRARY&#125;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="compile"><a href="#compile" class="headerlink" title="compile"></a>compile</h3><ol>
<li>混合编译, 注意-lcudart 顺序</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nvcc -O3 -c kernel.cu -o kernel.o</span><br><span class="line">g++ -L /usr/local/cuda/targets/x86_64-linux/lib/ preprocess.cpp kernel.o -o preprocess -lcuda -lcudart &amp;&amp; ./preprocess</span><br></pre></td></tr></table></figure>

<h2 id="unified-memory"><a href="#unified-memory" class="headerlink" title="unified memory"></a>unified memory</h2><ol>
<li>不明白原理时不推荐使用</li>
<li>优缺点<ul>
<li>优点<ul>
<li>简化代码编写和内存管理：cudaMallocManaged 可以简化 CPU 和 GPU 之间数据传递的代码，无需手动管理内存迁移。</li>
</ul>
</li>
<li>缺点<ul>
<li>可能降低性能：在某些情况下，统一内存可能会降低性能，例如在数据访问模式为稀疏的情况下。</li>
<li>可能增加内存占用：统一内存可能会增加内存占用，因为它需要在 CPU 和 GPU 内存中都保留一份数据副本。</li>
</ul>
</li>
</ul>
</li>
<li><code>cudaMalloc -&gt; cudaMallocManaged(&amp;x, N*sizeof(float));</code></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/21990899/23011500">使用 cudaMallocManaged 情况</a><ul>
<li>You are working on a Jetson device.</li>
</ul>
</li>
<li>runtime 负责 copy</li>
<li>unified memory cpu 访问时需要同一个线程。不同线程会 bus error。</li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/unified-memory-cuda-beginners/">Unified Memory for CUDA Beginners</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/">Maximizing Unified Memory Performance in CUDA</a></li>
</ol>
<h2 id="design"><a href="#design" class="headerlink" title="design"></a>design</h2><ol>
<li>lidar prprocess; 三重 for, 最外层作为 x, 一个线程执行一个最里面 for 的内容，</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">TransposeDim</span><span class="params">(<span class="type">int</span> kmax_num_point_pillar,</span></span></span><br><span class="line"><span class="params"><span class="function">                   <span class="type">int</span> kmax_num_point,</span></span></span><br><span class="line"><span class="params"><span class="function">                   <span class="type">int</span> kdim,</span></span></span><br><span class="line"><span class="params"><span class="function">                   <span class="type">int</span> voxel_num, <span class="type">float</span> *voxel_data, <span class="type">int8_t</span> *features_s8)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> kWC = kmax_num_point_pillar * kdim;</span><br><span class="line">  <span class="type">int</span> kHW = kmax_num_point * kmax_num_point_pillar;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> c = <span class="number">0</span>; c &lt; kdim; ++c)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> w = <span class="number">0</span>; w &lt; kmax_num_point_pillar; ++w)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> h = <span class="number">0</span>; h &lt; voxel_num; ++h)</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="type">int</span> old_index = h * kWC + w * kdim + c;</span><br><span class="line">        <span class="type">int</span> new_index = c * kHW + w * kmax_num_point + h;</span><br><span class="line">        <span class="type">float</span> features_tmp = <span class="built_in">round</span>(<span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(voxel_data[old_index]));</span><br><span class="line">        features_tmp = std::<span class="built_in">min</span>(std::<span class="built_in">max</span>(features_tmp, <span class="number">-128.f</span>), <span class="number">127.f</span>);</span><br><span class="line">        features_s8[new_index] = <span class="built_in">static_cast</span>&lt;<span class="type">int8_t</span>&gt;(features_tmp);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">TransposeDim5Kernel</span><span class="params">(<span class="type">int</span> kmax_num_point_pillar,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    <span class="type">int</span> kmax_num_point,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    <span class="type">int</span> kdim,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    <span class="type">int</span> voxel_num,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    <span class="type">float</span> *voxel_data,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    <span class="type">int8_t</span> *features_s8)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> c = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="type">int</span> w = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">  <span class="type">int</span> h = blockIdx.z * blockDim.z + threadIdx.z;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (c &lt; kdim &amp;&amp; w &lt; kmax_num_point_pillar &amp;&amp; h &lt; voxel_num)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="type">int</span> kWC = kmax_num_point_pillar * kdim;</span><br><span class="line">    <span class="type">int</span> kHW = kmax_num_point * kmax_num_point_pillar;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> old_index = h * kWC + w * kdim + c;</span><br><span class="line">    <span class="type">int</span> new_index = c * kHW + w * kmax_num_point + h;</span><br><span class="line">    <span class="type">float</span> features_tmp = <span class="built_in">round</span>(voxel_data[old_index]);</span><br><span class="line"></span><br><span class="line">    features_tmp = <span class="built_in">fmaxf</span>(<span class="built_in">fminf</span>(features_tmp, <span class="number">127.f</span>), <span class="number">-128.f</span>);</span><br><span class="line"></span><br><span class="line">    features_s8[new_index] = <span class="built_in">static_cast</span>&lt;<span class="type">int8_t</span>&gt;(features_tmp);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">4</span>, <span class="number">4</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">gridSize</span><span class="params">((kdim + blockSize.x - <span class="number">1</span>) / blockSize.x,</span></span></span><br><span class="line"><span class="params"><span class="function">                (kmax_num_point_pillar + blockSize.y - <span class="number">1</span>) / blockSize.y,</span></span></span><br><span class="line"><span class="params"><span class="function">                (voxel_num + blockSize.z - <span class="number">1</span>) / blockSize.z)</span></span>;</span><br><span class="line"></span><br><span class="line">TransposeDim5Kernel&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(kmax_num_point_pillar,</span><br><span class="line">                                               kmax_num_point,</span><br><span class="line">                                               kdim,</span><br><span class="line">                                               voxel_num,</span><br><span class="line">                                               d_voxel_data,</span><br><span class="line">                                               d_features_s8);</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/14/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><span class="page-number current">15</span><a class="page-number" href="/page/16/">16</a><span class="space">&hellip;</span><a class="page-number" href="/page/20/">20</a><a class="extend next" rel="next" href="/page/16/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="贾夕阳"
      src="/images/coder2.jpg">
  <p class="site-author-name" itemprop="name">贾夕阳</p>
  <div class="site-description" itemprop="description">深度学习/自动驾驶/C++/性能优化</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">192</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jiaxiyang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jiaxiyang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2025/01/18/cursor/" title="2025&#x2F;01&#x2F;18&#x2F;cursor&#x2F;">cursor</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/12/29/LLVM/" title="2024&#x2F;12&#x2F;29&#x2F;LLVM&#x2F;">LLVM</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/11/13/deformable-attention/" title="2024&#x2F;11&#x2F;13&#x2F;deformable-attention&#x2F;">deformable_attention</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/10/15/QNX/" title="2024&#x2F;10&#x2F;15&#x2F;QNX&#x2F;">QNX</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/09/24/qualcomm/" title="2024&#x2F;09&#x2F;24&#x2F;qualcomm&#x2F;">qualcomm</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">贾夕阳</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">587k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">8:54</span>
</div>

<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("06/26/2020 14:52:10");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : '[object Object]',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://jiaxiyang.github.io/page/15/',]
      });
      });
  </script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'g32ipLmEye1u5l6wBGRJt03S-gzGzoHsz',
      appKey     : 'zHgLkAICsZUl9Mf8LfdoVigP',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

  

  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>





 
</body>
</html>

