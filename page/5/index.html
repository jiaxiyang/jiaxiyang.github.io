<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0-rc2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jiaxiyang.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="深度学习&#x2F;自动驾驶&#x2F;C++&#x2F;性能优化">
<meta property="og:type" content="website">
<meta property="og:title" content="Xiyang">
<meta property="og:url" content="https://jiaxiyang.github.io/page/5/index.html">
<meta property="og:site_name" content="Xiyang">
<meta property="og:description" content="深度学习&#x2F;自动驾驶&#x2F;C++&#x2F;性能优化">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="贾夕阳">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://jiaxiyang.github.io/page/5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Xiyang</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WGS6S6YFJ6"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-WGS6S6YFJ6');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Xiyang</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Think twice, code once!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">196</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">44</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">55</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/jiaxiyang" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/02/22/pagedattention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/02/22/pagedattention/" class="post-title-link" itemprop="url">pagedattention</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-02-22 13:54:28 / 修改时间：14:17:17" itemprop="dateCreated datePublished" datetime="2024-02-22T13:54:28+08:00">2024-02-22</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/02/22/pagedattention/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/02/22/pagedattention/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>392</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="base"><a href="#base" class="headerlink" title="base"></a>base</h2><ol>
<li>attention 输入为 q k v 输出 q 维度类似的结果; 参数可能为各种 shape 信息 地址 head num, scale, mask 等。如果是 paged attention 需要处理 q k v 地址不连续的情况, 也需要考虑计算并行度问题</li>
<li>cuda core 计算的</li>
<li><a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/blob/main/csrc/attention/attention_kernels.cu">attention 代码</a></li>
</ol>
<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/638468472">从 FlashAttention 到 PagedAttention, 如何进一步优化 Attention 性能</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/673284781">vLLM 皇冠上的明珠：深入浅出理解 PagedAttention CUDA 实现</a><ul>
<li>vLLM 中，LLM 推理的 prefill 阶段 attention 计算使用第三方库 xformers 的优化实现，decoding 阶段 attention 计算则使用项目编译 CUDA 代码实现。具体代码在 vllm 的 csrc&#x2F;attention&#x2F;attention_kernels.cu 文件里，开发者洋洋洒洒写了八百多行 CUDA 代码。</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/02/03/flashattention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/02/03/flashattention/" class="post-title-link" itemprop="url">flashattention</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-02-03 14:55:42" itemprop="dateCreated datePublished" datetime="2024-02-03T14:55:42+08:00">2024-02-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-07-28 17:27:29" itemprop="dateModified" datetime="2025-07-28T17:27:29+08:00">2025-07-28</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/02/03/flashattention/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/02/03/flashattention/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="note"><a href="#note" class="headerlink" title="note"></a>note</h2><ol>
<li>重要：Q矩阵不在d维度上做tiling</li>
<li><a target="_blank" rel="noopener" href="https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf">flash attention公式计算</a><ul>
<li>注意融合的时候，oi &#x3D; oi-1 + aiV[i, :], ai和V[i, :]是外积</li>
</ul>
</li>
<li>softmax计算推导：指数和 &#x3D; 当前指数 + 前面指数和 * 修正参数(与max有关)</li>
<li>softmax必须分两步（2 loop) 来算， 但attention不需要得到softmax结果，根据公式一步就能算完, softmax需要指数和，attention计算可以只根据之前的结果(递推)</li>
<li>Q@K 或 softmax(Q@K) 被称为 attention score matrix</li>
<li>在计算机编程和软件项目中，csrc 通常是 “C Source” 的缩写，意为 “C 源代码”。即使 csrc 目录可能包含 C++ 代码，这个名字仍然被保留，因为它沿用了这一传统的命名惯例<ul>
<li>flash attention</li>
<li>pytorch</li>
<li>xformer</li>
<li>vllm</li>
</ul>
</li>
<li>(不考虑 softmax)Attention 不融合消耗大量内存，如果 n 很大 qk (atttion 矩阵 nxn)结果非常大; 如果 kenel 进行融合；分块计算，只需要保存中间结果，见 xformer 示意图：<a target="_blank" rel="noopener" href="https://twitter.com/fvsmassa/status/1580229170629849089">xformer link</a></li>
<li>FlashAttention 在<code>batch和heads两个维度上进行了并行化</code>：使用一个 thread block 来处理一个 attention head，总共需要 thread block 的数量等于 batch size × number of heads。每个 block 被调到到一个 SM 上运行，例如 A100 GPU 上有 108 个 SMs。当 block 数量很大时（例如 ≥80），这种调度方式是高效的，因为几乎可以有效利用 GPU 上所有计算资源。但是在处理长序列输入时，由于内存限制，通常会减小 batch size 和 head 数量，这样并行化成都就降低了。 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/645376942">link</a></li>
<li>FlashAttention-2 还在<code>序列长度这一维度上进行并行化</code>，显著提升了计算速度。此外，当 batch size 和 head 数量较小时，在序列长度上增加并行性有助于提高 GPU 占用率。</li>
<li><code>FlashAttention目的不是节约FLOPs，而是减少对HBM的访问。</code></li>
<li>重点是 FlashAttention 在<code>训练和预测过程中的结果和标准Attention一样</code>，对用户是无感的，而其他加速方法做不到这点。</li>
<li>FlashAttention does not read and write the large attention NxN matrix to HBM, FlashAttention 不会读取大型 NxN 注意力矩阵并将其写入 HBM</li>
<li>In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]—that is, carefully accounting for reads and writes to different levels of fast and slow memory</li>
<li>Our main goal is to avoid reading and writing the <code>attention matrix to and from HBM</code>. 没有优化前 multihead attation 每个 head 都需要一个 NxN attention matrix</li>
<li>The most common approach to accelerate memory-bound operations is kernel fusion: if there are multiple operations applied to the same input, the input can be loaded once from HBM, instead of multiple times for each operation. Compilers can automatically fuse many elementwise operations</li>
<li>注意训练时的 forward 和推理时的 forward 的区别; 训练时输入 N 固定; 推理是 LLM inference（或称为 decoding）</li>
<li>there have been many attempts to fuse several elementwise operations, such as fusing masking with softmax</li>
<li>Let N be the sequence length, d be the head dimension, and M be size of SRAM with d &lt;&#x3D; M &lt;&#x3D; Nd;. Standard attention (Algorithm ) requires <code>O(Nd + NN)</code> HBM accesses, while FlashAttention (Algorithm 1) requires <code>O(NNdd/M)</code> HBM accesses.</li>
<li>对于 d(64-128) 和 N（大约 100KB）的典型值，d2 比 M 小很多倍，并且因此，FlashAttention 所需的 HBM 访问次数比标准实现少很多倍。</li>
</ol>
<h3 id="flash-attention-v1"><a href="#flash-attention-v1" class="headerlink" title="flash attention v1"></a>flash attention v1</h3><ol>
<li>论文算法里注意shared memeory和register区别，on chip是register上   </li>
<li>论文里的算法是一个thread block要处理的流程, 是一个head的attention, <ul>
<li>We use 1 thread block to process one attention head</li>
<li>Q, K，V都会做tiling, loop处理</li>
</ul>
</li>
<li>NOTE: 论文算法里的block size跟thread block要处理的size没有关系, 只是划分数据用的   </li>
<li>QK运算时类似split-k, 在k维度上做并行，最后再sync</li>
<li>M 为shared memory大小，M&#x2F;4是算最大的tile Size，因为Q, K, V, O都需要tile数据在shared memory上， tile size越大，读写HBM次数越少<ul>
<li>注意这里的tile_size是thread block要处理的size， 不是warp</li>
</ul>
</li>
<li>为什么不把m和l直接放到shared memory中？<ul>
<li>因为sequence length可能比较大， shared memeory中放不下， 每个token有一个m，l, N长的Q, m和l共占用memeory为2N(sequence len); 每个block需要计算一个head的所有attention计算，m和l共需2N memory, 内循环每次都需要从HBM load Tr长度的m和l，更新完写回HBM <ul>
<li>为什么不复用li或mi? 因为Qtile对应一个mi和li，K和V在外层，内层的mi, li需要在K, V维度上累加，会混乱，结合<a target="_blank" rel="noopener" href="https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf">flash attention公式计算</a>图<ul>
<li>flash attention v2 l和m不用写回HBM，因为在Q上划分block，每个block只需要占用2Tr(Tr是划分的Block Size，不会很大)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>For each block, FlashAttention splits K and V across 4 warps while keeping Q accessible by all warps<ul>
<li>每个thread block内部还会划分threads(warp) 来计算<ul>
<li>triton是thread block粒度的编程， 不需要划分threads(warp)，只需要计算好每个thread block要算的数，和算法流程</li>
</ul>
</li>
<li>每个warp先算出O，m, l放回到shared memory中，然后再reduce, warp间需要同步</li>
</ul>
</li>
<li>code</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// params.b = batch_size, params.h = num_heads</span><br><span class="line">dim3 grid(params.b, params.h);</span><br><span class="line">kernel&lt;&lt;&lt;grid, Kernel_traits::THREADS, smem_size_dq_dk_dv, stream&gt;&gt;&gt;(params);</span><br></pre></td></tr></table></figure>

<h3 id="flash-attenion-v2"><a href="#flash-attenion-v2" class="headerlink" title="flash attenion v2"></a>flash attenion v2</h3><ol>
<li>论文里的forwar算法不是一个thread block要处理的流程，是一个head的attention, <ul>
<li>We use (seqlen_q &#x2F; kBlockM) thread blocks to process one attention head</li>
</ul>
</li>
<li>code</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// params.b = batch_size, params.h = num_heads</span><br><span class="line">const int num_m_block = (params.seqlen_q + Kernel_traits::kBlockM - 1) / Kernel_traits::kBlockM;</span><br><span class="line">dim3 grid(num_m_block, params.b, params.h);</span><br><span class="line">............</span><br><span class="line">      kernel&lt;&lt;&lt;grid, Kernel_traits::kNThreads, smem_size, stream&gt;&gt;&gt;(params);</span><br></pre></td></tr></table></figure>
<ol>
<li>Q维度上是增加thread block并行，Q_tile内部是warp并行</li>
<li>even within each thread block, we also have to decide how to partition the work between different warps. We typically use 4 or 8 warps per thread block,(128或256 threads)</li>
<li>QK运算是slice-k, warp之间不需要sync</li>
<li>没有使用M来设置tile size, 直接作为参数传递  </li>
<li>先讲述 FlashAttention-2 对 FlashAttention 的改进，从而<code>减少了非矩阵乘法运算（non-matmul）的FLOPs</code>。然后说明如何将任务分配给不同的<code>thread block</code>进行并行计算，充分利用 GPU 资源。最后描述了如何在一个 thread block 内部分配任务给<code>不同的warps</code>，以减少访问共<br>享内存次数。这些优化方案使得 FlashAttention-2 的性能提升了 2-3 倍</li>
<li>softmax 将 rescale 最后一起进行，中间结果可以多利用 tensor core 矩阵乘</li>
<li>Warp Specialization: warp之间进行异步</li>
<li>Multistage(流水线)：warp内进行异步<ul>
<li>至少double buffer, 占用更多shared memmory</li>
</ul>
</li>
</ol>
<h3 id="flash-attention-v3"><a href="#flash-attention-v3" class="headerlink" title="flash attention v3"></a>flash attention v3</h3><ol>
<li>有计算和计算之前的异步指令</li>
<li>在 Hopper 之前的架构，GEMM 计算是同步的，即 GEMM 计算完成后才可调度其他计算单元（如 SFU）进行计算（A100 新增的cp.asyncAPI 是计算和数据传输的异步，并非计算和计算的异步），但对于 Hopper 而言，由于 WGMMA 指令的异步性，我们可以同时进行 Tensor Core 和 CUDA Core&#x2F;SFU 的计算，通过合理地安排 Warpgroup 的调度策略，就可以使 Softmax 的运算时间被 GEMM overlap。</li>
<li>Warpgroup 指的是 4 个连续的 warps，共 128 个连续的 threads，正好对应了一个 SM 最多可并行计算的线程数。在 H100 上，我们可以以 Warpgroup 为粒度调度 GEMM 运算。</li>
<li>Warp Specialization + Ping-Pong Scheduling + Intra-warpgroup pipelining <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/17533058076">link</a><ul>
<li>warpgroup之间warp specialization</li>
<li>Warp Specialization 数据传输和计算overlap</li>
<li>Ping-Pong Scheduling 计算和计算之间overlap(注意：不是一个warpgroup内，是多个warpgroup之间， 一个group内是multistage) NOTE:只考虑计算</li>
<li>Intra-warpgroup pipelining 计算和计算之间的overlap, 类似multstage; NOTE:只考虑计算</li>
</ul>
</li>
<li>注意Warp Specialization, Multistage，  Intra-warpgroup overlapping, Ping-Pong Scheduling异步的区别</li>
<li>mulitstage里的stage2就是在for里面算多少个不相关tile的数据<ul>
<li>shared memory也有stage概念, 注意区分, 这里的multistage不一定要和shared memory stage相同，因为可能用不到shared memory, 直接使用寄存器</li>
</ul>
</li>
<li>2 stage是将wgmma0和softmax算一个stage，和wgmma1做流水， 3 stage是 wgmma0, softmax, wgmma1做流水</li>
</ol>
<h3 id="flash-decoding"><a href="#flash-decoding" class="headerlink" title="flash decoding"></a><a target="_blank" rel="noopener" href="https://crfm.stanford.edu/2023/10/12/flashdecoding.html">flash decoding</a></h3><ol>
<li>flash attention 是针对模型训练时的 forward 和 backward; 训练时的 forward 是并行运算的</li>
<li>flash decoding 是针对模型推理时的 inference； 推理时的 inference 是 Autoregressive(AR)</li>
<li>flash attention 的优化不适合直接应用于推理过程。因为在训练过程中，FlashAttention 对 batch size 和 query length 进行了并行化加速。而在推理过程中，<code>query length 通常为 1</code>，这意味着如果 batch size 小于 GPU 上的 SM 数量（例如 A100 上有 108 个 SMs），那么整个计算过程只使用了 GPU 的一小部分！特别是当上下文较长时，通常会减小 batch size 来适应 GPU 内存。例如 batch size &#x3D; 1 时，FlashAttention 对 GPU 利用率小于 1%！</li>
<li>流程：对 K 和 V 进行划分成多块，并行计算各块 attention，最后各块结果做 reduction</li>
</ol>
<h2 id="code"><a href="#code" class="headerlink" title="code"></a>code</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention/blob/61a777247900f6c2a37376f3ffd7134385fdc95c/setup.py#L133">编译 cuda</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention/blob/61a777247900f6c2a37376f3ffd7134385fdc95c/csrc/flash_attn/flash_api.cpp#L1462">pybind11 python interface</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/search?q=repo:Dao-AILab/flash-attention%20flash_attn_cuda&type=code">python using pybind11 api</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention/blob/61a777247900f6c2a37376f3ffd7134385fdc95c/csrc/flash_attn/flash_api.cpp#L317">mha_fwd</a><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention/blob/9c0e9ee86d0e0022b60deddb405c20ab77481582/flash_attn/flash_attn_interface.py#L51">python call place</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention/blob/61a777247900f6c2a37376f3ffd7134385fdc95c/csrc/flash_attn/flash_api.cpp#L221C17-L221C29">run_mha_fwd</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/search?q=repo:Dao-AILab/flash-attention%20run_mha_fwd_&type=code">run_mha_fwd 多种实现</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention/blob/5cdabc2809095b98c311283125c05d222500c8ff/csrc/flash_attn/src/flash_fwd_launch_template.h#L31">run_flash_fwd</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention/blob/5cdabc2809095b98c311283125c05d222500c8ff/csrc/flash_attn/src/flash_fwd_kernel.h#L1043">compute_attn kernel 实现</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention/blob/5cdabc2809095b98c311283125c05d222500c8ff/csrc/flash_attn/src/flash_fwd_kernel.h#L28">compute_attn_1rowblock 最关键实现函数</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention/blob/5cdabc2809095b98c311283125c05d222500c8ff/csrc/flash_attn/src/utils.h#L138">flash::gemm</a></li>
</ol>
<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/apex/blob/master/apex/contrib/csrc/fmha/fmha_api.cpp">软件架构参考 apex fmha</a><ul>
<li>fmha: fast_multihead_attn</li>
<li>We use FMHA code as a starting point, and apply two well-established techniques (tiling and recomputa-tion) to deal with long sequences and to save memory as mentioned; we can support much longer sequences (e.g., up to length 64K). We also support more head dimensions (16, 32, 64, 128) and broader GPU types (all Turing and Ampere GPUs at the time of writing).</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/01/26/cutlass/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/26/cutlass/" class="post-title-link" itemprop="url">cutlass</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-01-26 10:20:32" itemprop="dateCreated datePublished" datetime="2024-01-26T10:20:32+08:00">2024-01-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-11 14:39:56" itemprop="dateModified" datetime="2025-02-11T14:39:56+08:00">2025-02-11</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/01/26/cutlass/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/01/26/cutlass/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>12k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>10 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="note"><a href="#note" class="headerlink" title="note"></a>note</h2><ol>
<li>主动设置<code>--cache-control=all</code>选项来保证在进行 kernel profiling 的时候清空历史 cache 数据，通过设置<code>--clock-control=base</code>选项来锁定 GPU 运行的频率，避免动态调频对性能测量的影响。</li>
<li><code>Swizzle</code>主要是在到 8192 以后，且 B 矩阵 transpose 的情况下必须要用（不用则效率腰斩，调出来 SWIZZLE 最优是 8）</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cub">cutlass 参考了 cub 的设计</a></li>
<li>cutlass 是对 gemm 实现过程的各种级别的抽象；手写搬数时需要进行大量计算<ul>
<li>block tile 矩阵乘法内积； thread tile 矩阵乘法外积</li>
<li>global memory &lt;-&gt; shared memory &lt;-&gt; register &lt;-&gt; core</li>
<li>double buffering: shared memory, register</li>
<li>bank conflict: permute</li>
</ul>
</li>
<li>排列（英语：Permutation）是将相异对象或符号根据确定的顺序重排。每个顺序都称作一个排列</li>
<li>MMA &#x3D; matrix multiply accumulate</li>
<li>cutlass kernel 融合效果好</li>
<li>可以看一下 cutlass 发版历史，了解一下过程</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/v0.1.1/README.md">v0.1.1&#x2F;README.md</a><ul>
<li>thread block tile, warp tile, thread tile</li>
</ul>
</li>
<li>CUTLASS is a collection of CUDA C++ template abstractions for implementing high-performance <code>matrix-matrix multiplication (GEMM)</code> and <code>related computations</code> at all levels and scales within CUDA<ul>
<li><code>GEMM</code></li>
<li><code>related computations</code> 主要指的是接在 GEMM 后面的 activation 或者一些 pointwise 计算，比如 bias，scales，这些计算可以和 GEMM 融合在一起，从而减少访存压力，由于这类计算一般发生在 GEMM 之后，我们管这类计算叫 epilogue(中文：结语)</li>
</ul>
</li>
<li>大量使用模板以及各种各种手段，把尽可能多的事情放在编译期完成。</li>
<li>对用户暴露大量的实现策略，用户需要自己做出实现策略的选择。这是 CUTLASS 和 CUBLAS,CUDNN 的重要差别所在，CUBLAS 和 CUDNN 用户只需要描述计算问题，所有的策略选择在内部完成。这意味着从软件分层上来说，CUTLASS 可以是 CUBLAS 和 CUDNN 的 backend，换而言之，CUBLAS 和 CUDNN 可以是 CUTLASS 的用户</li>
<li>c++模板，只用包含头文件</li>
<li>CUDA Templates for Linear Algebra Subroutines and Solvers</li>
<li>CUTLASS 和 cuBLAS 都是与 NVIDIA GPUs 相关的库，专门用于高效地执行线性代数运算</li>
<li>大矩阵 GEMM 运算 CuTlass 可以显著提速,是目前 GPU 上最快的 GEMM 库。</li>
<li>它允许开发者使用模板元编程自定义和优化矩阵乘法（GEMM）等线性代数运算，更加灵活。</li>
<li>cuBLAS 提供了一个简单、标准的 BLAS 接口，易于使用，而 CUTLASS 提供了更多的定制性和灵活性，但需要更深入的理解和控制。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/functionality.md">缩写的含义</a><ul>
<li>N: Column Major Matrix</li>
<li>T: Row Major matrix</li>
<li>{N,T} x {N,T}: All combinations, i.e., NN, NT, TN, TT(blas 中约定 Normal(N)矩阵为列优先，T 表示 transpose，即对列优先的矩阵进行转置则为行优先)</li>
<li>f: floating point</li>
<li>s: signed int</li>
<li>b: bit</li>
<li>cf: complex float</li>
<li>bf16: bfloat16</li>
<li>tf32: tfloat32</li>
<li>Simt: Use Simt CUDA Core MMA(使用 cuda core)</li>
<li>TensorOp: Use Tensor Core MMA</li>
<li>SpTensorOp: Use Sparse Tensor Core MMA</li>
<li>WmmaTensorOp: Use WMMA abstraction to use Tensor Core MMA</li>
</ul>
</li>
<li><code>One of CUTLASS&#39;s design patterns is to define gemm argument objects that are constructible in host code and passed to kernels by value.</code> These may include pointers, strides, scalars, and other arguments needed by Gemm and its components.The benefits of this pattern are <code>(1.) a structured, composable strategy for passing host-constructible arguments to kernels and (2.) minimized initialization overhead on kernel entry.</code></li>
<li>group gemm 和 batch gemm 区别： group 是 batch 更通用的形式，允许有不同尺寸的矩阵，batch 则需要多个矩阵尺寸相同</li>
<li>三种 tile<ul>
<li>thread block tile(例如：128x128x32)</li>
<li>warp tile(例如：64x64x32)</li>
<li>mma tile(例如：16x8x16)</li>
</ul>
</li>
</ol>
<h2 id="cute-cuda-tensor-tools"><a href="#cute-cuda-tensor-tools" class="headerlink" title="cute (cuda tensor tools)"></a>cute (cuda tensor tools)</h2><ol>
<li>CUTLASS 对 stream-K GEMM 算法的实现，该算法使用了 CuTe 的许多特性。</li>
<li>One could summarize almost all CuTe use cases as follows:<ul>
<li>create Layouts,</li>
<li>create Tensors with those Layouts</li>
<li>invoke (either CuTe’s, or custom) algorithms on those Tensors.</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=PWWOGrLZtZg">youtube 介绍</a></li>
<li><code>Layout maps from coordinate space(s) to an index space</code>. 从坐标(逻辑存储)映射到索引(物理存储);</li>
<li>需要十分明确 coordinate 和 index 概念， 1D coordinate 和 index 是不同的</li>
<li>The map from a natural coordinate to an index is performed by taking the inner product of the natural coordinate with the Layout’s Stride. coordinate 和 stride 内积来实现到 index 的映射;</li>
<li><code>cute::crd2idx(c, shape, stride)</code>来计算 c(coordinate)的 index</li>
<li>直接根据 coordinate 顺序计算 index， 不要用想象， 用公式； 例如 A &#x3D; (2,2):(4,1); coordinate 顺序为 A(0) &#x3D; A(0, 0) &#x3D; 0; A(1) &#x3D; A(1, 0) &#x3D; 4x1 &#x3D; 4; A(2) &#x3D; A(0, 1) &#x3D; 1; A(3) &#x3D; A(1, 1) &#x3D; 5; 所以 A(c)为 0 4 1 5</li>
<li>Layout 对象调用,返回 index，layout(i)</li>
<li>The core abstraction of CuTe are the hierarchically multidimensional layouts which can be composed with data arrays to represent tensors.</li>
<li>tensor</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor = ptr + layout <span class="comment">## 根据coodinate得到元素value</span></span><br><span class="line">layout = shape + stride <span class="comment">## 根据coodinate得到index</span></span><br></pre></td></tr></table></figure>
<ol>
<li>ptr &lt;–&gt; coodinate &lt;–&gt; index, value</li>
<li><code>从右往左读</code> The map from an input coordinate to a natural coordinate is the application of a colexicographical order (reading right to left, instead of “lexicographical,” which reads left to right) within the Shape</li>
<li>shape and stride 是 IntTupple, A Layout is a tuple of (Shape, Stride).</li>
<li>CuTe 的 Tensor 类表示一个多维数组。数组的元素可以驻留在任何类型的内存中，包括<code>全局内存、共享内存和寄存器内存</code>。</li>
<li>Tensor 的行为完全由它的两个组件决定，这两个组件对应于它的两个模板参数： Engine 和 Layout 。Engine 表示元素的一维数组。当用户对 Tensor 执行数组访问时， Tensor 使用其 Layout 从逻辑坐标映射到一维索引。然后， Tensor 使用其 Engine 将一维索引映射到对该元素的引用。</li>
<li>tensor Ownership of the elements; Tensors can be owning or nonowning.<ul>
<li>Whether a Tensor is “owning” or “nonowning” depends entirely on its Engine.</li>
<li>ViewEngine 和 ConstViewEngine 包装指向各种内存的指针。</li>
<li>make_gmem_ptr(g) when g is a pointer to global memory, or make_smem_ptr(s) when s is a pointer to shared memory.</li>
</ul>
</li>
<li>print 会打印 tensor 存储空间所在的位置和 shape、stride 信息，而 print_tensor 除了以上信息还会打印 Tensor 中具体的每一个数值：</li>
<li>We wrap each MMA’s PTX instruction in an “Operation” struct.</li>
<li>For each Operation struct, we define a “Traits” struct that defines all of the meta-information needed to use the Operation. 对于每个操作结构，我们定义一个“Traits”结构，它定义使用该操作所需的所有信息。</li>
<li>mma ptx -&gt; mma_operation -&gt; mma_trait(封装各种信息) -&gt; mma_atom -&gt; tiled_mma -&gt; thr_mma -&gt; cute::gemm()</li>
<li>copy ptx -&gt; copy_operation -&gt; copy_trait -&gt; copy_atom -&gt; tiled_copy -&gt; thr_copy -&gt; cute::copy()</li>
<li>通过 Tensor 和 Layout 抽象我们可以实现对计算矩阵的分块；基于 Copy 抽象，我们可以完成块状矩阵 A、B 数据从 global 内存到寄存器的加载；通过 MMA 抽象我们可以利用 Tensor Core 完成寄存器上小块矩阵的乘法运算；再次通过 Copy 抽象，我们可以将寄存器上的结果拷贝到 global 内存，完成完整的 GEMM 运算</li>
<li>A has shape (M, K) and strides (1, ldA). Since A has stride 1 in the M mode, we say that A is <code>M major</code>. B has shape (N, K) and strides (1, ldB), so B is “N-major.” Similarly, C has shape (M, N) and strides (1, ldC), so C is “M major.”</li>
<li>layout 两个核心概念<ul>
<li>shape: define the coordinate mappings</li>
<li>stride: defines the index mappings, 在逻辑位置和物理（数据）做映射的时候每一个元素之间的差为 n</li>
</ul>
</li>
<li>thread layout and data layout</li>
<li>mapping correct threads to correct values for you computation</li>
<li>layout is a function</li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/column/c_1696937812497235968">CUDA 高性能编程</a></li>
<li>计算机中的内存是一维的线性地址空间，而数学计算问题所要处理的空间经常是高维的。如 GEMM（General Matrix Multiplication）问题的数学计算体系是二维计算空间，Deep Learning 计算体系是三维以上的计算空间(batch, height, width, channel, etc.)。如何高效的表达高维计算空间，如何高效便捷的将计算所要求的高维空间映射到一维空间变得越来越重要。历史上对该问题的探究可以分为三个阶段：<ul>
<li>第一阶段 BLAS 的 row&#x2F;col-major + leading dimension 描述阶段；</li>
<li>第二阶段 Tensor 的 shape + stride 阶段；</li>
<li>第三阶段为 Hierarchy Tensor 阶段。(组合设计模式实现？)</li>
</ul>
</li>
<li>引入有层次的描述（Layout）代数来表达计算空间和一维地址空间的映射问题。Layout 是一个数据排列的描述体系，其可以实现将<code>逻辑坐标</code>映射到<code>索引(物理)坐标</code>（offset 表示）</li>
<li>Tensor 是数据的表达，其表达一个相对独立且有结构的数据体，而 Tensor 内的数据排布则由 Layout 来表达</li>
<li>二维矩阵的描述和一维类似，shape 表示其逻辑形状，stride 表示具体的某个元素和物理空间的映射时的间隔量。</li>
<li>逻辑空间到物理空间的映射通过点积来完成。如 shape: (3,4); stride: (4, 1); cood: (2, 2), 物理 offset &#x3D; 2x4 + 2x1 &#x3D; 10</li>
<li>Layout 的本质是函数，其可以实现由一种坐标系统变换到一个表示偏移量的标量</li>
<li>Layout 的本质是函数，函数的本质是集合</li>
<li>((2, 4), (3, 5))，同样我们可以得到其 stride: ((3, 6), (1, 24)) <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/662089556">link</a><ul>
<li>(2, 4)表示行方向按 2 个一组，分 4 组</li>
<li>(3, 5)表示列方向按 3 个一组，分 5 组</li>
<li>(3, 6)表示行方向内层元素之间 stride 为 3，外层元素 stride 为 6</li>
<li>(1, 24)表示列方向内存元素之间 stride 为 1，外层元素 stride 为 24</li>
<li>行为：2x4 &#x3D; 8</li>
<li>列为：3x5 &#x3D; 15</li>
<li>(6, 4)表示：((6%2, 6&#x2F;2), (4%3, 4&#x2F;3)) &#x3D; ((0, 3), (1, 1))， offset &#x3D; 0<em>3 + 3</em>6 + 1<em>1 + 24</em>1 &#x3D; 43</li>
<li>((1, 3), (2, 4))位置如何找？在行上，先看 3，后看 1，确定外围 tensor 是第 4 个，内层 tensor 是第 1 个；列同理</li>
</ul>
</li>
<li>Layout 描述了数据的排列和底层存储位置关系，但 Layout 并没有指定存储。Tensor 就是在 Layout 的基础上包含了存储，即<code>Tensor = Layout + storage</code>, 数据存储的具体表现上可以是指针表达的数据或则是栈上数据（GPU 上表现为寄存器）</li>
<li>深度学习框架中的 Tensor 更强调数据的表达实体，通过 Tensor 实体与实体之间的计算产生新的 Tensor 实体，即<code>多份数据实体</code>，cute 中的 Tensor 更多的是对 Tensor 进行分解和组合等操作，而这些操作多是对 Layout 的变换（只是逻辑层面的数据组织形式），<code>底层的数据实体一般不变更</code>。</li>
<li>深度学习框架中的 Tensor 是用来表达数据实体，cute 中的 Tensor 是偏向描述的实体。</li>
<li>使用 Tensor 语义和工具能够更形象化的表达我们的逻辑，方便我们的思考，而 CUDA 的优化思路和技巧并不会因为 Tensor 的引入而变简单或困难。Tensor 只是工具，可以方便我们的表达，至于深层次的优化思路那还是对经验的挑战。</li>
<li>Tensor 表达虽然提供了很多方便，但也只限于表达的高效和便捷，如何对程序进行优化，Tensor 表达的引入并没有提供额外帮助，它仍然需要我们从别的途径来获得。即便如此，<code>表达和抽象依然无比重要</code>，那正如：伽罗瓦如果没有群这一表达工具，就难以解决多项式根的问题；杨振宁没有群这一工具就难以构思举世的杨-米尔斯理论。</li>
<li>抽象和工具让我们可以在更高的维度上思考。</li>
<li>cute 提供了 MMA 能力来完成 D &#x3D; A x B + C 的矩阵乘法运算，其针对指令封装，适配层，原子能力、块状 MMA、线程划分和执行进行了抽象，形成了 MMAOperation、MMA_Traits、MMA_Atom、TiledMMA、ThrMMA、cute::gemm 数据结构和函数，我们通过这些结构能够完成逻辑块状矩阵乘法的划分和执行。这些抽象通过软件分层设计使得各层次独立，我们不必关注底层细节，只需要从提供的模块中组合我们的逻辑即可，同时抽象的解偶设计，使得我们可以专注于顶层逻辑而降低对底层细节的要求。</li>
<li>swizzle 抽象， Layout 的作用是给定坐标返回 offset，而 swizzle 的作用则是给定 offset 返回 bank conflict free 的 offset。 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/671419093">link</a><ul>
<li>shared memory 读写都按逻辑坐标，真实物理坐标都是 bank free 的(太强了)</li>
<li>先从一维映射到逻辑二维，再从逻辑二维找到 bank free 二维，最后再返回一维</li>
<li>ibank &#x3D; irow ^ icol 真是妙啊</li>
<li>在做全局内存到共享内存数据搬运时，<code>思考模型是逻辑空间，而执行时需要考虑存储空间以避免bank conflict</code></li>
</ul>
</li>
<li><code>layout和swizzle都是一种抽象，可以使用逻辑的方式思考，具体存储交给映射关系</code></li>
<li><code>new layout language to describe coordinate and index bookkeepings; coordinate 是思考时用到的逻辑方式，具体的存储由 layout 映射关系来处理，可以将经历放在逻辑上</code></li>
<li><code>Any problem in computer science can be solved by another layer of indirection</code> 计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决;例如虚拟地址<ul>
<li>layout 和 swizzle 都可以看做是中间层或者代理</li>
</ul>
</li>
<li><code>loccal tile</code> 是 Tensor 中用户可以使用到的重要的函数，可以通过 tile 方法对 tensor 进行分块，通过 local_tile 可以实现从大的 tensor 中切取 tile 块，并且通过 coord 进行块的选取<ul>
<li>用于数据分块</li>
</ul>
</li>
<li><code>local partition</code>和 local tile 类似，现将大的 Tensor 按照 tile 大小进行分块，分块后每一块取出 coordinate 指定的元素拼拼成新的块<ul>
<li>用于给线程分数据</li>
</ul>
</li>
</ol>
<h2 id="cutlass-优化手段"><a href="#cutlass-优化手段" class="headerlink" title="cutlass 优化手段"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/677616101">cutlass 优化手段</a></h2><ol>
<li><code>bank conflict free的shared memory layout</code></li>
<li><code>thread block swizzle</code>：这个优化对于中大型矩阵乘法比较明显，更改了发射 block 的顺序，以增加 locality，从而提高 l2cache 的命中率，实现上非常简单，核心代码就是一个取余操作，但有用多级流水线(software pipeline)：2 条可以不要 async.copy 这个指令(sm80 才有的)，大于 2 条流水就需要了，原理上没什么，和 CPU 的多级流水一个道理，主要是指令的应用。</li>
<li><code>predicate iterator</code>：这个是一个软件层组件写法的优化，叫 predicate 的原因是，这个 iterator 会返回一个布尔值，在 gpu 的指令里是一个 special register，用来表示这块内存是不是需要 load，这个在软件层会涉及一些优化手段，比较有趣的是会在 host 侧 precompute 了哪些下标需要 load，用位运算来 mask，计算开销(位运算在 gpu 里开销较小)和存储开销(一个 byte 可以存 8 个 mask 值)都很小。为什么需要让存储开销很小？因为在 gpu 架构里，register 是很贵的，一个 thread 只能使用 255 个 register，如果超出了就会存在 local memory 里，register 读取很快，一个 cycle 就可以完成，而 local memory 就会慢非常多，register 用超了会非常非常影响性能！</li>
<li><code>shared memory重排搬出</code>：mma 指令计算完成之后，结果是存在 register 里的，且 register 中存储的数据是不连续的(32bits 连续)，原因是由于 mma 指令造成的，我们知道 vectorize load&#x2F;store 会提高访存带宽，所以我们可以在 shared memory 里重新排序，一并搬出。但并不是什么情况下重排都是正优化，因为重排还是会增加一次 shared memory store&#x2F;load，比如在小 channel 的 conv2d 中，直接从 register 搬出到 global memory 性能会更好</li>
<li><code>cooperative fetching和vectorize load</code>：这两个是 GPU 的一些基本优化方法，即尽量用更大的 data type 来搬运，以及尽量让一个 warp 里的不同线程是连续地访存同一块内存地址，原理可以参考</li>
<li><code>tiling description</code>: 提供了实例化方法，来调整 block 计算量和 warp 计算量，</li>
</ol>
<h2 id="type"><a href="#type" class="headerlink" title="type"></a>type</h2><ol>
<li><code>cutlass::half_t</code> 不支持 half</li>
<li><code>cutlass::bfloat16</code> Ampere ElementAccumulator 应该为 float, 看手册可以看到只支持 float32 的 accumulate <a target="_blank" rel="noopener" href="https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf">link</a><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu#L68">ampere_gemm_operand_reduction_fusion</a></li>
</ul>
</li>
</ol>
<h2 id="fusion"><a href="#fusion" class="headerlink" title="fusion"></a>fusion</h2><ol>
<li>为了减少核启动(launch)和内存访问的开销</li>
<li>It also removes kernel launch overhead 减少 kernel launch 开销</li>
<li>注意融合分两种：<ul>
<li>简单融合：简单将两个 kernel 合并成一个，并没有减少从主存加载数据次数，只是减少了 kernel 调度的开销</li>
<li>高效融合：利用 shared memory 减少从主存加载数据次数</li>
</ul>
</li>
<li>两个 gemm 融合， 关键在 k 维度， 假设 A@B@C， 分块时 A，B 矩阵滑动取数，结果放到 C 中，只能取一块，结果和 C 相乘的时候需要所有的分块矩阵</li>
</ol>
<h2 id="docs"><a href="#docs" class="headerlink" title="docs"></a>docs</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cutlass_3x_design.md">cutlass_3x_design</a><ul>
<li>CUTLASS 3.0 将其接口层与硬件分离，将它们集中在 GEMM 算法的自然结构周围，不依赖于任何特定的 GPU generation</li>
<li>CUTLASS 2.x 将 GEMM 操作的移动部分分解为一个层次结构，该层次结构密切反映了 GPU 架构的组织。然而，这种设计有时会导致耦合过于紧密，无法扩展到不适合同一架构层次结构的较新 GPU。</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/gemm_api_3x.md">gemm_api_3x</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/gemm_api.md">gemm_api</a><ul>
<li>包含层级结构</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/code_organization.md">代码组织</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/fundamental_types.md">数据类型</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/01_layout.md">CuTe layout</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md">efficient_gemm.md</a></li>
<li>最全流程图<br><img src="https://github.com/NVIDIA/cutlass/blob/main/media/images/gemm-structural-components.png" alt="gemm-structural-components"></li>
</ol>
<h3 id="build"><a href="#build" class="headerlink" title="build"></a>build</h3><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/quickstart.md">quickstart</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/quickstart.md#building-for-multiple-architectures">选择 arch</a></li>
<li>下不下来 google test, 修改 cmakelists.txt, 注释掉 gtest</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libgtest-dev</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/NVIDIA/cutlass.git</span><br><span class="line"><span class="built_in">export</span> CUDACXX=<span class="variable">$&#123;CUDA_INSTALL_PATH&#125;</span>/bin/nvcc</span><br><span class="line"><span class="built_in">export</span> PATH=/usr/local/cuda/bin/:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">mkdir</span> build &amp;&amp; <span class="built_in">cd</span> build</span><br><span class="line">cmake .. -DCUTLASS_NVCC_ARCHS=86</span><br><span class="line">make cutlass_profiler -j12</span><br><span class="line">./tools/profiler/cutlass_profiler --kernels=sgemm --m=4352 --n=4096 --k=4096</span><br></pre></td></tr></table></figure>

<h2 id="samples"><a href="#samples" class="headerlink" title="samples"></a>samples</h2><ol>
<li><p>00 basic gemm</p>
<ul>
<li>创建 cutlass::gemm::device::Gemm，CutlassGemm::Arguments 实例 gemm_operator 和 args</li>
<li>gemm_operator(args) launch kernel</li>
</ul>
</li>
<li><p>01 cutlass utilities</p>
<ul>
<li>HostTensor contributes management for both host and device memory. HostTensor allocates host and device memory upon construction. Basic element-wise operations on host memory synchronize device memory automatically.</li>
<li>cutlass::reference::host::Gemm</li>
<li>cutlass::reference::device::TensorFillRandomGaussian()</li>
<li>cutlass::reference::host::BlockFillSequential()</li>
<li>cutlass::reference::host::TensorEquals()</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/tree/main/examples/02_dump_reg_shmem">02_dump_reg_shmem</a></p>
<ul>
<li>Don’t use cuda-gdb to debug any heavy templated CUDA code. We just insert printf to dump the information to debug. We have some utilities to help you to do it. <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/issues/372#issuecomment-987532556">link</a></li>
</ul>
</li>
<li><p>41_fused_multi_head_attention</p>
</li>
<li><p>cute&#x2F;tutorial&#x2F;sgemm_nt_1.cu</p>
<ul>
<li>thread layout tA, tB 是读 AB 数据时线程 layout, tC 是计算 C 时的线程 layout</li>
<li>tAsA 搬 block tile 数据用， tCsA 计算 block tile 用</li>
<li>tCrC 每个线程算 8x8 数据， shared memory 中 A、B 数据也为 8x8，</li>
<li>使用 cuda core 计算，并没使用 tensor core</li>
<li>tAsA 表示 copy 时 thr_copy A 时 Shared memory 上的 thread 数据</li>
<li>tCsA, tCsB, tCrC 表示 mma 计算 thr_mma 用到的 A，B shared memory thread 数据和 C 寄存器数据; 提供线程号，则获得具体线程的数据划分能力，对给定的数据块进行划分，得到线程级的数据描述。</li>
<li>可以顺序赋值给 A, B, 类型改为 int, 来模仿各 tile layout</li>
<li>axpby:alpha x plus beta y <code>y = alpha*x + beta*y</code>; where x and y are vectors of n elements and alpha and beta are scalars.</li>
<li>axpy here is simply an abbreviation of alpha times x plus y.</li>
</ul>
</li>
</ol>
<h2 id="debug"><a href="#debug" class="headerlink" title="debug"></a>debug</h2><h3 id="cutlass-device-dump"><a href="#cutlass-device-dump" class="headerlink" title="cutlass device dump"></a><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/tools/util/include/cutlass/util/device_dump.h#L131">cutlass device dump</a></h3><ol>
<li>cutlass::debug::dump_shmem()<ul>
<li>每行 128B，bank: 4B x 32 &#x3D; 128B</li>
</ul>
</li>
<li>cutlass::debug::dump_fragment()</li>
</ol>
<h3 id="cute-tensor"><a href="#cute-tensor" class="headerlink" title="cute tensor"></a><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/include/cute/tensor.hpp#L985">cute tensor</a></h3><ol>
<li>cute::print(tensor)</li>
<li>cute::print_tensor(tensor); 可以打印各种维度</li>
<li>cute::print_tensor_os()</li>
<li>operator&lt;&lt;()</li>
<li>print 会打印 tensor 存储空间所在的位置和 shape、stride 信息，而 print_tensor 除了以上信息还会打印 Tensor 中具体的每一个数值：</li>
</ol>
<h3 id="cute-layout"><a href="#cute-layout" class="headerlink" title="cute layout"></a><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/include/cute/layout.hpp#L1694">cute layout</a></h3><ol>
<li>cute::print(layout): <code>print(layout.shape()); print(&quot;:&quot;); print(layout.stride());</code></li>
<li>cute::print_layout(layout)<ul>
<li>NOTE: 只能打印二维 layout; 代码会做检查 <code>CUTE_STATIC_ASSERT_V(rank(layout) == Int&lt;2&gt;&#123;&#125;);</code> 可以 slice 成二维的再打印</li>
</ul>
</li>
<li>cute::print_layout(tensor.layout());</li>
<li>operator&lt;&lt;(): <code> os &lt;&lt; shape(layout) &lt;&lt; &quot;:&quot; &lt;&lt; stride(layout);</code></li>
</ol>
<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=QLdUML5MCfE&t=42s">CUTLASS 3 0 Next Generation Composable and Reusable GPU Linear Algebra Library - TVMCon2023</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/">nvidia cutlass-linear-algebra-cuda</a></li>
<li><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3582016.3582018">Graphene: An IR for Optimized Tensor Computations on GPUs</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/column/c_1696937812497235968">CUDA 高性能编程</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/588953452">cutlass 源码导读（1）——API 与设计理念</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/01/25/OpenCL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/25/OpenCL/" class="post-title-link" itemprop="url">OpenCL</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-01-25 11:02:48 / 修改时间：11:03:42" itemprop="dateCreated datePublished" datetime="2024-01-25T11:02:48+08:00">2024-01-25</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/01/25/OpenCL/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/01/25/OpenCL/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>44</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="base"><a href="#base" class="headerlink" title="base"></a>base</h2><ol>
<li><code>OpenCL(Open Computing Language)</code></li>
</ol>
<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.khronos.org/opencl/">opencl</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/01/14/Efficient-LLM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/14/Efficient-LLM/" class="post-title-link" itemprop="url">Efficient-LLM</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-01-14 18:44:21" itemprop="dateCreated datePublished" datetime="2024-01-14T18:44:21+08:00">2024-01-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-02-22 14:09:16" itemprop="dateModified" datetime="2024-02-22T14:09:16+08:00">2024-02-22</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/01/14/Efficient-LLM/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/01/14/Efficient-LLM/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><h3 id="大模型推理综述-Towards-Efficient-Generative-Large-Language-Model-Serving-A-Survey-from-Algorithms-to-Systems"><a href="#大模型推理综述-Towards-Efficient-Generative-Large-Language-Model-Serving-A-Survey-from-Algorithms-to-Systems" class="headerlink" title="大模型推理综述 Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.15234">大模型推理综述 Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems</a></h3><ol>
<li>Context or initial or prefil 阶段: input token 是并行的，可以利用 flash attention 来加速</li>
<li>Generation or incremental or decode 阶段: input token 是 1， Autoregressive(AR)，使用 paged attention 来实现<ul>
<li>如果是 batch 为 1， 可以填充为 8 来利用 tensor core; 或者使用优化的 cuda core GEMV</li>
<li>解码阶段的过程主要由 GEMV（batch size&#x3D;1）或 flat GEMM（batch size &gt; 1）操作组成</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/Uue0SxH6W_tI8K4Zb0igLQ">中文解读</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/675101140">黄浴中文翻译</a></li>
<li><code>低延迟和高吞吐量</code>是 LLM 服务系统中的双重优化目标，代表了互补但往往相互冲突的目标，需要一种平衡的策略来优化单个任务的快速响应和指定时间内最大化处理任务量之间的权衡</li>
<li>除了注意计算之外，对于线性投影算子，最近还出现了用<code>通用矩阵向量积（GEMV，general matrix-vector product）代替GEMM的趋势</code>，更有效地处理小批量（即大小为 1）的情况。</li>
</ol>
<h3 id="Full-Stack-Optimization-of-Transformer-Inference-a-Survey"><a href="#Full-Stack-Optimization-of-Transformer-Inference-a-Survey" class="headerlink" title="Full Stack Optimization of Transformer Inference: a Survey"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.14017">Full Stack Optimization of Transformer Inference: a Survey</a></h3><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/677595064">黄浴中文翻译</a></li>
</ul>
<h3 id="Efficient-Large-Language-Models-A-Survey"><a href="#Efficient-Large-Language-Models-A-Survey" class="headerlink" title="Efficient Large Language Models: A Survey"></a><a target="_blank" rel="noopener" href="https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey">Efficient Large Language Models: A Survey</a></h3><h3 id="Efficient-Deep-Learning-Book"><a href="#Efficient-Deep-Learning-Book" class="headerlink" title="Efficient Deep Learning Book"></a><a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2022-05-03-4">Efficient Deep Learning Book</a></h3><ol>
<li><a target="_blank" rel="noopener" href="https://efficientdlbook.com/#download">https://efficientdlbook.com</a></li>
</ol>
<h2 id="sampling"><a href="#sampling" class="headerlink" title="sampling"></a>sampling</h2><ol>
<li>top-K</li>
<li>top-P</li>
</ol>
<h2 id="系统优化技术"><a href="#系统优化技术" class="headerlink" title="系统优化技术"></a>系统优化技术</h2><ol>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/">nvidia mastering-llm-techniques-inference-optimization</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.15234">大模型推理综述 Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems</a><ul>
<li>量化</li>
<li>并行计算</li>
<li>内存管理</li>
<li>请求调度</li>
<li>kernel 优化</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM/tree/main?tab=readme-ov-file#advanced-topics">tensorrt llm 高级主题</a><ul>
<li>量化</li>
<li>in-flight batching</li>
<li>attention</li>
<li>graph rewriting</li>
</ul>
</li>
</ol>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><ol>
<li>vllm</li>
<li>tensorrt-llm</li>
</ol>
<h2 id="attention-优化"><a href="#attention-优化" class="headerlink" title="attention 优化"></a>attention 优化</h2><ol>
<li>flash attention -&gt; flash attention v2 -&gt; flash decoding</li>
<li>xformers</li>
<li>paged attention</li>
<li>token attention</li>
<li>cublas</li>
<li>cutlass</li>
</ol>
<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/horseee/Awesome-Efficient-LLM">Awesome-Efficient-LLM</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/optimum/llm-perf-leaderboard">llm-perf-leaderboard</a><ul>
<li>lantency vs memory vs score</li>
</ul>
</li>
</ol>
<h2 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h2><ol>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/perplexity">PPL: perplexity</a></li>
<li>hugging face 显示指标<br><img src="https://i.ibb.co/THp5xHd/Ijz-O30-Ae96.png" alt="指标"></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/optimum/llm-perf-leaderboard">llm-perf-leaderboard</a><ul>
<li>lantency vs memory vs score</li>
<li>e2e lantency</li>
<li>e2e throughput</li>
<li>prefil lantency</li>
<li>decode throughput</li>
</ul>
</li>
<li>TTFT 表示 Time To First Token，TPOT 表示 Time Per Output Token。前者由初始相位处理速度驱动，而后者直接取决于增量解码中的每次迭代执行时间。</li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/01/11/blas/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/11/blas/" class="post-title-link" itemprop="url">blas</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-01-11 16:31:34" itemprop="dateCreated datePublished" datetime="2024-01-11T16:31:34+08:00">2024-01-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-11 11:34:53" itemprop="dateModified" datetime="2025-04-11T11:34:53+08:00">2025-04-11</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/01/11/blas/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/01/11/blas/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="basic"><a href="#basic" class="headerlink" title="basic"></a>basic</h2><ol>
<li>先选定 TileC，然后沿着 k 轴移动小块进行累加求和的策略为 sliced-k，它对于 m、n 维度较大的场景（m n 分块所需要的 block 数目足以填充所有的 SM）比较有效。<code>对于k比较大，而m、n比较小的场景，由于m、n较小而我们根据C来划分thread block，这时需要的thread block数目比较小，当这个数目无法填充所有的SM时，则存在很多SM无任务，而有任务的SM需却又需要循环多次的问题</code>，这时候可以考虑将 k 轴拆分成多段，每一段都计算一个 TileC 结果，最后再通过额外的累加过程将多段的结果进行求和，这种模式的任务划分方法成为 split-k 方法。<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/667521327">link</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.03598">Stream-K: Work-Centric Parallel Decomposition for Dense Matrix-Matrix Multiplication on the GPU</a><ul>
<li>矩阵乘分块方法： sliced-k, split-k, stream-k</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/u013701860/article/details/128674224">split-k</a><ul>
<li>k 维度由多个 thread block 计算， 减少单 thread block 的延迟， 增加并行性</li>
<li>算完之后需要同步多个TB</li>
</ul>
</li>
<li>split-k和slice-k区别：<ul>
<li>Split-K 是一种 矩阵乘法加速技术，把 C &#x3D; A x B 中 K 维度拆成多个部分 并行计算，最后结果归约（加和）。提高并行度，适用于 K 非常大、单个线程块处理不完时。</li>
<li>slice-K（或叫 tiled-K）把矩阵乘法中的 K 维度切分成多个 tile，每次只加载一小段 K 来乘， 通常用于 节省寄存器和共享内存使用，适用于单个线程块处理较大 K 的情形。</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#parallelized-reductions">efficient_gemm.md parallelized-reductions</a><ul>
<li>split-k</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3">各种 blas 实现</a></li>
<li>BLAS 是 Basic Linear Algebra Subprograms 的缩写,即基本线性代数子程序,它是一系列线性代数运算函数的标准说明和接口规范。通常情况下,BLAS 函数库会为矩阵和向量运算提供高效、经过优化的实现,这些函数被广泛使用于科学计算、机器学习、数据分析等领域需要大量数值计算的场景中。常见的 BLAS 函数标准说明有:<ul>
<li>BLAS level 1:向量-向量运算</li>
<li>BLAS level 2:矩阵-向量运算</li>
<li>BLAS level 3:矩阵-矩阵运算</li>
</ul>
</li>
<li><code>GEMM(General Matrix-Matrix Multiplication)</code> 是通用矩阵乘法,表示 <code>C = αAB + β*C</code>。它计算两个矩阵的乘积,是最常见的数值密集型运算之一。</li>
<li><code>GEMV(General Matrix-Vector Multiplication)</code>是矩阵向量乘法,表示<code>y = αAx + β*y</code>。它计算矩阵和向量的乘积得到一个向量。 αβ 是标量，A 是矩阵，xy 是向量</li>
<li><a target="_blank" rel="noopener" href="https://github.com/flame/how-to-optimize-gemm">how-to-optimize-gemm</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NervanaSystems/maxas/wiki/SGEMM">maxas&#x2F;wiki&#x2F;SGEMM</a></li>
<li>blas 库命名规则<br><img src="https://i.ibb.co/8MWgXfn/atg4bmz-C9l.png" alt="name rule"></li>
<li>sgemm: <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/AukKpQU0XDH1YrDH_U8i1g">link</a><ul>
<li>s: single</li>
<li>ge: general</li>
<li>mm:matirx matrix matrix product</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/350470257">矩阵乘法的内积与外积</a><ul>
<li>cuda thread tile 时用了外积，可以缓存 shared memory 到 register</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://kl66.top/2019/05/30/%E5%90%91%E9%87%8F%E5%92%8C%E7%9F%A9%E9%98%B5%E7%9A%84%E5%90%84%E7%A7%8D%E7%A7%AF/">向量和矩阵的各种积</a></li>
<li><code>lda</code> leading dimension of two-dimensional array used to store matrix A.</li>
<li><code>batch gemm</code> 拼接，在 thread block tile 时只计算对角线上的数据, 优化技术跟 gemm 一样，就是 thread block tile 方式不同</li>
</ol>
<h2 id="optimize"><a href="#optimize" class="headerlink" title="optimize"></a>optimize</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/flame/how-to-optimize-gemm">how-to-optimize-gemm</a><ul>
<li>列优先</li>
<li>Optimization2: 循环展开</li>
<li>Optimization_1x4_4: 使用内联</li>
<li>Optimization_1x4_5: 循环合并</li>
<li>Optimization_1x4_6: 使用寄存器：register double test;</li>
<li>Optimization_1x4_7: 减少 for 循环变量访存</li>
<li>Optimization_4x4_5 MMult_4x4_5.c: 可以显示 block 计算过程, 可以看出分块之后有很多访存可以合并, A 和 B 分别可以减少到 1&#x2F;4</li>
<li>Optimization_4x4_10: 使用 vector 计算 block</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/tpoisonooo/how-to-optimize-gemm/blob/master/README_ZH_CN.md">how-to-optimize-gemm C++ 中文版</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/njuhope/cuda_sgemm/tree/master">cuda_sgemm 旷视</a></li>
<li>(good)<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/441146275">CUDA GEMM 理论性能分析与 kernel 优化</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/410278370">CUDA 矩阵乘法终极优化指南</a><ul>
<li>如果是 float4(128bit)，一个 warp 访问 16B，32x16B 得分 4 次做，那么会把一个 warp 分成 0-7&#x2F;8-15&#x2F;16-23&#x2F;24-31 这么 4 组，只要组内没有 bank conflict 就行了。</li>
<li>shared memory: float4 0 和 8 线程确实读的都是 0-3bank，但是 0-7 8-15 是分别的两组，这时候一个 warp 会分成 4 组 4 次读取，在不同组是不会冲突的，只需要组内没有冲突就可以了。我之前说了 shared memory 是 32 bank，每 bank 4B，你要一个 warp 访问 float4，数据量是 32*16B，是 shared memory 一次性能提供的数据量的 4 倍，那么必然要分 4 次获得，然后架构设计就是我说的分组情况。如果你一个线程访问 float2，那么就会是 0-15 16-31 两组</li>
<li>shared memory 一次不冲突只能读 32x4B， 看一次读取的数据量，多于 32x4B 需要读取多次 n&#x2F;(32x4)，只要在 n&#x2F;(32x4) 次内读完就是最高效的。</li>
</ul>
</li>
<li>使用 float4 类型访存，用向量化的 LDG.128 和 STG.128 指令一次读 4 个元素，以减少指令数</li>
<li>block tile 时，第一个矩阵在 shared memory 上要按列存储，为了连续存储，好利用 LDG.128 和 STG.128 指令</li>
</ol>
<h2 id="blas"><a href="#blas" class="headerlink" title="blas"></a><a target="_blank" rel="noopener" href="https://www.netlib.org/blas/">blas</a></h2><ol>
<li>名词出自 blas 库，例如<ul>
<li>SGEMM: single matrix matrix multiply;</li>
<li>DGEMM: double matrix matrix multiply</li>
<li>SGEMV: single matrix vector multiply</li>
</ul>
</li>
</ol>
<h2 id="NVIDIA-cutlass"><a href="#NVIDIA-cutlass" class="headerlink" title="NVIDIA&#x2F;cutlass"></a><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass">NVIDIA&#x2F;cutlass</a></h2><h2 id="cuBLAS"><a href="#cuBLAS" class="headerlink" title="cuBLAS"></a><a target="_blank" rel="noopener" href="https://developer.nvidia.com/cublas">cuBLAS</a></h2><ol>
<li>CUDA Basic Linear Algebra Subprograms</li>
<li>cuBLAS（CUDA Basic Linear Algebra Subprograms）是由 NVIDIA 提供的官方库，它是经典 BLAS（Basic Linear Algebra Subprograms）库的 CUDA 实现。</li>
<li>列优先</li>
<li>cublasSgemmBatched 支持 batch 运算</li>
</ol>
<h2 id="OpenBLAS"><a href="#OpenBLAS" class="headerlink" title="OpenBLAS"></a><a target="_blank" rel="noopener" href="https://github.com/OpenMathLib/OpenBLAS">OpenBLAS</a></h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.intel.com/content/www/us/en/docs/onemkl/developer-reference-c/2023-2/cblas-gemm-001.html#GUID-97718E5C-6E0A-44F0-B2B1-A551F0F164B2">intel cblas interface</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.apple.com/documentation/accelerate/1513264-cblas_sgemm?language=objc">apple 接口文档</a></li>
<li><code>sudo apt-get install libopenblas-dev</code></li>
<li><a target="_blank" rel="noopener" href="https://github.com/OpenMathLib/OpenBLAS?tab=readme-ov-file#usage">usage</a></li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export OPENBLAS_NUM_THREADS=4</span><br><span class="line">export GOTO_NUM_THREADS=4</span><br><span class="line">export OMP_NUM_THREADS=4</span><br></pre></td></tr></table></figure>

<ol>
<li>If you compile this library with USE_OPENMP&#x3D;1, you should set the OMP_NUM_THREADS environment variable; OpenBLAS ignores OPENBLAS_NUM_THREADS and GOTO_NUM_THREADS when compiled with USE_OPENMP&#x3D;1.</li>
<li>For a general introduction to the BLAS routines, please refer to the extensive documentation of their reference implementation hosted at netlib: <a target="_blank" rel="noopener" href="https://www.netlib.org/blas">https://www.netlib.org/blas</a>. On that site you will likewise find documentation for the reference implementation of the higher-level library <code>LAPACK - the Linear Algebra Package</code> that comes included with OpenBLAS</li>
<li><a target="_blank" rel="noopener" href="https://github.com/OpenMathLib/OpenBLAS/blob/develop/docs/distributing.md#performance-and-runtime-behavior-related-build-options">performance-and-runtime-behavior-related-build-options</a></li>
<li>openblas.pc 中可以看到 <code>USE_OPENMP=0 MAX_THREADS=24</code></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1609.00076.pdf">论文</a></li>
</ol>
<h2 id="blis"><a href="#blis" class="headerlink" title="blis"></a><a target="_blank" rel="noopener" href="https://github.com/flame/blis">blis</a></h2><h2 id="hipBLAS"><a href="#hipBLAS" class="headerlink" title="hipBLAS"></a><a target="_blank" rel="noopener" href="https://github.com/ROCm/hipBLAS">hipBLAS</a></h2><h2 id="Intel-oneMKL"><a href="#Intel-oneMKL" class="headerlink" title="Intel oneMKL"></a><a target="_blank" rel="noopener" href="https://github.com/oneapi-src/oneMKL">Intel oneMKL</a></h2><ol>
<li>oneAPI Math Kernel Library (oneMKL)</li>
<li><a target="_blank" rel="noopener" href="https://github.com/oneapi-src/oneMKL?tab=readme-ov-file#supported-configurations">统一多种 blas</a></li>
</ol>
<h2 id="CLBlast"><a href="#CLBlast" class="headerlink" title="CLBlast"></a><a target="_blank" rel="noopener" href="https://github.com/CNugteren/CLBlast">CLBlast</a></h2>
      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/01/10/llama-cpp/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/10/llama-cpp/" class="post-title-link" itemprop="url">llama.cpp</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-01-10 17:07:30" itemprop="dateCreated datePublished" datetime="2024-01-10T17:07:30+08:00">2024-01-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-01-14 18:48:11" itemprop="dateModified" datetime="2024-01-14T18:48:11+08:00">2024-01-14</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/01/10/llama-cpp/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/01/10/llama-cpp/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="语言模型质量评测"><a href="#语言模型质量评测" class="headerlink" title="语言模型质量评测"></a>语言模型质量评测</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#perplexity-measuring-model-quality">perplexity-measuring-model-quality</a></li>
<li><code>./perplexity -m models/7B/ggml-model-q4_0.gguf -f wiki.test.raw</code><ul>
<li>使用 openblas 版本</li>
<li><code>OMP_NUM_THREADS=16 OPENBLAS_NUM_THREADS=16 ./build/bin/perplexity -m ../llama/llama-2-7b/ggml-model-q4_0.gguf -f wikitext-2-raw/wiki.test.raw</code></li>
</ul>
</li>
<li>7b q4_0 result:</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">llama_print_timings: prompt eval time = 11159747.56 ms / 335360 tokens (   33.28 ms per token,    30.05 tokens per second)</span><br><span class="line">Final estimate: PPL = 5.9621 +/- 0.03348</span><br></pre></td></tr></table></figure>

<ol>
<li>7b fp16 result:</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">llama_print_timings: prompt eval time = 9850067.93 ms / 335360 tokens (   29.37 ms per token,    34.05 tokens per second)</span><br><span class="line">Final estimate: PPL = 5.7962 +/- 0.03235</span><br></pre></td></tr></table></figure>

<h2 id="run-on-cpu"><a href="#run-on-cpu" class="headerlink" title="run on cpu"></a>run on cpu</h2><ol>
<li>build</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/ggerganov/llama.cpp</span><br><span class="line"><span class="built_in">cd</span> llama.cpp</span><br><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br><span class="line">cmake --build . --config Release -j16</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>convert llama2 model</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## llama repo下下载了7b模型</span></span><br><span class="line">python convert.py ../llama/llama-2-7b/</span><br><span class="line"></span><br><span class="line"><span class="comment">## q4_0量化</span></span><br><span class="line">./build/bin/quantize ../llama/llama-2-7b/ggml-model-f16.gguf ../llama/llama-2-7b/ggml-model-q4_0.gguf q4_0</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>run</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build/bin/main -m ../llama/llama-2-7b/ggml-model-f16.gguf -p <span class="string">&quot;Building a website can be done in 10 simple steps:\nStep 1:&quot;</span> -n 40 -e</span><br></pre></td></tr></table></figure>

<p>4.result</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">llama_print_timings:        load time =    2123.21 ms</span><br><span class="line">llama_print_timings:      sample time =      22.01 ms /    40 runs   (    0.55 ms per token,  1817.19 tokens per second)</span><br><span class="line">llama_print_timings: prompt eval time =     820.50 ms /    19 tokens (   43.18 ms per token,    23.16 tokens per second)</span><br><span class="line">llama_print_timings:        eval time =   15164.51 ms /    39 runs   (  388.83 ms per token,     2.57 tokens per second)</span><br><span class="line">llama_print_timings:       total time =   16023.55 ms</span><br></pre></td></tr></table></figure>

<h2 id="run-on-cpu-with-mpi-on"><a href="#run-on-cpu-with-mpi-on" class="headerlink" title="run on cpu with mpi on"></a>run on cpu with mpi on</h2><ol>
<li>build</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install libopenmpi-dev</span><br><span class="line">cmake -S . -B build -DLLAMA_MPI=ON</span><br><span class="line">cd build</span><br><span class="line">cmake --build . --config Release -j16</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>result</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">llama_print_timings:        load time =    2092.53 ms</span><br><span class="line">llama_print_timings:      sample time =      20.81 ms /    40 runs   (    0.52 ms per token,  1922.15 tokens per second)</span><br><span class="line">llama_print_timings: prompt eval time =     668.81 ms /    19 tokens (   35.20 ms per token,    28.41 tokens per second)</span><br><span class="line">llama_print_timings:        eval time =   10150.64 ms /    39 runs   (  260.27 ms per token,     3.84 tokens per second)</span><br><span class="line">llama_print_timings:       total time =   10855.62 ms</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># run with mulit core</span></span></span><br><span class="line">mpirun -np 8 ./build/bin/main -m ../llama/llama-2-7b/ggml-model-f16.gguf -p &quot;Building a website can be done in 10 simple steps:\nStep 1:&quot; -n 40 -e</span><br></pre></td></tr></table></figure>

<h2 id="openBlas"><a href="#openBlas" class="headerlink" title="openBlas"></a>openBlas</h2><ol>
<li>build</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd build</span><br><span class="line">cmake .. -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS</span><br><span class="line">cmake --build . --config Release -j16</span><br></pre></td></tr></table></figure>

<ol>
<li>result</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">./build/bin/main -m ../llama/llama-2-7b/ggml-model-f16.gguf -p <span class="string">&quot;Building a website can be done in 10 simple steps:\nStep 1:&quot;</span> -n 40 -e</span><br><span class="line">llama_print_timings:        load time =    2008.93 ms</span><br><span class="line">llama_print_timings:      sample time =      50.88 ms /   100 runs   (    0.51 ms per token,  1965.37 tokens per second)</span><br><span class="line">llama_print_timings: prompt <span class="built_in">eval</span> time =     647.12 ms /    19 tokens (   34.06 ms per token,    29.36 tokens per second)</span><br><span class="line">llama_print_timings:        <span class="built_in">eval</span> time =   25045.01 ms /    99 runs   (  252.98 ms per token,     3.95 tokens per second)</span><br><span class="line">llama_print_timings:       total time =   25779.49 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># OMP_NUM_THREADS=32 OPENBLAS_NUM_THREADS=32 都设置比较快</span></span><br><span class="line">OMP_NUM_THREADS=32 OPENBLAS_NUM_THREADS=32 ./build/bin/main -m ../llama/llama-2-7b/ggml-model-f16.gguf -p <span class="string">&quot;Building a website can be done in 10 simple steps:\nStep 1:&quot;</span> -n 40 -e</span><br><span class="line">llama_print_timings:        load time =    1977.25 ms</span><br><span class="line">llama_print_timings:      sample time =      19.27 ms /    40 runs   (    0.48 ms per token,  2076.20 tokens per second)</span><br><span class="line">llama_print_timings: prompt <span class="built_in">eval</span> time =     622.66 ms /    19 tokens (   32.77 ms per token,    30.51 tokens per second)</span><br><span class="line">llama_print_timings:        <span class="built_in">eval</span> time =    6629.75 ms /    39 runs   (  169.99 ms per token,     5.88 tokens per second)</span><br><span class="line">llama_print_timings:       total time =    7284.48 ms</span><br></pre></td></tr></table></figure>

<h2 id="cublas"><a href="#cublas" class="headerlink" title="cublas"></a>cublas</h2><ol>
<li>build</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1</span><br><span class="line">export PATH=/usr/local/cuda/bin/:$&#123;PATH&#125;</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake .. -DLLAMA_CUBLAS=ON</span><br><span class="line">cmake --build . --config Release -j16</span><br></pre></td></tr></table></figure>

<ol>
<li>result</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">llama_print_timings:        load time =    2225.07 ms</span><br><span class="line">llama_print_timings:      sample time =      21.19 ms /    40 runs   (    0.53 ms per token,  1887.33 tokens per second)</span><br><span class="line">llama_print_timings: prompt eval time =     814.74 ms /    19 tokens (   42.88 ms per token,    23.32 tokens per second)</span><br><span class="line">llama_print_timings:        eval time =   10324.64 ms /    39 runs   (  264.73 ms per token,     3.78 tokens per second)</span><br><span class="line">llama_print_timings:       total time =   11176.41 ms</span><br></pre></td></tr></table></figure>

<ol>
<li>q4_0 result</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">llama_print_timings:        load time =    1097.19 ms</span><br><span class="line">llama_print_timings:      sample time =      22.74 ms /    40 runs   (    0.57 ms per token,  1759.25 tokens per second)</span><br><span class="line">llama_print_timings: prompt eval time =     780.84 ms /    19 tokens (   41.10 ms per token,    24.33 tokens per second)</span><br><span class="line">llama_print_timings:        eval time =    6715.37 ms /    39 runs   (  172.19 ms per token,     5.81 tokens per second)</span><br><span class="line">llama_print_timings:       total time =    7536.04 ms</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/01/10/model-compression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/10/model-compression/" class="post-title-link" itemprop="url">model-compression</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-01-10 16:27:54 / 修改时间：16:36:12" itemprop="dateCreated datePublished" datetime="2024-01-10T16:27:54+08:00">2024-01-10</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/01/10/model-compression/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/01/10/model-compression/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>132</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="quantization-量化"><a href="#quantization-量化" class="headerlink" title="quantization 量化"></a>quantization 量化</h2><h2 id="pruning-剪枝"><a href="#pruning-剪枝" class="headerlink" title="pruning 剪枝"></a>pruning 剪枝</h2><h3 id="稀疏化"><a href="#稀疏化" class="headerlink" title="稀疏化"></a>稀疏化</h3><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/u013010889/article/details/53305595">scipy csr_matrix 和 csc_matrix 函数详解</a></li>
</ol>
<h2 id="knowledge-distillation-蒸馏"><a href="#knowledge-distillation-蒸馏" class="headerlink" title="knowledge distillation 蒸馏"></a>knowledge distillation 蒸馏</h2><h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://xailient.com/blog/4-popular-model-compression-techniques-explained/">4-popular-model-compression-techniques-explained</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/01/04/huggingface/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/04/huggingface/" class="post-title-link" itemprop="url">huggingface</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-01-04 11:22:30" itemprop="dateCreated datePublished" datetime="2024-01-04T11:22:30+08:00">2024-01-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-01-08 17:58:42" itemprop="dateModified" datetime="2024-01-08T17:58:42+08:00">2024-01-08</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/01/04/huggingface/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/01/04/huggingface/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main_classes/pipelines">pipeline</a></h2><ol>
<li>The pipelines are a great and easy way to use models for inference.</li>
<li>llama2</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use a pipeline as a high-level helper</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line">pipe = pipeline(<span class="string">&quot;text-generation&quot;</span>, model=<span class="string">&quot;./Llama-2-7b-hf&quot;</span>)</span><br><span class="line">pipe(<span class="string">&quot;how are you&quot;</span>)</span><br><span class="line"><span class="comment"># 查看帮助</span></span><br><span class="line"><span class="built_in">help</span>(pipeline)</span><br><span class="line"><span class="built_in">help</span>(pipe)</span><br></pre></td></tr></table></figure>

<h2 id="查看模型信息"><a href="#查看模型信息" class="headerlink" title="查看模型信息"></a>查看模型信息</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/saratbhargava/ai-blog-resources/blob/main/LLM/Llama_2_param_count.ipynb">基础信息</a></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load model directly</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM</span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;./Llama-2-7b-hf&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"><span class="keyword">from</span> prettytable <span class="keyword">import</span> PrettyTable</span><br><span class="line"></span><br><span class="line">table = PrettyTable([<span class="string">&#x27;Name&#x27;</span>, <span class="string">&#x27;Shape&#x27;</span>, <span class="string">&#x27;Param&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    param_count = param.numel()</span><br><span class="line">    table.add_row([name, param.shape, param_count])</span><br><span class="line"><span class="built_in">print</span>(table)</span><br><span class="line">num_parameters = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters())</span><br><span class="line"><span class="built_in">print</span>(num_parameters)</span><br></pre></td></tr></table></figure>

<h2 id="models"><a href="#models" class="headerlink" title="models"></a><a target="_blank" rel="noopener" href="https://huggingface.co/models">models</a></h2><ol>
<li>repo 包含<ul>
<li>config.json 每个架构一个 config.json <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/model_doc/llama2#transformers.LlamaConfig">llama config</a></li>
</ul>
</li>
</ol>
<h2 id="模型文件类型"><a href="#模型文件类型" class="headerlink" title="模型文件类型"></a><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/620641385/answer/3230090109">模型文件类型</a></h2><ol>
<li>支持 bin 或 safetensors 文件</li>
<li>safetensors 是谷歌开发的一种 TensorFlow Lite 模型文件格式，用于在移动设备上运行模型</li>
<li>bin 文件自存储模型的参数，不包含</li>
<li>pytorch 两种方式<ul>
<li>保存整个模型：保存整个模型的结构（代码）、参数 <code>torch.save(model, &#39;model.pth&#39;)</code></li>
<li>保存模型参数：仅保存模型的参数，而不保存模型的结构（代码）。<code>torch.save(model.state_dict(), &#39;model_params.pth&#39;</code></li>
</ul>
</li>
<li>有些模型保存未 gguf 格式，需要专门推理引擎才能使用</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md">gguf doc</a></li>
<li>gguf：It is a successor file format to GGML, GGMF and GGJT, and is designed to be unambiguous by containing all the information needed to load a model. It is also designed to be extensible, so that new features can be added to GGML without breaking compatibility with older models.</li>
<li>The .bin files that are used by llama.cpp allow users to easily share models in a single file. Except they had one big problem: lack of flexibility. You could not add additional information about the model.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/discussions/2948">hugging face models to gguf</a></li>
</ol>
<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/v4.36.1/zh/index">transformers 中文文档</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/blog/zh/llama2">blog</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/blog/zh/llama2">Llama 2 来袭 - 在 Hugging Face 上玩转它</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/01/01/tensorrt-llm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/01/tensorrt-llm/" class="post-title-link" itemprop="url">tensorrt-llm</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-01-01 20:33:30" itemprop="dateCreated datePublished" datetime="2024-01-01T20:33:30+08:00">2024-01-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-02-27 11:02:17" itemprop="dateModified" datetime="2024-02-27T11:02:17+08:00">2024-02-27</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/01/01/tensorrt-llm/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/01/01/tensorrt-llm/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>5.6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="base"><a href="#base" class="headerlink" title="base"></a>base</h2><ol>
<li><code>A TensorRT Toolbox for Optimized Large Language Model Inference</code></li>
<li>tensorrt-llm 使用了很多 tensorrt 插件，插件里调用 kernel <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture.md#plugins">link</a></li>
<li>大模型优化方法：tensorrt + plugins, 耗时的算子用专有库进行替换，如矩阵乘用 cutlass</li>
<li>TensorRT-LLM wraps TensorRT’s deep learning compiler—which includes optimized kernels from FasterTransformer, pre- and post-processing, and multi-GPU and multi-node communication—in a simple open-source Python API for defining, optimizing, and executing LLMs for inference in production.</li>
<li><a target="_blank" rel="noopener" href="https://nvidia.github.io/TensorRT-LLM/gpt_runtime.html#generation">generation</a></li>
<li><a target="_blank" rel="noopener" href="https://nvidia.github.io/TensorRT-LLM/batch_manager.html#gptmanager-design">gptmanager</a></li>
<li>TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines.</li>
<li><a target="_blank" rel="noopener" href="https://nvidia.github.io/TensorRT-LLM/">doc</a></li>
<li>TensorRT-LLM includes an API to implement Python and C++ runtimes. The role of the runtime components is to load the TensorRT engines and drive their execution. Typically, for an auto-regressive model like GPT, the runtime is in charge of loading the engine that implements <code>both the processing of the input sequence as well as the body of the generation loop</code>.</li>
<li>TensorRT-LLM 的运行时需要管理：<ul>
<li>模型加载及执行</li>
<li>对于 AR 模型，两阶段(prompt 和 decoding)都需要处理</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/gpt_runtime.md">decode only 执行流程</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/gpt_attention.md">attention 流程</a><ul>
<li>As a quick reminder, the multihead attention is the sequence of a batched matmul, a softmax and another batched matmul described in the Attention Is All You Need article. attention 计算是 batch 的</li>
<li>the current implementation supports two input modes: Padded and packed (non-padded). As the packed mode is always more memory-efficient and faster than the padded mode, support for padded mode may be removed in the future. 打包模式更节省内存并且更快</li>
</ul>
</li>
<li>重算可以不用考虑 kv cache, uniad 只考虑 5 帧：前 4 帧运行时 padding 到 5 帧，重算 attention; 第六帧开始，recompute sliding window; 只考虑窗口的 5 帧，位置编码一直都是 0-5（位置改变，kv cache 需要重新计算)。 普通的 sliding windows 都是</li>
</ol>
<h2 id="best-practice"><a href="#best-practice" class="headerlink" title="best practice"></a><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/perf_best_practices.md">best practice</a></h2><h2 id="memory"><a href="#memory" class="headerlink" title="memory"></a>memory</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/memory.md">显存使用</a></li>
</ol>
<h2 id="key-features"><a href="#key-features" class="headerlink" title="key-features"></a>key-features</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM/tree/main?tab=readme-ov-file#key-features">key-features 可学习如何优化</a></li>
</ol>
<h3 id="in-flight-batching"><a href="#in-flight-batching" class="headerlink" title="in-flight batching"></a>in-flight batching</h3><h3 id="paged-attention"><a href="#paged-attention" class="headerlink" title="paged attention"></a>paged attention</h3><h3 id="MHA-GQA-MQA"><a href="#MHA-GQA-MQA" class="headerlink" title="MHA GQA MQA"></a>MHA GQA MQA</h3><h3 id="Quantization"><a href="#Quantization" class="headerlink" title="Quantization"></a><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/precision.md">Quantization</a></h3><h3 id="Graph-Rewriting"><a href="#Graph-Rewriting" class="headerlink" title="Graph Rewriting"></a>Graph Rewriting</h3><ol>
<li>TensorRT-LLM 在将 LLM 模型编译为 TensorRT Engines 时会对神经网络进行优化，提升执行效率。</li>
</ol>
<h2 id="deubg"><a href="#deubg" class="headerlink" title="deubg"></a>deubg</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM/blob/6cc5e177ff2fb60b1aab3b03fa0534b5181cf0f1/cpp/tensorrt_llm/common/logger.cpp#L32">TLLM_LOG_LEVEL&#x3D;TRACE</a></li>
<li>打开 trace， 跟踪代码执行</li>
</ol>
<h2 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h2><ol>
<li><a target="_blank" rel="noopener" href="https://hub.docker.com/search?q=tensorrt_llm">docker hubs</a></li>
</ol>
<h2 id="install"><a href="#install" class="headerlink" title="install"></a>install</h2><ol>
<li>使用 docker <a target="_blank" rel="noopener" href="https://hub.docker.com/r/baseten/tensorrt_llm-release">baseten&#x2F;tensorrt_llm-release</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM/blob/v0.7.1/docs/source/installation.md#fetch-the-sources">fetch-the-sources</a> in docker</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM/blob/v0.7.1/docs/source/installation.md#build-tensorrt-llm">build-tensorrt-llm</a></li>
<li>可能需要先卸载 <code>pip uninstall tensorrt_llm</code>， 重新安装</li>
</ol>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><h3 id="ccdv-cnn-dailymail"><a href="#ccdv-cnn-dailymail" class="headerlink" title="ccdv&#x2F;cnn_dailymail"></a><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ccdv/cnn_dailymail">ccdv&#x2F;cnn_dailymail</a></h3><ol>
<li>gitee 镜像版本不太好使</li>
<li>下载之后传到服务器</li>
<li><a target="_blank" rel="noopener" href="https://github.com/abisee/cnn-dailymail/tree/master/url_lists">clone txt</a></li>
<li>修改 summarize.py；从本地 load 数据集</li>
<li><code>dataset = load_dataset(&quot;/mnt/data-2/home/xiyang.jia/TensorRT-LLM/examples/bloom/cnn_dailymail/cnn_dailymail.py&quot;, &quot;3.0.0&quot;)</code> 从本地加载数据集</li>
</ol>
<h2 id="vscode-setting"><a href="#vscode-setting" class="headerlink" title="vscode setting"></a>vscode setting</h2><ol>
<li>env settings</li>
</ol>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;env&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;PYDEVD_WARN_EVALUATION_TIMEOUT&quot;</span><span class="punctuation">:</span> <span class="string">&quot;500&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION&quot;</span><span class="punctuation">:</span> <span class="string">&quot;python&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;OPAL_PREFIX&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/opt/hpcx/ompi&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure>

<ol>
<li>launch.json</li>
</ol>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.2.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;configurations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Python: tensorrt&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;python&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;request&quot;</span><span class="punctuation">:</span> <span class="string">&quot;launch&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="comment">// &quot;program&quot;: &quot;$&#123;workspaceFolder&#125;/examples/summarize.py&quot;,</span></span><br><span class="line">      <span class="comment">// &quot;cwd&quot;: &quot;$&#123;workspaceFolder&#125;/examples/bloom/&quot;,</span></span><br><span class="line">      <span class="comment">// &quot;program&quot;: &quot;../summarize.py&quot;,</span></span><br><span class="line">      <span class="comment">// &quot;args&quot;: [</span></span><br><span class="line">      <span class="comment">//     &quot;--test_trt_llm&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;--hf_model_dir&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;./bloom/560M/&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;--data_type&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;fp16&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;--engine_dir&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;./bloom/560M/trt_engines/fp16/1-gpu/&quot;</span></span><br><span class="line">      <span class="comment">// ],</span></span><br><span class="line">      <span class="comment">// run llama test</span></span><br><span class="line">      <span class="attr">&quot;cwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;/examples/llama/&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;program&quot;</span><span class="punctuation">:</span> <span class="string">&quot;../run.py&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;args&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="string">&quot;--max_output_len=50&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;--tokenizer_dir&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;/mnt/data-2/home/xiyang.jia/Llama-2-7b-hf&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;--engine_dir=./tmp/llama/7B/trt_engines/fp16/1-gpu/&quot;</span></span><br><span class="line">      <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="comment">// build llama test</span></span><br><span class="line">      <span class="comment">// &quot;cwd&quot;: &quot;$&#123;workspaceFolder&#125;/examples/llama/&quot;,</span></span><br><span class="line">      <span class="comment">// &quot;program&quot;: &quot;./build.py&quot;,</span></span><br><span class="line">      <span class="comment">// &quot;args&quot;: [</span></span><br><span class="line">      <span class="comment">//     &quot;--model_dir&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;/mnt/data-2/home/xiyang.jia/Llama-2-7b-hf&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;--dtype&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;float16&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;--remove_input_padding&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;--use_gpt_attention_plugin&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;float16&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;--enable_context_fmha&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;--use_gemm_plugin&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;float16&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;--output_dir&quot;,</span></span><br><span class="line">      <span class="comment">//     &quot;./tmp/llama/7B/trt_engines/fp16/test/&quot;</span></span><br><span class="line">      <span class="comment">// ],</span></span><br><span class="line">      <span class="attr">&quot;env&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;PYDEVD_WARN_EVALUATION_TIMEOUT&quot;</span><span class="punctuation">:</span> <span class="string">&quot;500&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION&quot;</span><span class="punctuation">:</span> <span class="string">&quot;python&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;OPAL_PREFIX&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/opt/hpcx/ompi&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="punctuation">:</span> <span class="string">&quot;6,7&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;TLLM_LOG_LEVEL&quot;</span><span class="punctuation">:</span> <span class="string">&quot;DEBUG&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;console&quot;</span><span class="punctuation">:</span> <span class="string">&quot;integratedTerminal&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;justMyCode&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h2 id="examples"><a href="#examples" class="headerlink" title="examples"></a>examples</h2><ol>
<li><code>build.py</code> 每个 example 独占，用于构建模型和编译成 engine 模型; 需要模型配置：权重等</li>
<li>In addition, there are two shared files in the parent folder examples for inference and evaluation:<ul>
<li><code>run.py</code> to run the inference on an input text; 运行推理</li>
<li><code>summarize.py</code> to summarize the articles in the cnn_dailymail dataset. 总结故事</li>
</ul>
</li>
<li>需要 hf model; 例:llama-7B-hf; hf 是 Huggingface 对原始 llama-7B 模型的打包版本,</li>
<li>TensorRT-LLM LLaMA builds TensorRT engine(s) from HF checkpoint. If no checkpoint directory is specified, TensorRT-LLM will build engine(s) with dummy weights. 如果不设置 <code>--model_dir</code>, 使用随机权重</li>
<li>llama sample 运行时需要 config.json, <code>--tokenizer_dir</code>指定</li>
<li><a target="_blank" rel="noopener" href="https://gitee.com/hf-models/Llama-2-7b-hf">gitee.com&#x2F;hf-models&#x2F;Llama-2-7b-hf</a> gitee 下载要快很多 <code>git lfs clone https://gitee.com/hf-models/Llama-2-7b-hf</code></li>
<li>run 的时候加参数<code>--temperature=0.6 --top_k=10</code>可生成不一样内容</li>
<li><code>/usr/local/tensorrt/bin/trtexec --loadEngine=tmp/llama/7B/trt_engines/fp16/1-gpu/llama_float16_tp1_rank0.engine</code>会出错，未解决<ul>
<li><code>export LD_LIBRARY_PATH=/usr/local/tensorrt/lib/:$LD_LIBRARY_PATH</code></li>
</ul>
</li>
<li><code> Error[1]: [stdArchiveReader.cpp::stdArchiveReaderInitCommon::47] Error Code 1: Serialization (Serialization assertion stdVersionRead == kSERIALIZATION_VERSION failed.Version tag does not match. Note: Current Version: 226, Serialized Engine Version: 228)</code> tensorrt 版本有问题</li>
<li><a target="_blank" rel="noopener" href="https://github.com/hpcaitech/SwiftInfer?tab=readme-ov-file">tensorrt llm llama</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/20/">20</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="贾夕阳"
      src="/images/coder2.jpg">
  <p class="site-author-name" itemprop="name">贾夕阳</p>
  <div class="site-description" itemprop="description">深度学习/自动驾驶/C++/性能优化</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">196</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jiaxiyang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jiaxiyang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2025/12/29/claude-code/" title="2025&#x2F;12&#x2F;29&#x2F;claude-code&#x2F;">Claude Code</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2025/08/20/AI-coding/" title="2025&#x2F;08&#x2F;20&#x2F;AI-coding&#x2F;">AI coding</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2025/04/28/Architecture/" title="2025&#x2F;04&#x2F;28&#x2F;Architecture&#x2F;">Computer Architecture</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2025/04/18/pytest/" title="2025&#x2F;04&#x2F;18&#x2F;pytest&#x2F;">pytest</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2025/01/18/cursor/" title="2025&#x2F;01&#x2F;18&#x2F;cursor&#x2F;">cursor</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">贾夕阳</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">625k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">9:29</span>
</div>

<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("06/26/2020 14:52:10");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : '[object Object]',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://jiaxiyang.github.io/page/5/',]
      });
      });
  </script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'g32ipLmEye1u5l6wBGRJt03S-gzGzoHsz',
      appKey     : 'zHgLkAICsZUl9Mf8LfdoVigP',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

  

  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>





 
</body>
</html>

