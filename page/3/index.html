<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0-rc2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jiaxiyang.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="深度学习&#x2F;自动驾驶&#x2F;C++&#x2F;性能优化">
<meta property="og:type" content="website">
<meta property="og:title" content="Xiyang">
<meta property="og:url" content="https://jiaxiyang.github.io/page/3/index.html">
<meta property="og:site_name" content="Xiyang">
<meta property="og:description" content="深度学习&#x2F;自动驾驶&#x2F;C++&#x2F;性能优化">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="贾夕阳">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://jiaxiyang.github.io/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Xiyang</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WGS6S6YFJ6"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-WGS6S6YFJ6');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Xiyang</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Think twice, code once!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">166</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">44</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">55</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/jiaxiyang" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2023/12/30/gpt/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/30/gpt/" class="post-title-link" itemprop="url">gpt</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-12-30 21:01:57 / 修改时间：21:02:26" itemprop="dateCreated datePublished" datetime="2023-12-30T21:01:57+08:00">2023-12-30</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2023/12/30/gpt/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/12/30/gpt/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>26</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/openai/gpt-2">openai&#x2F;gpt-2 开源代码</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2023/12/26/jupyter/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/26/jupyter/" class="post-title-link" itemprop="url">jupyter</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-12-26 11:51:34 / 修改时间：15:57:05" itemprop="dateCreated datePublished" datetime="2023-12-26T11:51:34+08:00">2023-12-26</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2023/12/26/jupyter/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/12/26/jupyter/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>228</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="base"><a href="#base" class="headerlink" title="base"></a>base</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda create --name d2l python=3.9 -y</span><br><span class="line">conda activate d2l</span><br><span class="line">pip install torch==1.12.0 torchvision==0.13.0 d2l==0.17.6 RICE</span><br><span class="line">jupyter notebook <span class="comment"># 映射端口号 注意不用使用机器自带jupyter</span></span><br><span class="line">jupyter notebook --port 5900 <span class="comment"># 映射端口号, 可能不好使</span></span><br></pre></td></tr></table></figure>

<h2 id="shortkeys"><a href="#shortkeys" class="headerlink" title="shortkeys"></a>shortkeys</h2><ol>
<li><code>S-Enter</code> 运行并到下一 cell</li>
<li><code>C-Enter</code> 运行 code</li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2023/12/24/llama2-c/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/24/llama2-c/" class="post-title-link" itemprop="url">llama2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-12-24 18:23:45" itemprop="dateCreated datePublished" datetime="2023-12-24T18:23:45+08:00">2023-12-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-12-27 19:38:39" itemprop="dateModified" datetime="2023-12-27T19:38:39+08:00">2023-12-27</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2023/12/24/llama2-c/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/12/24/llama2-c/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="base"><a href="#base" class="headerlink" title="base"></a>base</h2><h3 id="llama2-参数量计算"><a href="#llama2-参数量计算" class="headerlink" title="llama2 参数量计算"></a>llama2 参数量计算</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/649125936">LLAMA2 的参数计算</a></li>
<li><code>窗口长度, 上下文长度(Context Length)</code>指的是模型在做预测时能够看到的上下文单词的数量。具体来说,在 transformer 等注意力机制的模型中,输入是一个定长的窗口,窗口中的每个词会通过自注意力机制关联到窗口中的其他词。这个窗口的长度就是上下文长度。</li>
<li><code>词向量</code>：embeding 层的输出就是词向量</li>
<li>推理时间复杂度：整体级别是 l<em>n</em>d^2+l<em>d</em>n^2，其中：d 是词向量维度，n 是窗口或序列长度，l：层数<ul>
<li>无论是窗口长度还是词向量维度，都会让推理时间呈平方指数级上升。</li>
<li>层数对推理时间的影响是线性的。</li>
<li>而词表大小对推理时间基本没有影响。</li>
</ul>
</li>
</ol>
<h3 id="长文本-输入输出更长-token"><a href="#长文本-输入输出更长-token" class="headerlink" title="长文本(输入输出更长 token)"></a>长文本(输入输出更长 token)</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/657210829">大模型长文本建模的难点与方案</a></li>
<li><a target="_blank" rel="noopener" href="https://36kr.com/p/2470939687950470">卷完参数后，大模型公司又盯上了“长文本”？</a></li>
</ol>
<h2 id="llama2-c"><a href="#llama2-c" class="headerlink" title="llama2.c"></a><a target="_blank" rel="noopener" href="https://github.com/karpathy/llama2.c">llama2.c</a></h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=zjkBMFhNj_g">Intro to Large Language Models</a> ka</li>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama/blob/main/llama/model.py">python 推理代码</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=oM4VmoabDAI">Coding LLaMA 2 from scratch in PyTorch - KV Cache, Grouped Query Attention, Rotary PE, RMSNorm</a></li>
<li>build and run</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/karpathy/llama2.c.git</span><br><span class="line"><span class="built_in">cd</span> llama2.c</span><br><span class="line">make run; make rundebug</span><br><span class="line">wget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin</span><br><span class="line">./run stories15M.bin</span><br><span class="line">make runfast  <span class="comment"># 加速运行， 80 tok/s =&gt; 180 tok/s</span></span><br><span class="line">make runomp &amp;&amp; OMP_NUM_THREADS=4 ./run out/model.bin <span class="comment"># 多核加速， from 80 tok/s =&gt; 320 tok/s 主要加速matmul</span></span><br></pre></td></tr></table></figure>

<h2 id="代码理解"><a href="#代码理解" class="headerlink" title="代码理解"></a>代码理解</h2><ol>
<li><code>Tokenizer, Transformer, Sampler</code> 三大部分</li>
<li>tokenizer 也是一个模型，需要训练</li>
<li>Tokenizer 的主要作用是将自然语言文本转换为机器学习模型可以理解的格式。这通常意味着将文本拆分成词汇、子词或字符单元（即 tokens），然后将这些 tokens 转换为数字 ID。这些 ID 对应于模型的词汇表中的索引。</li>
<li>当有 prompt 时，有预热过程，从第一个 prompt_tokens 开始推理，if we are still processing the input prompt, force the next prompt token， otherwise sample the next token from the logits</li>
<li>Sampler 的主要作用是根据模型输出的 logits（未归一化的概率对数）来决定下一个生成的 token</li>
<li>Temperature Scaling：通过温度参数调整 logits。温度较低（&lt;1）会使输出分布更加尖锐（更确定），温度较高（&gt;1）则使分布更平滑（更随机）</li>
<li>采样策略对生成文本的质量和多样性有显著影响。例如，argmax 采样可能导致非常重复和可预测的文本，而合适的随机采样可以增加多样性和创造性，同时保持文本的连贯性和可读性。通过合理配置 temperature 和 topp 参数，可以在随机性和确定性之间找到平衡，生成符合预期的文本。</li>
<li>top-p 采样：<ul>
<li>模型输出的 logits（未归一化的概率对数），转换为 softmax 概率</li>
<li>创建一个结构体数组，每个元素包含 token 的索引和对应的概率。</li>
<li>概率小于 cutoff 过滤掉</li>
<li>排序</li>
<li>计算累计概率， 累计概率超过 topp 后不选择</li>
<li>随机数*累计概率；找到这个随机数对应的累积概率区间内的 token。</li>
<li>返回所选 token 的索引</li>
<li>控制生成质量：通过调整 p 的值，可以控制生成文本的随机性和确定性。较低的 p 值会导致更确定性的输出（更少的 token 可供选择），而较高的 p 值会增加输出的多样性。</li>
<li>避免不合理的 token：Top-p 采样可以有效地避免选择那些极不可能的 token，这对于生成更加连贯和自然的文本至关重要。</li>
</ul>
</li>
<li>前馈神经网络（Feed-Forward Neural Network, <code>FFN</code>）</li>
<li><code>forward</code> 跟 all you need is attention 论文描述的结构非常像</li>
<li>相对位置编码（RoPE）：模型利用 RoPE 来给序列中的每个 token 引入位置信息。这种方式与原始 Transformer 中的绝对位置编码不同。</li>
<li>对于每个输入 token， 生成 q, k, v, q 用来和之前的 k 做 attention 得到 scores, scores 和 k 做加权得到多头输出; 缓存 k, v 用于之后的推理</li>
<li>q 和之前所有 k 做 attension, 结果在和 v 做加权</li>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/method/grouped-query-attention">grouped-query-attention</a></li>
<li>Config 中通过 n_heads 和 n_kv_heads 可以看出是 multi-head, group-query, multi-query； 相同时（不为 1）是 multi-head, 都为 1 时是 multi-query; n_kv_heads &lt; n_heads 时是 group_query</li>
<li>MQA，全称 Multi Query Attention, 而 GQA 则是前段时间 Google 提出的 MQA 变种，全称 Group-Query Attention。MHA（Multi-head Attention）是标准的多头注意力机制，h 个 Query、Key 和 Value 矩阵。</li>
<li>推理的过程是一个自回归的过程，也就是说前 i 次的 token 会作为第 i+1 次的预测数据送入模型，拿到第 i+1 次的推理 token。</li>
<li>embedding 还能起到降维的作用，将 one-hot 的[s,vocab_size]大小变成了[s,d]。</li>
<li>在大多数基于 Transformer 的模型中，embedding 层输出的词向量维度通常与 Transformer 的隐藏层(attention layer)维度（也称为 Transformer dimension）相匹配。</li>
<li>Config 中的 dim 和 hidden_dim:<ul>
<li>dim（Transformer Dimension）: 这个参数指的是 Transformer 模型中主要的隐藏层维度。在标准的 Transformer 模型中，这包括自注意力层（Self-Attention Layer）的输出维度和 embedding 层的维度。因此，dim 通常与 embedding 层输出的词向量的大小相匹配。</li>
<li>hidden_dim（Feed-Forward Network Dimension）: 这个参数指的是 Transformer 模型中前馈网络（Feed-Forward Network, FFN）层的内部隐藏层的维度。FFN 是 Transformer 每个注意力层之后的一个子层，它的维度通常与主要隐藏层维度不同。这个维度通常更大，用于在模型中引入额外的非线性。</li>
</ul>
</li>
<li>Config 中的窗口长度等于 seq_len， 词向量维度等于 dim</li>
</ol>
<h3 id="embedding-和-tokenizer-的区别"><a href="#embedding-和-tokenizer-的区别" class="headerlink" title="embedding 和 tokenizer 的区别"></a>embedding 和 tokenizer 的区别</h3><ol>
<li>Tokenizer 是第一步：它将原始文本转换为一系列的 token 索引。Embedding 是第二步：利用这些索引在 embedding 层中查找或生成每个 token 的向量表示。互补关系：Tokenizer 和 embedding 层一起工作，将自然语言文本转换为机器学习模型可以有效处理的数值形式。</li>
<li>tokenizer 负责将文本转换为一系列的 token，而 embedding 则负责将这些 token 转换为机器学习模型可以理解的语义向量。</li>
<li>通过 token_embedding_table 查找 token 对应的的向量</li>
</ol>
<h3 id="模型基本结构"><a href="#模型基本结构" class="headerlink" title="模型基本结构"></a>模型基本结构</h3><ol>
<li>层的堆叠：模型由多个相同的层堆叠而成，每层包含两个主要子模块：多头自注意力（Multi-Head Self-Attention）和前馈神经网络（Feed-Forward Neural Network, FFN）。</li>
</ol>
<h2 id="others"><a href="#others" class="headerlink" title="others"></a>others</h2><ol>
<li>openmp 加速会获得较大加速比, 编译选项加<code>-fopenmp -march=native</code></li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">matmul</span><span class="params">(<span class="type">float</span>* xout, <span class="type">float</span>* x, <span class="type">float</span>* w, <span class="type">int</span> n, <span class="type">int</span> d)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// W (d,n) @ x (n,) -&gt; xout (d,)</span></span><br><span class="line">    <span class="comment">// by far the most amount of time is spent inside this little function</span></span><br><span class="line">    <span class="type">int</span> i;</span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> omp parallel for private(i)</span></span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; d; i++) &#123;</span><br><span class="line">        <span class="type">float</span> val = <span class="number">0.0f</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; n; j++) &#123;</span><br><span class="line">            val += w[i * n + j] * x[j];</span><br><span class="line">        &#125;</span><br><span class="line">        xout[i] = val;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://chat.openai.com/c/4e520557-e827-40c5-9357-db76b5a5e6ba">chatgpt 解释</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/651248009">一文看懂 llama2(原理,模型,训练)</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/83814532">Embedding 的作用</a></li>
<li><a target="_blank" rel="noopener" href="https://kexue.fm/archives/9529">为什么现在的 LLM 都是 Decoder-only 的架构？</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2023/12/21/deep-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/21/deep-learning/" class="post-title-link" itemprop="url">deep-learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-12-21 13:18:24" itemprop="dateCreated datePublished" datetime="2023-12-21T13:18:24+08:00">2023-12-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-12-29 15:25:14" itemprop="dateModified" datetime="2023-12-29T15:25:14+08:00">2023-12-29</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2023/12/21/deep-learning/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/12/21/deep-learning/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><ol>
<li><p><code>Deep learning</code> is an approach to machine learning characterized by deep stacks of computations. This depth of computation is what has enabled deep learning models to disentangle the kinds of complex and hierarchical patterns found in the most challenging real-world datasets.</p>
</li>
<li><p><code>SGD</code>: 全称为 Stochastic Gradient Descent 即随机梯度下降,是机器学习中常用的优化算法,用于训练各种模型(如神经网络)寻找最优参数, optimizer</p>
</li>
<li><p><code>neuron</code> : the Linear Unit y &#x3D; wx + b; w: weight, b: bias</p>
</li>
<li><p><code>layers</code>: Neural networks typically organize their neurons into layers. When we collect together linear units having a common set of inputs we get a <code>dense layer</code>.</p>
</li>
<li><p>一个 layer 共享一个 bias: y &#x3D; w1 _ x1 + b1 + w2 _ x2 + b2 &#x3D;&#x3D;&gt; y &#x3D; w1 _ x1 + w2 _ x2 + b</p>
</li>
<li><p><code>ReLU</code>: rectified linear unit</p>
</li>
<li><p><code>Linear Unit + ReLU</code>: y &#x3D; max(0, w * x + b)</p>
</li>
<li><p>Without activation functions, neural networks can only learn linear relationships.</p>
</li>
<li><p>A <code>loss function</code> that measures how good the network’s predictions are.</p>
</li>
<li><p>An <code>optimizer</code> that can tell the network how to change its weights.</p>
</li>
<li><p><code>MAE</code>: mean absolute error; loss function, for regression</p>
</li>
<li><p>Each iteration’s sample of training data is called a <code>minibatch</code> (or often just “batch”), while a complete round of the training data is called an <code>epoch</code>.</p>
</li>
<li><p>The <code>learning rate</code> and the size of the <code>minibatches</code> are the two parameters that have the largest effect on how the SGD training proceeds.</p>
</li>
<li><p><code>Adam</code> is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is “self tuning”, in a sense). Adam is a great general-purpose optimizer.</p>
</li>
<li><p><code>Underfitting the training set</code> is when the loss is not as low as it could be because the model hasn’t learned enough signal.</p>
</li>
<li><p><code>Overfitting the training set</code> is when the loss is not as low as it could be because the model learned too much noise. The trick to training deep learning models is finding the best balance between the two.</p>
</li>
<li><p><code>Early Stopping</code>: stop the training whenever it seems the validation loss isn’t decreasing anymore. Interrupting the training this way is called early stopping. Once we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured.</p>
</li>
<li><p><code>dropout layer</code> we randomly drop out some fraction of a layer’s input units every step of training, making it much harder for the network to learn those spurious patterns in the training data. Instead, it has to search for broad, general patterns, whose weight patterns tend to be more robust. 可以纠正过拟合</p>
</li>
<li><p><code>Batch Normalization layer</code></p>
<ul>
<li>why? Features that tend to produce activations of very different sizes can make for unstable training behavior.</li>
<li>A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters.</li>
<li>做两次 normalize, 先基于输入的 batch 数据做， 后基于训练的均值和方差来做</li>
<li>Models with batchnorm tend to need fewer epochs to complete training. Moreover, batchnorm can also fix various problems that can cause the training to get “stuck”.</li>
<li>get better performance if you standardize your data before using it for training</li>
</ul>
</li>
<li><p>The main difference regression and classification is in the loss function we use and in what kind of outputs we want the final layer to produce. 主要区别是损失函数和最后一层的输出类型</p>
</li>
<li><p><code>Accuracy</code> is one of the many metrics in use for measuring success on a classification problem. Accuracy is the ratio of correct predictions to total predictions: <code>accuracy = number_correct / total</code></p>
</li>
<li><p><code>Cross-Entropy</code> 交叉熵</p>
<ul>
<li>Cross-entropy is a sort of measure for the distance from one probability distribution to another.</li>
<li>SGD needs a loss function that changes smoothly, but accuracy, being a ratio of counts, changes in “jumps”. So, we have to choose a substitute to act as the loss function. This substitute is the cross-entropy function.</li>
<li>With regression, our goal was to minimize the distance between the expected outcome and the predicted outcome. We chose MAE to measure this distance.</li>
<li>For classification, what we want instead is a distance between probabilities, and this is what cross-entropy provides.</li>
</ul>
</li>
<li><p><code>softmax</code> 也是激活函数， layer to layer; not functions of a single fold x; 在 softmax 函数的实现中减去最大值是一种数值稳定性的技巧。从所有输入值中减去同一个常数不会改变函数的输出。如果 x 很大，可能导致 exp(x)溢出</p>
</li>
<li><p><code>relu</code> 是 single x 的激活函数</p>
</li>
<li><p><code>MLP, CNN, RNN, Transformer</code> 四大深度学习架构 Multilayer Perceptron(MLP)</p>
</li>
<li><p>样本和特征, batch 是样本</p>
</li>
<li><p><code>正则化(Regularization)</code> 指的是在训练过程中添加额外信息以防止模型过度拟合的技术。</p>
<ul>
<li>L1 正则化:在损失函数中添加模型权重参数绝对值的和,使权重 decay 到 0,从而使模型更稀疏。</li>
<li>L2 正则化:在损失函数中添加模型权重参数平方和,惩罚大的参数值,使权重较为平均分布,避免个别权重参数过大。也称为权重衰减(weight decay)。</li>
<li>Early Stopping:在模型测试指标不再改善时中止训练,防止过拟合。</li>
<li>Dropout:以一定概率随机置部分节点为 0,增加模型泛化能力</li>
<li>Data Augmentation:人工生成更多训练数据,改善模型泛化能力。</li>
<li>Batch Normalization: 通过调整网络中间层的激活值，使其在训练时保持一个更稳定的分布。虽然其主要目的是加快训练过程，但它也有一定的正则化效果。</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html">Batch Normalization 计算</a></p>
<ul>
<li>全连接层<br>仿射变换和激活函数之间;对 minibatch 整体做 normalization</li>
<li>卷积<br>卷积层之后和非线性激活函数之前; 对每个通道分别做 normalization; NCHW, 固定 C; 对于 RGB， 相当于 R, G, B 单独做 normalization</li>
<li>预测时：均值和方差为整个训练数据集的样本均值和方差(或者学习的均值和方差)</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/LXP-Never/p/11566064.html">各种 normlization 方法， 带图</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.tensorflow.org/2022/11/whats-new-in-tensorflow-211.html">文本 normalization 图示</a></p>
<ul>
<li>layer norm:输入一句话直接对其输出做 norm，不用管其他句子</li>
</ul>
</li>
</ol>
<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/learn/intro-to-deep-learning">kaggle intro-to-deep-learning</a></li>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/code/ryanholbrook/deep-learning-animations-and-illustrations/notebook">sgd 动画</a></li>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/code/ryanholbrook/overfitting-and-underfitting">overfitting-and-underfitting</a></li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Activation_function">激活函数</a></li>
</ol>
<h3 id="术语表"><a href="#术语表" class="headerlink" title="术语表"></a>术语表</h3><ol>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/0evrjcivb5ArZGLQ4tGrmg">深度学习速查词典</a></li>
<li><a target="_blank" rel="noopener" href="https://developers.google.com/machine-learning/glossary?hl=zh-cn">google 机器学习术语表</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2023/12/19/multimodal/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/19/multimodal/" class="post-title-link" itemprop="url">multimodal</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-12-19 14:15:19 / 修改时间：14:15:36" itemprop="dateCreated datePublished" datetime="2023-12-19T14:15:19+08:00">2023-12-19</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2023/12/19/multimodal/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/12/19/multimodal/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>32</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://openai.com/research/clip">clip</a> <a target="_blank" rel="noopener" href="https://imzhanghao.com/2022/10/27/multimodal-learning/">multimodal-learning 中文解析</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2023/12/19/diffusion/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/19/diffusion/" class="post-title-link" itemprop="url">diffusion</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-12-19 13:08:48 / 修改时间：16:08:19" itemprop="dateCreated datePublished" datetime="2023-12-19T13:08:48+08:00">2023-12-19</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2023/12/19/diffusion/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/12/19/diffusion/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>173</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="papers"><a href="#papers" class="headerlink" title="papers"></a>papers</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/595866176">必读的 10 篇经典论文</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a> stable diffusion 的原型<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/CompVis/latent-diffusion">code</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/CompVis/stable-diffusion">stable-diffusion code</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.03458">Video Diffusion Models</a><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/lucidrains/video-diffusion-pytorch">code</a></li>
</ul>
</li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2023/12/18/ai-papers/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/18/ai-papers/" class="post-title-link" itemprop="url">ai-papers</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-12-18 15:58:38" itemprop="dateCreated datePublished" datetime="2023-12-18T15:58:38+08:00">2023-12-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-12-28 11:14:24" itemprop="dateModified" datetime="2023-12-28T11:14:24+08:00">2023-12-28</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2023/12/18/ai-papers/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/12/18/ai-papers/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>614</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="alexnet"><a href="#alexnet" class="headerlink" title="alexnet"></a><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">alexnet</a></h2><ol>
<li>关键是 end2end, 直接 rgb 到结果，不用做各种专业处理</li>
<li>CNN 关键是压缩(特征一层一层压缩)</li>
<li>SGD: 全称为 Stochastic Gradient Descent,即随机梯度下降,是机器学习中常用的优化算法,用于训练各种模型(如神经网络)寻找最优参数</li>
<li>dropout</li>
</ol>
<h2 id="resnet"><a href="#resnet" class="headerlink" title="resnet"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.03385">resnet</a></h2><ol>
<li>加残差， 能训练很深，计算量未增加</li>
</ol>
<h2 id="unet"><a href="#unet" class="headerlink" title="unet"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1505.04597">unet</a></h2><h2 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">transformer</a></h2><h2 id="ViT-Vision-Transformer"><a href="#ViT-Vision-Transformer" class="headerlink" title="ViT Vision Transformer,"></a>ViT Vision Transformer,</h2><h2 id="flash-attention"><a href="#flash-attention" class="headerlink" title="flash attention"></a>flash attention</h2><h2 id="PagedAttention"><a href="#PagedAttention" class="headerlink" title="PagedAttention"></a><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.06180.pdf">PagedAttention</a></h2><ol>
<li>分块 KV 缓存通过消除 KV 缓存引起的内存碎片化，增加了潜在的序列并发量，从而增加了系统吞吐量。</li>
<li>没有减少 KV cache</li>
<li>类似于现有的框架如 TRT-LLM、TGI 和 vLLM，DeepSpeed-FastGen 的目标是利用连续批处理和非连续 KV 缓存技术，以提升数据中心服务大型语言模型（LLM）的硬件利用率和响应速度。为了实现更高的性能，DeepSpeed-FastGen 提出了 SplitFuse 技术，它利用动态提示和生成分解, 统一来进一步改善连续批处理和系统吞吐量。</li>
</ol>
<h2 id="diffusion"><a href="#diffusion" class="headerlink" title="diffusion"></a>diffusion</h2><h2 id="stable-diffusion"><a href="#stable-diffusion" class="headerlink" title="stable diffusion"></a>stable diffusion</h2><h2 id="如何使用-arxiv"><a href="#如何使用-arxiv" class="headerlink" title="如何使用 arxiv"></a>如何使用 arxiv</h2><h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/labmlai/annotated_deep_learning_paper_implementations">labml.ai Deep Learning Paper Implementations</a><ul>
<li>colab 中有测试代码</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/labmlai/annotated_deep_learning_paper_implementations/tree/master?tab=readme-ov-file#highlighted-research-paper-pdfs">papers 画了重点</a></li>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/">paperwithcode</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLFXJ6jwg0qW-7UM8iUTj3qKqdhbQULP5I">李沐论文精度</a></li>
<li><a target="_blank" rel="noopener" href="https://zh.d2l.ai/">李沐《动手学深度学习》</a></li>
<li><a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/d2l-zh.pdf">《动手学深度学习》pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/pytorch-image-models">images-models-papaers</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2023/11/21/transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/11/21/transformer/" class="post-title-link" itemprop="url">transformer</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-11-21 17:20:45" itemprop="dateCreated datePublished" datetime="2023-11-21T17:20:45+08:00">2023-11-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-01-01 11:07:10" itemprop="dateModified" datetime="2024-01-01T11:07:10+08:00">2024-01-01</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2023/11/21/transformer/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/11/21/transformer/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="base"><a href="#base" class="headerlink" title="base"></a>base</h2><ol>
<li>硬 train 一发</li>
<li><a target="_blank" rel="noopener" href="https://zhouyifan.net/2022/11/12/20220925-Transformer/">attention</a></li>
<li><a target="_blank" rel="noopener" href="https://stats.stackexchange.com/a/424127">What exactly are keys, queries, and values in attention mechanisms?</a><ul>
<li>The key&#x2F;value&#x2F;query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best matched videos (values).</li>
<li>搜索是 query, 每个视频的信息是 key, query 和所有视频 key 做相关， 然后推荐最相关的视频(value)</li>
</ul>
</li>
<li>只不过解码器的输入是编码器的状态的加权和，而不再是一个简单的中间状态。每一个输出对每一个输入的权重叫做注意力，注意力的大小取决于输出和输入的相关关系</li>
<li>注意力机制能够无视序列的先后顺序，捕捉序列间的关系</li>
<li>RNN 本轮的输入状态取决于上一轮的输出状态，这使 RNN 的计算必须串行执行。因此，RNN 的训练通常比较缓慢。</li>
<li>transformer: 需要对每个输入向量做 3 次 transform, Wq, Wk, Wv，转换矩阵都是学习得到的</li>
<li>score matrix:自相关矩阵</li>
<li>预测时就相当于使用编码器(输入)和之前的输出来预测下一个输出； 考虑了输入和输出信息 <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=4Bdc55j80l8">Illustrated Guide to Transformers Neural Network: A step by step explanation</a> 图示非常好</li>
<li>多头是想模拟卷积，可以输出多个通道 由多个 feature，识别多个模式</li>
<li>模型训练好之后，推理时每个 token 对应的 q,k,v 都固定了，因为 token embedding 后的词向量是固定的，Q,K V 矩阵也是固定的</li>
<li>每一个 token 都会生成 q,k,v，用 q 去和所有 token 的 k 做相关，根据结果从 k 中提取特征; 例如： how are you How 的 q 会和每个 token 中的 k 提取出一些信息，根据这些信息决定下一个生成的字符，比如 you 中的 k 和 I 很相关（embeding 和训练 决定），从 you 中很可能提取出 I, 然后再根据 how are you I 中重复上一步动作，这就是为什么 token 要和自己做相关，所有 token 相关性是由 embeding 和训练一起决定的?</li>
<li>kv cache 的长度分别为 <code>head 长度 * 每个 head 向量维度 * token 个数 * 层数</code></li>
<li>transform llama 各层 shape 参数量和计算量</li>
<li>推理时才自回归；训练时用相关矩阵(下三角)一步算出所有输出，推理时每次算出当前值与之前 token 注意力，相当于在下三角矩阵加一层，只输出一个预测 token;</li>
<li>decode 训练时加掩码并行，同时输出； 推理时自回归一步一步输出</li>
<li>encode decoder 架构推理直接有输入和起始符输出，decode only 需要预热</li>
<li>batch 指的是在模型训练或推理的时候,同时输入模型的样本数量。<ul>
<li>Transformer 可处理变长序列输入。不同的样本序列长度本来就可能不同,每个 batch 中的最大 token 长度是固定的,短的序列会 padding 补 0。对超长序列做截断,限制最大 token 数</li>
<li>Transformer 模型的参数在不同的 batch 之间是共享的，而 KV cache 的共享则取决于具体的应用场景和模型配置。在自回归生成任务中，KV cache 通常是特定于单个样本的，不是共享的.</li>
<li>在 Transformer 模型中,batch size 越大,则 key-value 缓存需要占用的内存空间也越大。</li>
</ul>
</li>
<li>每个词向量(word vectors)代表了“词空间（word space）”中的一个点，具有相似含义的词的位置会更接近彼此。例如，在向量空间中与猫最接近的词包括狗、小猫和宠物。用实数向量表示单词（相对于“C-A-T”这样的字母串）的一个主要优点是，数字能够进行字母无法进行的运算。 <a target="_blank" rel="noopener" href="https://www.understandingai.org/p/large-language-models-explained-with">link</a></li>
<li>像 ChatGPT 这样的语言模型能够根据单词出现的上下文以不同的向量表示同一个词(前几层 transformer 自动修改)。有一个针对“bank（金融机构）”的向量，还有一个针对“bank（河岸）”的向量。</li>
<li>前几层专注于理解句子的语法,后面的层则致力于对整个段落的高层次理解。</li>
<li><code>注意力层</code>从提示的较早部分检索信息，而<code>前馈层</code>使语言模型能够“记住”未在提示中出现的信息。事实上，可以将前馈层视为模型从训练数据中学到的信息的数据库。记忆力在前馈层。靠前的前馈层更可能编码与特定单词相关的简单事实，例如“特朗普经常在唐纳德之后出现”。靠后的层则编码更复杂的关系，如“添加这个向量以将一个国家转换为其首都。</li>
<li>LLM 的一个关键创新之处在于，它们不需要显式标记的数据。相反，它们通过尝试预测文本段落中下一个单词来学习。几乎任何书面材料都适用于训练这些模型——从维基百科页面到新闻文章再到计算机代码。</li>
</ol>
<h3 id="关键操作"><a href="#关键操作" class="headerlink" title="关键操作"></a>关键操作</h3><ol>
<li>multi head attention</li>
<li>feed forward</li>
<li>layernorm</li>
<li>softmax</li>
<li>matmul</li>
<li>concat</li>
<li>linear(生成 q, k, v) Q, K, V</li>
</ol>
<h2 id="计算和内存统计"><a href="#计算和内存统计" class="headerlink" title="计算和内存统计"></a>计算和内存统计</h2><ol>
<li><a target="_blank" rel="noopener" href="https://kipp.ly/transformer-inference-arithmetic/">transformer-inference-arithmetic</a></li>
</ol>
<h2 id="hugging-face-transformers"><a href="#hugging-face-transformers" class="headerlink" title="hugging face transformers"></a><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">hugging face transformers</a></h2><ol>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/index">doc</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/installation#offline-mode">offline-model</a></li>
<li>模型文件页面右侧有对应使用方法</li>
<li>模型里有 onnx</li>
<li>只需要模型和对应的 config.json</li>
<li>sample: <a target="_blank" rel="noopener" href="https://huggingface.co/Xenova/llama2.c-stories15M/tree/main">llama2.c-stories15M</a></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;/home/xiyang/d/working/transformers/models/&quot;</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;/home/xiyang/d/working/transformers/models/&quot;</span>)</span><br><span class="line">model <span class="comment"># 查看模型结构</span></span><br></pre></td></tr></table></figure>

<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html">pytorch transformer_tutorial</a></li>
<li><a target="_blank" rel="noopener" href="https://nn.labml.ai/transformers/mha.html">nn.labml.ai&#x2F;transformers&#x2F;mha</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=4Bdc55j80l8">Illustrated Guide to Transformers Neural Network: A step by step explanation</a> 图示非常好</li>
<li><a target="_blank" rel="noopener" href="https://github.com/harvardnlp/annotated-transformer">harvardnlp&#x2F;annotated-transformer</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=nzqlFIcCSWQ">Transformer 论文逐段精读</a><br>代码带解说 李沐](<a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_attention-mechanisms/transformer.html">https://zh.d2l.ai/chapter_attention-mechanisms/transformer.html</a>)</li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/quicktour">quicktour</a> can run in colab</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.06732">Efficient Transformers: A Survey</a>s</li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/installation#fetch-models-and-tokenizers-to-use-offline">fetch-models-and-tokenizers-to-use-offline</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">huggingface&#x2F;transformers</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need (Transformer) 论文</a></li>
<li><a target="_blank" rel="noopener" href="https://zhouyifan.net/2022/11/12/20220925-Transformer/">(good)Attention Is All You Need (Transformer) 论文精读</a></li>
</ol>
<h3 id="李宏毅"><a href="#李宏毅" class="headerlink" title="李宏毅"></a><a target="_blank" rel="noopener" href="https://www.youtube.com/@HungyiLeeNTU/playlists">李宏毅</a></h3><ol>
<li><a target="_blank" rel="noopener" href="https://hackmd.io/@shaoeChen/rJlRfP7mL">Transformer</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=hYdO9CscNes">self attention</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/505105707">Self-attention 自注意力机制讲解 李宏毅版 v.s 吴恩达版</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2023/11/21/LLM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/11/21/LLM/" class="post-title-link" itemprop="url">LLM</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-11-21 16:24:03" itemprop="dateCreated datePublished" datetime="2023-11-21T16:24:03+08:00">2023-11-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-01-01 21:29:40" itemprop="dateModified" datetime="2024-01-01T21:29:40+08:00">2024-01-01</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2023/11/21/LLM/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/11/21/LLM/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="LLM-进化树"><a href="#LLM-进化树" class="headerlink" title="LLM 进化树"></a>LLM 进化树</h2><p><img src="https://github.com/Mooler0410/LLMsPracticalGuide/raw/main/imgs/tree.jpg" alt="LLM 进化树"></p>
<h2 id="concept"><a href="#concept" class="headerlink" title="concept"></a>concept</h2><ol>
<li><code>General Matrix Multiply (GeMM)</code></li>
<li>奇怪的话可以放到 midjouney(or dell-e) 画出图像，助于理解</li>
<li>token(令牌?) 是某个领域中一个抽象的语法或逻辑单元的称呼。在自然语言处理中,token 指一个文本串中基本的符号。比如一个句子可以被切分为多个词(word),每个词就是一个 token。</li>
<li>prompt 提示很重要</li>
<li>机器来找 prompt(提示)： hard prompt, soft prompt(adpter 放在 input), using reinforcement learning(加 generator), 让 llm 自己产生自己 prompt</li>
<li>toolformer: 使用工具</li>
<li>token 中文是字， 英文是 word piece, word 太多; unbreakable -&gt; un break able</li>
<li>model 本质是函数</li>
<li>prompting 给 chatgpt 催眠，设置限制，让 chatgpt 回答某方便问题，如设置中文聊天</li>
<li>neural editing 训练好模型改参数</li>
<li>machine unlearning 忘记曾经学过的东西， 遗忘某些涉密问题</li>
<li>hyperparameter 超参数，学习算法的参数，不是神经网络的参数</li>
<li>文字冒险游戏： chatgpt + midjourney + 语音</li>
<li>腾讯语音情感 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.13662">https://arxiv.org/abs/2301.13662</a> <a target="_blank" rel="noopener" href="https://dongchaoyang.top/InstructTTS/">https://dongchaoyang.top/InstructTTS/</a></li>
<li>embeddings:在某种程度上，就是用来降维的，降维的原理就是矩阵乘法 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/164502624">一文读懂 Embedding 的概念</a></li>
<li><a target="_blank" rel="noopener" href="https://platform.openai.com/tokenizer">tokenizer</a></li>
<li>tokenizer 是将文本进行分词,通常分为单词(word)或子词(subword)单元。它的目标是划分语料的基本符号。embeddings 是将语料库中的词或字符映射到 dense 向量表示。通常先经过 tokenizer 得到词单元,然后再映射为稠密词向量。tokenizer 侧重文本符号的划分,主要方法是基于词典或规则进行分割。embeddings 侧重语义信息的编码,主要方法是基于语料训练词向量。</li>
<li>AGI (Artificial General Intelligence): 人工通用智能或通用人工智能，是指在任何智能任务上都能表现得与人类智能相媲美的人工智能系统。</li>
<li>AIGC (AI-Generated Content): 人工智能生成内容，指的是通过人工智能算法和模型自动或半自动生成的内容。这包括文本、图像、音乐、视频和其他媒体形式。</li>
<li>大模型（Large Language Model，LLM）<br><img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/1_Welcome_GenerativeMeena_CL_V02_150521_v2_720_25fps.gif" alt="示意"></li>
<li>模型大小 7B(billion), 7B 表示 70 亿个可训练参数。吉比（gigabyte）是一个信息技术单位，。十亿（billion）是一个计数单位， 参数类型(float, int8)和数量(billion)决定模型的大小(gigabyte)</li>
<li>Bard 是由 Google 开发的生成式人工智能聊天机器人，最初基于大型语言模型的 LaMDA 系列，后来基于 PaLM2。</li>
<li>基础架构 transformer</li>
<li><a target="_blank" rel="noopener" href="https://claude.ai/chats">claude</a> chatgpt 主要竞争对手</li>
<li>webgpt: 产生的内容带引用网址, 先 gpt 处理文字， 得到关键字用于搜索引擎搜索，得到各网页结果，然后处理搜索结果，然后点选，处理网页中的内容，关联的收藏起来， 可以产生多个关键字， 多次搜索， 最终有多个收藏， 只处理这些收藏</li>
<li>chatgpt 文字接龙， bert 文字填空</li>
<li>大模型可以帮助完成强有力的事情 “A mouse riding on the head of an elephant, using reins to steer the giant creature.” (powered by Midjourney )</li>
<li>对于大模型的期待：专才(finetune, adapter)，通才(instruction learning， in-context learning)</li>
<li>finetune: update network parameters by gradient descent</li>
<li>adapter(efficient fine tuning): 大模型加入插件(例如：加一层)， 只调整新添加插件参数, 优势：不用调整大模型原始参数</li>
<li>in-context learning(示例学习): 给一些例子， 例子可能只是用于启动任务, 唤醒记忆。 更大的模型可能从例子学习到信息更多, 输入一些分类 feature 例子(直接给数字和 label)，可以将大模型变为一个分类器</li>
<li>learing in-context learning: 学习示例学习结果更好，但数据难收集</li>
<li>instruction learning</li>
<li>chain-of-thought(CoT) prompting: 训练时给出推论再给结果， 结果正确率会高很多。多次推论有不同的答案再投票.。 （chatgpt 默认列出计算过程，如果不让列详细过程， 结果可能会差）</li>
<li>参数越多，数据越多， 效果越好</li>
<li>emergent ability 顿悟时刻 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.07682.pdf">Emergent Abilities of Large Language Models</a> 10B</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2207.05221">Language Models (Mostly) Know What They Know</a></li>
<li>calibration: 大模型 softmax 分数越高，正确率越高，大模型知道知己是否在瞎掰</li>
<li>模型越大结果越差例子： <a target="_blank" rel="noopener" href="https://github.com/inverse-scaling/prize">inverse-scaling&#x2F;prize</a>, 更大的模型可能会顿悟，结果会更好; U-shaped U 型曲线</li>
<li><a target="_blank" rel="noopener" href="https://www.jmlr.org/papers/v23/21-0998.html">Switch Transformers</a> 训练时用所有参数，推理时只用部分参数，加快推理运行</li>
<li>从数据中学习语言：世界知识(尝试)， 语言知识</li>
<li>data preparation: 数据处理<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.11446">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.06499">Deduplicating Training Data Makes Language Models Better</a></li>
</ul>
</li>
<li>固定算力资源情况下：模型参数(思考)， data(学习) 成反比； 大模型小数据(思而不学)， 小模型大数据(学而不思); 学思应该平衡<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</a> 给出算力，参数量和数据量如何确定</li>
<li>LLaMa 使用了这个知识</li>
</ul>
</li>
<li>fintuning 和 reinforcement lerning 效果很好 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a> 小模型也可以胜大模型</li>
<li>openai 收齐了很多问题，所以即使 chatgpt 模型不那么大，也比更大模型好</li>
<li><code>MoE(Mixture of Experts)</code>模型指的是专家混合模型,是一类将多个模型专家进行混合的组合结构。例如： Mixtral 8x7B</li>
</ol>
<h2 id="生成内容"><a href="#生成内容" class="headerlink" title="生成内容"></a>生成内容</h2><ol>
<li>文本</li>
<li>语音</li>
<li>声音</li>
<li>图像</li>
<li>视频</li>
</ol>
<h2 id="生成方式"><a href="#生成方式" class="headerlink" title="生成方式"></a>生成方式</h2><ol>
<li><p>各个击破 Autoregressive(AR) model</p>
<ul>
<li>速度慢 无法并行</li>
<li>质量高</li>
<li>常用于文字生成</li>
</ul>
</li>
<li><p>一次到位 Non-autoregressive(NAR) model</p>
<ul>
<li>速度快</li>
<li>质量较差</li>
<li>冲用于图片生成</li>
</ul>
</li>
<li><p>各个击破和一次到位结合</p>
<ul>
<li>先各个击破产生中间产物(先决定大方向)，再一次到位</li>
<li>一次到位改成 N 次到位(先一次到位再各个击破) diffusion model</li>
</ul>
</li>
</ol>
<h3 id="多任务学习-Multi-Task-Learning-和多模态学习-Multimodal-Learning"><a href="#多任务学习-Multi-Task-Learning-和多模态学习-Multimodal-Learning" class="headerlink" title="多任务学习(Multi-Task Learning)和多模态学习(Multimodal Learning)"></a>多任务学习(Multi-Task Learning)和多模态学习(Multimodal Learning)</h3><ol>
<li><p>目标差异</p>
<ul>
<li>多任务学习的目标是同时学习多个相关的任务,在不同任务间实现知识迁移,从而 mutually improve 模型的泛化性能。</li>
<li>而多模态学习是为了建模和理解包含多个模态(文本、图像、语音等)的单一任务或场景。</li>
</ul>
</li>
<li><p>方法差异</p>
<ul>
<li>多任务学习通常是共享底层特征表示,在顶层分出多个 task-specific 的输出层。</li>
<li>多模态学习则更关注不同模态间的交互建模、对齐、融合,学习联合的媒体表征。</li>
</ul>
</li>
<li><p>应用差异</p>
<ul>
<li>多任务学习的应用更广泛,从计算机视觉、NLP 到健康领域都有。</li>
<li>多模态应用更集中在人机交互、信息检索、场景理解等领域。</li>
</ul>
</li>
<li><p>总结</p>
<ul>
<li>多任务学习 optimize 同一模型在不同任务上的泛化性能</li>
<li>多模态学习 optimize 不同媒体表征的融合,用于理解复杂的多模态场景或问题。</li>
</ul>
</li>
</ol>
<h3 id="多模态"><a href="#多模态" class="headerlink" title="多模态"></a>多模态</h3><ol>
<li><a target="_blank" rel="noopener" href="https://openai.com/research/clip">clip</a> <a target="_blank" rel="noopener" href="https://imzhanghao.com/2022/10/27/multimodal-learning/">multimodal-learning 中文解析</a></li>
</ol>
<h2 id="排名"><a href="#排名" class="headerlink" title="排名"></a>排名</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/SuperCLUE">SuperCLUE 中文通用大模型综合性基准</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">open_llm_leaderboard</a></li>
<li><a target="_blank" rel="noopener" href="https://arena.lmsys.org/">UC 伯克利发布大模型排行榜 需要查看 leaderboard</a></li>
</ol>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><ol>
<li><a target="_blank" rel="noopener" href="https://laion.ai/">laion</a> 5B images, 图像生成训练集<ul>
<li><a target="_blank" rel="noopener" href="https://rom1504.github.io/clip-retrieval/?back=https://knn.laion.ai&index=laion5B-H-14&useMclip=false">online search</a></li>
</ul>
</li>
</ol>
<h2 id="huggine-face"><a href="#huggine-face" class="headerlink" title="huggine face"></a>huggine face</h2><ol>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/models">models</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">open_llm_leaderboard</a></li>
</ol>
<h2 id="sota-models"><a href="#sota-models" class="headerlink" title="sota models"></a>sota models</h2><ol>
<li><a target="_blank" rel="noopener" href="https://openai.com/gpt-4">gpt-4</a></li>
<li><a target="_blank" rel="noopener" href="https://ai.meta.com/llama/">llama</a></li>
<li><a target="_blank" rel="noopener" href="https://ai.google/discover/palm2/">palm2</a></li>
<li><a target="_blank" rel="noopener" href="https://www.anthropic.com/index/claude-2">claude-2</a></li>
</ol>
<h2 id="llama"><a href="#llama" class="headerlink" title="llama"></a><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama">llama</a></h2><ol>
<li>读音： 拉马（西班牙语通话的意思)</li>
<li>clone 之后执行 download.sh, 需要官网申请的 url</li>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama-recipes/tree/main/demo_apps">demo_apps</a></li>
</ol>
<h2 id="gpt"><a href="#gpt" class="headerlink" title="gpt"></a>gpt</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let’s build GPT: from scratch, in code, spelled out.</a></li>
<li>GPT(generative pre-trained transformer)</li>
<li><a target="_blank" rel="noopener" href="https://github.com/run-llama/rags">rags</a> RAGs is a Streamlit app that lets you create a RAG pipeline from a data source using natural language.</li>
<li>chatgpt：文字接龙</li>
<li>gpt 自监督学习(pre train)得到的模型(基础模型)，chatgpt 在 gpt 基础上监督学习(finetune)</li>
<li>chatgpt 实际是分类问题， 从使用者角度是生成式学习（生成句子：多个分类问题）</li>
<li>chatgpt 评价是增强学习</li>
<li>chatgpt: gpt -&gt; 监督学习 -&gt; 增强学习</li>
</ol>
<h2 id="bing-copilot"><a href="#bing-copilot" class="headerlink" title="bing copilot"></a><a target="_blank" rel="noopener" href="https://www.bing.com/">bing copilot</a></h2><h2 id="precision-精度"><a href="#precision-精度" class="headerlink" title="precision 精度"></a>precision 精度</h2><ol>
<li>float32</li>
<li>float16</li>
<li>bfloat16</li>
<li>8bit</li>
<li>4bit</li>
<li>GPTQ</li>
</ol>
<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/Mooler0410/LLMsPracticalGuide">LLMsPracticalGuide</a></li>
<li><a target="_blank" rel="noopener" href="https://z7nobhiey2.feishu.cn/file/JTa0bZ38RohzwTx9zjucuDmunVe">oneflow 技术年货 2023</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=yiY4nPOzJEg&list=PLJV_el3uVTsOePyfmkfivYZ7Rqr2nMk3W">李宏毅 生成式 AI</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=zjkBMFhNj_g">Intro to Large Language Models</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLFXJ6jwg0qW-7UM8iUTj3qKqdhbQULP5I">李沐论文精度</a></li>
<li><a target="_blank" rel="noopener" href="https://openai.com/research/clip">openai.com&#x2F;research</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2023/11/15/shareX/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/11/15/shareX/" class="post-title-link" itemprop="url">shareX</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-11-15 14:11:38" itemprop="dateCreated datePublished" datetime="2023-11-15T14:11:38+08:00">2023-11-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-11-16 10:47:54" itemprop="dateModified" datetime="2023-11-16T10:47:54+08:00">2023-11-16</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2023/11/15/shareX/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/11/15/shareX/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>66</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="base"><a href="#base" class="headerlink" title="base"></a>base</h2><ol>
<li>设置上传 url<br><img src="https://i.ibb.co/3C66vgd/UQTv-UKxj-Vi.png" alt="设置上传url"></li>
</ol>
<h2 id="设置自定义服务器"><a href="#设置自定义服务器" class="headerlink" title="设置自定义服务器"></a>设置自定义服务器</h2><ol>
<li><a target="_blank" rel="noopener" href="https://api.imgbb.com/">imagebb 生成 key</a><br><img src="https://i.ibb.co/Dz3Xkyn/Jf4m-Fd-WXg-F.png" alt="生成key"></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ShareX/CustomUploaders/blob/master/imgbb.com.sxcu">自定义目标设置贴入，修改 key</a><br><img src="https://i.ibb.co/MM2mLg5/2z4l9e-PS2x.png" alt="修改key"><br><img src="https://i.ibb.co/TwmGMk8/BUHj-Zwd0ke.png" alt="key"></li>
</ol>
<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://getsharex.com/">shareX</a></li>
<li><a target="_blank" rel="noopener" href="https://imgbb.com/">图片共享</a></li>
<li><a target="_blank" rel="noopener" href="https://imgur.com/">imgur</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="贾夕阳"
      src="/images/coder2.jpg">
  <p class="site-author-name" itemprop="name">贾夕阳</p>
  <div class="site-description" itemprop="description">深度学习/自动驾驶/C++/性能优化</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">166</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jiaxiyang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jiaxiyang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2024/01/01/tensorrt-llm/" title="2024&#x2F;01&#x2F;01&#x2F;tensorrt-llm&#x2F;">tensorrt-llm</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/12/30/gpt/" title="2023&#x2F;12&#x2F;30&#x2F;gpt&#x2F;">gpt</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/12/26/jupyter/" title="2023&#x2F;12&#x2F;26&#x2F;jupyter&#x2F;">jupyter</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/12/24/llama2-c/" title="2023&#x2F;12&#x2F;24&#x2F;llama2-c&#x2F;">llama2</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/12/21/deep-learning/" title="2023&#x2F;12&#x2F;21&#x2F;deep-learning&#x2F;">deep-learning</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">贾夕阳</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">403k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">6:06</span>
</div>

<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("06/26/2020 14:52:10");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : '[object Object]',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://jiaxiyang.github.io/page/3/',]
      });
      });
  </script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'g32ipLmEye1u5l6wBGRJt03S-gzGzoHsz',
      appKey     : 'zHgLkAICsZUl9Mf8LfdoVigP',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

  

  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>





 
</body>
</html>

