<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0-rc2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jiaxiyang.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="深度学习&#x2F;自动驾驶&#x2F;C++&#x2F;性能优化">
<meta property="og:type" content="website">
<meta property="og:title" content="Xiyang">
<meta property="og:url" content="https://jiaxiyang.github.io/page/3/index.html">
<meta property="og:site_name" content="Xiyang">
<meta property="og:description" content="深度学习&#x2F;自动驾驶&#x2F;C++&#x2F;性能优化">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="贾夕阳">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://jiaxiyang.github.io/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Xiyang</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WGS6S6YFJ6"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-WGS6S6YFJ6');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Xiyang</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Think twice, code once!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">196</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">44</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">55</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/jiaxiyang" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2025/08/20/AI-coding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/20/AI-coding/" class="post-title-link" itemprop="url">AI coding</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-08-20 10:06:28 / 修改时间：10:06:54" itemprop="dateCreated datePublished" datetime="2025-08-20T10:06:28+08:00">2025-08-20</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2025/08/20/AI-coding/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/08/20/AI-coding/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>23</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="list"><a href="#list" class="headerlink" title="list"></a>list</h1><ol>
<li>cursor</li>
<li>claude</li>
<li>copilot</li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2025/04/28/Architecture/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/28/Architecture/" class="post-title-link" itemprop="url">Computer Architecture</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-04-28 16:38:02 / 修改时间：17:28:48" itemprop="dateCreated datePublished" datetime="2025-04-28T16:38:02+08:00">2025-04-28</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2025/04/28/Architecture/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/04/28/Architecture/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>224</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><h3 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h3><h4 id="GPGPU"><a href="#GPGPU" class="headerlink" title="GPGPU"></a>GPGPU</h4><ol>
<li>通用性强，支持多种计算任务，尤其擅长并行处理</li>
<li>GPGPU: 像一个万能工匠，什么都会干，但效率不是最顶级。</li>
<li>例: NVIDIA H100</li>
</ol>
<h3 id="DSA"><a href="#DSA" class="headerlink" title="DSA"></a>DSA</h3><ol>
<li>针对特定领域（如深度学习、图像处理）定制优化</li>
<li>DSA：像一个专门干水电活的师傅，水电这块特别牛，但别的活就不行了。</li>
<li>例: Google TPU</li>
</ol>
<h3 id="ASIC"><a href="#ASIC" class="headerlink" title="ASIC"></a>ASIC</h3><ol>
<li>完全为某个特定应用定制，效率极高</li>
<li>ASIC：像一台只会干一件事的机器人，干得飞快、极致省电，但换个任务就废了。</li>
<li>例: 比特币矿机芯片</li>
</ol>
<h2 id="TPU-架构"><a href="#TPU-架构" class="headerlink" title="TPU 架构"></a>TPU 架构</h2><ol>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/zy1SyXjbnH3Ix6GigArD0w">google TPU</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2025/04/18/pytest/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/18/pytest/" class="post-title-link" itemprop="url">pytest</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-04-18 13:59:50" itemprop="dateCreated datePublished" datetime="2025-04-18T13:59:50+08:00">2025-04-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-12-16 11:07:26" itemprop="dateModified" datetime="2025-12-16T11:07:26+08:00">2025-12-16</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2025/04/18/pytest/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/04/18/pytest/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://docs.pytest.org/en/stable/">doc</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.pytest.org/en/stable/how-to/usage.html">usage</a><ul>
<li><code>pytest --collect-only -q test_file</code> 列出测试文件，不执行</li>
<li><code>pytest test_mod.py</code></li>
<li><code>pytest testing/</code><ul>
<li>pytest 默认只收集以 test_*.py 或 *_test.py 命名的文件。</li>
</ul>
</li>
<li><code>pytest tests/test_mod.py::test_func</code></li>
<li><code>pytest tests/test_mod.py::TestClass</code></li>
<li><code>pytest tests/test_mod.py::TestClass::test_method</code></li>
<li><code>pytest -m slow</code> 运行标记为slow的测试</li>
<li><code>pytest @tests_to_run.txt</code> 从文件中读取要测试的内容</li>
<li><code>pytest --durations=10 --durations-min=1.0</code> 分析测试执行时间</li>
</ul>
</li>
</ol>
<h2 id="NOTE"><a href="#NOTE" class="headerlink" title="NOTE"></a>NOTE</h2><ol>
<li><p>html report:</p>
<ul>
<li>pip install  pytest-html</li>
<li>pytest –html&#x3D;report.html –self-contained-html python&#x2F;test&#x2F;unit&#x2F;sipu -n 16 -v -rxXs</li>
</ul>
</li>
<li><p><code>pip install pytest-xdist</code>pytest加速</p>
<ul>
<li>pytest -n 4</li>
<li>pytest -n auto</li>
</ul>
</li>
<li><p>pytest 默认只收集以 test_*.py 或 *_test.py 命名的文件。<a target="_blank" rel="noopener" href="https://docs.pytest.org/en/stable/getting-started.html#run-multiple-tests">link</a></p>
<ul>
<li>pytest will run all files of the form test_*.py or *_test.py in the current directory and its subdirectories.</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://docs.pytest.org/en/stable/explanation/goodpractices.html#conventions-for-python-test-discovery">conventions-for-python-test-discovery</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://docs.pytest.org/en/stable/explanation/goodpractices.html#choosing-a-test-layout">工程布局</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://docs.pytest.org/en/stable/explanation/pythonpath.html#import-modes">import-modes</a></p>
<ul>
<li>pytest 作为测试框架需要导入测试模块和 conftest.py 文件来执行。</li>
<li>conftest.py可以传递参数</li>
<li>conftest.py 是 pytest 框架中用于共享测试配置和夹具（fixtures）的特殊文件，放在你的测试目录中。它的主要作用是为多个测试文件提供通用的设置、夹具和钩子函数，避免重复代码。</li>
<li>conftest.py 不需要被 import，pytest 会自动发现它。</li>
<li>一个项目可以有多个 conftest.py 文件，分布在不同测试目录中，作用域是当前目录及其子目录。</li>
</ul>
</li>
</ol>
<h2 id="fixtures"><a href="#fixtures" class="headerlink" title="fixtures"></a><a target="_blank" rel="noopener" href="https://docs.pytest.org/en/stable/how-to/fixtures.html">fixtures</a></h2><ol>
<li>fixture（夹具）是 pytest 中一个非常强大和核心的机制，用来为测试函数提供初始化的资源或前置条件，比如：<ul>
<li>创建数据库连接</li>
<li>准备测试数据</li>
<li>打开文件</li>
<li>初始化配置等</li>
</ul>
</li>
<li>fixture 的优势<ul>
<li>让测试更清晰，避免重复初始化代码</li>
<li>易于组合和共享</li>
<li>可以集中在 conftest.py 中，适用于多个测试文件</li>
</ul>
</li>
</ol>
<h2 id="base"><a href="#base" class="headerlink" title="base"></a>base</h2><ol>
<li>默认情况下 pytest 会隐藏 print()<ul>
<li><code>pytest -s</code> 显示输出log</li>
</ul>
</li>
<li><code>pytest.mark.parametrize</code>可以设置参数组合, 可以设置xfail</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&quot;shape, block_size&quot;</span>, [</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params">(<span class="params"><span class="number">1</span>, <span class="number">256</span></span>), <span class="number">256</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params">(<span class="params"><span class="number">12</span>, <span class="number">1024</span></span>), <span class="number">256</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    pytest.param(<span class="params">(<span class="params"><span class="number">2</span>, <span class="number">512</span></span>), <span class="number">128</span>, marks=pytest.mark.xfail(<span class="params">reason=<span class="string">&quot;Act quant kernel not support 128 block size&quot;</span></span>)</span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    pytest.param(<span class="params">(<span class="params"><span class="number">2</span>, <span class="number">512</span></span>), <span class="number">512</span>, marks=pytest.mark.xfail(<span class="params">reason=<span class="string">&quot;Load op has bugs, to be fix&quot;</span></span>)</span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">]</span>);</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&quot;shape, block_size&quot;</span>, [</span></span></span><br><span class="line"><span class="params"><span class="meta">    *[</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params">shape, block_size</span>)</span></span></span><br><span class="line"><span class="params"><span class="meta">        <span class="keyword">for</span> shape <span class="keyword">in</span> [(<span class="params"><span class="number">1</span>, <span class="number">256</span></span>), (<span class="params"><span class="number">2</span>, <span class="number">512</span></span>), (<span class="params"><span class="number">12</span>, <span class="number">1024</span></span>)]</span></span></span><br><span class="line"><span class="params"><span class="meta">        <span class="keyword">for</span> block_size <span class="keyword">in</span> [<span class="number">256</span>, <span class="number">512</span>]</span></span></span><br><span class="line"><span class="params"><span class="meta">        <span class="keyword">if</span> shape[<span class="number">1</span>] &gt;= block_size</span></span></span><br><span class="line"><span class="params"><span class="meta">    ],</span></span></span><br><span class="line"><span class="params"><span class="meta">    </span></span></span><br><span class="line"><span class="params"><span class="meta">    pytest.param(<span class="params">(<span class="params"><span class="number">2</span>, <span class="number">512</span></span>), <span class="number">128</span>, marks=pytest.mark.xfail(<span class="params">reason=<span class="string">&quot;Act quant kernel not support 128 block size&quot;</span></span>)</span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">]</span>)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="meta">    <span class="string">&quot;kernel, M, N, K, input_type&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">    [</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params">tile_mma_32x32_f32_f16_f16_kernel, <span class="number">32</span>, <span class="number">32</span>, <span class="number">16</span>, torch.float16</span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params">tile_mma_32x32_f32_f16_f16_v2_kernel, <span class="number">32</span>, <span class="number">32</span>, <span class="number">16</span>, torch.float32</span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params">tile_mma_32x32_f32_f8e4m3_f8e4m3_kernel, <span class="number">32</span>, <span class="number">32</span>, <span class="number">32</span>, torch.float8_e4m3fn</span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params">tile_mma_32x32_f32_f8e4m3_f8e4m3_v2_kernel, <span class="number">32</span>, <span class="number">32</span>, <span class="number">32</span>, torch.float32</span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    ],</span></span></span><br><span class="line"><span class="params"><span class="meta">    ids=[</span></span></span><br><span class="line"><span class="params"><span class="meta">        <span class="string">&quot;f16_kernel&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">        <span class="string">&quot;f16_v2_kernel&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">        <span class="string">&quot;f8e4_kernel&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">        <span class="string">&quot;f8e4_v2_kernel&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">    ]</span></span></span><br><span class="line"><span class="params"><span class="meta"></span>)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&quot;dtype&quot;</span>, [torch.float32, torch.float16, torch.int32]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&quot;shape,permute_dims&quot;</span>, [</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params">(<span class="params"><span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span></span>), (<span class="params"><span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span></span>)</span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params">(<span class="params"><span class="number">4</span>, <span class="number">5</span></span>), (<span class="params"><span class="number">1</span>, <span class="number">0</span></span>)</span>),  <span class="comment"># transpose</span></span></span></span><br><span class="line"><span class="params"><span class="meta">]</span>)</span></span><br></pre></td></tr></table></figure>
<ol>
<li><code>pytest test_single_mma.py -s -k tile_mma_32x32_f32_f16_f16_kernel</code> 选择一个kernel执行, 加上 ids 让 -k 更好用</li>
<li>可以在test文件中加下面内容，方便测试，pytest 不会执行 if <strong>name</strong> &#x3D;&#x3D; “<strong>main</strong>“: 中的代码</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    test_act_quant_fp32_to_fp8((<span class="number">2</span>, <span class="number">256</span>), <span class="number">128</span>)</span><br></pre></td></tr></table></figure>


<h2 id="skip"><a href="#skip" class="headerlink" title="skip"></a><a target="_blank" rel="noopener" href="https://docs.pytest.org/en/stable/how-to/skipping.html">skip</a></h2><ol>
<li><a target="_blank" rel="noopener" href="https://docs.pytest.org/en/stable/how-to/skipping.html#skip-xfail-with-parametrize">skip-xfail-with-parametrize</a></li>
</ol>
<h2 id="管理输出"><a href="#管理输出" class="headerlink" title="管理输出"></a><a target="_blank" rel="noopener" href="https://docs.pytest.org/en/stable/how-to/output.html">管理输出</a></h2><ol>
<li><code>pytest -rxXs</code>  # show extra info on xfailed, xpassed, and skipped tests</li>
<li><code>pytest -q -v</code>  每个文件用-q显示， ci测试好用(good)</li>
<li>pytest –quiet          # quiet - less verbose - mode</li>
<li>pytest -q               # quiet - less verbose - mode (shortcut)</li>
<li>pytest -v               # increase verbosity, display individual test names</li>
<li>pytest -vv              # more verbose, display more details from the test output</li>
<li>pytest -vvv             # not a standard , but may be used for even more detail in certain setups</li>
<li><code>pytest -s</code> 加上 -s  可以看到 print 输出，方便调试。</li>
<li><code>pytest -k</code> -k 后支持模糊匹配函数名或参数中变量的字符串表示</li>
<li><code>pytest --no-header -v</code>列表形式输出</li>
<li>pytest 在默认“简洁模式”（verbosity&#x3D;0 或 -q）下，会用单个字符表示每个测试的执行结果：<ul>
<li><code>.</code> —— 测试通过 (passed)。每当一个测试成功时，pytest 就输出一个点号。 </li>
<li><code>F</code> —— 测试失败 (failed)，通常伴随断言错误或其他异常。 </li>
<li><code>E</code> —— 测试执行过程中发生未捕获的错误 (error)，例如 fixture 或 setup 中抛出的异常。 </li>
<li><code>s 或 S</code> —— 测试被跳过 (skipped)，使用 @pytest.mark.skip、@pytest.mark.skipif 或在测试中主动调用 pytest.skip() 时触发。 </li>
<li><code>x</code> —— 预期失败 (xfail)，标记为 @pytest.mark.xfail 的测试如果确实失败，就显示 x。 </li>
<li><code>X</code> —— 意外通过 (xpass)，当标记为 xfail 的测试反而通过时显示 X。</li>
</ul>
</li>
</ol>
<h2 id="debug"><a href="#debug" class="headerlink" title="debug"></a><a target="_blank" rel="noopener" href="https://docs.pytest.org/en/stable/how-to/failures.html">debug</a></h2><ol>
<li>(good) <code>pytest -s</code> 加上 -s  可以看到 print 输出，方便调试。</li>
<li>-k 后支持模糊匹配函数名或参数中变量的字符串表示<br><code>pytest test_single_mma.py -s -k tile_mma_32x32_f32_f16_f16_kernel</code> 选择一个kernel执行</li>
<li><code>pytest --pdb</code> 停在报错地方</li>
<li><code>pytest --trace</code>停在测试入口</li>
</ol>
<h2 id="profiling"><a href="#profiling" class="headerlink" title="profiling"></a>profiling</h2><ol>
<li><a target="_blank" rel="noopener" href="https://docs.pytest.org/en/stable/how-to/usage.html">usage</a><ul>
<li><code>pytest --durations=10 --durations-min=1.0</code> 分析测试执行时间</li>
</ul>
</li>
</ol>
<h2 id="pytest-ini"><a href="#pytest-ini" class="headerlink" title="pytest.ini"></a>pytest.ini</h2><p>1. </p>
<h2 id="triton"><a href="#triton" class="headerlink" title="triton"></a>triton</h2><ol>
<li>triton直接使用的pytest，没使用torch的TestCase</li>
<li>核心测试在<code>python/test/unit/language</code></li>
</ol>
<h2 id="pytorch-test"><a href="#pytorch-test" class="headerlink" title="pytorch test"></a>pytorch test</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/tree/main/test">samples</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#unit-testing">unit testing</a></li>
<li><a target="_blank" rel="noopener" href="https://gitee.com/ascend/pytorch/blob/master/CONTRIBUTING.zh.md#%E6%B5%8B%E8%AF%95%E7%94%A8%E4%BE%8B">doc</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2025/01/18/cursor/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/18/cursor/" class="post-title-link" itemprop="url">cursor</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-01-18 20:10:23" itemprop="dateCreated datePublished" datetime="2025-01-18T20:10:23+08:00">2025-01-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-08-07 15:10:19" itemprop="dateModified" datetime="2025-08-07T15:10:19+08:00">2025-08-07</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2025/01/18/cursor/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/01/18/cursor/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>565</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="import-vscode-config"><a href="#import-vscode-config" class="headerlink" title="import vscode config"></a>import vscode config</h2><ol>
<li>cursor settings -&gt; general -&gt; vs code import</li>
<li>git管理配置</li>
</ol>
<h2 id="short-keys"><a href="#short-keys" class="headerlink" title="short keys"></a>short keys</h2><ol>
<li><a target="_blank" rel="noopener" href="https://forum.cursor.com/t/change-ctrl-l-to-ctrl-i-control-l-is-for-clear-terminal/15310/3">terminal ctrl-l 问题</a></li>
<li>terminal: C-S-l</li>
</ol>
<h2 id="ignore-rules"><a href="#ignore-rules" class="headerlink" title="ignore rules"></a><a target="_blank" rel="noopener" href="https://docs.cursor.com/en/context/ignore-files">ignore rules</a></h2><ol>
<li>要查看索引文件路径： Cursor Settings &gt; Indexing &amp; Docs &gt; View included files</li>
<li>project rule：给model先说明先验证知识，@rule包含<ul>
<li>可以让ai自己先晚上一下rule</li>
</ul>
</li>
</ol>
<h2 id=""><a href="#" class="headerlink" title="@"></a><a target="_blank" rel="noopener" href="https://docs.cursor.com/en/guides/working-with-context#%40-symbol">@</a></h2><ol>
<li>当你明确知道要包含哪些文件、文件夹、网站或其他上下文信息时，@ 符号非常有用。越具体越好。</li>
</ol>
<h2 id="large-project"><a href="#large-project" class="headerlink" title="large project"></a><a target="_blank" rel="noopener" href="https://docs.cursor.com/zh/guides/advanced/large-codebases">large project</a></h2><h2 id="核心功能"><a href="#核心功能" class="headerlink" title="核心功能"></a>核心功能</h2><h3 id="tab"><a href="#tab" class="headerlink" title="tab"></a><a target="_blank" rel="noopener" href="https://docs.cursor.com/zh/tab/overview">tab</a></h3><h2 id="base"><a href="#base" class="headerlink" title="base"></a>base</h2><ol>
<li>inline edit: <code>C-k</code> -&gt; <code>C-M-k</code></li>
<li><code>@</code> chat里输入@</li>
<li>切换成vscode <code>    &quot;workbench.activityBar.orientation&quot;: &quot;vertical&quot;,</code></li>
<li>cusor settings -&gt; features -&gt; editor -&gt; show chat&#x2F;edit tooltip: 关闭<ul>
<li>下一行会提示</li>
</ul>
</li>
<li><code>&quot;cursor.cpp.enablePartialAccepts&quot;: true</code> ctlr + rightarrow 接受一个单词，设置快捷键 M-f</li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/12/29/LLVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/29/LLVM/" class="post-title-link" itemprop="url">LLVM</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-12-29 15:42:47" itemprop="dateCreated datePublished" datetime="2024-12-29T15:42:47+08:00">2024-12-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-11-03 16:01:32" itemprop="dateModified" datetime="2025-11-03T16:01:32+08:00">2025-11-03</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/12/29/LLVM/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/12/29/LLVM/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.9k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="basic"><a href="#basic" class="headerlink" title="basic"></a>basic</h2><ol>
<li><p>编译器组件交互方式：</p>
<ul>
<li>通过内存：通过监督工具Clang实现，该工具将每个LLVM组件作为一个库，并依赖内存中分配的数据结构将一个阶段的输出作为输入提供给另一个阶段</li>
<li>通过文件：通过用户实现，用户启动较小的独立工具，改工具将特定组件的结果写入磁盘文件，用户决定这个文件作为输入传给下一个</li>
</ul>
</li>
<li><p>像Clang这样的高级工具可以整合使用其他几个更小的工具，具体做法是链接霄工具的库来实现工具的功能，LLVM设计上十分重视以库的形式进行大量代码复用。</p>
</li>
<li><p>llc使用libLLVMCodeGen库实现部分功能，LLVM IR优化器opt使用libLLVMipa实现与目标无关的过程间优化。Clang使用上面两个库来代替llc和op。并向用户提供更简单的接口。</p>
</li>
<li><p>语法(Syntax)：关注代码的结构和形式，通过预定义的文法规则来判断源代码是否“写对了格式”。它是编译器前端的第一道防线，主要产生的是语法树。</p>
</li>
<li><p>语义(Semantic)：关注代码的意义和逻辑，确保程序各部分之间的关系正确、类型匹配、作用域清晰等。它依赖于语法树，并进一步填充具体的含义信息，为中间代码生成提供依据。</p>
</li>
</ol>
<h2 id="TableGen"><a href="#TableGen" class="headerlink" title="TableGen"></a>TableGen</h2><ol>
<li>TableGen 是 LLVM 的“数据模板语言” + “代码生成器”。它用一套 .td 文件定义规则，用工具 llvm-tblgen 自动生成 C++ 源码文件</li>
<li>TableGen 是 LLVM 的“编译器生成器”中的生成器。它本身不生成机器码，而是生成“生成机器码的代码”。</li>
<li>TableGen → llvm-tblgen → .inc(*.h.inc or *.cpp.inc)</li>
<li>.inc 文件通常是 C&#x2F;C++ 源码片段（include 文件），不是完整的编译单元。.inc的名字来自 “include” 的缩写，表示它需要通过 #include 被其他 C++ 文件包含。</li>
<li>.inc &#x3D; include 文件的缩写;主要是 TableGen 生成的 C++ 代码片段;不可独立编译，需要 #include</li>
<li>用途：<ul>
<li>指令编码表</li>
<li>寄存器表</li>
<li>汇编打印 &#x2F; 解析</li>
<li>DAG 指令选择规则</li>
<li>Clang 诊断信息 &#x2F; 内建函数 &#x2F; 编译器选项</li>
</ul>
</li>
<li>可以理解为：<ul>
<li>.td 文件是 “表定义”，人可读</li>
<li>.inc 文件是 “表实现”，编译器可用</li>
</ul>
</li>
</ol>
<h2 id="LLC"><a href="#LLC" class="headerlink" title="LLC"></a>LLC</h2><ol>
<li><code>llc --version</code></li>
<li><code>llc &lt;llir file without postfix&gt; --debug</code></li>
</ol>
<h2 id="LLVM-IR"><a href="#LLVM-IR" class="headerlink" title="LLVM IR"></a>LLVM IR</h2><ol>
<li>br: branch; “br”在LLVM IR (中间表示) 中是一个分支指令，它用于控制流跳转，可以是有条件的也可以是无条件的。<ul>
<li>br label %target</li>
<li>br i1 %cond, label %if.true, label %if.false</li>
</ul>
</li>
<li>LLVM IR 中的 phi 指令用于实现 SSA（静态单赋值）形式下的变量合并。在一个基本块（basic block）中，如果存在多个前驱块（predecessor），那么在该块的开始处就需要一个 phi 指令来选择正确的值。它的基本语法如下：<ul>
<li><code>%result = phi &lt;type&gt; [ %val1, %pred1 ], [ %val2, %pred2 ], …</code> </li>
<li>这表示：如果控制流是从前驱块 %pred1 进入当前块，则 %result 的值为 %val1；如果是从 %pred2 进入，则 %result 为 %val2，依此类推。</li>
</ul>
</li>
</ol>
<h1 id="Debug"><a href="#Debug" class="headerlink" title="Debug"></a>Debug</h1><ol>
<li><a target="_blank" rel="noopener" href="https://llvm.org/docs/ProgrammersManual.html#the-llvm-debug-macro-and-debug-option">the-llvm-debug-macro-and-debug-option</a></li>
<li>speed up build</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Using clang and lld speeds up the build, we recommend adding:</span></span><br><span class="line">-DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DLLVM_ENABLE_LLD=ON</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">CCache can drastically speed up further rebuilds, try adding:</span></span><br><span class="line">-DLLVM_CCACHE_BUILD=ON</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Optionally, using ASAN/UBSAN can find bugs early <span class="keyword">in</span> development, <span class="built_in">enable</span> with:</span></span><br><span class="line">-DLLVM_USE_SANITIZER=&quot;Address;Undefined&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Optionally, enabling integration tests as well</span></span><br><span class="line">-DMLIR_INCLUDE_INTEGRATION_TESTS=ON</span><br></pre></td></tr></table></figure>
<ol>
<li>install path <code>-DCMAKE_INSTALL_PREFIX=/data/xiyang/mlir-tutorial/install</code></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/11/13/deformable-attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/11/13/deformable-attention/" class="post-title-link" itemprop="url">deformable_attention</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-11-13 11:51:24 / 修改时间：11:51:49" itemprop="dateCreated datePublished" datetime="2024-11-13T11:51:24+08:00">2024-11-13</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/11/13/deformable-attention/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/11/13/deformable-attention/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p><strong>Deformable Attention</strong> 是一种在视觉任务中用于增强自注意力机制的技术，特别是在处理图像、视频等高维数据时，能够有效地处理不规则、非均匀的空间或时间分布。它的关键思想是通过对关注区域进行选择性采样，而不是计算所有位置的注意力权重，从而显著提高计算效率和精度，尤其是在处理大规模数据时。</p>
<p>Deformable Attention 最初应用于<strong>Deformable DETR</strong>（Deformable Detection Transformer）中，针对检测任务优化了传统的自注意力机制，后来被推广到其他任务，如图像分割、姿态估计等。下面是 Deformable Attention 的一些关键特性和原理：</p>
<h3 id="1-传统自注意力的瓶颈"><a href="#1-传统自注意力的瓶颈" class="headerlink" title="1. 传统自注意力的瓶颈"></a>1. <strong>传统自注意力的瓶颈</strong></h3><p>标准的自注意力机制（如在 Transformer 中使用的注意力机制）通常会计算输入的每一个位置与所有其他位置的关系，这导致了计算复杂度是输入序列长度的平方。在图像中，若处理的是高分辨率图像或长序列数据，这样的计算开销会非常巨大。具体来说：</p>
<ul>
<li><strong>计算复杂度</strong>：对于长序列，计算每一对位置之间的相似度需要 ( O(N^2) ) 的时间复杂度，其中 ( N ) 是序列的长度。对于图像数据而言，每个像素与所有其他像素的关系计算量也非常庞大。</li>
</ul>
<h3 id="2-Deformable-Attention-的设计思想"><a href="#2-Deformable-Attention-的设计思想" class="headerlink" title="2. Deformable Attention 的设计思想"></a>2. <strong>Deformable Attention 的设计思想</strong></h3><p>Deformable Attention 通过限制计算注意力的区域来减少计算量。与标准的全局自注意力机制不同，Deformable Attention 仅关注输入数据中的 <strong>少量关键位置</strong>，这些关键位置通过学习自适应地选择，能够有效地捕捉到重要的上下文信息。</p>
<ul>
<li><strong>选择性采样</strong>：而不是计算每个位置和所有其他位置的关系，Deformable Attention 只选择一些特定的、与当前查询相关的关键位置进行计算。这些关键位置通常是通过一个学习的<strong>采样位置</strong>来选择的，因此能关注到更多有用的信息。</li>
</ul>
<h3 id="3-Deformable-Attention-的实现原理"><a href="#3-Deformable-Attention-的实现原理" class="headerlink" title="3. Deformable Attention 的实现原理"></a>3. <strong>Deformable Attention 的实现原理</strong></h3><p>Deformable Attention 的核心思想是通过<strong>可变形采样</strong>（deformable sampling）来动态选择哪些位置会参与到注意力的计算中。这通常通过引入一个采样的位置信息来完成，该信息在训练过程中不断优化，以便能自适应地选择合适的区域。</p>
<ul>
<li><p><strong>采样位置的生成</strong>：在 Deformable Attention 中，会对每个查询（query）生成一组采样位置（采样点），这些位置不一定是规则网格上的点，而是通过某种方式（如通过网络学习的偏移量）选择的。采样位置通常会覆盖输入特征的局部区域，从而捕获重要的上下文信息。</p>
</li>
<li><p><strong>注意力计算</strong>：通过对选定的局部位置进行自注意力计算，可以大大减少计算量，因为只需要计算局部区域的注意力，而不是全局的每一对位置之间的关系。</p>
</li>
<li><p><strong>位置信息</strong>：Deformable Attention 采用相对位置信息来进一步增强模型的表达能力，能够处理不同位置间的空间关系。</p>
</li>
</ul>
<h3 id="4-Deformable-Attention-与标准自注意力的比较"><a href="#4-Deformable-Attention-与标准自注意力的比较" class="headerlink" title="4. Deformable Attention 与标准自注意力的比较"></a>4. <strong>Deformable Attention 与标准自注意力的比较</strong></h3><table>
<thead>
<tr>
<th>特性</th>
<th>标准自注意力</th>
<th>Deformable Attention</th>
</tr>
</thead>
<tbody><tr>
<td><strong>计算复杂度</strong></td>
<td>( O(N^2) )</td>
<td>( O(N \cdot M) )，( M \ll N )</td>
</tr>
<tr>
<td><strong>注意力区域</strong></td>
<td>全局注意力，计算所有位置间的关系</td>
<td>局部区域注意力，选择关键位置进行计算</td>
</tr>
<tr>
<td><strong>效率</strong></td>
<td>计算开销大，内存消耗大</td>
<td>更高效，内存消耗少</td>
</tr>
<tr>
<td><strong>灵活性</strong></td>
<td>对所有位置进行相同的注意力计算</td>
<td>动态选择关键位置，自适应性强</td>
</tr>
</tbody></table>
<h3 id="5-Deformable-Attention-的优势"><a href="#5-Deformable-Attention-的优势" class="headerlink" title="5. Deformable Attention 的优势"></a>5. <strong>Deformable Attention 的优势</strong></h3><ul>
<li><strong>计算效率</strong>：由于仅计算少量的关键位置，Deformable Attention 在计算效率上相较于传统的自注意力机制有显著提升，尤其是在处理大规模输入数据时（如高分辨率图像、长时间序列等）。</li>
<li><strong>灵活性与自适应性</strong>：Deformable Attention 可以根据任务和输入数据的不同，自适应地选择最相关的位置进行注意力计算，而不是使用固定的网格结构。这样可以更灵活地处理复杂的空间或时间关系。</li>
<li><strong>减少计算冗余</strong>：在传统自注意力中，每个位置都与所有其他位置进行计算，这会带来大量冗余计算。Deformable Attention 通过局部采样减少了这些冗余计算，特别是在高分辨率图像的情况下，计算的效率提升尤为明显。</li>
</ul>
<h3 id="6-应用领域"><a href="#6-应用领域" class="headerlink" title="6. 应用领域"></a>6. <strong>应用领域</strong></h3><p>Deformable Attention 在多个领域中取得了显著的成果，特别是在计算机视觉任务中：</p>
<ul>
<li><p><strong>目标检测</strong>：Deformable DETR（Deformable Detection Transformer）是 Deformable Attention 的一个重要应用，改进了传统 DETR 模型在目标检测中的性能，尤其是在处理大尺度图像时更为高效。</p>
</li>
<li><p><strong>图像分割</strong>：Deformable Attention 也被用于图像分割任务，能够有效地捕捉到图像中的细节信息，特别是在处理不规则形状和边界时。</p>
</li>
<li><p><strong>姿态估计</strong>：通过对特定关键点的注意力计算，Deformable Attention 在姿态估计任务中也得到了广泛应用，能够更准确地捕捉人体姿态变化。</p>
</li>
</ul>
<h3 id="7-Deformable-Attention-的进一步发展"><a href="#7-Deformable-Attention-的进一步发展" class="headerlink" title="7. Deformable Attention 的进一步发展"></a>7. <strong>Deformable Attention 的进一步发展</strong></h3><p>Deformable Attention 的应用仍在不断扩展，研究者正在探索其在其他领域的潜力，例如：</p>
<ul>
<li><strong>自然语言处理</strong>：在自然语言处理中，Deformable Attention 可能能够帮助捕捉长文本中的重要上下文信息，尤其是当文本存在复杂的依赖关系时。</li>
<li><strong>视频处理</strong>：Deformable Attention 可以在视频处理任务中，通过自适应选择关键帧或区域，从而提高效率并降低计算量。</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Deformable Attention 通过对注意力计算的局部化和选择性采样，大大提高了计算效率和灵活性，尤其在处理高维数据（如高分辨率图像和长序列）时展现了优势。它在计算机视觉领域，特别是目标检测和图像分割中，取得了显著的成果，并且有潜力应用于其他领域，如自然语言处理和视频分析等。</p>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/10/15/QNX/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/10/15/QNX/" class="post-title-link" itemprop="url">QNX</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-10-15 17:01:16" itemprop="dateCreated datePublished" datetime="2024-10-15T17:01:16+08:00">2024-10-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-10-16 10:50:09" itemprop="dateModified" datetime="2024-10-16T10:50:09+08:00">2024-10-16</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/10/15/QNX/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/10/15/QNX/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>518</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="base"><a href="#base" class="headerlink" title="base"></a>base</h2><ol>
<li>QNX Software Development Platform (SDP) 需要购买, 包含<ul>
<li>QNX OS</li>
<li>QNX Tool Suite</li>
<li>QNX Software Center</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://blackberry.qnx.com/en/products/foundation-software/qnx-software-development-platform">SDP link</a></li>
<li><a target="_blank" rel="noopener" href="https://marketplace.visualstudio.com/items?itemName=qnx.qnx-vscode">VSCode 插件</a></li>
<li>QNX Neutrino 是操作系统</li>
<li>QNX 适合需要高度可靠性和安全性的应用，FreeRTOS 适合需要轻量级和可移植性的应用，VxWorks 适合需要高度可靠性和实时性能的应用，而 RTLinux 适合需要利用 Linux 的功能和资源的应用。</li>
<li>首先，宝马在进行基于 Linux 的自动辅助项目时，由于 Linux 作为一个宏内核的操作系统，需要对内核进行裁剪以满足功能安全要求。然而，无论他们怎么裁剪，都无法彻底清除干净，最终导致系统崩溃。</li>
<li>汽车电子架构已从硬件驱动发展到软件定义。 从数字驾驶舱到高级驾驶辅助系统 (ADAS)，再到自动驾驶控制器等，现都在系统级芯片 (SoC)上运行，它们通常还需要虚拟化，以管理多个操作系统和混合关键性。</li>
<li>在座舱领域，我们注意到，传统的芯片供应商已逐渐被高通取代。在国内，黑莓 QNX 占据了 99.9%的基础软件份额，其中 80%以上的项目选择高通作为首选芯片供应商。座舱领域有一个有趣的组合——QQA，即 Qualcomm、QNX 和 Android。</li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/09/24/qualcomm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/09/24/qualcomm/" class="post-title-link" itemprop="url">qualcomm</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-09-24 17:20:38" itemprop="dateCreated datePublished" datetime="2024-09-24T17:20:38+08:00">2024-09-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-10-16 10:47:12" itemprop="dateModified" datetime="2024-10-16T10:47:12+08:00">2024-10-16</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/09/24/qualcomm/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/09/24/qualcomm/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>a</p>
<h2 id="战略"><a href="#战略" class="headerlink" title="战略"></a>战略</h2><ol>
<li>市面上真正能提供舱驾融合&#x2F;舱驾一体计算平台的厂商并不多，英伟达 DRIVE Thor、高通骁龙 Flex SoC 以及黑芝麻武当 C1200 家族，是目前市面上少数相对成熟的产品。DRIVE Thor 以千 T 级的算力著称，而高通 Flex SoC 以及黑芝麻 C1200 是性价比派系的代表</li>
<li><a target="_blank" rel="noopener" href="https://www.sohu.com/a/788526882_121124477">高通第四代汽车 SoC 芯片分析，聚焦 CPU，AI 无所谓</a><ul>
<li>高通更多地考虑汽车座舱和 L2 级的舱驾一体，这样的市场定位不需要太高的 AI 性能，45TOPS 足以满足 99.9%的应用场景，座舱领域所谓的大模型 AI 都完全依赖云端计算，端侧部署成本太高，可能高达数千美元，即便端侧部署，体验也和云端计算有很大差距。</li>
<li>高通手机领域收入 62 亿美元，汽车业务收入 6.03 亿美元，IoT 业务收入 12 亿美元。</li>
<li>对于 AI 运算的 NPU，还是高通的六角 DSP，缺乏亮点，乏善可陈，高通一句话带过，没有多说。</li>
</ul>
</li>
<li>到 2024 年，Ride 智驾平台经过 4 年的演进，已经形成了从前视一体机（RV1 Lite）到支持城市 NOA（SA8650P）的完整谱系。这一代的 8650 和 8620，尤其强化了性价比的标签。</li>
<li>Flex SoC 的第一款产品 8775</li>
<li>为了向舱驾融合和跨域计算提供强大的中央计算平台，高通率先推出了汽车行业首款同时支持数字座舱和先进驾驶辅助系统（ADAS）的可扩展系列 SoC——Snapdragon Ride Flex。</li>
<li>高通技术公司提供高性能中央计算 SoC——Snapdragon Ride Flex，旨在跨异构计算资源支持混合关键级工作负载，以单颗 SoC 同时支持数字座舱、ADAS 和 AD 功能</li>
<li>手机芯片大厂联发科正携手 AI 芯片大厂英伟达（NVIDIA）开发基于 Arm 架构的 AI PC 处理器；在去年 5 月底的 COMPUTEX 2023 台北电脑展上，联发科携手英伟达共同宣布两家公司将在汽车芯片领域进行合作。联发科技将利用小芯片高速互联技术，开发整合有英伟达的 GPU 的车用 SoC 处理器，共同为新一代智能汽车提供解决方案。</li>
</ol>
<h2 id="base"><a href="#base" class="headerlink" title="base"></a>base</h2><ol>
<li>模型文件 Deep Learning Container (DLC)</li>
<li><a target="_blank" rel="noopener" href="https://chipsandcheese.com/2023/10/04/qualcomms-hexagon-dsp-and-now-npu/">qualcomms-hexagon-dsp-and-now-npu</a><ul>
<li>dsp 到 npu</li>
<li>有专用的矩阵乘指令</li>
<li>Hexagon 的 NPU 每个周期可以完成大量 16K 乘法累加运算</li>
</ul>
</li>
<li>Developers can get one step closer to the silicon and improve the performance of their AI models on Qualcomm® AI accelerators: Qualcomm® Kryo™ CPU, Qualcomm® Adreno™ GPU and Qualcomm® Hexagon™ NPU.<ul>
<li>Hexagon Arch: 可以指 DSP 和 NPU, 一个意思</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/overview.html#supported-snapdragon-devices">架构列表</a><ul>
<li>Hexagon arch: V66, V68, V69, V73, V75</li>
<li>SM8650 是 V75 架构</li>
</ul>
</li>
<li>Hexagon 是高通公司一系列数字信号处理器(DSP) 和后来的神经处理单元(NPU) 产品的品牌名称。 Hexagon 也称为 QDSP6，代表“第六代数字信号处理器”。据高通称，Hexagon 架构旨在为各种应用提供低功耗的性能。 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Qualcomm_Hexagon">link</a></li>
<li>Hexagon Tensor Accelerator (HTA) for AI acceleration.</li>
<li>Vector Execution (HVX)</li>
<li><a target="_blank" rel="noopener" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/backend.html">执行后端</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/api.html">QNN API</a></li>
</ol>
<h2 id="AI-stack"><a href="#AI-stack" class="headerlink" title="AI stack"></a>AI stack</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.qualcomm.com/developer/artificial-intelligence#overview">overview</a></li>
<li><a target="_blank" rel="noopener" href="https://www.qualcomm.com/developer/software/neural-processing-sdk-for-ai">neural-processing-sdk-for-ai</a><ul>
<li>(SNPE?)</li>
<li>The Qualcomm® Neural Processing SDK is engineered to help developers save time and effort in optimizing performance of trained neural networks on devices with Qualcomm® AI products.</li>
<li><a target="_blank" rel="noopener" href="https://www.qualcomm.com/developer/software/neural-processing-sdk-for-ai#tutorials">tutorials</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/introduction.html">doc introduction</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.qualcomm.com/developer/software/qualcomm-ai-engine-direct-sdk">ai-engine-direct-sdk</a><ul>
<li>Qualcomm AI Engine Direct is also referred to as Qualcomm Neural Network (QNN)</li>
<li>The Qualcomm® AI Engine Direct SDK provides lower-level, unified APIs for AI development. 低级别 API, 用于调用多种异构硬件核心</li>
<li>CPU, GPU, NPU, cDSP, standalone tensor acclerator，</li>
<li><a target="_blank" rel="noopener" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/introduction.html">doc introduction</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.qualcomm.com/developer/software/ai-model-efficiency-toolkit">ai-model-efficiency-toolkit</a><ul>
<li>工具集：压缩， 量化，编译，网络搜索</li>
</ul>
</li>
</ol>
<h2 id="AI-hub"><a href="#AI-hub" class="headerlink" title="AI hub"></a>AI hub</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.qualcomm.com/developer/artificial-intelligence#overview">overview</a></li>
</ol>
<h3 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h3><ol>
<li><a target="_blank" rel="noopener" href="https://app.aihub.qualcomm.com/docs/hub/getting_started.html">aihub quick-example-pytorch</a><ul>
<li>需要先注册</li>
<li>sample model 会传到服务器进行 compile, profile 和 run; 服务器上会记录中间 onnx, 编译好的模型(*.so)，预测运行时间，消耗的内存，layer 个数及 cycle 数</li>
</ul>
</li>
</ol>
<h2 id="doc"><a href="#doc" class="headerlink" title="doc"></a>doc</h2><ol>
<li><a target="_blank" rel="noopener" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-77512-1/hexagon-dsp-sdk-collection-landing-page.html?product=1601111740010422">hexagon sdk</a></li>
</ol>
<h2 id="硬件"><a href="#硬件" class="headerlink" title="硬件"></a>硬件</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.eefocus.com/article/1726287.html">佐思汽研《2024 年自动驾驶 SoC 研究报告》</a></li>
<li>SA8650 有 100TOPS 的 AI 算力，高于主要竞品英伟达 ORIN-N，同时图形输出能力很强，最高支持 4 个屏幕。 SA8650 可以对应 12 个摄像头，即 8 个 800 万像素，4 个 400 万像素。 SA8650 功耗大概 25-40 瓦，超过 25 瓦就最好采用水冷设计，目前 SA8650 的设计方案都是水冷</li>
</ol>
<h2 id="QNN"><a href="#QNN" class="headerlink" title="QNN"></a>QNN</h2><ol>
<li>下载<a target="_blank" rel="noopener" href="https://www.qualcomm.com/developer/software/qualcomm-ai-engine-direct-sdk">https://www.qualcomm.com/developer/software/qualcomm-ai-engine-direct-sdk</a> 点击右上角 get software<ul>
<li>wget -c <a target="_blank" rel="noopener" href="https://apigwx-aws.qualcomm.com/qsc/public/v1/api/download/software/qualcomm_neural_processing_sdk/v2.26.0.240828.zip">https://apigwx-aws.qualcomm.com/qsc/public/v1/api/download/software/qualcomm_neural_processing_sdk/v2.26.0.240828.zip</a></li>
<li>文档路径在${QNN_SDK_ROOT}&#x2F;docs</li>
</ul>
</li>
<li><code>qnn-throughput-net-run --config resnet50.json --output=resnet50_out.json</code> 运行模型<ul>
<li>配置 json 里包含 backends(.so 文件), models(.bin 文件), contexts, testCase 设置</li>
</ul>
</li>
<li>交叉编译<ul>
<li>需要指定 ${QNN_AARCH64_LINUX_OE_GCC_93}&#x2F;sysroots <a target="_blank" rel="noopener" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/setup.html#toolchains">OE</a></li>
<li>examples&#x2F;QNN&#x2F;NetRun&#x2F;linuxOE&#x2F;oe-linux-qnn-net-run.sh</li>
<li><a target="_blank" rel="noopener" href="https://github.com/quic-yocto/meta-qcom-qim-product-sdk?tab=readme-ov-file">yacto 编译</a></li>
</ul>
</li>
<li>模拟器<ul>
<li>先看</li>
</ul>
</li>
</ol>
<h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><ol>
<li>查看 npu 利用率<ul>
<li>sysMonApp 低级别工具，会提供详细信息</li>
<li>sysprofiler_app 高级别工具，基于 sysMonApp <code>sysprofiler_app --cdsp  --cdsp1 --live</code> 有些版本不支持同事看两个</li>
</ul>
</li>
</ol>
<h2 id="NOTE"><a href="#NOTE" class="headerlink" title="NOTE"></a>NOTE</h2><ol>
<li>先看一下开发板系统，是 ubuntu 还是 oe</li>
<li>linux 版本系统不是很稳定(会死机？)，推荐用 qnx 系统</li>
<li>npu(htp) 可以编程， simd</li>
<li>模型部署自定义插件是关键，插件在 npu 上运行</li>
<li>有 x86 模拟器</li>
<li>bin 文件和 so 文件区别：.bin 文件通常只包含模型的数据（如权重、偏置等），而 .so 文件是与硬件交互的可执行代码库，包含对模型推理的具体实现。在部署时，通常会将 .bin 文件中的模型数据加载到 .so 文件中的推理引擎中，然后由该引擎在高通硬件上执行推理操作。</li>
<li>QNX Software Development Platform (SDP) 需要购买</li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/07/03/triton/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/07/03/triton/" class="post-title-link" itemprop="url">triton</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-07-03 15:04:53" itemprop="dateCreated datePublished" datetime="2024-07-03T15:04:53+08:00">2024-07-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-12-24 14:45:52" itemprop="dateModified" datetime="2025-12-24T14:45:52+08:00">2025-12-24</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/07/03/triton/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/07/03/triton/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>27k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>25 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="llvm-objdump"><a href="#llvm-objdump" class="headerlink" title="llvm-objdump"></a>llvm-objdump</h2><ol>
<li><code>/share_data/triton/llvm_19_dir/251205/bin/llvm-objdump --mattr=+m,+c,+f,+a,+xsiorigin -zCDS &#39;/data/users/jiaxiyang/siorigin_triton/.triton/cache/M32BYEN7L42EZJG5X3YO773ONZ4AROFGAJPD45W3CBWOKVVXRRRA/alu_tile_kernel.elf&#39;  &gt; f32.log</code> 反汇编</li>
</ol>
<h2 id="lit-llvm-integrated-tester"><a href="#lit-llvm-integrated-tester" class="headerlink" title="lit (llvm integrated tester)"></a><a target="_blank" rel="noopener" href="https://llvm.org/docs/CommandGuide/lit.html">lit (llvm integrated tester)</a></h2><ol>
<li><a target="_blank" rel="noopener" href="https://mlir.llvm.org/getting_started/TestingGuide/">MLIR testing guide</a></li>
<li>The MLIR framework encourages existing best practices, e.g. writing and maintaining an IR spec, building an IR verifier, providing the ability to dump and parse MLIR files to text, writing extensive unit tests with the FileCheck tool, and building the infrastructure as a set of modular libraries that can be combined in new ways.</li>
<li><a target="_blank" rel="noopener" href="https://llvm.org/docs/CommandGuide/FileCheck.html">FileCheck</a></li>
<li><a target="_blank" rel="noopener" href="https://mlir.llvm.org/getting_started/TestingGuide/">MLIR lit</a></li>
<li>是 LLVM&#x2F;MLIR 标准的 测试运行框架。</li>
<li>在 MLIR 中，90% 的测试都是用 lit + FileCheck 组成。</li>
<li>可以测试多种文件，python也可以，需要# RUN %PYTHON %s 2&gt;&amp;1</li>
<li>FileCheck 用来匹配输出; 删掉可以查看输出结果</li>
<li>直接使用triton-op %s来验证是否有解析错误</li>
<li>MLIR 的绝大部分测试（.mlir、.td、.cpp 等）都依赖 lit 来运行。<ul>
<li><code># RUN: mlir-opt %s -convert-scf-to-cf | FileCheck %s</code> mlir文件注释是要执行的命令; 用 .mlir 文件作为输入，运行某些 pass，最后用 FileCheck 验证输出。</li>
<li>mlir-opt可以加–debug</li>
</ul>
</li>
<li>解释</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// RUN: triton-opt %s -split-input-file -verify-diagnostics --debug</span><br><span class="line">含义是：lit 会用 `triton-opt` 读取这个文件 `%s`，并加上两个关键选项：</span><br><span class="line">- `-split-input-file`：按文件中的 `// -----` 分隔，把一个文件**拆成多个子模块/子测试**，每个子块单独跑一遍。</span><br><span class="line">- `-verify-diagnostics`：让 MLIR 检查每个块中 `// expected-error` 之类的注释，**验证诊断（报错/警告）是否按预期出现**。</span><br></pre></td></tr></table></figure>
<ol>
<li>看中间结果</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// RUN: mlir-translate -mlir-to-llvmir %s | llc -mtriple=riscv64-rv64gv-linux-gnu -mcpu=sipu100 | FileCheck %s</span><br><span class="line">改为</span><br><span class="line">// RUN: mlir-translate -mlir-to-llvmir %s | tee /dev/tty | llc -mtriple=riscv64-rv64gv-linux-gnu -mcpu=sipu100 | tee /dev/tty | FileCheck %s</span><br><span class="line">或</span><br><span class="line">// RUN: mlir-translate -mlir-to-llvmir %s &gt; tranlate.llc &amp;&amp; llc -mtriple=riscv64-rv64gv-linux-gnu -mcpu=sipu100 tranlate.llc -o tranlate.llc.s &amp;&amp; FileCheck %s &lt; tranlate.llc.s</span><br><span class="line">或 去掉 FileCheck</span><br><span class="line">// RUN: mlir-translate -mlir-to-llvmir %s | llc -mtriple=riscv64-rv64gv-linux-gnu -mcpu=sipu100</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol>
<li>修改triton Makefile代码test-lint命令加上<code>-v</code>, 使用 <code>make test-lint</code>命令可以打印出测试的例子<ul>
<li>到build目录下<code>lit -v test/</code>执行，可以看到每一个测试测试状态, test是路径,测试路径下的所有文件</li>
<li><code>lit --show-tests test/</code> 只查看不执行</li>
<li><code>lit -v test/TritonSIPU/IR/linear-layout-attribute.mlir</code>只执行一个测试</li>
<li><code>lit -a tile-indexed-load-store.mlir</code> 执行一个文件的测试</li>
<li><code>lit -a --filter &#39;linear-layout-attribute.mlir&#39; test/</code> filter过滤，filter_out排除</li>
<li><code>-a</code> 输出测试过程中的详细信息，包含测试的输入输出</li>
<li><code>--debug</code>打印配置信息</li>
</ul>
</li>
<li>cp build目录下 <code>lit.site.cfg.py</code> 到test&#x2F; 文件夹下，就能在test目录下直接运行lit</li>
</ol>
<h2 id="NOTE"><a href="#NOTE" class="headerlink" title="NOTE"></a>NOTE</h2><ol>
<li>pytorch triton调度：Triton 的 kernel 调度是 PyTorch（准确说是 TorchInductor）根据图优化结果、张量形状和硬件信息来决策的，Triton 只是负责生成和编译 kernel，不负责调度策略本身。</li>
<li>triton是thread block粒度的编程， 不需要划分threads(warp)，只需要计算好每个thread block要算的数，和算法流程<ul>
<li>flash—attention v1 和 v2处理的数据不同，v1 一个thread block处理1个head attention, v2一个thread block处理 1&#x2F;num_m_blocks个head attention</li>
</ul>
</li>
<li>kernel 入参没用最好删除，编译出的ttir和driver.py参数可能对不上</li>
<li>参考mlir dialect td<a target="_blank" rel="noopener" href="https://github.com/llvm/llvm-project/blob/2e82a17f4e71a833cc3ca4a832bd14a5ef537616/mlir/include/mlir/Dialect/Arith/IR/ArithOps.td">llvm-project milr dialect td defination</a></li>
</ol>
<h2 id="value构造"><a href="#value构造" class="headerlink" title="value构造"></a>value构造</h2><ol>
<li>include&#x2F;triton&#x2F;Conversion&#x2F;TritonGPUToLLVM&#x2F;Utility.h<ul>
<li>i64_val(3)</li>
<li>#define i64_val(…) LLVM::createConstantI64(loc, rewriter, <strong>VA_ARGS</strong>)</li>
</ul>
</li>
</ol>
<h2 id="recompile"><a href="#recompile" class="headerlink" title="recompile"></a>recompile</h2><ol>
<li>triton在什么情况下会recompile kernel</li>
<li>不同输入向量化长度不一样？输入数据长度能影响llir生成？测试发现确实变了， BLOCK_SIZE&#x3D;2048时会生成两种llir</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/triton-lang/triton/blob/99b5e296ca21ba21b6c02dc48391422e53f25ffb/python/triton/runtime/jit.py#L580</span><br><span class="line">https://github.com/triton-lang/triton/blob/99b5e296ca21ba21b6c02dc48391422e53f25ffb/python/triton/runtime/jit.py#L320</span><br><span class="line">https://github.com/triton-lang/triton/blob/99b5e296ca21ba21b6c02dc48391422e53f25ffb/python/triton/backends/compiler.py#L79</span><br><span class="line">变量是16的倍数会特殊处理， specialization参数的key会加D</span><br><span class="line">key:  [(&#x27;*i32&#x27;, &#x27;D&#x27;), (&#x27;*i32&#x27;, &#x27;D&#x27;), (&#x27;i32&#x27;, &#x27;&#x27;)]&#123;&#x27;num_warps&#x27;: 1, &#x27;debug&#x27;: False&#125;</span><br><span class="line">key:  [(&#x27;*i32&#x27;, &#x27;D&#x27;), (&#x27;*i32&#x27;, &#x27;D&#x27;), (&#x27;i32&#x27;, &#x27;D&#x27;)]&#123;&#x27;num_warps&#x27;: 1, &#x27;debug&#x27;: False&#125;</span><br><span class="line">所以n在16的倍数和非16倍数之间切换时会recompile</span><br></pre></td></tr></table></figure>

<h2 id="mask"><a href="#mask" class="headerlink" title="mask"></a>mask</h2><ol>
<li>在 Triton 中，mask 是控制 是否参与计算 的一个非常重要的机制，通常用于 避免越界访问 或 条件执行。在编写 Triton kernel 时，mask 一般配合 tl.where, tl.load, tl.store 等指令使用。</li>
<li>load, store只是避免越界访问，where才是mask数据执行的</li>
<li><a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/607c50cc9fdd2541db88b5a8681164f081dd71ad/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/LoadStoreOpToLLVM.cpp#L210">load根据ptr和mask计算向量化load长度</a></li>
<li>triton-cpu<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton-cpu/blob/0625715c271426eb61dd37c43deb8bc954bf4f23/third_party/cpu/lib/TritonToTritonCPU/ConvertMemoryOps.cpp#L184">两种情况三种处理方式</a></li>
</ul>
</li>
</ol>
<h2 id="tutorial"><a href="#tutorial" class="headerlink" title="tutorial"></a>tutorial</h2><h3 id="06-fused-attention"><a href="#06-fused-attention" class="headerlink" title="06 fused-attention"></a>06 fused-attention</h3><ol>
<li>“on-band” 通常指的是靠近注意力矩阵主对角线的部分，也就是当前 token 更多地关注自己附近的 token。</li>
<li>“offband” 指的是距离对角线较远的注意力，也就是 token 关注的位置比较远（长距离依赖）</li>
</ol>
<h2 id="libdevice"><a href="#libdevice" class="headerlink" title="libdevice"></a>libdevice</h2><ol>
<li>调用库</li>
<li><a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/7a83ed78d95e0afcf89ea25289347fb079d47756/python/tutorials/07-extern-functions.py#L43">07-extern-functions.py</a></li>
</ol>
<h2 id="cuda和triton区别"><a href="#cuda和triton区别" class="headerlink" title="cuda和triton区别"></a>cuda和triton区别</h2><ol>
<li><a target="_blank" rel="noopener" href="https://chatgpt.com/share/67dcd467-b918-8004-886d-5d3381653365">gpt</a></li>
<li>编程粒度： cuda 是thread, triton 是thread block</li>
</ol>
<h2 id="ALU-和-SFU"><a href="#ALU-和-SFU" class="headerlink" title="ALU 和 SFU"></a>ALU 和 SFU</h2><ol>
<li>ALU（Arithmetic Logic Unit） </li>
<li>SFU（Special Function Unit） </li>
<li>ALU（Arithmetic Logic Unit） 和 SFU（Special Function Unit） 是现代处理器（尤其是 GPU）中经常并存的两类计算单元</li>
</ol>
<h2 id="对数"><a href="#对数" class="headerlink" title="对数"></a>对数</h2><ol>
<li>mma生成比较矩阵txt</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">result = (matmul_outputs_sipu.to(<span class="string">&quot;cpu&quot;</span>)- matmul_outputs_golden).<span class="built_in">abs</span>() &gt; <span class="number">0.1</span>  <span class="comment"># 误差大于 0.1 的地方设为 True</span></span><br><span class="line">result = result.to(torch.<span class="built_in">int</span>)  <span class="comment"># 转换为整数 (1 表示误差超出，0 表示正常)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 NumPy 加速写入</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.savetxt(<span class="string">&quot;result1.txt&quot;</span>, result.cpu().numpy()[:<span class="number">64</span>, :<span class="number">64</span>], fmt=<span class="string">&quot;%d&quot;</span>)</span><br><span class="line"></span><br><span class="line">result = (C - golden_C).<span class="built_in">abs</span>() &gt; <span class="number">0.1</span></span><br><span class="line">result = result.to(torch.<span class="built_in">int</span>)  <span class="comment"># 将布尔值转换为整数</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;result1.txt&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(result.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(result.shape[<span class="number">1</span>]):</span><br><span class="line">            f.write(<span class="string">f&quot;<span class="subst">&#123;result[i, j].item()&#125;</span> &quot;</span>)</span><br><span class="line">        f.write(<span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="coordinate-到-index的映射"><a href="#coordinate-到-index的映射" class="headerlink" title="coordinate 到 index的映射"></a>coordinate 到 index的映射</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#  坐标(x, y)到index的转换, 将M行N列矩阵划分为（m, n) 的block,block内部行优先存储，block之间也是行优先</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#  block index </span></span><br><span class="line"><span class="comment">#  block_row = x // m</span></span><br><span class="line"><span class="comment">#  block_col = y // n</span></span><br><span class="line"><span class="comment">#  blocks_per_row = N // n</span></span><br><span class="line"><span class="comment">#  block_index = block_row * blocks_per_row + block_col = x // m * (N // n) + y // n</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#  local index</span></span><br><span class="line"><span class="comment">#  local_row = x % m</span></span><br><span class="line"><span class="comment">#  local_col = y % n</span></span><br><span class="line"><span class="comment">#  local_index = local_row * n + local_col = x % m * n + y % n</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#  global index</span></span><br><span class="line"><span class="comment">#  global_index = block_index * m * n + local_index = (x // m * (N // n) + y // n) * m * n + x % m * n + y % n</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (x, y) -&gt; (x // m * (N // n) + y // n) * m * n + x % m * n + y % n</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#坐标(x, y)到index的转换, 将M行N列矩阵划分为（m, n) 的block,block内部列优先存储，block之间也是列优先</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#  block index </span></span><br><span class="line"><span class="comment">#  block_row = x // m</span></span><br><span class="line"><span class="comment">#  block_col = y // n</span></span><br><span class="line"><span class="comment">#  blocks_per_col = M // m</span></span><br><span class="line"><span class="comment">#  block_index = block_col * blocks_per_col + block_row = y // n * (M // m) + x // m</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#  local index</span></span><br><span class="line"><span class="comment">#  local_row = x % m</span></span><br><span class="line"><span class="comment">#  local_col = y % n</span></span><br><span class="line"><span class="comment">#  local_index = local_col * m + local_row = y % n * m + x % m</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#  global index</span></span><br><span class="line"><span class="comment">#  global_index = block_index * m * n + local_index = (y // n * (M // m) + x // m) * m * n + y % n * m + x % m</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (x, y) -&gt; (y // n * (M // m) + x // m) * m * n + y % n * m + x % m</span></span><br></pre></td></tr></table></figure>


<h2 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a><a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/tree/main/bin">Tools</a></h2><ol>
<li>vscode mlir 插件配置 triton-lsp<ul>
<li>op可以跳转到src定义</li>
<li>loc可以看行数</li>
<li>log.txt中可以看outline，能找到IR Dump Before</li>
<li>需要注册自定义dialet <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/bin/RegisterTritonDialects.h">link</a>：头文件和registry.insert dialect, 注册pass</li>
</ul>
</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;siorigin/include/Dialect/TritonSIPU/IR/Dialect.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;siorigin/include/TritonToTritonSIPU/Passes.h&quot;</span></span></span><br><span class="line">mlir::triton::siorigin::<span class="built_in">registerConvertLoadStoreOpsPass</span>();</span><br><span class="line">registry.insert&lt;mlir::triton::siorigin::TritonSIPUDialect&gt;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol>
<li><a target="_blank" rel="noopener" href="https://mlir.llvm.org/docs/Tools/MLIRLSP/#build-system-integration-1">tablegen_compile_commands.yml</a><ul>
<li>配置mlir tblgen-lsp-server 和 tablen_compile_commands.yml</li>
<li>可以从td文件跳转到生成的定义</li>
<li>hover</li>
</ul>
</li>
<li>triton-opt<ul>
<li>将中间结果存下来，使用triton-opt定位问题</li>
</ul>
</li>
<li>triton-llvm-opt</li>
<li>graphiviz<ul>
<li><code>triton-opt matmul_kernel.ttir --view-op-graph --allow-unregistered-dialect &gt; test.dot 2&gt;&amp;1</code></li>
<li><a target="_blank" rel="noopener" href="https://marketplace.visualstudio.com/items?itemName=tintinweb.graphviz-interactive-preview">vscode plugin open dot file</a></li>
<li><a target="_blank" rel="noopener" href="https://mlir.llvm.org/docs/Passes/#-view-op-graph">view-op-graph</a></li>
</ul>
</li>
</ol>
<h3 id="triton-tensor-layout"><a href="#triton-tensor-layout" class="headerlink" title="triton-tensor-layout"></a>triton-tensor-layout</h3><ol>
<li>bin&#x2F;triton-tensor-layout.cpp 有测试命令</li>
<li><code>triton-tensor-layout -i input.mlir -t &quot;tensor&lt;128xf32&gt;&quot; --use-hw-view  -o output1.mlir</code> <ul>
<li>input.mlir只包含类似 <code>#blocked = #ttg.blocked&lt;&#123;sizePerThread = [1], threadsPerWarp = [1], warpsPerCTA = [1], order = [0]&#125;&gt;</code>的语句</li>
<li>–use-hw-view 从硬件的view打印， 默认是data的view<ul>
<li>hw下warp下面一列代表一个thread</li>
</ul>
</li>
</ul>
</li>
<li><code>triton-tensor-layout -l &quot;#ttg.blocked&lt;&#123;sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]&#125;&gt;&quot; -t &quot;tensor&lt;16x16xf16&gt;&quot;</code> 注意：用ttg，不用triton_gpu</li>
<li><code>triton-tensor-layout -l &quot;#ttg.nvidia_mma&lt;&#123;versionMajor = 3, versionMinor = 0, warpsPerCTA = [8, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0], instrShape = [16, 256, 32]&#125;&gt;&quot; -t &quot;tensor&lt;128x256xf16&gt;&quot;</code><ul>
<li>unittest&#x2F;Dialect&#x2F;TritonGPU&#x2F;DumpLayoutTest.cpp里有测试代码</li>
<li>关键代码: shared layout, distribute layout<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">std::string mlir::triton::gpu::<span class="built_in">getLayoutStr</span>(RankedTensorType tensorType,</span><br><span class="line">                                            <span class="type">bool</span> useHWPointOfView) &#123;</span><br><span class="line">  <span class="keyword">auto</span> layout = tensorType.<span class="built_in">getEncoding</span>();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// tensorType is needed later on (e.g., getDimSize(j)), so we still have to</span></span><br><span class="line">  <span class="comment">// pass it as a param</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">auto</span> sharedLayout = mlir::<span class="built_in">dyn_cast</span>&lt;SharedEncodingAttr&gt;(layout)) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">getSharedLayoutStr</span>(tensorType, useHWPointOfView);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">auto</span> distributedLayout =</span><br><span class="line">                 mlir::<span class="built_in">dyn_cast</span>&lt;DistributedEncodingTrait&gt;(layout)) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">getDistributedLayoutStr</span>(tensorType, useHWPointOfView);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// else unimplemented, return error</span></span><br><span class="line">  llvm::<span class="built_in">report_fatal_error</span>(<span class="string">&quot;Unimplemented usage of getLayoutStr&quot;</span>);</span><br><span class="line">  <span class="keyword">return</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="lit"><a href="#lit" class="headerlink" title="lit"></a>lit</h3><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><ol>
<li>tt.splat 操作用于将一个标量值扩展为一个张量，其中所有元素都具有相同的标量值。</li>
<li>tt.broadcast 操作用于将一个较小的张量扩展为一个较大的张量，通过复制其元素来匹配目标张量的形状。</li>
<li>NOTE: tutorial 中的BLOCK_SIZE和cuda中的block_size不是一样的， BLOCK_SIZE是data size, block size是cuda中一个block有多少thread<ul>
<li>BLOCK_SIZE 是一个编译时常量，用来指定每个 kernel 实例（也可以看作是“tile”或“block”）要处理的数据量，即每个程序实例负责处理多少个元素。</li>
<li>block_size 通常是指每个 block 中的线程总数（即 num_warps × 32）以及每个线程负责的工作量（sizePerThread）</li>
<li>Triton 编译器和运行时会根据这个 BLOCK_SIZE 自动确定实际的线程组织（比如如何分配到 warps 中），从而隐式地管理 num_warps 和 sizePerThread</li>
<li>教程中定义的 BLOCK_SIZE 并没有和我们之前讨论的概念冲突，而是用一种更高层的抽象来表达同样的思想。它既代表了一个 block 中需要处理的数据总量，也间接影响了内部线程数和每个线程的工作量，只不过这些底层细节由 Triton 自动处理了。</li>
<li><code>BLOCK_SIZE = block_size × sizePerThread = num_warps x threadsPerWarp × sizePerThread</code></li>
</ul>
</li>
<li>当输入不是const, 如何传递tl.constexp? BLOCK_SIZE &#x3D; triton.next_power_of_2(n_cols)</li>
</ol>
<h2 id="适配-DSL"><a href="#适配-DSL" class="headerlink" title="适配 DSL"></a>适配 DSL</h2><ol>
<li>intel<br><img src="https://i.ibb.co/cF3yF0K/e-Hk7ps-HTk-M.png" alt="intel backend architecture"></li>
<li><a target="_blank" rel="noopener" href="https://github.com/intel/intel-xpu-backend-for-triton">intel-xpu-backend-for-triton</a></li>
<li>高通<br><img src="https://i.ibb.co/6NMJ8Yk/xo5-Pqj-RPDT.png" alt="architecture"><br><img src="https://i.ibb.co/9VLhpNk/du-T6l-PNQfg.png" alt="pytorch"></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ONrKkI7KhU4&list=PLc_vA1r0qoiTjlrINKUuFrI8Ptoopm8Vz&index=18">Triton Conference 2024: Afternoon Session</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/microsoft/triton-shared">microsoft&#x2F;triton-shared</a></li>
<li><a target="_blank" rel="noopener" href="https://llvm.org/docs/TableGen/ProgRef.html">tablegen 来生成 MLIR 代码</a></li>
</ol>
<h2 id="tensor和线程layout"><a href="#tensor和线程layout" class="headerlink" title="tensor和线程layout"></a>tensor和线程layout</h2><ol>
<li><p>要实现峰值性能，不仅需要利用专用硬件单元，还需要精心设计张量布局和转换, tensor layout and layout conversion</p>
</li>
<li><p>在编译过程中，Triton 的 Python 代码首先被翻译成 Triton 方言 ( tt )，然后再翻译成 TritonGPU 方言 ( ttg )。在此过程中，每个张量都与特定的布局相关联，以充分利用现代 GPU 上可用的硬件功能单元。</p>
</li>
<li><p>layout不仅针对load store, 不同的op有不同的layout偏好，需要进行layout转换来使用最佳layout, 此时，需要进行数据重排</p>
</li>
<li><p>lowering过程不仅需要sizePerThread信息，还需要contiguity信息， contiguity表明逻辑和物理的映射连续性; </p>
<ul>
<li>有了thread要处理的数据和数据的连续性信息才能lowering</li>
</ul>
</li>
<li><p>tensor划分的顺序从左到右，sizePerThread -&gt; threadsPerWarp -&gt; warpsPerCTA …; 如果和tensor shape不匹配，需要广播</p>
<ul>
<li>左边是最里面的维度, 是移动最快的维度。</li>
</ul>
</li>
<li><p>只有memory相关的操作才关心sizePerThread；其他op在reg上都是连续的   </p>
</li>
<li><p>sizePerThread表明一个thread一次可连续处理的数据， elementPerThread是一个thread需要处理的数， sizePerThread由contiguity和硬件限制等参数算出，sizePerThread[order[0]]，只会，设置order[0]对应维度的sizePerThread，其他为1</p>
<ul>
<li>具体逻辑见 lib&#x2F;Dialect&#x2F;TritonGPU&#x2F;Transforms&#x2F;Coalesce.cpp</li>
</ul>
</li>
<li><p>sizePerThread 和 elementPerThread 有区别，sizePerThread根据contiguity算的，elementPerThread是一个线程真正要算的数</p>
<ul>
<li>The legacy Triton layout system requires each layout to define its own interface methods—such as <code>the number of elements per thread</code> and <code>the number of contiguous elements</code></li>
</ul>
</li>
<li><p>sizePerThread对应reg, threadsPerWarp对应thread, warpsPerCTA对应warp</p>
</li>
<li><p>sizePerThread &#x3D; [2, 2], threadsPerWarp &#x3D; [8, 4], warpsPerCTA &#x3D; [1, 2]. 因为此时sizePerThread<em>threadsPerWarp</em>warpsPerCTA &#x3D; 16x16, 小于tensor的shape 32x32, 所以这个16x16的layout会按照[2, 2]的shape进行广播, 填满整个32x32的tensor.</p>
</li>
<li><p><code>./triton-tensor-layout -l &quot;#triton_gpu.blocked&lt;&#123;sizePerThread = [1, 1], threadsPerWarp = [8, 4], warpsPerCTA = [2, 2], order = [1, 0]&#125;&gt;&quot; -t &quot;tensor&lt;16x8xf32&gt;&quot;</code></p>
</li>
<li><p>ConvertTritonToTritonGPU pass加的layout信息</p>
<ul>
<li>typeconverter里重新构造了tensor type， 加了layout信息, 检测到是tensor, 就添加默认layout信息</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">  <span class="built_in">addConversion</span>([<span class="keyword">this</span>](RankedTensorType tensorType) -&gt; RankedTensorType &#123;</span><br><span class="line">  <span class="comment">// types with encoding are already in the right format</span></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> check for layout encodings more specifically</span></span><br><span class="line">  <span class="keyword">if</span> (tensorType.<span class="built_in">getEncoding</span>())</span><br><span class="line">    <span class="keyword">return</span> tensorType;</span><br><span class="line">  ArrayRef&lt;<span class="type">int64_t</span>&gt; shape = tensorType.<span class="built_in">getShape</span>();</span><br><span class="line">  triton::gpu::BlockedEncodingAttr encoding =</span><br><span class="line">      <span class="built_in">getDefaultBlockedEncoding</span>(<span class="keyword">this</span>-&gt;context, shape, <span class="keyword">this</span>-&gt;numWarps,</span><br><span class="line">                                <span class="keyword">this</span>-&gt;threadsPerWarp, <span class="keyword">this</span>-&gt;numCTAs);</span><br><span class="line">  <span class="keyword">return</span> RankedTensorType::<span class="built_in">get</span>(shape, tensorType.<span class="built_in">getElementType</span>(), encoding);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></li>
<li><p><code>ptr+index &lt;-shape stride-&gt; coodinate &lt;-layout-&gt; thread id</code></p>
<ul>
<li>tensor提供coodinate到index, value的映射</li>
<li>layout提供coodinate到thread id映射, layout(coodinate) &#x3D; thread id</li>
</ul>
</li>
<li><p>不同的Layout可以看作是不同的映射函数，代表了不同访问模式。</p>
</li>
<li><p>Layout是tensor coodinate到线程的映射，we define a layout as a function that maps a multi-dimensional tensor index to a set of integers T corresponding to the indices of the CUDA threads allowed to access some data at index i. <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/672720213">link</a></p>
<ul>
<li>coodinate到index是由tensor axis info决定的</li>
</ul>
</li>
<li><p>triton也通过Layout来表征Thread对数据(tensor)的访问模式，例如blocked和blocked1分别表示A,B两个Operand的访问模式，即每个Thread取连续的8个f16等等。</p>
<ul>
<li>tensor&lt;64x64x!tt.ptr<f32>, #blocked1&gt; 既有tensor, 也有layout, data layout </li>
<li>AxisInfo 会记录张量各轴的尺寸、步幅以及与硬件映射相关的信息，用于指导后续的内存合并（coalesce）和其它优化操作，“AxisInfo”并不是存储张量所有信息的容器，它主要关注那些对高效内存访存至关重要的轴信息（例如各轴的大小、步幅、排列顺序等），而完整的张量信息还可能分布在其他 IR 属性或数据结构中。因此，可以说，Triton 的 AxisInfo 分析提取并表达了张量中与访存和并行调度密切相关的那部分信息。</li>
</ul>
</li>
<li><p>data layout 是 TritonGPU Dialect 的 Type system 的关键，确定了 Data(各层级memory中的Tensor) 到 thread 之间的映射关系。</p>
</li>
<li><p>总的来说 tensor 为数据在内存中的物理组织提供静态描述，而layout 则通过运行时索引决定每个线程应处理数据中的哪一部分。两者必须配合，才能让每个执行单元正确且高效地读取、处理和写回数据，从而充分发挥 GPU 的并行计算能力</p>
</li>
<li><p>nvidia 在ttgir生成后layout相关优化已经做完了，lowering到llvm ir根据ttgir 方案来，不会再做layout的优化</p>
</li>
<li><p>ttgir里已经有每个thread要算数据的所有信息</p>
<ul>
<li>可以删除compiler.py除了add_convert_to_ttgpuir pass之外的pass看看结果正确性和ir</li>
</ul>
</li>
<li><p>load之后数据在reg上, 后面的数据不用关心contiguty和size_per_thread; 只用看shape</p>
<ul>
<li>只有load和store考虑连续性的问题，后面的计算op不需要考虑，已经加载到reg上了</li>
</ul>
</li>
<li><p>coalesce pass里会分析continuty等信息来确认一次load和store多少数据,改变load store layout并添加conver layout op   </p>
</li>
<li><p>coalesce添加新的layout并添加convert_layout; remove_layout负责layout的传播和删除</p>
</li>
</ol>
<h3 id="linear-layout"><a href="#linear-layout" class="headerlink" title="linear layout"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.23819">linear layout</a></h3><ol>
<li>注意 linear layout表示的是输出到输入的索引，给定输出index，经过linear map左乘，得到输入index； Vin &#x3D; T@Vout</li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/PDFshzgcj_udaFu3aJr1tQ">详细介绍</a><ul>
<li>复合 Composition 讲的比较好</li>
</ul>
</li>
<li>linear layout可以表示多种含义：<ul>
<li>作为tonsor属性：硬件位置和tensor 逻辑index映射</li>
<li>作为op的属性：tensor 逻辑index之间的映射</li>
<li>两个tensor和一个convert_layout op; convert_layout op不带属性<ul>
<li>直接使用linear 组合功能</li>
</ul>
</li>
<li>两个tensor和一个op，都带linear layout属性：<ul>
<li>比两个tensor之间转换多加了逻辑之间的转换</li>
<li>结合起来可以得到硬件位置之间的映射; 硬件指令功能也用linear layout表示，通过组合来Lowering</li>
</ul>
</li>
</ul>
</li>
<li><code>https://github.com/triton-lang/triton/blob/main/include/triton/Tools/LinearLayout.h</code> 官方说明</li>
<li><code>In Triton, a linear layout (LL) is a function that maps from a &quot;hardware location&quot; to a &quot;logical tensor index&quot;.</code></li>
<li><code>To summarize, a linear layout is a function from tuples of integers to tuples of integers.  We specify some key values of the function, and then we can compute all the other values using the linearity rule.</code></li>
<li>是一种映射:<code>硬件索引 -&gt; tensor 坐标索引</code>; 查找表，linear layout是查找表的一种压缩<ul>
<li>tensor坐标到存储tensor memory的index并不从linear layout来获取</li>
</ul>
</li>
<li>只要给硬件位置basic vector对应的输出tensor index(coodinatate)，就可以算出 linear map？<ul>
<li>从硬件输入维度和tensor index可以看出输入输出维度</li>
<li>类似线性方程组，给定几个输入输出，可以得到线性方程组的参数</li>
</ul>
</li>
<li>对于 tensor layout，我们通常会问一个问题：某个线程 index（例如 [lane_x, warp_y, thread_z]）负责访问张量中的哪个 [i, j, k] 元素？<ul>
<li>作者的关键想法是：这种映射可以写成一个线性映射：[i, j, k]^T &#x3D; M × [lane_x, warp_y, thread_z]^T   （模 2 运算）其中 M 是一个 𝔽₂ 上的矩阵，形状为 [张量维度 × 硬件维度]</li>
</ul>
</li>
<li>矩阵layout M 就定义了“线程 index 到 tensor index”之间的布局映射。<ul>
<li>M没什么含义，就是线性方程组参数，是一种映射projection;</li>
<li>M的行列长度需要和硬件的[reg, thread, warp]与tensor的[i, j, k]二进制位数匹配上</li>
</ul>
</li>
<li>linear layout是一种表示：表明tensor 张量元素分布并映射到计算和内存层次结构；不同的linear layout表明tensor的映射不同。映射关系可以通过linear map来表示；两个linear layout转换也可以通过linear map来转换。   </li>
<li>linear layout: <code>x = M * p</code>; M是二进制矩阵，p是硬件二进制编码向量，x是对应的tenosr 坐标，x算出来是一列，前几位表示j, 后几位表示i<ul>
<li>linear的意思应该是通过线性运算就能做layout映射和转换</li>
<li><code>T_index = A * HW_index^T</code></li>
</ul>
</li>
<li>linear layout是从硬件寄存器找tensor 逻辑index, <code>一个reg只对应一个逻辑index, 之前的layout是从tensor 逻辑地址找硬件寄存器；一个逻辑index可以对应多个硬件reg</code>, 输入是硬件位置二进制编码，输出是tensor 逻辑坐标。<ul>
<li>使用layout 二进制矩阵来进行layout来计算</li>
</ul>
</li>
<li>linear layout和linear map是有区别的，linear map可以将一种layout转化为另一种</li>
<li>linear layout永远是二维的，输入输出可以是各种维度：输入是三维，会变成一维，如i, j, k, shape是（2， 4， 8）， 分别用1， 2，3bit表示，那么输入就是6bit的一维向量，表示的范围为2<em>4</em>8 &#x3D; 2的6次方 &#x3D; 64；输出也一样<ul>
<li>输入是行，输出是列</li>
</ul>
</li>
<li>linear layout的base vector,基向量<ul>
<li>表示label所在列为1对应的bit位所在行号的2的指数结果，从0开始；如下面warp&#x3D;2 -&gt; (8),最后一列为1的行号是3， 2的3次方为8</li>
<li>ob表示二进制</li>
<li>输出维度为1，dim0为2的4次方16 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">   shape: [16]</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">ttg.blocked&lt;&#123;sizePerThread = [1], threadsPerWarp = [4], warpsPerCTA = [4], order = [0]&#125;&gt;</span></span><br><span class="line">RepOrder: [0]</span><br><span class="line">TotalElemsPerThread: 1</span><br><span class="line">ElemsPerThread: [1]</span><br><span class="line">- register is a size 1 dimension</span><br><span class="line">- lane=1 -&gt; (1)</span><br><span class="line">  lane=2 -&gt; (2)</span><br><span class="line">- warp=1 -&gt; (4)</span><br><span class="line">  warp=2 -&gt; (8)</span><br><span class="line">- block is a size 1 dimension</span><br><span class="line">where out dims are: [dim0 (size 16)]</span><br><span class="line">0b1000</span><br><span class="line">0b0100</span><br><span class="line">0b0010</span><br><span class="line">0b0001</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>二维<ul>
<li>输出是二维的，前6bit表示第一维，后7bit表示第二维，register&#x3D;1 -&gt; (1, 0) 表示第一维的第一列的第一位为1； register&#x3D;8 -&gt; (0, 16)表示第4列的第二维的第5位为1, 表明第四个reg的位置决定了第二维第5位的值<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">shape: [64, 128]</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">ttg.blocked&lt;&#123;sizePerThread = [8, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [1, 2], CTASplitNum = [1, 2], CTAOrder = [1, 0]&#125;&gt;</span></span><br><span class="line">th 4 * 4 = 16 -&gt; 64</span><br><span class="line">RepOrder: [0, 1]</span><br><span class="line">TotalElemsPerThread: 32</span><br><span class="line">ElemsPerThread: [8, 4]</span><br><span class="line">- register=1 -&gt; (1, 0) </span><br><span class="line">  register=2 -&gt; (2, 0)</span><br><span class="line">  register=4 -&gt; (4, 0)</span><br><span class="line">  register=8 -&gt; (0, 16)</span><br><span class="line">  register=16 -&gt; (0, 32)</span><br><span class="line">- lane=1 -&gt; (8, 0)</span><br><span class="line">  lane=2 -&gt; (16, 0)</span><br><span class="line">  lane=4 -&gt; (32, 0)</span><br><span class="line">  lane=8 -&gt; (0, 1)</span><br><span class="line">  lane=16 -&gt; (0, 2)</span><br><span class="line">- warp=1 -&gt; (0, 4)</span><br><span class="line">  warp=2 -&gt; (0, 8)</span><br><span class="line">- block=1 -&gt; (0, 64)</span><br><span class="line">where out dims are: [dim0 (size 64 b6), dim1 (size 128 b7)]</span><br><span class="line">0b1000000000000</span><br><span class="line">0b0100000000000</span><br><span class="line">0b0010000000000</span><br><span class="line">0b0000010000000</span><br><span class="line">0b0000001000000</span><br><span class="line">0b0000000100000</span><br><span class="line">0b0000000010000</span><br><span class="line">0b0000000001000</span><br><span class="line">0b0000000000100</span><br><span class="line">0b0000000000010</span><br><span class="line">0b0001000000000</span><br><span class="line">0b0000100000000</span><br><span class="line">0b0000000000001</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>linear layout某一列可以为全0，如warp2为0，表明没有用完warp</li>
<li>linear layout的行列的bit数表明有输入输出有多少种情况，用一个矩阵可以表明输入和输出的各种组合</li>
<li><code>include/triton/Tools/LinearLayout.h</code>有说明</li>
<li><code>unittest/Tools/LinearLayoutTest.cpp</code> 有测试<ul>
<li><code>./unittest/Tools/LinearLayout  --gtest_filter=LinearLayoutTest.Empty</code></li>
<li>linear layout base vector &#x3D; 0, 表示repeat; 当前列决定不了输出，所以会和其他输入的结果重复</li>
<li>输入输出可以先将bit个数转成shape, bit位不是很熟悉</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/unittest/Dialect/TritonGPU/LinearLayoutConversionsTest.cpp">LinearLayoutConversionsTest</a><ul>
<li>Linear Layout	所有张量元素在内存中按一维线性排列</li>
<li><code>unittest/Dialect/TritonGPU/LinearLayoutConversions --gtest_filter=LinearLayoutConversionsTest.ShapeLargerThanLayout</code></li>
</ul>
</li>
<li>Every tensor layout is modeled as a linear function—a matrix—that maps physical resource indices into a logical tensor of size n2 using binary arithmetic on the bits of the input and the output.<ul>
<li>每个张量布局都被建模为一个线性函数（矩阵），它使用对输入和输出位的二进制算术将物理资源索引映射到大小为n方的逻辑张量。</li>
<li>矩阵是一个映射， 将物理资源映射为逻辑张量的索引</li>
</ul>
</li>
<li>We define a Linear Layout as a linear map between (labeled) vector spaces over F2<ul>
<li>For example, we can define layout L as:  L : Reg × Thr × Wrp → 𝔽₂ⁿ × 𝔽₂ᵐ (n, m是坐标shape的二进制长度, 如16x32, n&#x3D;4, m&#x3D;5)</li>
</ul>
</li>
<li>使用linear layout布局的传播就比较好实现，直接使用组合从一种layout到另一种layout</li>
<li>linear layout工具<ul>
<li>矢量化：不用再手动算Contiguous elements等信息，根据L来算</li>
<li>Broadcasting: 数据小于线程，多个线程算一个数据，数据多余线程，一个线程算多个数据，需要广播。之前的layout很难处理；linear layout只要按需改L矩阵</li>
</ul>
</li>
<li>Linear Layouts 作为编译器的一部分，实现了布局推理（inference）、布局转换（conversion）以及代码生成（codegen）的一体化流程。通过矩阵操作，编译器能够自动选择最优布局、插入转换并调度硬件指令，而不再依赖繁杂的专门代码或手工优化</li>
<li>Triton Linear Layout 是 编译器中端优化阶段的核心技术，专注于解决张量布局的硬件映射与转换问题。其通过代数建模统一了 NVIDIA&#x2F;AMD 等硬件的布局规则，显著降低编译器开发复杂性和错误率，并为高性能算子（如 GEMM）提供接近硬件的峰值性能。</li>
<li>之前的做法是为每种 layout（blocked, swizzled, pitch-linear, tiled 等）硬编码映射逻辑，很容易造成代码重复、不可组合、维护难。   </li>
<li>能看到 layout × thread 的过程吗？可从 Triton IR 或 TTGIR 中看到地址偏移计算指令，含 mul, add, bitwise 等<ul>
<li>ttgir的layout（之前block) 可以确认每个线程计算的数据， linear layout也可以</li>
</ul>
</li>
<li>同一个数据，其实面对两个标号：<code>空间逻辑位置和thread的全局标号</code>，layout则是建立二者之间的映射关系，这样编程的时候就知道当前的thread应该操作哪个数据，进而推广到当前的一簇thread（比如一个warp，一个warp group）应该操作哪一个集合内的数据<ul>
<li>每个thread由于所处的warp，cluster，block不同，也有自己唯一的全局标号&lt;block_id, warp_id, lane_id&gt;</li>
</ul>
</li>
<li>阶段	               传统方案     	Linear Layout 方案<ul>
<li>前端 → TTGIR      硬编码布局声明	布局定义为 F2 矩阵</li>
<li>布局转换	       手工编写转换规则	矩阵乘法（A * B^{-1}）</li>
<li>后端代码生成	   通用共享内存重排	映射至硬件原语（如 ldmatrix）</li>
</ul>
</li>
<li>如果你有一个张量 A[i,j,k]，分配到 thread block、warp、lane 的顺序是错综复杂的，传统做法需要手写映射函数。现在，只需用一个二进制矩阵 M_layout，你就能表达这种从 hardware index → tensor index 的映射：<ul>
<li>tensor_index &#x3D; M_layout × hardware_index（mod 2</li>
</ul>
</li>
<li>ttgir生成的blocked mma 等 layout 可以tolinear_layout然后再使用，根据自己硬件的特性可以对to linear layout做hack</li>
<li>Triton’s operations fall into four categories: (1) computation, (2) memory (global, shared, tensor, etc.), (3) layout conversion, and (4) shape operations.<ul>
<li>Triton 的操作分为四类：(1) 计算，(2) 内存（全局、共享、张量等），(3) 布局转换，以及 (4) 形状操作。</li>
</ul>
</li>
</ol>
<h4 id="从linear-layout获取每个thread要处理的数据"><a href="#从linear-layout获取每个thread要处理的数据" class="headerlink" title="从linear layout获取每个thread要处理的数据"></a>从linear layout获取每个thread要处理的数据</h4><ol>
<li>LinearLayout::apply()函数接收硬件二进制indices，输出tensor index；输入只是常量，是int32</li>
<li>SmallVector&lt;std::pair&lt;StringAttr, Value&gt;&gt; applyLinearLayout， 输入输出都是变量，是value<ul>
<li>输出有label + value组成，几维的输出结果的size就是几</li>
</ul>
</li>
</ol>
<h3 id="convert-layout"><a href="#convert-layout" class="headerlink" title="convert_layout"></a><a target="_blank" rel="noopener" href="https://superjomn.github.io/posts/triton-mlir-publish/#convertlayoutop">convert_layout</a></h3><ol>
<li>convert_layout： %out &#x3D; convert_layout %in: tensor&lt;128x128xf32, #layoutA&gt; -&gt; tensor&lt;128x128xf32, #layoutB&gt; 用线程去读取 layoutA 下的数据，然后写入 layoutB 对应的位置。<ul>
<li>编译器会自动生成一套数据搬运逻辑（通常是多个线程协作 copy），实现这个转换。</li>
</ul>
</li>
<li>convert_layout 的本质是：重新安排 tensor 数据在内存中的排列顺序，以便适配不同的线程访问模式或硬件特性。<ul>
<li>convert_layout 的目的：在计算或搬运之前，把数据布局变换成 适合当前线程访问方式 的形式。</li>
<li>不是直接在 global memory 上重排，而是： 每个 thread 从 global memory 按原 layout 读取数据到自己的寄存器， 然后根据目标 layout 的规则，通过 thread 之间的 shuffle、store 等操作，将数据搬运成新布局</li>
<li><a target="_blank" rel="noopener" href="https://chatgpt.com/share/68819589-cd78-8004-864f-95b3b46e7577">gpt</a></li>
<li>load之后的blocked layout是在寄存器上</li>
<li>#blocked -&gt; #shared ，代表数据从 register file 存储到 shared memory 中，</li>
<li>#blocked -&gt; #blocked1 ，代表数据从 register file 存储到register file 中，thread内部只需要改变顺序，thread间可能需要shared memory处理</li>
<li>#mma -&gt; #blocked ，正常是 DotOp 的输出转换为更简单的 layout 来进一步计算，由于涉及到跨 thread 间的数据传递，因此一般会借由 shared memory 中转一次</li>
</ul>
</li>
</ol>
<h2 id="axis-info"><a href="#axis-info" class="headerlink" title="axis info"></a>axis info</h2><ol>
<li>先Analysis再Transform <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/687394750">link</a></li>
<li>include&#x2F;triton&#x2F;Analysis&#x2F;AxisInfo.h 中有解释contiguity、divisibility 和 constancy, 是用于描述张量（Tensor）在不同维度上的内存布局和值的分布特性的重要属性，它们通常用于优化编译器的代码生成，特别是在 GPU 上生成高效的内存访问模式时。<ul>
<li><a target="_blank" rel="noopener" href="https://chatgpt.com/share/6826a649-edc8-8004-bb2e-fe9255d2b68e">gpt</a></li>
</ul>
</li>
<li>打印axis info代码位置：<a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/676227a023b88d48f59d660df6c12d98630ed240/lib/Analysis/AxisInfo.cpp#L1336">link</a></li>
<li>可以获取Value的axis info</li>
<li>contiguity	最短的连续整数段长度	内存布局是否紧密</li>
<li>divisibility	每段起始值能被整除的最大 2 的幂次	值是否有规律、对齐</li>
<li>constancy	最短的相同数值序列长度	是否包含重复常数，用于优化广播等</li>
<li>每个属性是多为的，dim 0表示最高维，最后面一个数是最低维</li>
<li>AxisInfo Analysis<ul>
<li>TRITON_LLVM_DEBUG_ONLY&#x3D;”tritongpu-coalesce” 打印AxisInfo</li>
<li>Contiguity: 连续性，当数据（如数组或向量）的各个元素在内存中按照顺序排列、地址连续时，我们称该数据结构是连续的</li>
<li>Divisibility：可分性, 在处理数组或矩阵时，如果数据维度的长度能整除分块大小，则可以将数据分割为连续且大小均一的块（例如在 tiling 操作中），这有助于内存访问的局部性和缓存利用率。<ul>
<li>tt.divisibility 属性就是用来给出对数据（通常是指针或者整数值）的整除性（或对齐性）的保证</li>
<li>这种机制类似于其他编译器中通过“对齐”说明符（如 attribute((aligned(16))) 等）提供的优化提示，但在 Triton IR 中以一种专用的方式表达。</li>
</ul>
</li>
<li>Constancy(Constant Value): 指这个Tensor是常量，得到其常量的长度，对于%1是通过arith.constant得到的，其长度为128，constant value为1</li>
</ul>
</li>
<li>gpu blocked layout是线程分配数据的layout，不是数据的layout   </li>
<li>访存合并需要看load store的oprand的<code>线程访问顺序和数据的排布</code>决定是不是需要合并访存  sample:</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#blocked1 = #triton_gpu.blocked&lt;&#123;sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]&#125;&gt;</span><br><span class="line">%10 = tt.addptr %7, %9 : tensor&lt;64x64x!tt.ptr&lt;f32&gt;, #blocked1&gt;, tensor&lt;64x64xi32, #blocked1&gt;</span><br><span class="line">%19 = tt.load %10, %cst, %cst_0 &#123;cache = 1 : i32, evict = 1 : i32, isVolatile = false&#125; : tensor&lt;64x64xf32, #blocked1&gt;</span><br><span class="line">// %10 的AxisInfo contiguity = [1, 64], divisibility = [4, 16], constancy = [1, 1], constant_value = &lt;none&gt;</span><br><span class="line">// %10的blocked threadsPerWarp = [32, 1]表明线程按列取数，AxisInfo  contiguity = [1, 64]表明数据是行优先的，load的时候访存效率不高，需要访存合并coalesce,  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="lowering"><a href="#lowering" class="headerlink" title="lowering"></a>lowering</h2><h3 id="for"><a href="#for" class="headerlink" title="for"></a><a target="_blank" rel="noopener" href="https://mlir.llvm.org/docs/Dialects/SCFDialect/#scffor-scfforop">for</a></h3><ol>
<li>for -&gt; scf -&gt; cf -&gt; llvm.cf -&gt; br</li>
</ol>
<h3 id="make-range"><a href="#make-range" class="headerlink" title="make_range"></a>make_range</h3><ol>
<li>tt.make_range决定每一个线程计算的数据量，triton是block level的编程，通过make_range变成thread level</li>
<li>make_range里会获取cluster, block, warp, thread的id, 会将数据划分到id对应index, ttir里无layout信息，ttgir里有，根据ttgir来lowering<ul>
<li>TTIR 中的操作是抽象的张量级操作，没有说明具体由哪个线程处理什么数据。</li>
<li>TTGIR 明确了：<ul>
<li>每个线程处理哪些元素</li>
<li>线程的线程索引&#x2F;维度</li>
<li>block&#x2F;grid 的结构</li>
</ul>
</li>
<li><code>./triton-tensor-layout -l &quot;#triton_gpu.blocked&lt;&#123;sizePerThread = [1, 1], threadsPerWarp = [8, 4], warpsPerCTA = [2, 2], order = [1, 0]&#125;&gt;&quot; -t &quot;tensor&lt;16x8xf32&gt;&quot;</code></li>
<li><code>python/build/cmake.linux-x86_64-cpython-3.10/bin/triton-tensor-layout -l &quot;#ttg.blocked&lt;&#123;sizePerThread = [1], threadsPerWarp = [1], warpsPerCTA = [1], order = [0]&#125;&gt;&quot; -t=&quot;tensor&lt;256xf32&gt;&quot;</code></li>
</ul>
</li>
<li>sipu 中tile_format输入时， make_range决定数据偏移</li>
</ol>
<h2 id="base"><a href="#base" class="headerlink" title="base"></a>base</h2><ol>
<li>intrinsic, asm, builtin<ul>
<li>asm（内联汇编）手动在 C&#x2F;C++ 代码中嵌入汇编指令，提供最底层、最精细的控制，但可移植性最差，需要针对不同架构单独编写汇编语句。 </li>
<li>intrinsic（编译器内建函数）看似普通函数调用，编译器将其“内联”替换为对应硬件指令或指令序列，既能保证性能，又提高了可维护性和跨平台性（在支持该 intrinsic 的架构上直接生成指令，否则回退至通用实现）。 </li>
<li>builtin（GCC&#x2F;Clang 内置函数）以 _<em>builtin</em> 前缀出现，无须包含头文件，由编译器原生识别并优化（如 __builtin_popcount 可映射为硬件 POPCNT 指令），兼具内建函数与语言扩展的特点，可用于原子操作、数学计算等多种场景</li>
</ul>
</li>
<li>intrinsic（内在函数）和 asm（Inline assembly 内联汇编）各有优缺点：内在函数提供了一种较高级、便捷且安全的方式调用底层指令，同时让编译器能充分优化代码；而内联汇编则提供了极致的控制能力，但也带来可移植性和维护性上的挑战。</li>
<li>在编译器和编程语言文档中，”intrinsic” 通常指由编译器直接内置的函数，用于提高效率，将函数调用转化为内联代码或特定的机器指令。”builtin” 则多用作 “intrinsic” 的同义词</li>
<li>builtin是编译器内置，intrinsic需要头文件</li>
<li>builtin是更广泛的“编译器扩展”集合：除了映射硬件指令外，还包括诸如分支预测提示（__builtin_expect）、类型检查（__builtin_types_compatible_p）、内存屏障、数学函数（__builtin_sin）等</li>
<li>用c++写kernel时用builtin, 被编译成llir时，转换成intrinsic; c++也能调用intrinsic，例如arm neon intrinsic</li>
<li>Intrinsic 是编译器提供的特殊函数，通常直接映射到特定的机器指令。它们允许程序员以函数的形式使用底层硬件指令，从而提高性能。Builtin 是编译器内置的函数，通常以 _<em>builtin</em> 为前缀，提供了一些没有头文件也可以使用的功能，如数学运算、位操作等。这些函数由编译器直接识别和处理，可能被优化为特定的机器指令。</li>
<li>当你要调用特定指令集（如 SSE、NEON）或直接操作特殊寄存器，用 intrinsic；当你要利用编译器提供的各种低级优化或扩展（分支预测、内存屏障、内置数学函数等），用 builtin。</li>
<li>polyhedral compilation，  scheduling languages与triton<ul>
<li>如果你做编译优化（LLVM, 编译器后端开发）→ 选择 Polyhedral Compilation</li>
<li>如果你优化计算图（Halide, TVM, 计算调度）→ 选择 Scheduling Languages</li>
<li>如果你优化深度学习（GPU kernel 加速）→ 选择 Triton</li>
</ul>
</li>
<li>假设要优化矩阵乘法 C &#x3D; A × B：<ul>
<li>Polyhedral Compilation：自动分析 for 循环依赖，调整 i, j, k 计算顺序，进行 loop tiling 和 向量化。生成适用于 CPU&#x2F;GPU 的高效代码。</li>
<li>Scheduling Languages（Halide, TVM）：允许用户手动设定计算顺序，比如先计算小块矩阵、再合并结果。适用于自动调度和搜索最优策略。</li>
<li>Triton：直接编写 triton.kernel，自动生成高效 CUDA 代码。通过 memory tiling 和 寄存器优化 提高 GPU 执行效率。</li>
</ul>
</li>
<li>第一次编译时会在&#x2F;tmp目录下生成main.cpp然后编译成运行kernel时所需so，编译后删除main.cpp，生成内容见driver.py。<ul>
<li>main.cpp不包含main函数，有函数用于调用kernel</li>
<li>main.cpp中包含生成的kenerl路径, 需要加载</li>
</ul>
</li>
<li>使得算子开发的部分复杂度分配给上层 kernel 开发用户，部分复杂度分配给底层的 Triton compiler。同时对接上层框架的工作则交给了上层软件栈，使得 Triton 能够专注在自己这个层次需要解决的核心问题上，找到了一个比较不错的”product&#x2F;technology” fit。</li>
<li>Triton 的核心设计思想—-Block-wise 编程，Block 上面的归用户，Block 内部的归 Triton compiler 自动化处理。相应地，Block 内部的优化细节，也交由 Triton compiler 处理了。</li>
<li>Triton 这个项目目前还更像是一个满足细分场景(高效高性能 Kernel 开发)需求的“小而美”的项目</li>
<li>triton kernel 中的计算是 block level, 跟据 ptr + offset 直接计算一个 block，block 具体如何划分成 thread 由编译器决定<ul>
<li>通过 arrange + stride 来计算 block 中每个数据的 offset</li>
</ul>
</li>
<li>注意代码中的 pid 含义和 cuda 的不同：triton 表示 block id, cuda 表示 thread id</li>
<li>自定义优化搜索空间，减少搜索时间</li>
<li>使用 triton 来做 codegen</li>
<li>triton 的 language 语法确实很简单，相比较 cuda 来说，它能够帮我们快速验证一些 idea，同时给出比 cublas 性能相当的算子。</li>
<li>Using Triton, you only need to know that a program is divided into multiple blocks</li>
<li>大部分的框架都以 python 的 DSL 暴露给用户，然后用户通过写对应的 python 语法，调用已经用 C++&#x2F;CUDA 或者 assemble 写好的高性能组件。</li>
<li><code>pip show triton</code> &#x2F;usr&#x2F;local&#x2F;lib&#x2F;python3.10&#x2F;dist-packages&#x2F;triton&#x2F;_C&#x2F;libtriton.so</li>
<li>支持 nvidia, AMD GPU； &#x2F;usr&#x2F;local&#x2F;lib&#x2F;python3.10&#x2F;dist-packages&#x2F;triton&#x2F;backends&#x2F;amd&#x2F;lib&#x2F;libamdhip64.so</li>
<li><code>~/.triton/cache</code> cache 路径, 里面有 ptx, cubin, 各种 ir 文件 <a target="_blank" rel="noopener" href="https://chatgpt.com/share/6750222c-3c44-8004-b0a4-483b2d45dead">link</a><br><img src="https://i.ibb.co/qFsBwJv/YMws76l-KOI.png" alt="各文件作用"><ul>
<li>TTIR（Triton Tensor Intermediate Representation）TTIR 是 Triton 编译器的一个中间表示，专门用于描述张量级别的操作。它是一个较高层次的中间表示，关注计算任务中的张量操作和计算模式。</li>
<li>TTGIR（Triton Tensor GPU Intermediate Representation）TTGIR 是 Triton 的低层中间表示，专注于描述与 GPU 硬件架构相关的计算细节。它建立在 TTIR 之上，但更加贴近 GPU 的执行模型。</li>
</ul>
</li>
<li>Triton, a <code>language</code> and <code>compiler</code> for writing highly efficient custom Deep-Learning primitives. The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs.</li>
<li>An open-source python-like programming language which enables researchers with no CUDA experience to write highly efficient GPU code – most of the time on par with what an expert would be able to produce</li>
<li>开发效率<br><img src="https://i.ibb.co/NYSS9WV/Xrz-Qku-ZLSo.png" alt="img"></li>
<li>领域特定语言（domain-specific language DSL)</li>
<li>本质上来说，Triton 和 TVM&#x2F;XLA 这类工作的定位有所不同，如果说 TVM&#x2F;XLA 是比较纯正的 AI 编译器的话，Triton 更像是一个面向 AI 加速器算子开发的领域开发语言，为了能够将用户使用 Triton 语言开发的 kernel 映射到具体硬件上的执行码，需要设计开发相应的 Triton compiler 来完成这层映射。所以当我们说 Triton 的时候，其实隐指了 Triton 语言+Triton 编译器这两个事物的综合体。</li>
<li>从 triton 的源码来看，triton 目前在 NV 的 GPU 上已经有了一套自己比较成熟的 mapping 路线，通过先对 python 语言层，也就是 triton DSL 进行抽象，得到 AST，然后将 AST 中的每个节点 lower 到 Triton Dialect 上，Triton Dialect 则是一个比较贴近上层语言表达的 IR，他的主要作用则是为了保持用户在书写对应算法时的准确性。接下来会进一步被 map 到 TritonGPU Dialect 上，那么 TritonGPU Dialect 则是一个更加贴近 GPU 层面的 IR，它则是为了具体的性能优化而设计。图中其他的蓝色模块，比如 SCF，Arith，Tensor 等都是 MLIR 生态中已经被实现好并且广为使用的 Dialect。这些 Dialect 会一起和 TritonGPU Dialect 共存，然后被 lower 到对应的 LLVM Dialect，LLVM Dialect 则是最贴近 LLVM IR 的一层设计，从 LLVM Dialect 到 LLVM IR 的转换是非常容易的，最终代码就会被接入到 LLVM 的 NVPTX 的后端，从而生成后续能跑在 GPU 上的高性能 machine code. <a target="_blank" rel="noopener" href="http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/OpenAI%20Triton%20MLIR%20%E7%AC%AC%E9%9B%B6%E7%AB%A0%20%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/">link</a></li>
<li>和 cuda pytorch 区别 <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=AtbnRIzpwho&t=273s">link</a><br><img src="https://i.ibb.co/qszbhN4/NYmmor-R0l-P.png" alt="区别"></li>
<li>编译器是一个很好的生产力工具，能够帮助做很多手工的任务</li>
<li>triton 支持的语言特性是 Python 的一个子集<ul>
<li>no dict</li>
<li>no meta-programming</li>
<li>no slicing</li>
<li>no indexing</li>
<li>…</li>
</ul>
</li>
<li>triton v2 <a target="_blank" rel="noopener" href="https://www.jokeren.tech/slides/triton_next.pdf">link</a><ul>
<li>MLIR(Triton dialect, TritonGPU dialect)</li>
<li>clean layout concepts(like cutlass cute)<ul>
<li>low overhead time: Cache and fetch kernels using efficient signatures</li>
</ul>
</li>
<li>debugging: triton.language.print</li>
<li>Profiler interface</li>
</ul>
</li>
<li>查看 grid</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">grid = <span class="keyword">lambda</span> meta: (triton.cdiv(n_elements, meta[<span class="string">&#x27;BLOCK_SIZE&#x27;</span>]), )</span><br><span class="line">grid_size = grid(&#123;<span class="string">&#x27;BLOCK_SIZE&#x27;</span>: <span class="number">1024</span>&#125;) <span class="comment"># grid是lamda函数，需要先调用才能看结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Grid size: <span class="subst">&#123;grid_size&#125;</span>&#x27;</span>) <span class="comment"># (97, )</span></span><br></pre></td></tr></table></figure>

<h2 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h2><ol>
<li>triton.jit</li>
<li>triton.autotune</li>
<li>triton.compile</li>
<li>torch.compile</li>
</ol>
<h2 id="debug"><a href="#debug" class="headerlink" title="debug"></a>debug</h2><ol>
<li>C++ 代码里：<code>#include &quot;llvm/Support/Debug.h&quot; llvm::DebugFlag = true;</code></li>
<li><a target="_blank" rel="noopener" href="https://mlir.llvm.org/getting_started/Debugging/">MLIR debug</a><ul>
<li>mlir-opt</li>
</ul>
</li>
<li>使用<code>llvm::sys::PrintStackTrace(llvm::errs());</code> 来打印堆栈，注意需要设置llvm-symbolizer路径<ul>
<li>export PATH&#x3D;$PATH:&#x2F;share_data&#x2F;triton&#x2F;llvm_19_dir&#x2F;250925&#x2F;bin</li>
</ul>
</li>
<li>export MLIR_ENABLE_DIAGNOSTICS&#x3D;warnings,remarks;来控制诊断信息<br>op-&gt;emitRemark()<br>mlir::emitRemark</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (<span class="keyword">auto</span> enableDiagnostics =</span><br><span class="line">        triton::tools::<span class="built_in">getStrEnv</span>(<span class="string">&quot;MLIR_ENABLE_DIAGNOSTICS&quot;</span>);</span><br><span class="line">    !enableDiagnostics.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">  llvm::SmallVector&lt;std::string, <span class="number">3</span>&gt; storage;</span><br><span class="line">  <span class="built_in">parseCommaSeparatedValues</span>(enableDiagnostics, storage);</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span> &amp;str : storage) &#123;</span><br><span class="line">    <span class="keyword">if</span> (str == <span class="string">&quot;warnings&quot;</span>) &#123;</span><br><span class="line">      showWarnings = <span class="literal">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (str == <span class="string">&quot;remarks&quot;</span>) &#123;</span><br><span class="line">      showRemarks = <span class="literal">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (str == <span class="string">&quot;stacktraces&quot;</span>) &#123;</span><br><span class="line">      showStacktraces = <span class="literal">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (str == <span class="string">&quot;operations&quot;</span>) &#123;</span><br><span class="line">      showOperations = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// we show errors by default, so no need to set it</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li>llvm_unreachable(“unsupported emitBaseIndexForLayout”);</li>
<li>op-&gt;emitError(“Expected at least one tensor input operand.”);<ul>
<li>自动附带 operation 的位置信息（location），方便用户或开发者快速定位 IR 中出错的地方。</li>
<li>返回值是一个 InFlightDiagnostic 对象，可以用于链式追加更详细的信息</li>
</ul>
</li>
<li><code>./python/triton/_C/libtriton.so</code> libtrion位置</li>
<li><a target="_blank" rel="noopener" href="https://triton-lang.org/main/programming-guide/chapter-3/debugging.html">官方指导</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton?tab=readme-ov-file#tips-for-building">源码 debug</a></li>
<li><code>TRITON_ENABLE_LLVM_DEBUG=1</code> 打印详细信息</li>
<li><code>TRITON_LLVM_DEBUG_ONLY=&quot;tritongpu-remove-layout-conversions,regalloc&quot;</code> 只打印一些pass log, 使用LDBG宏</li>
<li><code>TRITON_INTERPRET=1</code> 使用 Triton 解释器而不是在 GPU 上运行。您可以在内核代码中插入 Python 断点！NOTE:注意真正运行的时候不要打开</li>
<li><code>TRITON_HOME=/home/data</code></li>
<li><code>MLIR_ENABLE_DUMP=1</code> 只dump mlir, 不dump llvm ir</li>
<li><code>LLVM_IR_ENABLE_DUMP=1</code></li>
<li><code>TRITON_ALWAYS_COMPILE=1</code> 每次都重新编译</li>
<li><code>TRITON_PRINT_AUTOTUNING=1</code>可以打印 autotune 选择的最快配置 prints out the best autotuning config and total time spent for each kernel after autotuning is complete.</li>
</ol>
<h2 id="c-调用-triton-kernel"><a href="#c-调用-triton-kernel" class="headerlink" title="c++调用 triton kernel"></a>c++调用 triton kernel</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/python/test/unit/tools/test_aot.py">test aot</a><ul>
<li>vscode testing 下的 pytest 来测试 debug 模式可以看到生成的文件路径, 右键测试程序进入 debug</li>
<li>能自动生成 kernel.h, kernel.c, libkernel.so, test.c 不用手写</li>
<li>python3 -m pytest -v python&#x2F;test&#x2F;unit&#x2F;tools 运行测试程序</li>
<li>python3 -m pytest -v -k test_compile_link_matmul python&#x2F;test&#x2F;unit&#x2F;tools&#x2F;test_aot.py</li>
<li>compile.py 编译</li>
<li>compile_aot_kernels 编译 kernel</li>
<li><a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/fd691c67ac20958a67693358186d877790f5f48f/python/triton/tools/link.py#L222">load 函数</a></li>
</ul>
</li>
<li>在 c++ load triton 生成的 ptx 或者 cubin 文件</li>
</ol>
<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/OpenAI%20Triton%20MLIR%20%E7%AC%AC%E9%9B%B6%E7%AB%A0%20%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/">非常好的中文教程</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jokeren.tech/slides/triton_next.pdf">triton next</a><ul>
<li>why triton</li>
<li>triton 的定位</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/613244988">谈谈对 OpenAI Triton 的一些理解</a></li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/07/01/talk-skills/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/07/01/talk-skills/" class="post-title-link" itemprop="url">talk_skills</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-07-01 15:41:15 / 修改时间：17:38:40" itemprop="dateCreated datePublished" datetime="2024-07-01T15:41:15+08:00">2024-07-01</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/07/01/talk-skills/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/07/01/talk-skills/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>290</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="技术分享"><a href="#技术分享" class="headerlink" title="技术分享"></a>技术分享</h2><h3 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h3><ol>
<li>清晰简洁<ul>
<li>语言要清晰简洁，避免使用过多的专业术语。</li>
<li>使用简单易懂的例子来说明复杂的概念。</li>
</ul>
</li>
<li>视觉辅助<ul>
<li>使用图表、图片和动画来帮助解释概念。</li>
<li>确保幻灯片内容简洁，不要过多的文字。</li>
</ul>
</li>
<li>互动性<ul>
<li>提出问题并邀请听众参与讨论。</li>
<li>使用现场演示或代码运行来增加互动性。</li>
</ul>
</li>
<li>实战演练<ul>
<li>事先进行多次演练，熟悉内容和演示流程。</li>
<li>准备好应对可能出现的问题或故障。</li>
</ul>
</li>
<li>自信和热情<ul>
<li>保持自信和热情，感染听众。</li>
<li>展示你对所分享内容的兴趣和信心。</li>
</ul>
</li>
<li>时间管理<ul>
<li>控制好每个部分的时间，避免超时或内容过于简略。</li>
<li>根据听众的反应灵活调整节奏。</li>
</ul>
</li>
</ol>
<h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><ol>
<li>自我介绍</li>
<li>引言<ul>
<li>介绍主题与分享目的</li>
</ul>
</li>
<li>大纲</li>
<li>具体内容</li>
<li>总结<ul>
<li>回顾关键点，并提出可能的下一步</li>
</ul>
</li>
<li>QA</li>
<li>反馈和改进</li>
</ol>
<h2 id="会议组织"><a href="#会议组织" class="headerlink" title="会议组织"></a>会议组织</h2><h2 id="演讲"><a href="#演讲" class="headerlink" title="演讲"></a>演讲</h2>
      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/20/">20</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="贾夕阳"
      src="/images/coder2.jpg">
  <p class="site-author-name" itemprop="name">贾夕阳</p>
  <div class="site-description" itemprop="description">深度学习/自动驾驶/C++/性能优化</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">196</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jiaxiyang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jiaxiyang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2025/12/29/claude-code/" title="2025&#x2F;12&#x2F;29&#x2F;claude-code&#x2F;">claude code</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2025/08/20/AI-coding/" title="2025&#x2F;08&#x2F;20&#x2F;AI-coding&#x2F;">AI coding</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2025/04/28/Architecture/" title="2025&#x2F;04&#x2F;28&#x2F;Architecture&#x2F;">Computer Architecture</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2025/04/18/pytest/" title="2025&#x2F;04&#x2F;18&#x2F;pytest&#x2F;">pytest</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2025/01/18/cursor/" title="2025&#x2F;01&#x2F;18&#x2F;cursor&#x2F;">cursor</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">贾夕阳</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">628k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">9:31</span>
</div>

<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("06/26/2020 14:52:10");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : '[object Object]',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://jiaxiyang.github.io/page/3/',]
      });
      });
  </script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'g32ipLmEye1u5l6wBGRJt03S-gzGzoHsz',
      appKey     : 'zHgLkAICsZUl9Mf8LfdoVigP',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

  

  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>





 
</body>
</html>

