[{"title":"Claude Code","url":"/2025/12/29/claude-code/","content":"\n## 快捷键\n\n| 快捷键 | 功能 |\n|--------|------|\n| `S-Tab` | 切换模式 |\n| `C-d C-d` | 退出 |\n| `C-r` | 搜索历史 |\n| `ESC ESC` | 回退历史，编辑之前的 prompt |\n\n## 常用命令\n\n| 命令 | 功能 |\n|------|------|\n| `/init` | 创建 CLAUDE.md，扫描代码仓库建立工程上下文 |\n| `/cost` | 显示花费 |\n| `/model` | 选择模型 |\n| `/resume` | 选择历史对话 |\n| `/clear` | 重置上下文窗口（建议任务间频繁使用） |\n| `/ide` | 连接外部终端到 VS Code |\n\n## IDE 集成\n\nCLI 和 IDE 插件连接：\n- 在 VS Code 集成终端运行 `claude` 会自动与 IDE 集成\n- 外部终端使用 `/ide` 命令连接到 VS Code\n- tmux 首次使用需全部关闭重启\n- 确保 IDE CLI 可用：VS Code 需要 `code` 命令，Cursor 需要 `cursor` 命令\n- 参考：https://code.claude.com/docs/en/vs-code#standalone-cli-not-connecting-to-ide\n\nVS Code 快捷键：`View: Focus into Secondary Side Bar` 绑定到 `C-2`\n\n## 工作流\n\n**推荐流程：**\n- Explore → Plan → Code → Commit\n- Write tests → Commit → Code → Iterate → Commit\n- Write code → Screenshot result → Iterate\n\n**优化技巧：**\n- 指令要具体明确\n- 提供截图给 Claude\n- 大型任务使用 Markdown 文件作为 checklist\n- 多 Claude 协作：一个写代码，另一个验证；或一个写测试，另一个实现\n\n## Skill（技能）\n\n### 什么是 Skill\n\nSkill 是 Markdown 文件，教导 Claude 如何执行特定任务。与 Slash Command 不同，Skill 由 Claude **自动触发**（基于请求内容语义匹配）。\n\n### 创建 Skill\n\n**目录位置：**\n```bash\n# 个人 Skill（跨所有项目）\n~/.claude/skills/skill-name/SKILL.md\n\n# 项目 Skill（仅限该仓库）\n.claude/skills/skill-name/SKILL.md\n```\n\n**SKILL.md 结构：**\n```yaml\n---\nname: my-skill              # 只能用小写字母、数字和连字符\ndescription: 技能描述，Claude 用它决定何时应用 Skill\nallowed-tools: Read, Grep   # 可选：限制可用工具\nmodel: claude-opus-4-5-20251101  # 可选：指定模型\n---\n\n# 技能标题\n\n## 说明\n详细步骤...\n\n## 示例\n具体用法...\n```\n\n### Skill 示例：生成提交信息\n\n```yaml\n---\nname: generating-commit-messages\ndescription: 从 git diff 生成清晰的提交信息。写提交信息或审查暂存更改时使用。\n---\n\n# 生成提交信息\n\n## 说明\n1. 运行 `git diff --staged` 查看更改\n2. 建议提交信息包括：\n   - 摘要不超过 50 个字符\n   - 详细描述\n   - 受影响的组件\n\n## 最佳实践\n- 使用现在时\n- 解释什么和为什么，而不是如何\n```\n\n### Skill 示例：代码审查\n\n```yaml\n---\nname: code-review\ndescription: 根据团队标准审查代码质量和最佳实践。\nallowed-tools: Read, Grep, Glob, Bash(git diff:*)\n---\n\n# 代码审查\n\n## 检查项\n\n### 安全性\n- 没有硬编码的密钥或凭证\n- 输入验证是否充分\n\n### 性能\n- 是否有不必要的循环\n- 算法复杂度是否合理\n\n### 代码风格\n- 命名约定是否一致\n- 是否遵循项目规范\n```\n\n### Skill vs Slash Command\n\n| 方面 | Skill | Slash Command |\n|------|-------|---------------|\n| 触发方式 | Claude 自动决定 | 显式输入 `/command` |\n| 复杂性 | 复杂工作流 | 简单提示 |\n| 文件结构 | 目录 + SKILL.md + 支持文件 | 单一 .md 文件 |\n\n### 使用技巧\n\n- 在请求中使用与 Skill 描述相似的关键词\n- 检查可用 Skill：`What Skills are available?`\n- 重启 Claude Code 加载新 Skill\n\n## Plugin（插件）\n\n### 什么是 Plugin\n\nPlugin 是 Claude Code 的扩展包，可包含：\n- 斜杠命令（Commands）\n- 技能（Skills）\n- 事件钩子（Hooks）\n- MCP 服务器\n- LSP 服务器\n\n**用途**：跨项目复用、团队共享、通过 Marketplace 分发\n\n### Plugin vs 独立配置\n\n| 方式 | 命令格式 | 适用场景 |\n|------|----------|----------|\n| 独立配置 `.claude/` | `/hello` | 个人、项目特定、快速实验 |\n| Plugin | `/plugin-name:hello` | 团队共享、跨项目、版本管理 |\n\n### 目录结构\n\n```\nmy-plugin/\n├── .claude-plugin/\n│   └── plugin.json      # 必需：插件清单\n├── commands/            # 斜杠命令\n│   └── hello.md\n├── skills/              # 技能\n│   └── code-review/\n│       └── SKILL.md\n├── hooks/               # 钩子配置\n│   └── hooks.json\n├── .mcp.json            # MCP 服务器\n└── scripts/             # 脚本\n```\n\n### 创建 Plugin\n\n**1. 创建清单 `.claude-plugin/plugin.json`：**\n\n```json\n{\n  \"name\": \"my-plugin\",\n  \"description\": \"我的第一个插件\",\n  \"version\": \"1.0.0\",\n  \"author\": { \"name\": \"Your Name\" }\n}\n```\n\n**2. 添加命令 `commands/hello.md`：**\n\n```markdown\n---\ndescription: 问候用户\n---\n\n# Hello 命令\n\n用 \"$ARGUMENTS\" 问候用户。\n```\n\n**3. 测试 Plugin：**\n\n```bash\nclaude --plugin-dir ./my-plugin\n```\n\n使用：`/my-plugin:hello World`\n\n### Plugin 示例：代码审查\n\n```\ncode-review-plugin/\n├── .claude-plugin/\n│   └── plugin.json\n├── commands/\n│   └── review.md\n└── skills/\n    └── pr-review/\n        └── SKILL.md\n```\n\n**plugin.json：**\n```json\n{\n  \"name\": \"code-review\",\n  \"description\": \"代码审查工具\",\n  \"version\": \"1.0.0\"\n}\n```\n\n**commands/review.md：**\n```markdown\n---\ndescription: 启动代码审查\n---\n\n检查代码质量、最佳实践、安全问题。\n```\n\n### 安装与管理\n\n```bash\n# 安装（用户范围）\nclaude plugin install plugin-name@marketplace\n\n# 安装到项目（与团队共享）\nclaude plugin install plugin-name@marketplace --scope project\n\n# 启用/禁用\nclaude plugin enable plugin-name\nclaude plugin disable plugin-name\n\n# 卸载\nclaude plugin uninstall plugin-name\n```\n\n### 环境变量\n\n在 hooks、MCP 配置中使用 `${CLAUDE_PLUGIN_ROOT}` 获取插件根目录：\n\n```json\n{\n  \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/run.sh\"\n}\n```\n\n### Plugin vs Skill\n\n| 特性 | Plugin | Skill |\n|------|--------|-------|\n| 定义 | 完整扩展包 | 教 Claude 做某事的文档 |\n| 触发 | 用户显式调用 `/plugin:cmd` | Claude 自动匹配 |\n| 包含 | Commands + Skills + Hooks + MCP | 单个 SKILL.md |\n\n## 权限与调试\n\n```bash\n# 跳过权限确认（危险模式）\nclaude --dangerously-skip-permissions\nalias claude='claude --dangerously-skip-permissions'\n\n# 调试模式（会生成 log 文件）\nclaude --debug\n```\n\n## WSL 配置\n\n```bash\n# 端口转发\nssh -N -L 18080:10.96.11.131:3100 yxsqw@10.193.64.77\n\n# 设置 API 地址\nexport ANTHROPIC_BASE_URL=\"http://localhost:18080/api\"\n```\n\n## 自定义斜杠命令\n\n### 创建位置\n\n在 `.claude/commands/` 目录创建 `.md` 文件\n\n### 示例：博客部署命令\n\n**`.claude/commands/deploy.md`：**\n\n```markdown\n---\ndescription: 部署博客到 GitHub Pages\n---\n\n# 博客部署命令\n\n1. Git commit 并 push：\n   ```bash\n   git add -A && git commit -m \"update\" && git push\n   ```\n\n2. Hexo deploy：\n   ```bash\n   hexo deploy\n   ```\n```\n\n使用：`/deploy`\n\n## prompt\n1. 扩展思考 - 处理复杂任务时使用 \"think hard\" 或 \"think deeply\" 触发深度推理\n   - \"think\" - 触发基本扩展思考\n   - \"think hard\"、\"think more\"、\"think longer\" - 触发更深层思考\n\n## 参考链接\n\n- [Common Workflow](https://code.claude.com/docs/en/common-workflows)\n- [Claude Code Best Practices](https://www.anthropic.com/engineering/claude-code-best-practices)\n- [Skills 官方文档](https://code.claude.com/docs/en/skills.md)\n- [Slash Commands 文档](https://code.claude.com/docs/en/slash-commands.md)\n- [Plugin 官方文档](https://code.claude.com/docs/en/plugins.md)\n- [Plugin Marketplace](https://code.claude.com/docs/en/plugin-marketplaces.md)\n"},{"title":"AI coding","url":"/2025/08/20/AI-coding/","content":"\n\n# list\n1. cursor\n1. claude\n1. copilot\n"},{"title":"Computer Architecture","url":"/2025/04/28/Architecture/","content":"\n## 架构\n### GPU\n#### GPGPU\n1. 通用性强，支持多种计算任务，尤其擅长并行处理\n1. GPGPU: 像一个万能工匠，什么都会干，但效率不是最顶级。\n1. 例: NVIDIA H100\n\n### DSA\n1. 针对特定领域（如深度学习、图像处理）定制优化\n1. DSA：像一个专门干水电活的师傅，水电这块特别牛，但别的活就不行了。\n1. 例: Google TPU\n\n### ASIC\n1. 完全为某个特定应用定制，效率极高\n1. ASIC：像一台只会干一件事的机器人，干得飞快、极致省电，但换个任务就废了。\n1. 例: 比特币矿机芯片\n\n\n## TPU 架构\n1. [google TPU](https://mp.weixin.qq.com/s/zy1SyXjbnH3Ix6GigArD0w)\n"},{"title":"pytest","url":"/2025/04/18/pytest/","content":"\n## links\n1. [doc](https://docs.pytest.org/en/stable/)\n1. [usage](https://docs.pytest.org/en/stable/how-to/usage.html)\n   - `pytest --collect-only -q test_file` 列出测试文件，不执行\n   - `pytest test_mod.py`\n   - `pytest testing/`\n     - pytest 默认只收集以 test_*.py 或 *_test.py 命名的文件。\n   - `pytest tests/test_mod.py::test_func`\n   - `pytest tests/test_mod.py::TestClass`\n   - `pytest tests/test_mod.py::TestClass::test_method`\n   - `pytest -m slow` 运行标记为slow的测试\n   - `pytest @tests_to_run.txt` 从文件中读取要测试的内容\n   - `pytest --durations=10 --durations-min=1.0` 分析测试执行时间\n   \n\n## NOTE\n1. html report:\n   - pip install  pytest-html\n   - pytest --html=report.html --self-contained-html python/test/unit/sipu -n 16 -v -rxXs\n\n1. `pip install pytest-xdist`pytest加速\n   - pytest -n 4\n   - pytest -n auto\n1. pytest 默认只收集以 test_*.py 或 *_test.py 命名的文件。[link](https://docs.pytest.org/en/stable/getting-started.html#run-multiple-tests)\n   - pytest will run all files of the form test_*.py or *_test.py in the current directory and its subdirectories.\n1. [conventions-for-python-test-discovery](https://docs.pytest.org/en/stable/explanation/goodpractices.html#conventions-for-python-test-discovery)\n1. [工程布局](https://docs.pytest.org/en/stable/explanation/goodpractices.html#choosing-a-test-layout)\n1. [import-modes](https://docs.pytest.org/en/stable/explanation/pythonpath.html#import-modes)\n   - pytest 作为测试框架需要导入测试模块和 conftest.py 文件来执行。\n   - conftest.py可以传递参数\n   - conftest.py 是 pytest 框架中用于共享测试配置和夹具（fixtures）的特殊文件，放在你的测试目录中。它的主要作用是为多个测试文件提供通用的设置、夹具和钩子函数，避免重复代码。\n   - conftest.py 不需要被 import，pytest 会自动发现它。\n   - 一个项目可以有多个 conftest.py 文件，分布在不同测试目录中，作用域是当前目录及其子目录。\n\n## [fixtures](https://docs.pytest.org/en/stable/how-to/fixtures.html)\n1. fixture（夹具）是 pytest 中一个非常强大和核心的机制，用来为测试函数提供初始化的资源或前置条件，比如：\n   - 创建数据库连接\n   - 准备测试数据\n   - 打开文件\n   - 初始化配置等\n1. fixture 的优势\n   - 让测试更清晰，避免重复初始化代码\n   - 易于组合和共享\n   - 可以集中在 conftest.py 中，适用于多个测试文件\n\n## base\n1. 默认情况下 pytest 会隐藏 print()\n   - `pytest -s` 显示输出log\n1. `pytest.mark.parametrize`可以设置参数组合, 可以设置xfail\n\n``` python\n@pytest.mark.parametrize(\"shape, block_size\", [\n    ((1, 256), 256),\n    ((12, 1024), 256),\n    pytest.param((2, 512), 128, marks=pytest.mark.xfail(reason=\"Act quant kernel not support 128 block size\")),\n    pytest.param((2, 512), 512, marks=pytest.mark.xfail(reason=\"Load op has bugs, to be fix\")),\n]);\n\n@pytest.mark.parametrize(\"shape, block_size\", [\n    *[\n        (shape, block_size)\n        for shape in [(1, 256), (2, 512), (12, 1024)]\n        for block_size in [256, 512]\n        if shape[1] >= block_size\n    ],\n    \n    pytest.param((2, 512), 128, marks=pytest.mark.xfail(reason=\"Act quant kernel not support 128 block size\")),\n])\n\n@pytest.mark.parametrize(\n    \"kernel, M, N, K, input_type\",\n    [\n        (tile_mma_32x32_f32_f16_f16_kernel, 32, 32, 16, torch.float16),\n        (tile_mma_32x32_f32_f16_f16_v2_kernel, 32, 32, 16, torch.float32),\n        (tile_mma_32x32_f32_f8e4m3_f8e4m3_kernel, 32, 32, 32, torch.float8_e4m3fn),\n        (tile_mma_32x32_f32_f8e4m3_f8e4m3_v2_kernel, 32, 32, 32, torch.float32),\n    ],\n    ids=[\n        \"f16_kernel\",\n        \"f16_v2_kernel\",\n        \"f8e4_kernel\",\n        \"f8e4_v2_kernel\",\n    ]\n)\n\n\n@pytest.mark.parametrize(\"dtype\", [torch.float32, torch.float16, torch.int32])\n@pytest.mark.parametrize(\"shape,permute_dims\", [\n    ((2, 3, 4), (0, 2, 1)),\n    ((4, 5), (1, 0)),  # transpose\n])\n```\n1. `pytest test_single_mma.py -s -k tile_mma_32x32_f32_f16_f16_kernel` 选择一个kernel执行, 加上 ids 让 -k 更好用\n1. 可以在test文件中加下面内容，方便测试，pytest 不会执行 if __name__ == \"__main__\": 中的代码\n\n``` python\nif __name__ == \"__main__\":\n    test_act_quant_fp32_to_fp8((2, 256), 128)\n```\n\n\n## [skip](https://docs.pytest.org/en/stable/how-to/skipping.html)\n1. [skip-xfail-with-parametrize](https://docs.pytest.org/en/stable/how-to/skipping.html#skip-xfail-with-parametrize)\n\n\n## [管理输出](https://docs.pytest.org/en/stable/how-to/output.html)\n\n1. `pytest -rxXs`  # show extra info on xfailed, xpassed, and skipped tests\n1. `pytest -q -v`  每个文件用-q显示， ci测试好用(good)\n1. pytest --quiet          # quiet - less verbose - mode\n1. pytest -q               # quiet - less verbose - mode (shortcut)\n1. pytest -v               # increase verbosity, display individual test names\n1. pytest -vv              # more verbose, display more details from the test output\n1. pytest -vvv             # not a standard , but may be used for even more detail in certain setups\n1. `pytest -s` 加上 -s  可以看到 print 输出，方便调试。\n1. `pytest -k` -k 后支持模糊匹配函数名或参数中变量的字符串表示\n1. `pytest --no-header -v`列表形式输出\n1. pytest 在默认“简洁模式”（verbosity=0 或 -q）下，会用单个字符表示每个测试的执行结果：\n   - `.` —— 测试通过 (passed)。每当一个测试成功时，pytest 就输出一个点号。 \n   - `F` —— 测试失败 (failed)，通常伴随断言错误或其他异常。 \n   - `E` —— 测试执行过程中发生未捕获的错误 (error)，例如 fixture 或 setup 中抛出的异常。 \n   - `s 或 S` —— 测试被跳过 (skipped)，使用 @pytest.mark.skip、@pytest.mark.skipif 或在测试中主动调用 pytest.skip() 时触发。 \n   - `x` —— 预期失败 (xfail)，标记为 @pytest.mark.xfail 的测试如果确实失败，就显示 x。 \n   - `X` —— 意外通过 (xpass)，当标记为 xfail 的测试反而通过时显示 X。\n\n## [debug](https://docs.pytest.org/en/stable/how-to/failures.html)\n1. (good) `pytest -s` 加上 -s  可以看到 print 输出，方便调试。\n1. -k 后支持模糊匹配函数名或参数中变量的字符串表示\n   `pytest test_single_mma.py -s -k tile_mma_32x32_f32_f16_f16_kernel` 选择一个kernel执行\n1. `pytest --pdb` 停在报错地方\n1. `pytest --trace`停在测试入口\n\n## profiling\n1. [usage](https://docs.pytest.org/en/stable/how-to/usage.html)\n   - `pytest --durations=10 --durations-min=1.0` 分析测试执行时间\n\n## pytest.ini\n1. \n\n\n## triton\n1. triton直接使用的pytest，没使用torch的TestCase\n1. 核心测试在`python/test/unit/language`\n\n\n## pytorch test\n1. [samples](https://github.com/pytorch/pytorch/tree/main/test)\n1. [unit testing](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#unit-testing)\n1. [doc](https://gitee.com/ascend/pytorch/blob/master/CONTRIBUTING.zh.md#%E6%B5%8B%E8%AF%95%E7%94%A8%E4%BE%8B)\n"},{"title":"cursor","url":"/2025/01/18/cursor/","content":"\n\n## import vscode config\n1. cursor settings -> general -> vs code import\n1. git管理配置\n\n\n## short keys\n1. [terminal ctrl-l 问题](https://forum.cursor.com/t/change-ctrl-l-to-ctrl-i-control-l-is-for-clear-terminal/15310/3)\n1. terminal: C-S-l\n\n## [ignore rules](https://docs.cursor.com/en/context/ignore-files)\n1. 要查看索引文件路径： Cursor Settings > Indexing & Docs > View included files\n1. project rule：给model先说明先验证知识，@rule包含\n   - 可以让ai自己先晚上一下rule\n\n## [@](https://docs.cursor.com/en/guides/working-with-context#%40-symbol)\n1. 当你明确知道要包含哪些文件、文件夹、网站或其他上下文信息时，@ 符号非常有用。越具体越好。\n\n## [large project](https://docs.cursor.com/zh/guides/advanced/large-codebases)\n\n## 核心功能\n### [tab](https://docs.cursor.com/zh/tab/overview)\n\n\n\n\n\n## base\n1. inline edit: `C-k` -> `C-M-k`\n1. `@` chat里输入@\n1. 切换成vscode `    \"workbench.activityBar.orientation\": \"vertical\",`\n1. cusor settings -> features -> editor -> show chat/edit tooltip: 关闭\n   - 下一行会提示\n1. `\"cursor.cpp.enablePartialAccepts\": true` ctlr + rightarrow 接受一个单词，设置快捷键 M-f   \n"},{"title":"LLVM","url":"/2024/12/29/LLVM/","content":"\n## basic\n1. 编译器组件交互方式：\n   - 通过内存：通过监督工具Clang实现，该工具将每个LLVM组件作为一个库，并依赖内存中分配的数据结构将一个阶段的输出作为输入提供给另一个阶段\n   - 通过文件：通过用户实现，用户启动较小的独立工具，改工具将特定组件的结果写入磁盘文件，用户决定这个文件作为输入传给下一个\n1. 像Clang这样的高级工具可以整合使用其他几个更小的工具，具体做法是链接霄工具的库来实现工具的功能，LLVM设计上十分重视以库的形式进行大量代码复用。\n1. llc使用libLLVMCodeGen库实现部分功能，LLVM IR优化器opt使用libLLVMipa实现与目标无关的过程间优化。Clang使用上面两个库来代替llc和op。并向用户提供更简单的接口。\n\n   \n1. 语法(Syntax)：关注代码的结构和形式，通过预定义的文法规则来判断源代码是否“写对了格式”。它是编译器前端的第一道防线，主要产生的是语法树。\n1. 语义(Semantic)：关注代码的意义和逻辑，确保程序各部分之间的关系正确、类型匹配、作用域清晰等。它依赖于语法树，并进一步填充具体的含义信息，为中间代码生成提供依据。\n\n\n## TableGen\n1. TableGen 是 LLVM 的“数据模板语言” + “代码生成器”。它用一套 .td 文件定义规则，用工具 llvm-tblgen 自动生成 C++ 源码文件\n1. TableGen 是 LLVM 的“编译器生成器”中的生成器。它本身不生成机器码，而是生成“生成机器码的代码”。\n1. TableGen → llvm-tblgen → .inc(*.h.inc or *.cpp.inc)\n1. .inc 文件通常是 C/C++ 源码片段（include 文件），不是完整的编译单元。.inc的名字来自 “include” 的缩写，表示它需要通过 #include 被其他 C++ 文件包含。\n1. .inc = include 文件的缩写;主要是 TableGen 生成的 C++ 代码片段;不可独立编译，需要 #include\n1. 用途：\n   - 指令编码表\n   - 寄存器表\n   - 汇编打印 / 解析\n   - DAG 指令选择规则\n   - Clang 诊断信息 / 内建函数 / 编译器选项\n1. 可以理解为：\n   - .td 文件是 “表定义”，人可读\n   - .inc 文件是 “表实现”，编译器可用\n\n## LLC\n1. `llc --version`\n1. `llc <llir file without postfix> --debug`\n\n## LLVM IR\n1. br: branch; \"br\"在LLVM IR (中间表示) 中是一个分支指令，它用于控制流跳转，可以是有条件的也可以是无条件的。\n   - br label %target\n   - br i1 %cond, label %if.true, label %if.false\n1. LLVM IR 中的 phi 指令用于实现 SSA（静态单赋值）形式下的变量合并。在一个基本块（basic block）中，如果存在多个前驱块（predecessor），那么在该块的开始处就需要一个 phi 指令来选择正确的值。它的基本语法如下：\n   - `%result = phi <type> [ %val1, %pred1 ], [ %val2, %pred2 ], …` \n   - 这表示：如果控制流是从前驱块 %pred1 进入当前块，则 %result 的值为 %val1；如果是从 %pred2 进入，则 %result 为 %val2，依此类推。\n\n\n\n# Debug\n1. [the-llvm-debug-macro-and-debug-option](https://llvm.org/docs/ProgrammersManual.html#the-llvm-debug-macro-and-debug-option)\n1. speed up build\n\n``` shell\n# Using clang and lld speeds up the build, we recommend adding:\n-DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DLLVM_ENABLE_LLD=ON\n\n# CCache can drastically speed up further rebuilds, try adding:\n-DLLVM_CCACHE_BUILD=ON\n\n# Optionally, using ASAN/UBSAN can find bugs early in development, enable with:\n-DLLVM_USE_SANITIZER=\"Address;Undefined\"\n\n# Optionally, enabling integration tests as well\n-DMLIR_INCLUDE_INTEGRATION_TESTS=ON\n```\n1. install path `-DCMAKE_INSTALL_PREFIX=/data/xiyang/mlir-tutorial/install`\n"},{"title":"deformable_attention","url":"/2024/11/13/deformable-attention/","content":"\n### 介绍\n\n**Deformable Attention** 是一种在视觉任务中用于增强自注意力机制的技术，特别是在处理图像、视频等高维数据时，能够有效地处理不规则、非均匀的空间或时间分布。它的关键思想是通过对关注区域进行选择性采样，而不是计算所有位置的注意力权重，从而显著提高计算效率和精度，尤其是在处理大规模数据时。\n\nDeformable Attention 最初应用于**Deformable DETR**（Deformable Detection Transformer）中，针对检测任务优化了传统的自注意力机制，后来被推广到其他任务，如图像分割、姿态估计等。下面是 Deformable Attention 的一些关键特性和原理：\n\n### 1. **传统自注意力的瓶颈**\n\n标准的自注意力机制（如在 Transformer 中使用的注意力机制）通常会计算输入的每一个位置与所有其他位置的关系，这导致了计算复杂度是输入序列长度的平方。在图像中，若处理的是高分辨率图像或长序列数据，这样的计算开销会非常巨大。具体来说：\n\n- **计算复杂度**：对于长序列，计算每一对位置之间的相似度需要 \\( O(N^2) \\) 的时间复杂度，其中 \\( N \\) 是序列的长度。对于图像数据而言，每个像素与所有其他像素的关系计算量也非常庞大。\n\n### 2. **Deformable Attention 的设计思想**\n\nDeformable Attention 通过限制计算注意力的区域来减少计算量。与标准的全局自注意力机制不同，Deformable Attention 仅关注输入数据中的 **少量关键位置**，这些关键位置通过学习自适应地选择，能够有效地捕捉到重要的上下文信息。\n\n- **选择性采样**：而不是计算每个位置和所有其他位置的关系，Deformable Attention 只选择一些特定的、与当前查询相关的关键位置进行计算。这些关键位置通常是通过一个学习的**采样位置**来选择的，因此能关注到更多有用的信息。\n\n### 3. **Deformable Attention 的实现原理**\n\nDeformable Attention 的核心思想是通过**可变形采样**（deformable sampling）来动态选择哪些位置会参与到注意力的计算中。这通常通过引入一个采样的位置信息来完成，该信息在训练过程中不断优化，以便能自适应地选择合适的区域。\n\n- **采样位置的生成**：在 Deformable Attention 中，会对每个查询（query）生成一组采样位置（采样点），这些位置不一定是规则网格上的点，而是通过某种方式（如通过网络学习的偏移量）选择的。采样位置通常会覆盖输入特征的局部区域，从而捕获重要的上下文信息。\n- **注意力计算**：通过对选定的局部位置进行自注意力计算，可以大大减少计算量，因为只需要计算局部区域的注意力，而不是全局的每一对位置之间的关系。\n\n- **位置信息**：Deformable Attention 采用相对位置信息来进一步增强模型的表达能力，能够处理不同位置间的空间关系。\n\n### 4. **Deformable Attention 与标准自注意力的比较**\n\n| 特性           | 标准自注意力                     | Deformable Attention                 |\n| -------------- | -------------------------------- | ------------------------------------ |\n| **计算复杂度** | \\( O(N^2) \\)                     | \\( O(N \\cdot M) \\)，\\( M \\ll N \\)    |\n| **注意力区域** | 全局注意力，计算所有位置间的关系 | 局部区域注意力，选择关键位置进行计算 |\n| **效率**       | 计算开销大，内存消耗大           | 更高效，内存消耗少                   |\n| **灵活性**     | 对所有位置进行相同的注意力计算   | 动态选择关键位置，自适应性强         |\n\n### 5. **Deformable Attention 的优势**\n\n- **计算效率**：由于仅计算少量的关键位置，Deformable Attention 在计算效率上相较于传统的自注意力机制有显著提升，尤其是在处理大规模输入数据时（如高分辨率图像、长时间序列等）。\n- **灵活性与自适应性**：Deformable Attention 可以根据任务和输入数据的不同，自适应地选择最相关的位置进行注意力计算，而不是使用固定的网格结构。这样可以更灵活地处理复杂的空间或时间关系。\n- **减少计算冗余**：在传统自注意力中，每个位置都与所有其他位置进行计算，这会带来大量冗余计算。Deformable Attention 通过局部采样减少了这些冗余计算，特别是在高分辨率图像的情况下，计算的效率提升尤为明显。\n\n### 6. **应用领域**\n\nDeformable Attention 在多个领域中取得了显著的成果，特别是在计算机视觉任务中：\n\n- **目标检测**：Deformable DETR（Deformable Detection Transformer）是 Deformable Attention 的一个重要应用，改进了传统 DETR 模型在目标检测中的性能，尤其是在处理大尺度图像时更为高效。\n- **图像分割**：Deformable Attention 也被用于图像分割任务，能够有效地捕捉到图像中的细节信息，特别是在处理不规则形状和边界时。\n\n- **姿态估计**：通过对特定关键点的注意力计算，Deformable Attention 在姿态估计任务中也得到了广泛应用，能够更准确地捕捉人体姿态变化。\n\n### 7. **Deformable Attention 的进一步发展**\n\nDeformable Attention 的应用仍在不断扩展，研究者正在探索其在其他领域的潜力，例如：\n\n- **自然语言处理**：在自然语言处理中，Deformable Attention 可能能够帮助捕捉长文本中的重要上下文信息，尤其是当文本存在复杂的依赖关系时。\n- **视频处理**：Deformable Attention 可以在视频处理任务中，通过自适应选择关键帧或区域，从而提高效率并降低计算量。\n\n### 总结\n\nDeformable Attention 通过对注意力计算的局部化和选择性采样，大大提高了计算效率和灵活性，尤其在处理高维数据（如高分辨率图像和长序列）时展现了优势。它在计算机视觉领域，特别是目标检测和图像分割中，取得了显著的成果，并且有潜力应用于其他领域，如自然语言处理和视频分析等。\n"},{"title":"QNX","url":"/2024/10/15/QNX/","content":"\n## base\n\n1. QNX Software Development Platform (SDP) 需要购买, 包含\n   - QNX OS\n   - QNX Tool Suite\n   - QNX Software Center\n1. [SDP link](https://blackberry.qnx.com/en/products/foundation-software/qnx-software-development-platform)\n1. [VSCode 插件](https://marketplace.visualstudio.com/items?itemName=qnx.qnx-vscode)\n1. QNX Neutrino 是操作系统\n1. QNX 适合需要高度可靠性和安全性的应用，FreeRTOS 适合需要轻量级和可移植性的应用，VxWorks 适合需要高度可靠性和实时性能的应用，而 RTLinux 适合需要利用 Linux 的功能和资源的应用。\n1. 首先，宝马在进行基于 Linux 的自动辅助项目时，由于 Linux 作为一个宏内核的操作系统，需要对内核进行裁剪以满足功能安全要求。然而，无论他们怎么裁剪，都无法彻底清除干净，最终导致系统崩溃。\n1. 汽车电子架构已从硬件驱动发展到软件定义。 从数字驾驶舱到高级驾驶辅助系统 (ADAS)，再到自动驾驶控制器等，现都在系统级芯片 (SoC)上运行，它们通常还需要虚拟化，以管理多个操作系统和混合关键性。\n1. 在座舱领域，我们注意到，传统的芯片供应商已逐渐被高通取代。在国内，黑莓 QNX 占据了 99.9%的基础软件份额，其中 80%以上的项目选择高通作为首选芯片供应商。座舱领域有一个有趣的组合——QQA，即 Qualcomm、QNX 和 Android。\n"},{"title":"qualcomm","url":"/2024/09/24/qualcomm/","content":"\na\n\n## 战略\n\n1. 市面上真正能提供舱驾融合/舱驾一体计算平台的厂商并不多，英伟达 DRIVE Thor、高通骁龙 Flex SoC 以及黑芝麻武当 C1200 家族，是目前市面上少数相对成熟的产品。DRIVE Thor 以千 T 级的算力著称，而高通 Flex SoC 以及黑芝麻 C1200 是性价比派系的代表\n1. [高通第四代汽车 SoC 芯片分析，聚焦 CPU，AI 无所谓](https://www.sohu.com/a/788526882_121124477)\n   - 高通更多地考虑汽车座舱和 L2 级的舱驾一体，这样的市场定位不需要太高的 AI 性能，45TOPS 足以满足 99.9%的应用场景，座舱领域所谓的大模型 AI 都完全依赖云端计算，端侧部署成本太高，可能高达数千美元，即便端侧部署，体验也和云端计算有很大差距。\n   - 高通手机领域收入 62 亿美元，汽车业务收入 6.03 亿美元，IoT 业务收入 12 亿美元。\n   - 对于 AI 运算的 NPU，还是高通的六角 DSP，缺乏亮点，乏善可陈，高通一句话带过，没有多说。\n1. 到 2024 年，Ride 智驾平台经过 4 年的演进，已经形成了从前视一体机（RV1 Lite）到支持城市 NOA（SA8650P）的完整谱系。这一代的 8650 和 8620，尤其强化了性价比的标签。\n1. Flex SoC 的第一款产品 8775\n1. 为了向舱驾融合和跨域计算提供强大的中央计算平台，高通率先推出了汽车行业首款同时支持数字座舱和先进驾驶辅助系统（ADAS）的可扩展系列 SoC——Snapdragon Ride Flex。\n1. 高通技术公司提供高性能中央计算 SoC——Snapdragon Ride Flex，旨在跨异构计算资源支持混合关键级工作负载，以单颗 SoC 同时支持数字座舱、ADAS 和 AD 功能\n1. 手机芯片大厂联发科正携手 AI 芯片大厂英伟达（NVIDIA）开发基于 Arm 架构的 AI PC 处理器；在去年 5 月底的 COMPUTEX 2023 台北电脑展上，联发科携手英伟达共同宣布两家公司将在汽车芯片领域进行合作。联发科技将利用小芯片高速互联技术，开发整合有英伟达的 GPU 的车用 SoC 处理器，共同为新一代智能汽车提供解决方案。\n\n## base\n\n1. 模型文件 Deep Learning Container (DLC)\n1. [qualcomms-hexagon-dsp-and-now-npu](https://chipsandcheese.com/2023/10/04/qualcomms-hexagon-dsp-and-now-npu/)\n   - dsp 到 npu\n   - 有专用的矩阵乘指令\n   - Hexagon 的 NPU 每个周期可以完成大量 16K 乘法累加运算\n1. Developers can get one step closer to the silicon and improve the performance of their AI models on Qualcomm® AI accelerators: Qualcomm® Kryo™ CPU, Qualcomm® Adreno™ GPU and Qualcomm® Hexagon™ NPU.\n   - Hexagon Arch: 可以指 DSP 和 NPU, 一个意思\n1. [架构列表](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/overview.html#supported-snapdragon-devices)\n   - Hexagon arch: V66, V68, V69, V73, V75\n   - SM8650 是 V75 架构\n1. Hexagon 是高通公司一系列数字信号处理器(DSP) 和后来的神经处理单元(NPU) 产品的品牌名称。 Hexagon 也称为 QDSP6，代表“第六代数字信号处理器”。据高通称，Hexagon 架构旨在为各种应用提供低功耗的性能。 [link](https://en.wikipedia.org/wiki/Qualcomm_Hexagon)\n1. Hexagon Tensor Accelerator (HTA) for AI acceleration.\n1. Vector Execution (HVX)\n1. [执行后端](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/backend.html)\n1. [QNN API](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/api.html)\n\n## AI stack\n\n1. [overview](https://www.qualcomm.com/developer/artificial-intelligence#overview)\n1. [neural-processing-sdk-for-ai](https://www.qualcomm.com/developer/software/neural-processing-sdk-for-ai)\n   - (SNPE?)\n   - The Qualcomm® Neural Processing SDK is engineered to help developers save time and effort in optimizing performance of trained neural networks on devices with Qualcomm® AI products.\n   - [tutorials](https://www.qualcomm.com/developer/software/neural-processing-sdk-for-ai#tutorials)\n   - [doc introduction](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/introduction.html)\n1. [ai-engine-direct-sdk](https://www.qualcomm.com/developer/software/qualcomm-ai-engine-direct-sdk)\n   - Qualcomm AI Engine Direct is also referred to as Qualcomm Neural Network (QNN)\n   - The Qualcomm® AI Engine Direct SDK provides lower-level, unified APIs for AI development. 低级别 API, 用于调用多种异构硬件核心\n   - CPU, GPU, NPU, cDSP, standalone tensor acclerator，\n   - [doc introduction](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/introduction.html)\n1. [ai-model-efficiency-toolkit](https://www.qualcomm.com/developer/software/ai-model-efficiency-toolkit)\n   - 工具集：压缩， 量化，编译，网络搜索\n\n## AI hub\n\n1. [overview](https://www.qualcomm.com/developer/artificial-intelligence#overview)\n\n### sample\n\n1. [aihub quick-example-pytorch](https://app.aihub.qualcomm.com/docs/hub/getting_started.html)\n   - 需要先注册\n   - sample model 会传到服务器进行 compile, profile 和 run; 服务器上会记录中间 onnx, 编译好的模型(\\*.so)，预测运行时间，消耗的内存，layer 个数及 cycle 数\n\n## doc\n\n1. [hexagon sdk](https://docs.qualcomm.com/bundle/publicresource/topics/80-77512-1/hexagon-dsp-sdk-collection-landing-page.html?product=1601111740010422)\n\n## 硬件\n\n1. [佐思汽研《2024 年自动驾驶 SoC 研究报告》](https://www.eefocus.com/article/1726287.html)\n1. SA8650 有 100TOPS 的 AI 算力，高于主要竞品英伟达 ORIN-N，同时图形输出能力很强，最高支持 4 个屏幕。 SA8650 可以对应 12 个摄像头，即 8 个 800 万像素，4 个 400 万像素。 SA8650 功耗大概 25-40 瓦，超过 25 瓦就最好采用水冷设计，目前 SA8650 的设计方案都是水冷\n\n## QNN\n\n1. 下载https://www.qualcomm.com/developer/software/qualcomm-ai-engine-direct-sdk 点击右上角 get software\n   - wget -c https://apigwx-aws.qualcomm.com/qsc/public/v1/api/download/software/qualcomm_neural_processing_sdk/v2.26.0.240828.zip\n   - 文档路径在${QNN_SDK_ROOT}/docs\n1. `qnn-throughput-net-run --config resnet50.json --output=resnet50_out.json` 运行模型\n   - 配置 json 里包含 backends(.so 文件), models(.bin 文件), contexts, testCase 设置\n1. 交叉编译\n   - 需要指定 ${QNN_AARCH64_LINUX_OE_GCC_93}/sysroots [OE](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/setup.html#toolchains)\n   - examples/QNN/NetRun/linuxOE/oe-linux-qnn-net-run.sh\n   - [yacto 编译](https://github.com/quic-yocto/meta-qcom-qim-product-sdk?tab=readme-ov-file)\n1. 模拟器\n   - 先看\n\n## 工具\n\n1. 查看 npu 利用率\n   - sysMonApp 低级别工具，会提供详细信息\n   - sysprofiler_app 高级别工具，基于 sysMonApp `sysprofiler_app --cdsp  --cdsp1 --live` 有些版本不支持同事看两个\n\n## NOTE\n\n1. 先看一下开发板系统，是 ubuntu 还是 oe\n1. linux 版本系统不是很稳定(会死机？)，推荐用 qnx 系统\n1. npu(htp) 可以编程， simd\n1. 模型部署自定义插件是关键，插件在 npu 上运行\n1. 有 x86 模拟器\n1. bin 文件和 so 文件区别：.bin 文件通常只包含模型的数据（如权重、偏置等），而 .so 文件是与硬件交互的可执行代码库，包含对模型推理的具体实现。在部署时，通常会将 .bin 文件中的模型数据加载到 .so 文件中的推理引擎中，然后由该引擎在高通硬件上执行推理操作。\n1. QNX Software Development Platform (SDP) 需要购买\n"},{"title":"triton","url":"/2024/07/03/triton/","content":"\n## llvm-objdump\n1. `/share_data/triton/llvm_19_dir/251205/bin/llvm-objdump --mattr=+m,+c,+f,+a,+xsiorigin -zCDS '/data/users/jiaxiyang/siorigin_triton/.triton/cache/M32BYEN7L42EZJG5X3YO773ONZ4AROFGAJPD45W3CBWOKVVXRRRA/alu_tile_kernel.elf'  > f32.log` 反汇编\n\n\n## [lit (llvm integrated tester)](https://llvm.org/docs/CommandGuide/lit.html)\n\n1. [MLIR testing guide](https://mlir.llvm.org/getting_started/TestingGuide/)\n1. The MLIR framework encourages existing best practices, e.g. writing and maintaining an IR spec, building an IR verifier, providing the ability to dump and parse MLIR files to text, writing extensive unit tests with the FileCheck tool, and building the infrastructure as a set of modular libraries that can be combined in new ways.\n1. [FileCheck](https://llvm.org/docs/CommandGuide/FileCheck.html)\n1. [MLIR lit](https://mlir.llvm.org/getting_started/TestingGuide/)\n1. 是 LLVM/MLIR 标准的 测试运行框架。\n1. 在 MLIR 中，90% 的测试都是用 lit + FileCheck 组成。\n1. 可以测试多种文件，python也可以，需要# RUN %PYTHON %s 2>&1\n1. FileCheck 用来匹配输出; 删掉可以查看输出结果\n1. 直接使用triton-op %s来验证是否有解析错误\n1. MLIR 的绝大部分测试（.mlir、.td、.cpp 等）都依赖 lit 来运行。\n   - `# RUN: mlir-opt %s -convert-scf-to-cf | FileCheck %s` mlir文件注释是要执行的命令; 用 .mlir 文件作为输入，运行某些 pass，最后用 FileCheck 验证输出。\n   - mlir-opt可以加--debug\n1. 解释\n\n``` mlir\n  // RUN: triton-opt %s -split-input-file -verify-diagnostics --debug\n  含义是：lit 会用 `triton-opt` 读取这个文件 `%s`，并加上两个关键选项：\n  - `-split-input-file`：按文件中的 `// -----` 分隔，把一个文件**拆成多个子模块/子测试**，每个子块单独跑一遍。\n  - `-verify-diagnostics`：让 MLIR 检查每个块中 `// expected-error` 之类的注释，**验证诊断（报错/警告）是否按预期出现**。\n```\n1. 看中间结果\n\n``` mlir\n// RUN: mlir-translate -mlir-to-llvmir %s | llc -mtriple=riscv64-rv64gv-linux-gnu -mcpu=sipu100 | FileCheck %s\n改为\n// RUN: mlir-translate -mlir-to-llvmir %s | tee /dev/tty | llc -mtriple=riscv64-rv64gv-linux-gnu -mcpu=sipu100 | tee /dev/tty | FileCheck %s\n或\n// RUN: mlir-translate -mlir-to-llvmir %s > tranlate.llc && llc -mtriple=riscv64-rv64gv-linux-gnu -mcpu=sipu100 tranlate.llc -o tranlate.llc.s && FileCheck %s < tranlate.llc.s\n或 去掉 FileCheck\n// RUN: mlir-translate -mlir-to-llvmir %s | llc -mtriple=riscv64-rv64gv-linux-gnu -mcpu=sipu100\n\n```\n\n1. 修改triton Makefile代码test-lint命令加上`-v`, 使用 `make test-lint`命令可以打印出测试的例子\n   - 到build目录下`lit -v test/`执行，可以看到每一个测试测试状态, test是路径,测试路径下的所有文件\n   - `lit --show-tests test/` 只查看不执行\n   - `lit -v test/TritonSIPU/IR/linear-layout-attribute.mlir`只执行一个测试\n   - `lit -a tile-indexed-load-store.mlir` 执行一个文件的测试\n   - `lit -a --filter 'linear-layout-attribute.mlir' test/` filter过滤，filter_out排除\n   - `-a` 输出测试过程中的详细信息，包含测试的输入输出\n   - `--debug`打印配置信息\n1. cp build目录下 `lit.site.cfg.py` 到test/ 文件夹下，就能在test目录下直接运行lit   \n\n## NOTE\n1. pytorch triton调度：Triton 的 kernel 调度是 PyTorch（准确说是 TorchInductor）根据图优化结果、张量形状和硬件信息来决策的，Triton 只是负责生成和编译 kernel，不负责调度策略本身。\n1. triton是thread block粒度的编程， 不需要划分threads(warp)，只需要计算好每个thread block要算的数，和算法流程\n   - flash—attention v1 和 v2处理的数据不同，v1 一个thread block处理1个head attention, v2一个thread block处理 1/num_m_blocks个head attention\n1. kernel 入参没用最好删除，编译出的ttir和driver.py参数可能对不上\n1. 参考mlir dialect td[llvm-project milr dialect td defination](https://github.com/llvm/llvm-project/blob/2e82a17f4e71a833cc3ca4a832bd14a5ef537616/mlir/include/mlir/Dialect/Arith/IR/ArithOps.td)\n## value构造\n1. include/triton/Conversion/TritonGPUToLLVM/Utility.h\n   - i64_val(3)\n   - #define i64_val(...) LLVM::createConstantI64(loc, rewriter, __VA_ARGS__)\n\n## recompile\n1. triton在什么情况下会recompile kernel\n1. 不同输入向量化长度不一样？输入数据长度能影响llir生成？测试发现确实变了， BLOCK_SIZE=2048时会生成两种llir\n\n``` shell\nhttps://github.com/triton-lang/triton/blob/99b5e296ca21ba21b6c02dc48391422e53f25ffb/python/triton/runtime/jit.py#L580\nhttps://github.com/triton-lang/triton/blob/99b5e296ca21ba21b6c02dc48391422e53f25ffb/python/triton/runtime/jit.py#L320\nhttps://github.com/triton-lang/triton/blob/99b5e296ca21ba21b6c02dc48391422e53f25ffb/python/triton/backends/compiler.py#L79\n变量是16的倍数会特殊处理， specialization参数的key会加D\nkey:  [('*i32', 'D'), ('*i32', 'D'), ('i32', '')]{'num_warps': 1, 'debug': False}\nkey:  [('*i32', 'D'), ('*i32', 'D'), ('i32', 'D')]{'num_warps': 1, 'debug': False}\n所以n在16的倍数和非16倍数之间切换时会recompile\n```\n\n## mask\n1. 在 Triton 中，mask 是控制 是否参与计算 的一个非常重要的机制，通常用于 避免越界访问 或 条件执行。在编写 Triton kernel 时，mask 一般配合 tl.where, tl.load, tl.store 等指令使用。\n1. load, store只是避免越界访问，where才是mask数据执行的\n1. [load根据ptr和mask计算向量化load长度](https://github.com/triton-lang/triton/blob/607c50cc9fdd2541db88b5a8681164f081dd71ad/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/LoadStoreOpToLLVM.cpp#L210)\n1. triton-cpu\n   - [两种情况三种处理方式](https://github.com/triton-lang/triton-cpu/blob/0625715c271426eb61dd37c43deb8bc954bf4f23/third_party/cpu/lib/TritonToTritonCPU/ConvertMemoryOps.cpp#L184)\n\n## tutorial\n### 06 fused-attention\n1. “on-band” 通常指的是靠近注意力矩阵主对角线的部分，也就是当前 token 更多地关注自己附近的 token。\n1. “offband” 指的是距离对角线较远的注意力，也就是 token 关注的位置比较远（长距离依赖）\n\n## libdevice\n1. 调用库\n1. [07-extern-functions.py](https://github.com/triton-lang/triton/blob/7a83ed78d95e0afcf89ea25289347fb079d47756/python/tutorials/07-extern-functions.py#L43)\n\n\n## cuda和triton区别\n1. [gpt](https://chatgpt.com/share/67dcd467-b918-8004-886d-5d3381653365)\n1. 编程粒度： cuda 是thread, triton 是thread block\n\n## ALU 和 SFU\n1. ALU（Arithmetic Logic Unit） \n1. SFU（Special Function Unit） \n1. ALU（Arithmetic Logic Unit） 和 SFU（Special Function Unit） 是现代处理器（尤其是 GPU）中经常并存的两类计算单元\n\n## 对数\n1. mma生成比较矩阵txt\n\n``` python\nresult = (matmul_outputs_sipu.to(\"cpu\")- matmul_outputs_golden).abs() > 0.1  # 误差大于 0.1 的地方设为 True\nresult = result.to(torch.int)  # 转换为整数 (1 表示误差超出，0 表示正常)\n\n# 使用 NumPy 加速写入\nimport numpy as np\nnp.savetxt(\"result1.txt\", result.cpu().numpy()[:64, :64], fmt=\"%d\")\n\nresult = (C - golden_C).abs() > 0.1\nresult = result.to(torch.int)  # 将布尔值转换为整数\nwith open(\"result1.txt\", \"w\") as f:\n    for i in range(result.shape[0]):\n        for j in range(result.shape[1]):\n            f.write(f\"{result[i, j].item()} \")\n        f.write(\"\\n\")\n```\n\n## coordinate 到 index的映射\n\n``` python\n\n#  坐标(x, y)到index的转换, 将M行N列矩阵划分为（m, n) 的block,block内部行优先存储，block之间也是行优先\n\n#  block index \n#  block_row = x // m\n#  block_col = y // n\n#  blocks_per_row = N // n\n#  block_index = block_row * blocks_per_row + block_col = x // m * (N // n) + y // n\n\n#  local index\n#  local_row = x % m\n#  local_col = y % n\n#  local_index = local_row * n + local_col = x % m * n + y % n\n\n#  global index\n#  global_index = block_index * m * n + local_index = (x // m * (N // n) + y // n) * m * n + x % m * n + y % n\n\n# (x, y) -> (x // m * (N // n) + y // n) * m * n + x % m * n + y % n\n\n#坐标(x, y)到index的转换, 将M行N列矩阵划分为（m, n) 的block,block内部列优先存储，block之间也是列优先\n\n#  block index \n#  block_row = x // m\n#  block_col = y // n\n#  blocks_per_col = M // m\n#  block_index = block_col * blocks_per_col + block_row = y // n * (M // m) + x // m\n\n#  local index\n#  local_row = x % m\n#  local_col = y % n\n#  local_index = local_col * m + local_row = y % n * m + x % m\n\n#  global index\n#  global_index = block_index * m * n + local_index = (y // n * (M // m) + x // m) * m * n + y % n * m + x % m\n\n# (x, y) -> (y // n * (M // m) + x // m) * m * n + y % n * m + x % m\n```\n\n\n## [Tools](https://github.com/triton-lang/triton/tree/main/bin)\n\n1. vscode mlir 插件配置 triton-lsp\n   - op可以跳转到src定义\n   - loc可以看行数\n   - log.txt中可以看outline，能找到IR Dump Before\n   - 需要注册自定义dialet [link](https://github.com/triton-lang/triton/blob/main/bin/RegisterTritonDialects.h)：头文件和registry.insert dialect, 注册pass\n\n``` c++\n#include \"siorigin/include/Dialect/TritonSIPU/IR/Dialect.h\"\n#include \"siorigin/include/TritonToTritonSIPU/Passes.h\"\nmlir::triton::siorigin::registerConvertLoadStoreOpsPass();\nregistry.insert<mlir::triton::siorigin::TritonSIPUDialect>;\n\n```\n\n1. [tablegen_compile_commands.yml](https://mlir.llvm.org/docs/Tools/MLIRLSP/#build-system-integration-1)\n   - 配置mlir tblgen-lsp-server 和 tablen_compile_commands.yml\n   - 可以从td文件跳转到生成的定义\n   - hover\n1. triton-opt\n   - 将中间结果存下来，使用triton-opt定位问题\n1. triton-llvm-opt\n1. graphiviz\n   - `triton-opt matmul_kernel.ttir --view-op-graph --allow-unregistered-dialect > test.dot 2>&1`\n   - [vscode plugin open dot file](https://marketplace.visualstudio.com/items?itemName=tintinweb.graphviz-interactive-preview)\n   - [view-op-graph](https://mlir.llvm.org/docs/Passes/#-view-op-graph)\n   \n### triton-tensor-layout\n1. bin/triton-tensor-layout.cpp 有测试命令\n1. `triton-tensor-layout -i input.mlir -t \"tensor<128xf32>\" --use-hw-view  -o output1.mlir` \n   - input.mlir只包含类似 `#blocked = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [1], warpsPerCTA = [1], order = [0]}>`的语句\n   - --use-hw-view 从硬件的view打印， 默认是data的view\n     - hw下warp下面一列代表一个thread\n1. `triton-tensor-layout -l \"#ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\" -t \"tensor<16x16xf16>\"` 注意：用ttg，不用triton_gpu\n2. `triton-tensor-layout -l \"#ttg.nvidia_mma<{versionMajor = 3, versionMinor = 0, warpsPerCTA = [8, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0], instrShape = [16, 256, 32]}>\" -t \"tensor<128x256xf16>\"`\n   - unittest/Dialect/TritonGPU/DumpLayoutTest.cpp里有测试代码\n   - 关键代码: shared layout, distribute layout\n``` c++\nstd::string mlir::triton::gpu::getLayoutStr(RankedTensorType tensorType,\n                                            bool useHWPointOfView) {\n  auto layout = tensorType.getEncoding();\n\n  // tensorType is needed later on (e.g., getDimSize(j)), so we still have to\n  // pass it as a param\n  if (auto sharedLayout = mlir::dyn_cast<SharedEncodingAttr>(layout)) {\n    return getSharedLayoutStr(tensorType, useHWPointOfView);\n  } else if (auto distributedLayout =\n                 mlir::dyn_cast<DistributedEncodingTrait>(layout)) {\n    return getDistributedLayoutStr(tensorType, useHWPointOfView);\n  }\n\n  // else unimplemented, return error\n  llvm::report_fatal_error(\"Unimplemented usage of getLayoutStr\");\n  return \"\";\n}\n```\n\n### lit\n\n\n## 语法\n1. tt.splat 操作用于将一个标量值扩展为一个张量，其中所有元素都具有相同的标量值。\n1. tt.broadcast 操作用于将一个较小的张量扩展为一个较大的张量，通过复制其元素来匹配目标张量的形状。\n1. NOTE: tutorial 中的BLOCK_SIZE和cuda中的block_size不是一样的， BLOCK_SIZE是data size, block size是cuda中一个block有多少thread\n   - BLOCK_SIZE 是一个编译时常量，用来指定每个 kernel 实例（也可以看作是“tile”或“block”）要处理的数据量，即每个程序实例负责处理多少个元素。\n   - block_size 通常是指每个 block 中的线程总数（即 num_warps × 32）以及每个线程负责的工作量（sizePerThread）\n   - Triton 编译器和运行时会根据这个 BLOCK_SIZE 自动确定实际的线程组织（比如如何分配到 warps 中），从而隐式地管理 num_warps 和 sizePerThread\n   - 教程中定义的 BLOCK_SIZE 并没有和我们之前讨论的概念冲突，而是用一种更高层的抽象来表达同样的思想。它既代表了一个 block 中需要处理的数据总量，也间接影响了内部线程数和每个线程的工作量，只不过这些底层细节由 Triton 自动处理了。\n   - `BLOCK_SIZE = block_size × sizePerThread = num_warps x threadsPerWarp × sizePerThread`\n1. 当输入不是const, 如何传递tl.constexp? BLOCK_SIZE = triton.next_power_of_2(n_cols)\n\n## 适配 DSL\n\n1. intel\n   ![intel backend architecture](https://i.ibb.co/cF3yF0K/e-Hk7ps-HTk-M.png)\n1. [intel-xpu-backend-for-triton](https://github.com/intel/intel-xpu-backend-for-triton)\n1. 高通\n   ![architecture](https://i.ibb.co/6NMJ8Yk/xo5-Pqj-RPDT.png)\n   ![pytorch](https://i.ibb.co/9VLhpNk/du-T6l-PNQfg.png)\n1. [Triton Conference 2024: Afternoon Session](https://www.youtube.com/watch?v=ONrKkI7KhU4&list=PLc_vA1r0qoiTjlrINKUuFrI8Ptoopm8Vz&index=18)\n1. [microsoft/triton-shared](https://github.com/microsoft/triton-shared)\n1. [tablegen 来生成 MLIR 代码](https://llvm.org/docs/TableGen/ProgRef.html)\n\n## tensor和线程layout\n\n1. 要实现峰值性能，不仅需要利用专用硬件单元，还需要精心设计张量布局和转换, tensor layout and layout conversion\n1. 在编译过程中，Triton 的 Python 代码首先被翻译成 Triton 方言 ( tt )，然后再翻译成 TritonGPU 方言 ( ttg )。在此过程中，每个张量都与特定的布局相关联，以充分利用现代 GPU 上可用的硬件功能单元。\n1. layout不仅针对load store, 不同的op有不同的layout偏好，需要进行layout转换来使用最佳layout, 此时，需要进行数据重排\n1. lowering过程不仅需要sizePerThread信息，还需要contiguity信息， contiguity表明逻辑和物理的映射连续性; \n   - 有了thread要处理的数据和数据的连续性信息才能lowering\n1. tensor划分的顺序从左到右，sizePerThread -> threadsPerWarp -> warpsPerCTA ...; 如果和tensor shape不匹配，需要广播\n   - 左边是最里面的维度, 是移动最快的维度。\n1. 只有memory相关的操作才关心sizePerThread；其他op在reg上都是连续的   \n1. sizePerThread表明一个thread一次可连续处理的数据， elementPerThread是一个thread需要处理的数， sizePerThread由contiguity和硬件限制等参数算出，sizePerThread[order[0]]，只会，设置order[0]对应维度的sizePerThread，其他为1\n   - 具体逻辑见 lib/Dialect/TritonGPU/Transforms/Coalesce.cpp\n1. sizePerThread 和 elementPerThread 有区别，sizePerThread根据contiguity算的，elementPerThread是一个线程真正要算的数\n   - The legacy Triton layout system requires each layout to define its own interface methods—such as `the number of elements per thread` and `the number of contiguous elements`   \n1. sizePerThread对应reg, threadsPerWarp对应thread, warpsPerCTA对应warp\n1. sizePerThread = [2, 2], threadsPerWarp = [8, 4], warpsPerCTA = [1, 2]. 因为此时sizePerThread*threadsPerWarp*warpsPerCTA = 16x16, 小于tensor的shape 32x32, 所以这个16x16的layout会按照[2, 2]的shape进行广播, 填满整个32x32的tensor.\n1. `./triton-tensor-layout -l \"#triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 4], warpsPerCTA = [2, 2], order = [1, 0]}>\" -t \"tensor<16x8xf32>\"`\n1. ConvertTritonToTritonGPU pass加的layout信息\n   - typeconverter里重新构造了tensor type， 加了layout信息, 检测到是tensor, 就添加默认layout信息\n\n   ``` c++\n    addConversion([this](RankedTensorType tensorType) -> RankedTensorType {\n    // types with encoding are already in the right format\n    // TODO: check for layout encodings more specifically\n    if (tensorType.getEncoding())\n      return tensorType;\n    ArrayRef<int64_t> shape = tensorType.getShape();\n    triton::gpu::BlockedEncodingAttr encoding =\n        getDefaultBlockedEncoding(this->context, shape, this->numWarps,\n                                  this->threadsPerWarp, this->numCTAs);\n    return RankedTensorType::get(shape, tensorType.getElementType(), encoding);\n  });\n   ```\n1. `ptr+index <-shape stride-> coodinate <-layout-> thread id`\n   - tensor提供coodinate到index, value的映射\n   - layout提供coodinate到thread id映射, layout(coodinate) = thread id\n1. 不同的Layout可以看作是不同的映射函数，代表了不同访问模式。\n1. Layout是tensor coodinate到线程的映射，we define a layout as a function that maps a multi-dimensional tensor index to a set of integers T corresponding to the indices of the CUDA threads allowed to access some data at index i. [link](https://zhuanlan.zhihu.com/p/672720213)\n   - coodinate到index是由tensor axis info决定的\n1. triton也通过Layout来表征Thread对数据(tensor)的访问模式，例如blocked和blocked1分别表示A,B两个Operand的访问模式，即每个Thread取连续的8个f16等等。\n   - tensor<64x64x!tt.ptr<f32>, #blocked1> 既有tensor, 也有layout, data layout \n   - AxisInfo 会记录张量各轴的尺寸、步幅以及与硬件映射相关的信息，用于指导后续的内存合并（coalesce）和其它优化操作，“AxisInfo”并不是存储张量所有信息的容器，它主要关注那些对高效内存访存至关重要的轴信息（例如各轴的大小、步幅、排列顺序等），而完整的张量信息还可能分布在其他 IR 属性或数据结构中。因此，可以说，Triton 的 AxisInfo 分析提取并表达了张量中与访存和并行调度密切相关的那部分信息。\n1. data layout 是 TritonGPU Dialect 的 Type system 的关键，确定了 Data(各层级memory中的Tensor) 到 thread 之间的映射关系。\n1. 总的来说 tensor 为数据在内存中的物理组织提供静态描述，而layout 则通过运行时索引决定每个线程应处理数据中的哪一部分。两者必须配合，才能让每个执行单元正确且高效地读取、处理和写回数据，从而充分发挥 GPU 的并行计算能力\n1. nvidia 在ttgir生成后layout相关优化已经做完了，lowering到llvm ir根据ttgir 方案来，不会再做layout的优化\n1. ttgir里已经有每个thread要算数据的所有信息\n   - 可以删除compiler.py除了add_convert_to_ttgpuir pass之外的pass看看结果正确性和ir\n1. load之后数据在reg上, 后面的数据不用关心contiguty和size_per_thread; 只用看shape\n   - 只有load和store考虑连续性的问题，后面的计算op不需要考虑，已经加载到reg上了\n1. coalesce pass里会分析continuty等信息来确认一次load和store多少数据,改变load store layout并添加conver layout op   \n1. coalesce添加新的layout并添加convert_layout; remove_layout负责layout的传播和删除\n\n### [linear layout](https://arxiv.org/abs/2505.23819)\n1. 注意 linear layout表示的是输出到输入的索引，给定输出index，经过linear map左乘，得到输入index； Vin = T@Vout\n1. [详细介绍](https://mp.weixin.qq.com/s/PDFshzgcj_udaFu3aJr1tQ)\n   - 复合 Composition 讲的比较好\n1.  linear layout可以表示多种含义：\n    - 作为tonsor属性：硬件位置和tensor 逻辑index映射\n    - 作为op的属性：tensor 逻辑index之间的映射\n    - 两个tensor和一个convert_layout op; convert_layout op不带属性\n      - 直接使用linear 组合功能\n    - 两个tensor和一个op，都带linear layout属性：\n      - 比两个tensor之间转换多加了逻辑之间的转换\n      - 结合起来可以得到硬件位置之间的映射; 硬件指令功能也用linear layout表示，通过组合来Lowering\n1. `https://github.com/triton-lang/triton/blob/main/include/triton/Tools/LinearLayout.h` 官方说明\n1. `In Triton, a linear layout (LL) is a function that maps from a \"hardware location\" to a \"logical tensor index\".`\n1. `To summarize, a linear layout is a function from tuples of integers to tuples of integers.  We specify some key values of the function, and then we can compute all the other values using the linearity rule.`\n1. 是一种映射:`硬件索引 -> tensor 坐标索引`; 查找表，linear layout是查找表的一种压缩\n   - tensor坐标到存储tensor memory的index并不从linear layout来获取\n1. 只要给硬件位置basic vector对应的输出tensor index(coodinatate)，就可以算出 linear map？\n   -  从硬件输入维度和tensor index可以看出输入输出维度\n   - 类似线性方程组，给定几个输入输出，可以得到线性方程组的参数\n1. 对于 tensor layout，我们通常会问一个问题：某个线程 index（例如 [lane_x, warp_y, thread_z]）负责访问张量中的哪个 [i, j, k] 元素？\n   - 作者的关键想法是：这种映射可以写成一个线性映射：[i, j, k]^T = M × [lane_x, warp_y, thread_z]^T   （模 2 运算）其中 M 是一个 𝔽₂ 上的矩阵，形状为 [张量维度 × 硬件维度]\n1. 矩阵layout M 就定义了“线程 index 到 tensor index”之间的布局映射。\n   - M没什么含义，就是线性方程组参数，是一种映射projection;\n   - M的行列长度需要和硬件的[reg, thread, warp]与tensor的[i, j, k]二进制位数匹配上\n1. linear layout是一种表示：表明tensor 张量元素分布并映射到计算和内存层次结构；不同的linear layout表明tensor的映射不同。映射关系可以通过linear map来表示；两个linear layout转换也可以通过linear map来转换。   \n1. linear layout: `x = M * p`; M是二进制矩阵，p是硬件二进制编码向量，x是对应的tenosr 坐标，x算出来是一列，前几位表示j, 后几位表示i\n   - linear的意思应该是通过线性运算就能做layout映射和转换\n   - `T_index = A * HW_index^T`\n1. linear layout是从硬件寄存器找tensor 逻辑index, `一个reg只对应一个逻辑index, 之前的layout是从tensor 逻辑地址找硬件寄存器；一个逻辑index可以对应多个硬件reg`, 输入是硬件位置二进制编码，输出是tensor 逻辑坐标。\n   - 使用layout 二进制矩阵来进行layout来计算\n1. linear layout和linear map是有区别的，linear map可以将一种layout转化为另一种\n1. linear layout永远是二维的，输入输出可以是各种维度：输入是三维，会变成一维，如i, j, k, shape是（2， 4， 8）， 分别用1， 2，3bit表示，那么输入就是6bit的一维向量，表示的范围为2*4*8 = 2的6次方 = 64；输出也一样\n   - 输入是行，输出是列\n1. linear layout的base vector,基向量\n   - 表示label所在列为1对应的bit位所在行号的2的指数结果，从0开始；如下面warp=2 -> (8),最后一列为1的行号是3， 2的3次方为8\n   - ob表示二进制\n   - 输出维度为1，dim0为2的4次方16 \n``` shell\n   shape: [16]\n#ttg.blocked<{sizePerThread = [1], threadsPerWarp = [4], warpsPerCTA = [4], order = [0]}>\nRepOrder: [0]\nTotalElemsPerThread: 1\nElemsPerThread: [1]\n- register is a size 1 dimension\n- lane=1 -> (1)\n  lane=2 -> (2)\n- warp=1 -> (4)\n  warp=2 -> (8)\n- block is a size 1 dimension\nwhere out dims are: [dim0 (size 16)]\n0b1000\n0b0100\n0b0010\n0b0001\n```\n1. 二维\n   - 输出是二维的，前6bit表示第一维，后7bit表示第二维，register=1 -> (1, 0) 表示第一维的第一列的第一位为1； register=8 -> (0, 16)表示第4列的第二维的第5位为1, 表明第四个reg的位置决定了第二维第5位的值\n``` shell\nshape: [64, 128]\n#ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [1, 2], CTASplitNum = [1, 2], CTAOrder = [1, 0]}>\nth 4 * 4 = 16 -> 64\nRepOrder: [0, 1]\nTotalElemsPerThread: 32\nElemsPerThread: [8, 4]\n- register=1 -> (1, 0) \n  register=2 -> (2, 0)\n  register=4 -> (4, 0)\n  register=8 -> (0, 16)\n  register=16 -> (0, 32)\n- lane=1 -> (8, 0)\n  lane=2 -> (16, 0)\n  lane=4 -> (32, 0)\n  lane=8 -> (0, 1)\n  lane=16 -> (0, 2)\n- warp=1 -> (0, 4)\n  warp=2 -> (0, 8)\n- block=1 -> (0, 64)\nwhere out dims are: [dim0 (size 64 b6), dim1 (size 128 b7)]\n0b1000000000000\n0b0100000000000\n0b0010000000000\n0b0000010000000\n0b0000001000000\n0b0000000100000\n0b0000000010000\n0b0000000001000\n0b0000000000100\n0b0000000000010\n0b0001000000000\n0b0000100000000\n0b0000000000001\n```\n1. linear layout某一列可以为全0，如warp2为0，表明没有用完warp\n1. linear layout的行列的bit数表明有输入输出有多少种情况，用一个矩阵可以表明输入和输出的各种组合\n1. `include/triton/Tools/LinearLayout.h`有说明\n1. `unittest/Tools/LinearLayoutTest.cpp` 有测试\n   - `./unittest/Tools/LinearLayout  --gtest_filter=LinearLayoutTest.Empty`\n   - linear layout base vector = 0, 表示repeat; 当前列决定不了输出，所以会和其他输入的结果重复\n   - 输入输出可以先将bit个数转成shape, bit位不是很熟悉\n1. [LinearLayoutConversionsTest](https://github.com/triton-lang/triton/blob/main/unittest/Dialect/TritonGPU/LinearLayoutConversionsTest.cpp)\n   - Linear Layout\t所有张量元素在内存中按一维线性排列\n   - `unittest/Dialect/TritonGPU/LinearLayoutConversions --gtest_filter=LinearLayoutConversionsTest.ShapeLargerThanLayout`\n1.  Every tensor layout is modeled as a linear function—a matrix—that maps physical resource indices into a logical tensor of size n2 using binary arithmetic on the bits of the input and the output.\n    - 每个张量布局都被建模为一个线性函数（矩阵），它使用对输入和输出位的二进制算术将物理资源索引映射到大小为n方的逻辑张量。\n    - 矩阵是一个映射， 将物理资源映射为逻辑张量的索引\n1. We define a Linear Layout as a linear map between (labeled) vector spaces over F2\n   - For example, we can define layout L as:  L : Reg × Thr × Wrp → 𝔽₂ⁿ × 𝔽₂ᵐ (n, m是坐标shape的二进制长度, 如16x32, n=4, m=5)\n1. 使用linear layout布局的传播就比较好实现，直接使用组合从一种layout到另一种layout\n1. linear layout工具\n   - 矢量化：不用再手动算Contiguous elements等信息，根据L来算\n   - Broadcasting: 数据小于线程，多个线程算一个数据，数据多余线程，一个线程算多个数据，需要广播。之前的layout很难处理；linear layout只要按需改L矩阵\n1.  Linear Layouts 作为编译器的一部分，实现了布局推理（inference）、布局转换（conversion）以及代码生成（codegen）的一体化流程。通过矩阵操作，编译器能够自动选择最优布局、插入转换并调度硬件指令，而不再依赖繁杂的专门代码或手工优化\n1. Triton Linear Layout 是 编译器中端优化阶段的核心技术，专注于解决张量布局的硬件映射与转换问题。其通过代数建模统一了 NVIDIA/AMD 等硬件的布局规则，显著降低编译器开发复杂性和错误率，并为高性能算子（如 GEMM）提供接近硬件的峰值性能。\n1. 之前的做法是为每种 layout（blocked, swizzled, pitch-linear, tiled 等）硬编码映射逻辑，很容易造成代码重复、不可组合、维护难。   \n1. 能看到 layout × thread 的过程吗？可从 Triton IR 或 TTGIR 中看到地址偏移计算指令，含 mul, add, bitwise 等\n   - ttgir的layout（之前block) 可以确认每个线程计算的数据， linear layout也可以\n1. 同一个数据，其实面对两个标号：`空间逻辑位置和thread的全局标号`，layout则是建立二者之间的映射关系，这样编程的时候就知道当前的thread应该操作哪个数据，进而推广到当前的一簇thread（比如一个warp，一个warp group）应该操作哪一个集合内的数据\n   - 每个thread由于所处的warp，cluster，block不同，也有自己唯一的全局标号<block_id, warp_id, lane_id>\n1. 阶段\t               传统方案     \tLinear Layout 方案\n   - 前端 → TTGIR      硬编码布局声明\t布局定义为 F2 矩阵\n   - 布局转换\t       手工编写转换规则\t矩阵乘法（A * B^{-1}）\n   - 后端代码生成\t   通用共享内存重排\t映射至硬件原语（如 ldmatrix）\n1. 如果你有一个张量 A[i,j,k]，分配到 thread block、warp、lane 的顺序是错综复杂的，传统做法需要手写映射函数。现在，只需用一个二进制矩阵 M_layout，你就能表达这种从 hardware index → tensor index 的映射：\n   - tensor_index = M_layout × hardware_index（mod 2\n1. ttgir生成的blocked mma 等 layout 可以tolinear_layout然后再使用，根据自己硬件的特性可以对to linear layout做hack\n1. Triton’s operations fall into four categories: (1) computation, (2) memory (global, shared, tensor, etc.), (3) layout conversion, and (4) shape operations.\n   - Triton 的操作分为四类：(1) 计算，(2) 内存（全局、共享、张量等），(3) 布局转换，以及 (4) 形状操作。\n#### 从linear layout获取每个thread要处理的数据\n1. LinearLayout::apply()函数接收硬件二进制indices，输出tensor index；输入只是常量，是int32\n1. SmallVector<std::pair<StringAttr, Value>> applyLinearLayout， 输入输出都是变量，是value\n   - 输出有label + value组成，几维的输出结果的size就是几\n\n\n### [convert_layout](https://superjomn.github.io/posts/triton-mlir-publish/#convertlayoutop)\n1. convert_layout： %out = convert_layout %in: tensor<128x128xf32, #layoutA> -> tensor<128x128xf32, #layoutB> 用线程去读取 layoutA 下的数据，然后写入 layoutB 对应的位置。\n   - 编译器会自动生成一套数据搬运逻辑（通常是多个线程协作 copy），实现这个转换。\n1. convert_layout 的本质是：重新安排 tensor 数据在内存中的排列顺序，以便适配不同的线程访问模式或硬件特性。\n   - convert_layout 的目的：在计算或搬运之前，把数据布局变换成 适合当前线程访问方式 的形式。\n   - 不是直接在 global memory 上重排，而是： 每个 thread 从 global memory 按原 layout 读取数据到自己的寄存器， 然后根据目标 layout 的规则，通过 thread 之间的 shuffle、store 等操作，将数据搬运成新布局\n   - [gpt](https://chatgpt.com/share/68819589-cd78-8004-864f-95b3b46e7577)\n   - load之后的blocked layout是在寄存器上\n   - #blocked -> #shared ，代表数据从 register file 存储到 shared memory 中，\n   - #blocked -> #blocked1 ，代表数据从 register file 存储到register file 中，thread内部只需要改变顺序，thread间可能需要shared memory处理\n   - #mma -> #blocked ，正常是 DotOp 的输出转换为更简单的 layout 来进一步计算，由于涉及到跨 thread 间的数据传递，因此一般会借由 shared memory 中转一次\n   \n## axis info\n1. 先Analysis再Transform [link](https://zhuanlan.zhihu.com/p/687394750)\n1. include/triton/Analysis/AxisInfo.h 中有解释contiguity、divisibility 和 constancy, 是用于描述张量（Tensor）在不同维度上的内存布局和值的分布特性的重要属性，它们通常用于优化编译器的代码生成，特别是在 GPU 上生成高效的内存访问模式时。\n   - [gpt](https://chatgpt.com/share/6826a649-edc8-8004-bb2e-fe9255d2b68e)\n1. 打印axis info代码位置：[link](https://github.com/triton-lang/triton/blob/676227a023b88d48f59d660df6c12d98630ed240/lib/Analysis/AxisInfo.cpp#L1336)\n1. 可以获取Value的axis info\n1. contiguity\t最短的连续整数段长度\t内存布局是否紧密\n1. divisibility\t每段起始值能被整除的最大 2 的幂次\t值是否有规律、对齐\n1. constancy\t最短的相同数值序列长度\t是否包含重复常数，用于优化广播等\n1. 每个属性是多为的，dim 0表示最高维，最后面一个数是最低维\n1. AxisInfo Analysis\n   - TRITON_LLVM_DEBUG_ONLY=\"tritongpu-coalesce\" 打印AxisInfo\n   - Contiguity: 连续性，当数据（如数组或向量）的各个元素在内存中按照顺序排列、地址连续时，我们称该数据结构是连续的\n   - Divisibility：可分性, 在处理数组或矩阵时，如果数据维度的长度能整除分块大小，则可以将数据分割为连续且大小均一的块（例如在 tiling 操作中），这有助于内存访问的局部性和缓存利用率。\n     - tt.divisibility 属性就是用来给出对数据（通常是指针或者整数值）的整除性（或对齐性）的保证\n     - 这种机制类似于其他编译器中通过“对齐”说明符（如 attribute((aligned(16))) 等）提供的优化提示，但在 Triton IR 中以一种专用的方式表达。\n   - Constancy(Constant Value): 指这个Tensor是常量，得到其常量的长度，对于%1是通过arith.constant得到的，其长度为128，constant value为1\n1. gpu blocked layout是线程分配数据的layout，不是数据的layout   \n1. 访存合并需要看load store的oprand的`线程访问顺序和数据的排布`决定是不是需要合并访存  sample:\n\n``` mlir\n#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n%10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n%19 = tt.load %10, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked1>\n// %10 的AxisInfo contiguity = [1, 64], divisibility = [4, 16], constancy = [1, 1], constant_value = <none>\n// %10的blocked threadsPerWarp = [32, 1]表明线程按列取数，AxisInfo  contiguity = [1, 64]表明数据是行优先的，load的时候访存效率不高，需要访存合并coalesce,  \n\n```\n\n## lowering\n### [for](https://mlir.llvm.org/docs/Dialects/SCFDialect/#scffor-scfforop)\n1. for -> scf -> cf -> llvm.cf -> br\n\n### make_range\n1. tt.make_range决定每一个线程计算的数据量，triton是block level的编程，通过make_range变成thread level\n1. make_range里会获取cluster, block, warp, thread的id, 会将数据划分到id对应index, ttir里无layout信息，ttgir里有，根据ttgir来lowering\n   - TTIR 中的操作是抽象的张量级操作，没有说明具体由哪个线程处理什么数据。\n   - TTGIR 明确了：\n     - 每个线程处理哪些元素\n     - 线程的线程索引/维度\n     - block/grid 的结构\n   - `./triton-tensor-layout -l \"#triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 4], warpsPerCTA = [2, 2], order = [1, 0]}>\" -t \"tensor<16x8xf32>\"`\n   - `python/build/cmake.linux-x86_64-cpython-3.10/bin/triton-tensor-layout -l \"#ttg.blocked<{sizePerThread = [1], threadsPerWarp = [1], warpsPerCTA = [1], order = [0]}>\" -t=\"tensor<256xf32>\"`\n2. sipu 中tile_format输入时， make_range决定数据偏移\n\n\n## base\n1. intrinsic, asm, builtin\n   - asm（内联汇编）手动在 C/C++ 代码中嵌入汇编指令，提供最底层、最精细的控制，但可移植性最差，需要针对不同架构单独编写汇编语句。 \n   - intrinsic（编译器内建函数）看似普通函数调用，编译器将其“内联”替换为对应硬件指令或指令序列，既能保证性能，又提高了可维护性和跨平台性（在支持该 intrinsic 的架构上直接生成指令，否则回退至通用实现）。 \n   - builtin（GCC/Clang 内置函数）以 __builtin_ 前缀出现，无须包含头文件，由编译器原生识别并优化（如 __builtin_popcount 可映射为硬件 POPCNT 指令），兼具内建函数与语言扩展的特点，可用于原子操作、数学计算等多种场景\n1. intrinsic（内在函数）和 asm（Inline assembly 内联汇编）各有优缺点：内在函数提供了一种较高级、便捷且安全的方式调用底层指令，同时让编译器能充分优化代码；而内联汇编则提供了极致的控制能力，但也带来可移植性和维护性上的挑战。\n1. 在编译器和编程语言文档中，\"intrinsic\" 通常指由编译器直接内置的函数，用于提高效率，将函数调用转化为内联代码或特定的机器指令。\"builtin\" 则多用作 \"intrinsic\" 的同义词\n1. builtin是编译器内置，intrinsic需要头文件\n1. builtin是更广泛的“编译器扩展”集合：除了映射硬件指令外，还包括诸如分支预测提示（__builtin_expect）、类型检查（__builtin_types_compatible_p）、内存屏障、数学函数（__builtin_sin）等\n1. 用c++写kernel时用builtin, 被编译成llir时，转换成intrinsic; c++也能调用intrinsic，例如arm neon intrinsic\n1. Intrinsic 是编译器提供的特殊函数，通常直接映射到特定的机器指令。它们允许程序员以函数的形式使用底层硬件指令，从而提高性能。Builtin 是编译器内置的函数，通常以 __builtin_ 为前缀，提供了一些没有头文件也可以使用的功能，如数学运算、位操作等。这些函数由编译器直接识别和处理，可能被优化为特定的机器指令。\n1. 当你要调用特定指令集（如 SSE、NEON）或直接操作特殊寄存器，用 intrinsic；当你要利用编译器提供的各种低级优化或扩展（分支预测、内存屏障、内置数学函数等），用 builtin。\n1. polyhedral compilation，  scheduling languages与triton\n   - 如果你做编译优化（LLVM, 编译器后端开发）→ 选择 Polyhedral Compilation\n   - 如果你优化计算图（Halide, TVM, 计算调度）→ 选择 Scheduling Languages\n   - 如果你优化深度学习（GPU kernel 加速）→ 选择 Triton\n1. 假设要优化矩阵乘法 C = A × B：\n   - Polyhedral Compilation：自动分析 for 循环依赖，调整 i, j, k 计算顺序，进行 loop tiling 和 向量化。生成适用于 CPU/GPU 的高效代码。\n   - Scheduling Languages（Halide, TVM）：允许用户手动设定计算顺序，比如先计算小块矩阵、再合并结果。适用于自动调度和搜索最优策略。\n   - Triton：直接编写 triton.kernel，自动生成高效 CUDA 代码。通过 memory tiling 和 寄存器优化 提高 GPU 执行效率。   \n1. 第一次编译时会在/tmp目录下生成main.cpp然后编译成运行kernel时所需so，编译后删除main.cpp，生成内容见driver.py。\n   - main.cpp不包含main函数，有函数用于调用kernel\n   - main.cpp中包含生成的kenerl路径, 需要加载\n1. 使得算子开发的部分复杂度分配给上层 kernel 开发用户，部分复杂度分配给底层的 Triton compiler。同时对接上层框架的工作则交给了上层软件栈，使得 Triton 能够专注在自己这个层次需要解决的核心问题上，找到了一个比较不错的\"product/technology\" fit。\n1. Triton 的核心设计思想----Block-wise 编程，Block 上面的归用户，Block 内部的归 Triton compiler 自动化处理。相应地，Block 内部的优化细节，也交由 Triton compiler 处理了。\n1. Triton 这个项目目前还更像是一个满足细分场景(高效高性能 Kernel 开发)需求的“小而美\"的项目\n1. triton kernel 中的计算是 block level, 跟据 ptr + offset 直接计算一个 block，block 具体如何划分成 thread 由编译器决定\n   - 通过 arrange + stride 来计算 block 中每个数据的 offset\n1. 注意代码中的 pid 含义和 cuda 的不同：triton 表示 block id, cuda 表示 thread id\n1. 自定义优化搜索空间，减少搜索时间\n1. 使用 triton 来做 codegen\n1. triton 的 language 语法确实很简单，相比较 cuda 来说，它能够帮我们快速验证一些 idea，同时给出比 cublas 性能相当的算子。\n1. Using Triton, you only need to know that a program is divided into multiple blocks\n1. 大部分的框架都以 python 的 DSL 暴露给用户，然后用户通过写对应的 python 语法，调用已经用 C++/CUDA 或者 assemble 写好的高性能组件。\n1. `pip show triton` /usr/local/lib/python3.10/dist-packages/triton/\\_C/libtriton.so\n1. 支持 nvidia, AMD GPU； /usr/local/lib/python3.10/dist-packages/triton/backends/amd/lib/libamdhip64.so\n1. `~/.triton/cache` cache 路径, 里面有 ptx, cubin, 各种 ir 文件 [link](https://chatgpt.com/share/6750222c-3c44-8004-b0a4-483b2d45dead)\n   ![各文件作用](https://i.ibb.co/qFsBwJv/YMws76l-KOI.png)\n   - TTIR（Triton Tensor Intermediate Representation）TTIR 是 Triton 编译器的一个中间表示，专门用于描述张量级别的操作。它是一个较高层次的中间表示，关注计算任务中的张量操作和计算模式。\n   - TTGIR（Triton Tensor GPU Intermediate Representation）TTGIR 是 Triton 的低层中间表示，专注于描述与 GPU 硬件架构相关的计算细节。它建立在 TTIR 之上，但更加贴近 GPU 的执行模型。\n1. Triton, a `language` and `compiler` for writing highly efficient custom Deep-Learning primitives. The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs.\n1. An open-source python-like programming language which enables researchers with no CUDA experience to write highly efficient GPU code -- most of the time on par with what an expert would be able to produce\n1. 开发效率\n   ![img](https://i.ibb.co/NYSS9WV/Xrz-Qku-ZLSo.png)\n1. 领域特定语言（domain-specific language DSL)\n1. 本质上来说，Triton 和 TVM/XLA 这类工作的定位有所不同，如果说 TVM/XLA 是比较纯正的 AI 编译器的话，Triton 更像是一个面向 AI 加速器算子开发的领域开发语言，为了能够将用户使用 Triton 语言开发的 kernel 映射到具体硬件上的执行码，需要设计开发相应的 Triton compiler 来完成这层映射。所以当我们说 Triton 的时候，其实隐指了 Triton 语言+Triton 编译器这两个事物的综合体。\n1. 从 triton 的源码来看，triton 目前在 NV 的 GPU 上已经有了一套自己比较成熟的 mapping 路线，通过先对 python 语言层，也就是 triton DSL 进行抽象，得到 AST，然后将 AST 中的每个节点 lower 到 Triton Dialect 上，Triton Dialect 则是一个比较贴近上层语言表达的 IR，他的主要作用则是为了保持用户在书写对应算法时的准确性。接下来会进一步被 map 到 TritonGPU Dialect 上，那么 TritonGPU Dialect 则是一个更加贴近 GPU 层面的 IR，它则是为了具体的性能优化而设计。图中其他的蓝色模块，比如 SCF，Arith，Tensor 等都是 MLIR 生态中已经被实现好并且广为使用的 Dialect。这些 Dialect 会一起和 TritonGPU Dialect 共存，然后被 lower 到对应的 LLVM Dialect，LLVM Dialect 则是最贴近 LLVM IR 的一层设计，从 LLVM Dialect 到 LLVM IR 的转换是非常容易的，最终代码就会被接入到 LLVM 的 NVPTX 的后端，从而生成后续能跑在 GPU 上的高性能 machine code. [link](http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/OpenAI%20Triton%20MLIR%20%E7%AC%AC%E9%9B%B6%E7%AB%A0%20%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/)\n1. 和 cuda pytorch 区别 [link](https://www.youtube.com/watch?v=AtbnRIzpwho&t=273s)\n   ![区别](https://i.ibb.co/qszbhN4/NYmmor-R0l-P.png)\n1. 编译器是一个很好的生产力工具，能够帮助做很多手工的任务\n1. triton 支持的语言特性是 Python 的一个子集\n   - no dict\n   - no meta-programming\n   - no slicing\n   - no indexing\n   - ...\n1. triton v2 [link](https://www.jokeren.tech/slides/triton_next.pdf)\n   - MLIR(Triton dialect, TritonGPU dialect)\n   - clean layout concepts(like cutlass cute)\n     - low overhead time: Cache and fetch kernels using efficient signatures\n   - debugging: triton.language.print\n   - Profiler interface\n1. 查看 grid\n\n```python\ngrid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\ngrid_size = grid({'BLOCK_SIZE': 1024}) # grid是lamda函数，需要先调用才能看结果\nprint(f'Grid size: {grid_size}') # (97, )\n```\n\n## 装饰器\n\n1. triton.jit\n1. triton.autotune\n1. triton.compile\n1. torch.compile\n\n## debug\n1. C++ 代码里：`#include \"llvm/Support/Debug.h\" llvm::DebugFlag = true;`\n1. [MLIR debug](https://mlir.llvm.org/getting_started/Debugging/)\n   - mlir-opt\n1. 使用`llvm::sys::PrintStackTrace(llvm::errs());` 来打印堆栈，注意需要设置llvm-symbolizer路径\n   - export PATH=$PATH:/share_data/triton/llvm_19_dir/250925/bin\n1. export MLIR_ENABLE_DIAGNOSTICS=warnings,remarks;来控制诊断信息  \n   op->emitRemark()\n   mlir::emitRemark\n\n``` c++\n        if (auto enableDiagnostics =\n                triton::tools::getStrEnv(\"MLIR_ENABLE_DIAGNOSTICS\");\n            !enableDiagnostics.empty()) {\n          llvm::SmallVector<std::string, 3> storage;\n          parseCommaSeparatedValues(enableDiagnostics, storage);\n          for (auto &str : storage) {\n            if (str == \"warnings\") {\n              showWarnings = true;\n            } else if (str == \"remarks\") {\n              showRemarks = true;\n            } else if (str == \"stacktraces\") {\n              showStacktraces = true;\n            } else if (str == \"operations\") {\n              showOperations = true;\n            }\n            // we show errors by default, so no need to set it\n          }\n        }\n```\n\n1. llvm_unreachable(\"unsupported emitBaseIndexForLayout\");\n1.  op->emitError(\"Expected at least one tensor input operand.\");\n    - 自动附带 operation 的位置信息（location），方便用户或开发者快速定位 IR 中出错的地方。\n    - 返回值是一个 InFlightDiagnostic 对象，可以用于链式追加更详细的信息\n1. `./python/triton/_C/libtriton.so` libtrion位置\n1. [官方指导](https://triton-lang.org/main/programming-guide/chapter-3/debugging.html)\n1. [源码 debug](https://github.com/triton-lang/triton?tab=readme-ov-file#tips-for-building)\n1. `TRITON_ENABLE_LLVM_DEBUG=1` 打印详细信息\n1. `TRITON_LLVM_DEBUG_ONLY=\"tritongpu-remove-layout-conversions,regalloc\"` 只打印一些pass log, 使用LDBG宏\n1. `TRITON_INTERPRET=1` 使用 Triton 解释器而不是在 GPU 上运行。您可以在内核代码中插入 Python 断点！NOTE:注意真正运行的时候不要打开\n1. `TRITON_HOME=/home/data`\n1. `MLIR_ENABLE_DUMP=1` 只dump mlir, 不dump llvm ir\n1. `LLVM_IR_ENABLE_DUMP=1`\n1. `TRITON_ALWAYS_COMPILE=1` 每次都重新编译\n1. `TRITON_PRINT_AUTOTUNING=1`可以打印 autotune 选择的最快配置 prints out the best autotuning config and total time spent for each kernel after autotuning is complete.\n\n## c++调用 triton kernel\n\n1. [test aot](https://github.com/triton-lang/triton/blob/main/python/test/unit/tools/test_aot.py)\n   - vscode testing 下的 pytest 来测试 debug 模式可以看到生成的文件路径, 右键测试程序进入 debug\n   - 能自动生成 kernel.h, kernel.c, libkernel.so, test.c 不用手写\n   - python3 -m pytest -v python/test/unit/tools 运行测试程序\n   - python3 -m pytest -v -k test_compile_link_matmul python/test/unit/tools/test_aot.py\n   - compile.py 编译\n   - compile_aot_kernels 编译 kernel\n   - [load 函数](https://github.com/triton-lang/triton/blob/fd691c67ac20958a67693358186d877790f5f48f/python/triton/tools/link.py#L222)\n1. 在 c++ load triton 生成的 ptx 或者 cubin 文件\n\n## links\n\n1. [非常好的中文教程](http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/OpenAI%20Triton%20MLIR%20%E7%AC%AC%E9%9B%B6%E7%AB%A0%20%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/)\n1. [triton next](https://www.jokeren.tech/slides/triton_next.pdf)\n   - why triton\n   - triton 的定位\n1. [谈谈对 OpenAI Triton 的一些理解](https://zhuanlan.zhihu.com/p/613244988)\n"},{"title":"talk_skills","url":"/2024/07/01/talk-skills/","content":"\n## 技术分享\n\n### 技巧\n\n1. 清晰简洁\n   - 语言要清晰简洁，避免使用过多的专业术语。\n   - 使用简单易懂的例子来说明复杂的概念。\n1. 视觉辅助\n   - 使用图表、图片和动画来帮助解释概念。\n   - 确保幻灯片内容简洁，不要过多的文字。\n1. 互动性\n   - 提出问题并邀请听众参与讨论。\n   - 使用现场演示或代码运行来增加互动性。\n1. 实战演练\n   - 事先进行多次演练，熟悉内容和演示流程。\n   - 准备好应对可能出现的问题或故障。\n1. 自信和热情\n   - 保持自信和热情，感染听众。\n   - 展示你对所分享内容的兴趣和信心。\n1. 时间管理\n   - 控制好每个部分的时间，避免超时或内容过于简略。\n   - 根据听众的反应灵活调整节奏。\n\n### 流程\n\n1. 自我介绍\n1. 引言\n   - 介绍主题与分享目的\n1. 大纲\n1. 具体内容\n1. 总结\n   - 回顾关键点，并提出可能的下一步\n1. QA\n1. 反馈和改进\n\n## 会议组织\n\n## 演讲\n"},{"title":"agent","url":"/2024/06/27/agent/","content":"\n## base\n\n1. 智能体（Agent）是一个能够感知其环境并采取行动以实现某些目标的实体。智能体的基本组件包括：\n   - 感知：智能体通过传感器获取环境信息。\n   - 决策：智能体基于感知到的信息以及其内部状态做出决策。\n   - 行动：智能体通过执行器与环境互动，以实现特定目标。\n1. 智能体可以是非常简单的（例如基于规则的系统）或者非常复杂的（例如自主驾驶汽车）。智能体可以是软件程序，也可以是物理机器人。\n1. 具身智能（Embodied Intelligence）是人工智能（AI）和机器人学中的一个概念，强调智能体（agent）不仅仅通过抽象的计算或逻辑推理展现智能，而是通过与物理环境的互动和感知展现智能。具体来说，具身智能认为身体在智能行为中起着关键作用，智能体必须能够感知其环境、作出反应并采取行动，而不仅仅是处理信息或进行推理。\n1. 具身智能是人工智能的一个发展领域，指一种智能系统或机器能够通过感知和交互与环境进行实时互动的能力。 可以简单理解为各种不同形态的机器人，让它们在真实的物理环境下执行各种各样的任务，来完成人工智能的进化过程。 具身智能机器人是“具身智能”的实体形态，有望成为 AI 的最终载体。\n"},{"title":"uniad","url":"/2024/06/25/uniad/","content":"\n## base\n\n1. 传统范式: `感知-决策-规控`\n1. 感知（Perception）：感知是指系统通过各种传感器来感知周围环境的能力，包括识别和理解道路、车辆、行人、障碍物等元素。自动驾驶系统通常会使用多种传感器，如摄像头、激光雷达、毫米波雷达、GPS、超声波传感器等。这些传感器能够提供不同类型的信息，如图像、距离、速度、方向等。感知算法会对这些数据进行处理和分析，从而识别和跟踪周围环境中的各种对象，形成对周围环境的理解。\n1. 决策（Decision-making）：决策是指系统基于感知到的环境信息，以及预先设定的目标和规则，来制定行动计划的能力。在决策过程中，系统需要考虑诸如避障、保持车辆安全、遵循交通规则、到达目的地等因素。决策算法会根据当前环境的状态，以及系统的目标和约束条件，选择最佳的行动策略。这可能涉及到路径规划、速度控制、车道保持、转向等决策。\n1. 控制（Control）：控制是指将决策产生的行动指令转化为实际车辆控制的过程。控制系统负责实现决策中确定的行动，使车辆按照规划的路径和速度行驶，并且保持在安全的状态下。控制系统通常会包括车辆动力系统控制、转向控制、制动控制等方面。这些控制器会根据决策器提供的指令，以及传感器反馈的实时信息，对车辆进行精确的控制，以实现自动驾驶的目标。 这三个技术模块在自动驾驶系统中密切配合，形成了一个完整的闭环系统。感知模块提供环境信息，决策模块根据环境信息做出决策，控制模块将决策转化为实际行动。这种分层的设计使得自动驾驶系统能够根据周围环境实时变化，做出正确的决策，并安全地控制车辆行驶。\n1. 自动驾驶算法框架的各种设计比较。\n   - 大多数工业解决方案是对不同的任务部署不同的模型。\n   - 多任务学习架构，多个任务头共享一个主干网络（特征提取器）。\n   - 端到端学习范式，其联合了感知与规划模块\n\n## links\n\n1. [All in One | UniAD：一种包含全栈自动驾驶算法的统一架构设计](https://developer.volcengine.com/articles/7382373299495862310)\n1. [repo](https://github.com/OpenDriveLab/UniAD)\n"},{"title":"invest","url":"/2024/06/18/invest/","content":"\n## 时间价值\n\n## 终值 FV\n\n1. [excel 计算器](https://support.content.office.net/en-us/coach/excelcoach_FV_PartII.html)\n\n## 净现值 NPV\n\n1. 将贷款净现值与本金做对比\n1. 注意按年算的\n1. execel 上计算`=NPV(rate, B2:B291)`: 和原始投入做比较，\n   - rate 为银行利率或通货膨胀率\n   - rate 按 0.3\n1. 搜中国通货膨胀率\n1. 房贷注意减去房租，只要通货膨胀在 1%就是划算的\n1. 注意算首付利息(都按现值算)\n1. 都按现值算有问题，未考虑首付无利息问题，按未来值(终值)\n1. 付出的终值 - 资产的终值\n\n## 内部收益率(IRR)\n\n1. 有计算器\n1. 资金流入现值总额与资金流出现值总额相等、净现值等于零时的折现率。\n1. 用于计算投资是否可行\n1. 采用内部收益率（IRR）进行项目评估的时候，如果内部收益率大于当前的折现率，那么其净现值就会大于零\n\n## 折现率\n\n## 复利\n\n1. [复利计算器](https://www.xiadun.net/gongju/fuli.html)\n\n## link\n\n1. [NPV（净现值）与 IRR（内部收益率）](https://zhuanlan.zhihu.com/p/260938216?utm_psn=1786379550101471233)\n"},{"title":"probability_theory","url":"/2024/06/18/probability-theory/","content":"\n## 先验概率\n\n## 后验概率\n\n## 最大似然估计\n\n## [排列组合](https://zh.wikipedia.org/wiki/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6)\n\n## [概率论](https://zh.wikipedia.org/wiki/%E6%A6%82%E7%8E%87%E8%AE%BA)\n"},{"title":"llm.c","url":"/2024/06/14/llm-c/","content":"\n## download models\n\n1. gitee\n\n```shell\ngit clone https://gitee.com/hf-models/openai-community-gpt2.git\ncd gpt2\napt install git-lfs\ngit lfs pull\n```\n\n## links\n\n1. [karpathy/llm.c](https://github.com/karpathy/llm.c)\n1. [gitee hugging face gpt2](https://gitee.com/hf-models/openai-community-gpt2.git)\n"},{"title":"bev","url":"/2024/06/12/bev/","content":"\n## base\n\n1. 具体到智能驾驶系统，因为感知、预测、决策和规划等模块，都是在 3D 空间内进行的，而摄像头看到的图像信息，只是真实物理世界在透视视图（Perspective View）下的投影，从图像得到的信息，需要经过复杂的后处理才能使用，信息损失也很多。而`将视觉信息转换到 BEV 空间，则可以很方便地连接感知与下游的规划控制模块`。\n1. BEV 空间内的感知任务，在精度上也有优势。做 2D 感知时，面对远处的物体，可能几个像素的误差便可能导致几十米的真实误差，而在 BEV 空间内训练模型时，对远处误差的损失（loss）更加明显，所以感知结果也会更准确一些。\n1. BEV 在线构建的语义地图可以一定程度上代替高精地图的作用，在城市 NOA 中发挥重要作用。\n1. HDMapNet、BEVSegFormer、BEVerse 等方法，可以基于多摄像头信息，将 BEV 空间内的车道线、路沿、斑马线等静态目标物，在线生成局部语义地图供下游规控任务使用，大大降低了语义地图的构建和维护成本，从而可能会给行业带来地图构建和更新的新范式。\n\n## bevdet\n\n## bevformer\n\n## bevfuison\n\n1. [bev_pool code](https://github.com/mit-han-lab/bevfusion/tree/main/mmdet3d/ops/bev_pool/src)\n\n## bev pool\n\n1. bevfusion 先实现\n1. Voxel Pooling 用来把多视角的 3D 特征 F3d(视锥特征) 聚合成统一的基于 BEV 的特征 Fbev 。\n   - voxel 体素，对应 3d 特征点， pixel 像素，对应 2d 特征点\n1. [BEVPoolv2: A Cutting-edge Implementation of BEVDet Toward Deployment](https://arxiv.org/abs/2211.17111)\n1. [BEVDet 的 Voxel pooling 新版本 BEVPoolv2: A Cutting-edge Implementation of BEVDet Toward Deplo](https://blog.csdn.net/djfjkj52/article/details/138722692)\n1. 根据 NDHW voxel index(3d 特征点到 bev pillar 映射索引) 和 NDHWC frustum feature（视锥特征) 可以得到 bev feature\n1. 估计深度原理可能是近大远小，同一个物体如果占用了很多像素则说明物体离的近，占用很少像素说明离的远\n\n## links\n\n1. [一文读懂 BEV 空间内的特征级融合](https://zhuanlan.zhihu.com/p/539951496)\n   - 前融合：是指把各传感器的数据采集后，经过数据同步后，对这些原始数据进行融合\n   - 中融合：就是先将各个传感器通过神经网络模型提取中间层特征（即有效特征），再对多种传感器的有效主要特征进行融合，从而更有可能得到最佳推理。\n   - 后融合：是指各传感器针对目标物体单独进行深度学习模型推理，从而各自输出带有传感器自身属性的结果，并在决策层进行融合，\n1. [一文读懂 BEV 自底向上方法：LSS 和 BEVDepth](https://zhuanlan.zhihu.com/p/567880155)\n1. [NVIDIA-AI-IOT/Lidar_AI_Solution/](https://github.com/NVIDIA-AI-IOT/Lidar_AI_Solution/)\n"},{"title":"op","url":"/2024/05/09/op/","content":"\n## plan\n\n1. 基础工程\n   - unit test\n   - benchmark\n   - ci\n1. 加速技术\n   - simd\n1. profiling 工具\n   - perf\n   - nsight system/compute\n1. 编程语言\n   - cuda\n     - cutlass\n   - triton\n   - halide\n   - opencl\n1. 平台\n   - gpu\n   - arm\n   - dsp\n1. 自定义算子\n   - (重要)注意插件和自定义算子区别：C++ or Cuda 自定义算子可以直接被 pytorch 使用，不用封装成插件，也不用为导出 onnx 模型做一些额外工作。导出 onnx 时需要做一些操作，在运行 trt 模型时需要将算子封装成 plugin\n   - c++算子：cuda 实现，c++封装，python 调用，不用实现前后 forward\n   - pytorch 自定义算子：需要继承 torch.autograd.Function，实现前后 forward, 可以是 python 算子；如果是 C 算子，在 python 中调用 c++ or cuda, 如果不考虑导出 onnx 不用做封装，导出的话需要 symbolic，opset15 更简单 [link](https://leimao.github.io/blog/PyTorch-Custom-ONNX-Operator-Export/) [详解 PyTorch 编译并调用自定义 CUDA 算子的三种方式](https://godweiyang.com/2021/03/21/torch-cpp-cuda-2/)\n     - 隐藏 cuda 实现：编译 cuda 到 cubin， 在 cpp 中读 cubin，再用 pytorch setup.py 编译 [link](https://chatgpt.com/share/8fb13ae5-3d21-4c1e-be5a-7648d8b2ea9a)\n     - cuda 需要 torch tensor 包一层，tensorrt 插件不用\n     - [triton](https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html)\n   - tensorrt 插件：将 c++ or cuda 实现的算子封装成 tensorrt plugin\n   - [plugin](https://github.com/godweiyang/NN-CUDA-Example)\n   - tensorrt 两种方法支持自定义 op\n     - onnx graphsurgeon 修改 op [link](https://leimao.github.io/blog/TensorRT-Custom-Plugin-Example/)\n     - pytorch 直接导出带自定义 op 的 onnx [link](https://leimao.github.io/blog/PyTorch-Custom-ONNX-Operator-Export/)\n   - [numba jit 加速](https://numba.readthedocs.io/en/stable/user/examples.html)\n1. 常见算子列表\n1. 大模型算子\n1. 库\n   - openblas\n   - cudnn\n   - cublas\n   - [Tengine](https://github.com/OAID/Tengine) (久未更新代码)\n   - tensorrt-llm plugins kernel\n1. 算子融合\n   - conv + elementwise(relu, add, BN ....)\n   - elementwise + elementwise\n   - conv + conv\n1. python 驱动测试 (开发板上可能无开发环境，需要 c++单测)\n   - setup.py 编译 kernel\n   - pybind11 封装 kernel\n   - python 直接调用 kernel，可用于训练和推理\n   - python 调用 trt plugin 来验证 plugin 正确性\n1. 训练加速\n   - 正向反向传播算子\n   - [torch.autograd.function](https://pytorch.org/docs/stable/autograd.html)\n     - 实现函数的前向传播、反向传播，已经封装好了，理解起来很快，看一个案例就能理解。\n   - viztracer + nsys + ncu + nvtx 分析性能瓶颈\n   - 多卡加速\n   - ddp\n   - dataloader 取数据的能力和计算的能力要平衡\n   - 1 gpu 4 worker\n   - 读数据和解码耗时\n     - cpu: [pywuffs](https://github.com/dev0x13/pywuffs) png 更有效，jpg 和 cv 差不太多\n     - gpu: [nvImageCodec](https://github.com/NVIDIA/nvImageCodec/tree/main/example/python)\n       - png decode 也是 cpu 做的\n     - torch: [vision io](https://pytorch.org/vision/main/io.html)\n   - 当 cpu 是瓶颈的时候，增加 worker 数量对数据读取帮助不大，反而会增加内存的消耗，将低 worker 数可以减少存储的消耗\n1. 推理加速（主要是减少 latency, 增大吞吐可以数据并行，多台机器，每台运行不同数据)\n   - 单机多卡\n   - 分布式\n1. 量化支持\n1. 调试\n   - vscode + nsight + cuda-gdb\n1. dump 实测数据方式\n   - [torch save](https://pytorch.org/tutorials/beginner/saving_loading_models.html#save)\n     - op 输入输出先写到一个 dict 里，一起 save;\n     - 追加功能，读 pt 文件到 dict, 插入新的键值，重新写\n   - pickle(python 自带)\n1. python 与 C++ 交互方式\n   - [pytorch/extension-cpp](https://github.com/pytorch/extension-cpp.git)\n     - c++ op forward backward => pybind11 => python op(torch.autograd.Fuction) with forward and backward => layer(torch.nn.Module) forward 中调用 op.apply()\n     - onnx export 时 symbol 会替换 module 中 forward 调用继承了 torch.autograd.Function 的自定义算子 apply 函数\n   - pytorch tensor + pybind11\n   - [Setuptools example](https://github.com/pybind/python_example)\n   - [pytorch 之 c++/cuda 拓展](https://zhuanlan.zhihu.com/p/459955492)\n   - [PyTorch 源码解读之 cpp_extension：揭秘 C++/CUDA 算子实现和调用全流程](https://zhuanlan.zhihu.com/p/348555597)\n   - [pytorch setup.py](https://github.com/pytorch/pytorch/blob/main/setup.py)\n   - [mmdetection3d](https://github.com/open-mmlab/mmdetection3d)\n   - [BEVFusion](https://github.com/open-mmlab/mmdetection3d/tree/main/projects/BEVFusion)\n   - `pip install -v -e .`\n1. torch compile\n   - [doc](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)\n   - 需要 pytorch 2\n\n## base\n\n1. [算子基本概念](https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/80RC1alpha003/devguide/opdevg/tbeaicpudevg/atlasopdev_10_0006.html)\n"},{"title":"dsp","url":"/2024/05/06/dsp/","content":"\n## cadence\n\n1. [Tensilica Vision DSP Family](https://www.cadence.com/content/dam/cadence-www/global/en_US/documents/solutions/automotive/tensilica-vision-dsp-wp.pdf)\n   - 包含 P6, Q8 参数\n\n## VLIW\n\n1. VLIW 架构在嵌入式市场，如 DSP，领域获得了成功应用。\n1. 总而言之，VLIW 指令集架构简化了硬件设计，将最复杂的工作推给了编译器。\n1. 通常，常用的应用中，发现足够的并行指令填满 VLIW 的可执行单元是非常困难的。这就导致 VLIW 的利用率较低，即出现设计/宣称处理能力很高，但是实际的性能却较低的现象。\n"},{"title":"ollama","url":"/2024/03/19/ollama/","content":"\n## base\n\n1. [ollama/ollama](https://github.com/ollama/ollama)\n1. [ollama-ui/ollama-ui](https://github.com/ollama-ui/ollama-ui)\n1. [open-webui/open-webui](https://github.com/open-webui/open-webui)\n1. 可以在多个操作系统上开启大语言模型服务\n1. windows 先安装 ollama 工具，在 powershell 上可以运行\n1. `ollama run llama2`\n1. `ollama run gemma:2b`\n1. `ollama serve`\n"},{"title":"gemma","url":"/2024/02/29/gemma/","content":"\n## [gemma.cpp](https://github.com/google/gemma.cpp)\n\n1. 使用了 hightway 加速\n1. [下载模型](https://www.kaggle.com/models/google/gemma/frameworks/gemmaCpp?select=tokenizer.spm)\n1. `./build/gemma --verbosity 2 --tokenizer tokenizer.spm --compressed_weights 2b-it-sfp.sbs --model 2b-it`\n1. [关键代码](https://github.com/google/gemma.cpp/blob/main/gemma.cc)\n   - GenerateImpl() 包含 prefill 和 generate 两个阶段\n   - prefill 阶段使用了 Prefill()函数\n     - 每次处理 kPrefillBatchSize 个输入，循环处理\n   - generate 阶段使用了 Transformer()，Softmax(), SampleTopK()函数\n1. add_definitions(-DPROFILER_ENABLED=1), 打开 hightway profiler, 命令行%q 退出，[link](https://github.com/google/gemma.cpp/issues/73#issuecomment-1973230523)\n"},{"title":"pagedattention","url":"/2024/02/22/pagedattention/","content":"\n## base\n\n1.  attention 输入为 q k v 输出 q 维度类似的结果; 参数可能为各种 shape 信息 地址 head num, scale, mask 等。如果是 paged attention 需要处理 q k v 地址不连续的情况, 也需要考虑计算并行度问题\n1.  cuda core 计算的\n1.  [attention 代码](https://github.com/vllm-project/vllm/blob/main/csrc/attention/attention_kernels.cu)\n\n## links\n\n1. [从 FlashAttention 到 PagedAttention, 如何进一步优化 Attention 性能](https://zhuanlan.zhihu.com/p/638468472)\n1. [vLLM 皇冠上的明珠：深入浅出理解 PagedAttention CUDA 实现](https://zhuanlan.zhihu.com/p/673284781)\n   - vLLM 中，LLM 推理的 prefill 阶段 attention 计算使用第三方库 xformers 的优化实现，decoding 阶段 attention 计算则使用项目编译 CUDA 代码实现。具体代码在 vllm 的 csrc/attention/attention_kernels.cu 文件里，开发者洋洋洒洒写了八百多行 CUDA 代码。\n"},{"title":"flashattention","url":"/2024/02/03/flashattention/","content":"\n## note\n1. 重要：Q矩阵不在d维度上做tiling\n1. [flash attention公式计算](https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf)\n   - 注意融合的时候，oi = oi-1 + aiV[i, :], ai和V[i, :]是外积\n1. softmax计算推导：指数和 = 当前指数 + 前面指数和 * 修正参数(与max有关)\n1. softmax必须分两步（2 loop) 来算， 但attention不需要得到softmax结果，根据公式一步就能算完, softmax需要指数和，attention计算可以只根据之前的结果(递推)\n1. Q@K 或 softmax(Q@K) 被称为 attention score matrix\n1. 在计算机编程和软件项目中，csrc 通常是 \"C Source\" 的缩写，意为 \"C 源代码\"。即使 csrc 目录可能包含 C++ 代码，这个名字仍然被保留，因为它沿用了这一传统的命名惯例\n   - flash attention\n   - pytorch\n   - xformer\n   - vllm\n1. (不考虑 softmax)Attention 不融合消耗大量内存，如果 n 很大 qk (atttion 矩阵 nxn)结果非常大; 如果 kenel 进行融合；分块计算，只需要保存中间结果，见 xformer 示意图：[xformer link](https://twitter.com/fvsmassa/status/1580229170629849089)\n1. FlashAttention 在`batch和heads两个维度上进行了并行化`：使用一个 thread block 来处理一个 attention head，总共需要 thread block 的数量等于 batch size × number of heads。每个 block 被调到到一个 SM 上运行，例如 A100 GPU 上有 108 个 SMs。当 block 数量很大时（例如 ≥80），这种调度方式是高效的，因为几乎可以有效利用 GPU 上所有计算资源。但是在处理长序列输入时，由于内存限制，通常会减小 batch size 和 head 数量，这样并行化成都就降低了。 [link](https://zhuanlan.zhihu.com/p/645376942)\n1. FlashAttention-2 还在`序列长度这一维度上进行并行化`，显著提升了计算速度。此外，当 batch size 和 head 数量较小时，在序列长度上增加并行性有助于提高 GPU 占用率。\n1. `FlashAttention目的不是节约FLOPs，而是减少对HBM的访问。`\n1. 重点是 FlashAttention 在`训练和预测过程中的结果和标准Attention一样`，对用户是无感的，而其他加速方法做不到这点。\n1. FlashAttention does not read and write the large attention NxN matrix to HBM, FlashAttention 不会读取大型 NxN 注意力矩阵并将其写入 HBM\n1. In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]—that is, carefully accounting for reads and writes to different levels of fast and slow memory\n1. Our main goal is to avoid reading and writing the `attention matrix to and from HBM`. 没有优化前 multihead attation 每个 head 都需要一个 NxN attention matrix\n1. The most common approach to accelerate memory-bound operations is kernel fusion: if there are multiple operations applied to the same input, the input can be loaded once from HBM, instead of multiple times for each operation. Compilers can automatically fuse many elementwise operations\n1. 注意训练时的 forward 和推理时的 forward 的区别; 训练时输入 N 固定; 推理是 LLM inference（或称为 decoding）\n1. there have been many attempts to fuse several elementwise operations, such as fusing masking with softmax\n1. Let N be the sequence length, d be the head dimension, and M be size of SRAM with d <= M <= Nd;. Standard attention (Algorithm ) requires `O(Nd + NN)` HBM accesses, while FlashAttention (Algorithm 1) requires `O(NNdd/M)` HBM accesses.\n1. 对于 d(64-128) 和 N（大约 100KB）的典型值，d2 比 M 小很多倍，并且因此，FlashAttention 所需的 HBM 访问次数比标准实现少很多倍。\n\n### flash attention v1\n1. 论文算法里注意shared memeory和register区别，on chip是register上   \n1. 论文里的算法是一个thread block要处理的流程, 是一个head的attention, \n   - We use 1 thread block to process one attention head\n   - Q, K，V都会做tiling, loop处理\n1. NOTE: 论文算法里的block size跟thread block要处理的size没有关系, 只是划分数据用的   \n1. QK运算时类似split-k, 在k维度上做并行，最后再sync\n1. M 为shared memory大小，M/4是算最大的tile Size，因为Q, K, V, O都需要tile数据在shared memory上， tile size越大，读写HBM次数越少\n   - 注意这里的tile_size是thread block要处理的size， 不是warp\n1. 为什么不把m和l直接放到shared memory中？\n   - 因为sequence length可能比较大， shared memeory中放不下， 每个token有一个m，l, N长的Q, m和l共占用memeory为2N(sequence len); 每个block需要计算一个head的所有attention计算，m和l共需2N memory, 内循环每次都需要从HBM load Tr长度的m和l，更新完写回HBM \n     - 为什么不复用li或mi? 因为Qtile对应一个mi和li，K和V在外层，内层的mi, li需要在K, V维度上累加，会混乱，结合[flash attention公式计算](https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf)图\n       - flash attention v2 l和m不用写回HBM，因为在Q上划分block，每个block只需要占用2Tr(Tr是划分的Block Size，不会很大)\n1. For each block, FlashAttention splits K and V across 4 warps while keeping Q accessible by all warps\n   - 每个thread block内部还会划分threads(warp) 来计算\n     - triton是thread block粒度的编程， 不需要划分threads(warp)，只需要计算好每个thread block要算的数，和算法流程\n   - 每个warp先算出O，m, l放回到shared memory中，然后再reduce, warp间需要同步\n1. code\n\n``` cuda\n// params.b = batch_size, params.h = num_heads\ndim3 grid(params.b, params.h);\nkernel<<<grid, Kernel_traits::THREADS, smem_size_dq_dk_dv, stream>>>(params);\n```\n\n### flash attenion v2\n1. 论文里的forwar算法不是一个thread block要处理的流程，是一个head的attention, \n   - We use (seqlen_q / kBlockM) thread blocks to process one attention head\n1. code\n\n``` cuda\n// params.b = batch_size, params.h = num_heads\nconst int num_m_block = (params.seqlen_q + Kernel_traits::kBlockM - 1) / Kernel_traits::kBlockM;\ndim3 grid(num_m_block, params.b, params.h);\n............\n      kernel<<<grid, Kernel_traits::kNThreads, smem_size, stream>>>(params);\n```\n1. Q维度上是增加thread block并行，Q_tile内部是warp并行\n1. even within each thread block, we also have to decide how to partition the work between different warps. We typically use 4 or 8 warps per thread block,(128或256 threads)\n1. QK运算是slice-k, warp之间不需要sync\n1. 没有使用M来设置tile size, 直接作为参数传递  \n1. 先讲述 FlashAttention-2 对 FlashAttention 的改进，从而`减少了非矩阵乘法运算（non-matmul）的FLOPs`。然后说明如何将任务分配给不同的`thread block`进行并行计算，充分利用 GPU 资源。最后描述了如何在一个 thread block 内部分配任务给`不同的warps`，以减少访问共\n   享内存次数。这些优化方案使得 FlashAttention-2 的性能提升了 2-3 倍\n1. softmax 将 rescale 最后一起进行，中间结果可以多利用 tensor core 矩阵乘\n1. Warp Specialization: warp之间进行异步\n1. Multistage(流水线)：warp内进行异步\n   - 至少double buffer, 占用更多shared memmory\n\n### flash attention v3\n1. 有计算和计算之前的异步指令\n1. 在 Hopper 之前的架构，GEMM 计算是同步的，即 GEMM 计算完成后才可调度其他计算单元（如 SFU）进行计算（A100 新增的cp.asyncAPI 是计算和数据传输的异步，并非计算和计算的异步），但对于 Hopper 而言，由于 WGMMA 指令的异步性，我们可以同时进行 Tensor Core 和 CUDA Core/SFU 的计算，通过合理地安排 Warpgroup 的调度策略，就可以使 Softmax 的运算时间被 GEMM overlap。\n1. Warpgroup 指的是 4 个连续的 warps，共 128 个连续的 threads，正好对应了一个 SM 最多可并行计算的线程数。在 H100 上，我们可以以 Warpgroup 为粒度调度 GEMM 运算。\n1. Warp Specialization + Ping-Pong Scheduling + Intra-warpgroup pipelining [link](https://zhuanlan.zhihu.com/p/17533058076)\n   - warpgroup之间warp specialization\n   - Warp Specialization 数据传输和计算overlap\n   - Ping-Pong Scheduling 计算和计算之间overlap(注意：不是一个warpgroup内，是多个warpgroup之间， 一个group内是multistage) NOTE:只考虑计算\n   - Intra-warpgroup pipelining 计算和计算之间的overlap, 类似multstage; NOTE:只考虑计算\n1. 注意Warp Specialization, Multistage，  Intra-warpgroup overlapping, Ping-Pong Scheduling异步的区别\n1. mulitstage里的stage2就是在for里面算多少个不相关tile的数据\n   - shared memory也有stage概念, 注意区分, 这里的multistage不一定要和shared memory stage相同，因为可能用不到shared memory, 直接使用寄存器\n1. 2 stage是将wgmma0和softmax算一个stage，和wgmma1做流水， 3 stage是 wgmma0, softmax, wgmma1做流水   \n\n### [flash decoding](https://crfm.stanford.edu/2023/10/12/flashdecoding.html)\n\n1. flash attention 是针对模型训练时的 forward 和 backward; 训练时的 forward 是并行运算的\n1. flash decoding 是针对模型推理时的 inference； 推理时的 inference 是 Autoregressive(AR)\n1. flash attention 的优化不适合直接应用于推理过程。因为在训练过程中，FlashAttention 对 batch size 和 query length 进行了并行化加速。而在推理过程中，`query length 通常为 1`，这意味着如果 batch size 小于 GPU 上的 SM 数量（例如 A100 上有 108 个 SMs），那么整个计算过程只使用了 GPU 的一小部分！特别是当上下文较长时，通常会减小 batch size 来适应 GPU 内存。例如 batch size = 1 时，FlashAttention 对 GPU 利用率小于 1%！\n1. 流程：对 K 和 V 进行划分成多块，并行计算各块 attention，最后各块结果做 reduction\n\n## code\n\n1. [编译 cuda](https://github.com/Dao-AILab/flash-attention/blob/61a777247900f6c2a37376f3ffd7134385fdc95c/setup.py#L133)\n1. [pybind11 python interface](https://github.com/Dao-AILab/flash-attention/blob/61a777247900f6c2a37376f3ffd7134385fdc95c/csrc/flash_attn/flash_api.cpp#L1462)\n1. [python using pybind11 api](https://github.com/search?q=repo%3ADao-AILab/flash-attention%20flash_attn_cuda&type=code)\n1. [mha_fwd](https://github.com/Dao-AILab/flash-attention/blob/61a777247900f6c2a37376f3ffd7134385fdc95c/csrc/flash_attn/flash_api.cpp#L317)\n   - [python call place](https://github.com/Dao-AILab/flash-attention/blob/9c0e9ee86d0e0022b60deddb405c20ab77481582/flash_attn/flash_attn_interface.py#L51)\n1. [run_mha_fwd](https://github.com/Dao-AILab/flash-attention/blob/61a777247900f6c2a37376f3ffd7134385fdc95c/csrc/flash_attn/flash_api.cpp#L221C17-L221C29)\n1. [run_mha_fwd 多种实现](https://github.com/search?q=repo%3ADao-AILab%2Fflash-attention%20run_mha_fwd_&type=code)\n1. [run_flash_fwd](https://github.com/Dao-AILab/flash-attention/blob/5cdabc2809095b98c311283125c05d222500c8ff/csrc/flash_attn/src/flash_fwd_launch_template.h#L31)\n1. [compute_attn kernel 实现](https://github.com/Dao-AILab/flash-attention/blob/5cdabc2809095b98c311283125c05d222500c8ff/csrc/flash_attn/src/flash_fwd_kernel.h#L1043)\n1. [compute_attn_1rowblock 最关键实现函数](https://github.com/Dao-AILab/flash-attention/blob/5cdabc2809095b98c311283125c05d222500c8ff/csrc/flash_attn/src/flash_fwd_kernel.h#L28)\n1. [flash::gemm](https://github.com/Dao-AILab/flash-attention/blob/5cdabc2809095b98c311283125c05d222500c8ff/csrc/flash_attn/src/utils.h#L138)\n\n## links\n\n1. [软件架构参考 apex fmha](https://github.com/NVIDIA/apex/blob/master/apex/contrib/csrc/fmha/fmha_api.cpp)\n   - fmha: fast_multihead_attn\n   - We use FMHA code as a starting point, and apply two well-established techniques (tiling and recomputa-tion) to deal with long sequences and to save memory as mentioned; we can support much longer sequences (e.g., up to length 64K). We also support more head dimensions (16, 32, 64, 128) and broader GPU types (all Turing and Ampere GPUs at the time of writing).\n"},{"title":"cutlass","url":"/2024/01/26/cutlass/","content":"\n## note\n\n1. 主动设置`--cache-control=all`选项来保证在进行 kernel profiling 的时候清空历史 cache 数据，通过设置`--clock-control=base`选项来锁定 GPU 运行的频率，避免动态调频对性能测量的影响。\n1. `Swizzle`主要是在到 8192 以后，且 B 矩阵 transpose 的情况下必须要用（不用则效率腰斩，调出来 SWIZZLE 最优是 8）\n1. [cutlass 参考了 cub 的设计](https://github.com/NVIDIA/cub)\n1. cutlass 是对 gemm 实现过程的各种级别的抽象；手写搬数时需要进行大量计算\n   - block tile 矩阵乘法内积； thread tile 矩阵乘法外积\n   - global memory <-> shared memory <-> register <-> core\n   - double buffering: shared memory, register\n   - bank conflict: permute\n1. 排列（英语：Permutation）是将相异对象或符号根据确定的顺序重排。每个顺序都称作一个排列\n1. MMA = matrix multiply accumulate\n1. cutlass kernel 融合效果好\n1. 可以看一下 cutlass 发版历史，了解一下过程\n1. [v0.1.1/README.md](https://github.com/NVIDIA/cutlass/blob/v0.1.1/README.md)\n   - thread block tile, warp tile, thread tile\n1. CUTLASS is a collection of CUDA C++ template abstractions for implementing high-performance `matrix-matrix multiplication (GEMM)` and `related computations` at all levels and scales within CUDA\n   - `GEMM`\n   - `related computations` 主要指的是接在 GEMM 后面的 activation 或者一些 pointwise 计算，比如 bias，scales，这些计算可以和 GEMM 融合在一起，从而减少访存压力，由于这类计算一般发生在 GEMM 之后，我们管这类计算叫 epilogue(中文：结语)\n1. 大量使用模板以及各种各种手段，把尽可能多的事情放在编译期完成。\n1. 对用户暴露大量的实现策略，用户需要自己做出实现策略的选择。这是 CUTLASS 和 CUBLAS,CUDNN 的重要差别所在，CUBLAS 和 CUDNN 用户只需要描述计算问题，所有的策略选择在内部完成。这意味着从软件分层上来说，CUTLASS 可以是 CUBLAS 和 CUDNN 的 backend，换而言之，CUBLAS 和 CUDNN 可以是 CUTLASS 的用户\n1. c++模板，只用包含头文件\n1. CUDA Templates for Linear Algebra Subroutines and Solvers\n1. CUTLASS 和 cuBLAS 都是与 NVIDIA GPUs 相关的库，专门用于高效地执行线性代数运算\n1. 大矩阵 GEMM 运算 CuTlass 可以显著提速,是目前 GPU 上最快的 GEMM 库。\n1. 它允许开发者使用模板元编程自定义和优化矩阵乘法（GEMM）等线性代数运算，更加灵活。\n1. cuBLAS 提供了一个简单、标准的 BLAS 接口，易于使用，而 CUTLASS 提供了更多的定制性和灵活性，但需要更深入的理解和控制。\n1. [缩写的含义](https://github.com/NVIDIA/cutlass/blob/main/media/docs/functionality.md)\n   - N: Column Major Matrix\n   - T: Row Major matrix\n   - {N,T} x {N,T}: All combinations, i.e., NN, NT, TN, TT(blas 中约定 Normal(N)矩阵为列优先，T 表示 transpose，即对列优先的矩阵进行转置则为行优先)\n   - f: floating point\n   - s: signed int\n   - b: bit\n   - cf: complex float\n   - bf16: bfloat16\n   - tf32: tfloat32\n   - Simt: Use Simt CUDA Core MMA(使用 cuda core)\n   - TensorOp: Use Tensor Core MMA\n   - SpTensorOp: Use Sparse Tensor Core MMA\n   - WmmaTensorOp: Use WMMA abstraction to use Tensor Core MMA\n1. `One of CUTLASS's design patterns is to define gemm argument objects that are constructible in host code and passed to kernels by value.` These may include pointers, strides, scalars, and other arguments needed by Gemm and its components.The benefits of this pattern are `(1.) a structured, composable strategy for passing host-constructible arguments to kernels and (2.) minimized initialization overhead on kernel entry.`\n1. group gemm 和 batch gemm 区别： group 是 batch 更通用的形式，允许有不同尺寸的矩阵，batch 则需要多个矩阵尺寸相同\n1. 三种 tile\n   - thread block tile(例如：128x128x32)\n   - warp tile(例如：64x64x32)\n   - mma tile(例如：16x8x16)\n\n## cute (cuda tensor tools)\n\n1. CUTLASS 对 stream-K GEMM 算法的实现，该算法使用了 CuTe 的许多特性。\n1. One could summarize almost all CuTe use cases as follows:\n   - create Layouts,\n   - create Tensors with those Layouts\n   - invoke (either CuTe's, or custom) algorithms on those Tensors.\n1. [youtube 介绍](https://www.youtube.com/watch?v=PWWOGrLZtZg)\n1. `Layout maps from coordinate space(s) to an index space`. 从坐标(逻辑存储)映射到索引(物理存储);\n1. 需要十分明确 coordinate 和 index 概念， 1D coordinate 和 index 是不同的\n1. The map from a natural coordinate to an index is performed by taking the inner product of the natural coordinate with the Layout's Stride. coordinate 和 stride 内积来实现到 index 的映射;\n1. `cute::crd2idx(c, shape, stride)`来计算 c(coordinate)的 index\n1. 直接根据 coordinate 顺序计算 index， 不要用想象， 用公式； 例如 A = (2,2):(4,1); coordinate 顺序为 A(0) = A(0, 0) = 0; A(1) = A(1, 0) = 4x1 = 4; A(2) = A(0, 1) = 1; A(3) = A(1, 1) = 5; 所以 A(c)为 0 4 1 5\n1. Layout 对象调用,返回 index，layout(i)\n1. The core abstraction of CuTe are the hierarchically multidimensional layouts which can be composed with data arrays to represent tensors.\n1. tensor\n\n```sh\ntensor = ptr + layout ## 根据coodinate得到元素value\nlayout = shape + stride ## 根据coodinate得到index\n```\n1. ptr <--> coodinate <--> index, value\n1. `从右往左读` The map from an input coordinate to a natural coordinate is the application of a colexicographical order (reading right to left, instead of \"lexicographical,\" which reads left to right) within the Shape\n1. shape and stride 是 IntTupple, A Layout is a tuple of (Shape, Stride).\n1. CuTe 的 Tensor 类表示一个多维数组。数组的元素可以驻留在任何类型的内存中，包括`全局内存、共享内存和寄存器内存`。\n1. Tensor 的行为完全由它的两个组件决定，这两个组件对应于它的两个模板参数： Engine 和 Layout 。Engine 表示元素的一维数组。当用户对 Tensor 执行数组访问时， Tensor 使用其 Layout 从逻辑坐标映射到一维索引。然后， Tensor 使用其 Engine 将一维索引映射到对该元素的引用。\n1. tensor Ownership of the elements; Tensors can be owning or nonowning.\n   - Whether a Tensor is \"owning\" or \"nonowning\" depends entirely on its Engine.\n   - ViewEngine 和 ConstViewEngine 包装指向各种内存的指针。\n   - make_gmem_ptr(g) when g is a pointer to global memory, or make_smem_ptr(s) when s is a pointer to shared memory.\n1. print 会打印 tensor 存储空间所在的位置和 shape、stride 信息，而 print_tensor 除了以上信息还会打印 Tensor 中具体的每一个数值：\n1. We wrap each MMA's PTX instruction in an \"Operation\" struct.\n1. For each Operation struct, we define a \"Traits\" struct that defines all of the meta-information needed to use the Operation. 对于每个操作结构，我们定义一个“Traits”结构，它定义使用该操作所需的所有信息。\n1. mma ptx -> mma_operation -> mma_trait(封装各种信息) -> mma_atom -> tiled_mma -> thr_mma -> cute::gemm()\n1. copy ptx -> copy_operation -> copy_trait -> copy_atom -> tiled_copy -> thr_copy -> cute::copy()\n1. 通过 Tensor 和 Layout 抽象我们可以实现对计算矩阵的分块；基于 Copy 抽象，我们可以完成块状矩阵 A、B 数据从 global 内存到寄存器的加载；通过 MMA 抽象我们可以利用 Tensor Core 完成寄存器上小块矩阵的乘法运算；再次通过 Copy 抽象，我们可以将寄存器上的结果拷贝到 global 内存，完成完整的 GEMM 运算\n1. A has shape (M, K) and strides (1, ldA). Since A has stride 1 in the M mode, we say that A is `M major`. B has shape (N, K) and strides (1, ldB), so B is \"N-major.\" Similarly, C has shape (M, N) and strides (1, ldC), so C is \"M major.\"\n1. layout 两个核心概念\n   - shape: define the coordinate mappings\n   - stride: defines the index mappings, 在逻辑位置和物理（数据）做映射的时候每一个元素之间的差为 n\n1. thread layout and data layout\n1. mapping correct threads to correct values for you computation\n1. layout is a function\n1. [CUDA 高性能编程](https://www.zhihu.com/column/c_1696937812497235968)\n1. 计算机中的内存是一维的线性地址空间，而数学计算问题所要处理的空间经常是高维的。如 GEMM（General Matrix Multiplication）问题的数学计算体系是二维计算空间，Deep Learning 计算体系是三维以上的计算空间(batch, height, width, channel, etc.)。如何高效的表达高维计算空间，如何高效便捷的将计算所要求的高维空间映射到一维空间变得越来越重要。历史上对该问题的探究可以分为三个阶段：\n   - 第一阶段 BLAS 的 row/col-major + leading dimension 描述阶段；\n   - 第二阶段 Tensor 的 shape + stride 阶段；\n   - 第三阶段为 Hierarchy Tensor 阶段。(组合设计模式实现？)\n1. 引入有层次的描述（Layout）代数来表达计算空间和一维地址空间的映射问题。Layout 是一个数据排列的描述体系，其可以实现将`逻辑坐标`映射到`索引(物理)坐标`（offset 表示）\n1. Tensor 是数据的表达，其表达一个相对独立且有结构的数据体，而 Tensor 内的数据排布则由 Layout 来表达\n1. 二维矩阵的描述和一维类似，shape 表示其逻辑形状，stride 表示具体的某个元素和物理空间的映射时的间隔量。\n1. 逻辑空间到物理空间的映射通过点积来完成。如 shape: (3,4); stride: (4, 1); cood: (2, 2), 物理 offset = 2x4 + 2x1 = 10\n1. Layout 的本质是函数，其可以实现由一种坐标系统变换到一个表示偏移量的标量\n1. Layout 的本质是函数，函数的本质是集合\n1. ((2, 4), (3, 5))，同样我们可以得到其 stride: ((3, 6), (1, 24)) [link](https://zhuanlan.zhihu.com/p/662089556)\n   - (2, 4)表示行方向按 2 个一组，分 4 组\n   - (3, 5)表示列方向按 3 个一组，分 5 组\n   - (3, 6)表示行方向内层元素之间 stride 为 3，外层元素 stride 为 6\n   - (1, 24)表示列方向内存元素之间 stride 为 1，外层元素 stride 为 24\n   - 行为：2x4 = 8\n   - 列为：3x5 = 15\n   - (6, 4)表示：((6%2, 6/2), (4%3, 4/3)) = ((0, 3), (1, 1))， offset = 0*3 + 3*6 + 1*1 + 24*1 = 43\n   - ((1, 3), (2, 4))位置如何找？在行上，先看 3，后看 1，确定外围 tensor 是第 4 个，内层 tensor 是第 1 个；列同理\n1. Layout 描述了数据的排列和底层存储位置关系，但 Layout 并没有指定存储。Tensor 就是在 Layout 的基础上包含了存储，即`Tensor = Layout + storage`, 数据存储的具体表现上可以是指针表达的数据或则是栈上数据（GPU 上表现为寄存器）\n1. 深度学习框架中的 Tensor 更强调数据的表达实体，通过 Tensor 实体与实体之间的计算产生新的 Tensor 实体，即`多份数据实体`，cute 中的 Tensor 更多的是对 Tensor 进行分解和组合等操作，而这些操作多是对 Layout 的变换（只是逻辑层面的数据组织形式），`底层的数据实体一般不变更`。\n1. 深度学习框架中的 Tensor 是用来表达数据实体，cute 中的 Tensor 是偏向描述的实体。\n1. 使用 Tensor 语义和工具能够更形象化的表达我们的逻辑，方便我们的思考，而 CUDA 的优化思路和技巧并不会因为 Tensor 的引入而变简单或困难。Tensor 只是工具，可以方便我们的表达，至于深层次的优化思路那还是对经验的挑战。\n1. Tensor 表达虽然提供了很多方便，但也只限于表达的高效和便捷，如何对程序进行优化，Tensor 表达的引入并没有提供额外帮助，它仍然需要我们从别的途径来获得。即便如此，`表达和抽象依然无比重要`，那正如：伽罗瓦如果没有群这一表达工具，就难以解决多项式根的问题；杨振宁没有群这一工具就难以构思举世的杨-米尔斯理论。\n1. 抽象和工具让我们可以在更高的维度上思考。\n1. cute 提供了 MMA 能力来完成 D = A x B + C 的矩阵乘法运算，其针对指令封装，适配层，原子能力、块状 MMA、线程划分和执行进行了抽象，形成了 MMAOperation、MMA_Traits、MMA_Atom、TiledMMA、ThrMMA、cute::gemm 数据结构和函数，我们通过这些结构能够完成逻辑块状矩阵乘法的划分和执行。这些抽象通过软件分层设计使得各层次独立，我们不必关注底层细节，只需要从提供的模块中组合我们的逻辑即可，同时抽象的解偶设计，使得我们可以专注于顶层逻辑而降低对底层细节的要求。\n1. swizzle 抽象， Layout 的作用是给定坐标返回 offset，而 swizzle 的作用则是给定 offset 返回 bank conflict free 的 offset。 [link](https://zhuanlan.zhihu.com/p/671419093)\n   - shared memory 读写都按逻辑坐标，真实物理坐标都是 bank free 的(太强了)\n   - 先从一维映射到逻辑二维，再从逻辑二维找到 bank free 二维，最后再返回一维\n   - ibank = irow ^ icol 真是妙啊\n   - 在做全局内存到共享内存数据搬运时，`思考模型是逻辑空间，而执行时需要考虑存储空间以避免bank conflict`\n1. `layout和swizzle都是一种抽象，可以使用逻辑的方式思考，具体存储交给映射关系`\n1. `new layout language to describe coordinate and index bookkeepings; coordinate 是思考时用到的逻辑方式，具体的存储由 layout 映射关系来处理，可以将经历放在逻辑上`\n1. `Any problem in computer science can be solved by another layer of indirection` 计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决;例如虚拟地址\n   - layout 和 swizzle 都可以看做是中间层或者代理\n1. `loccal tile` 是 Tensor 中用户可以使用到的重要的函数，可以通过 tile 方法对 tensor 进行分块，通过 local_tile 可以实现从大的 tensor 中切取 tile 块，并且通过 coord 进行块的选取\n   - 用于数据分块\n1. `local partition`和 local tile 类似，现将大的 Tensor 按照 tile 大小进行分块，分块后每一块取出 coordinate 指定的元素拼拼成新的块\n   - 用于给线程分数据\n\n## [cutlass 优化手段](https://zhuanlan.zhihu.com/p/677616101)\n\n1. `bank conflict free的shared memory layout`\n1. `thread block swizzle`：这个优化对于中大型矩阵乘法比较明显，更改了发射 block 的顺序，以增加 locality，从而提高 l2cache 的命中率，实现上非常简单，核心代码就是一个取余操作，但有用多级流水线(software pipeline)：2 条可以不要 async.copy 这个指令(sm80 才有的)，大于 2 条流水就需要了，原理上没什么，和 CPU 的多级流水一个道理，主要是指令的应用。\n1. `predicate iterator`：这个是一个软件层组件写法的优化，叫 predicate 的原因是，这个 iterator 会返回一个布尔值，在 gpu 的指令里是一个 special register，用来表示这块内存是不是需要 load，这个在软件层会涉及一些优化手段，比较有趣的是会在 host 侧 precompute 了哪些下标需要 load，用位运算来 mask，计算开销(位运算在 gpu 里开销较小)和存储开销(一个 byte 可以存 8 个 mask 值)都很小。为什么需要让存储开销很小？因为在 gpu 架构里，register 是很贵的，一个 thread 只能使用 255 个 register，如果超出了就会存在 local memory 里，register 读取很快，一个 cycle 就可以完成，而 local memory 就会慢非常多，register 用超了会非常非常影响性能！\n1. `shared memory重排搬出`：mma 指令计算完成之后，结果是存在 register 里的，且 register 中存储的数据是不连续的(32bits 连续)，原因是由于 mma 指令造成的，我们知道 vectorize load/store 会提高访存带宽，所以我们可以在 shared memory 里重新排序，一并搬出。但并不是什么情况下重排都是正优化，因为重排还是会增加一次 shared memory store/load，比如在小 channel 的 conv2d 中，直接从 register 搬出到 global memory 性能会更好\n1. `cooperative fetching和vectorize load`：这两个是 GPU 的一些基本优化方法，即尽量用更大的 data type 来搬运，以及尽量让一个 warp 里的不同线程是连续地访存同一块内存地址，原理可以参考\n1. `tiling description`: 提供了实例化方法，来调整 block 计算量和 warp 计算量，\n\n## type\n\n1. `cutlass::half_t` 不支持 half\n1. `cutlass::bfloat16` Ampere ElementAccumulator 应该为 float, 看手册可以看到只支持 float32 的 accumulate [link](https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf)\n   - [ampere_gemm_operand_reduction_fusion](https://github.com/NVIDIA/cutlass/blob/main/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu#L68)\n\n## fusion\n\n1. 为了减少核启动(launch)和内存访问的开销\n1. It also removes kernel launch overhead 减少 kernel launch 开销\n1. 注意融合分两种：\n   - 简单融合：简单将两个 kernel 合并成一个，并没有减少从主存加载数据次数，只是减少了 kernel 调度的开销\n   - 高效融合：利用 shared memory 减少从主存加载数据次数\n1. 两个 gemm 融合， 关键在 k 维度， 假设 A@B@C， 分块时 A，B 矩阵滑动取数，结果放到 C 中，只能取一块，结果和 C 相乘的时候需要所有的分块矩阵\n\n## docs\n\n1. [cutlass_3x_design](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cutlass_3x_design.md)\n   - CUTLASS 3.0 将其接口层与硬件分离，将它们集中在 GEMM 算法的自然结构周围，不依赖于任何特定的 GPU generation\n   - CUTLASS 2.x 将 GEMM 操作的移动部分分解为一个层次结构，该层次结构密切反映了 GPU 架构的组织。然而，这种设计有时会导致耦合过于紧密，无法扩展到不适合同一架构层次结构的较新 GPU。\n1. [gemm_api_3x](https://github.com/NVIDIA/cutlass/blob/main/media/docs/gemm_api_3x.md)\n1. [gemm_api](https://github.com/NVIDIA/cutlass/blob/main/media/docs/gemm_api.md)\n   - 包含层级结构\n1. [代码组织](https://github.com/NVIDIA/cutlass/blob/main/media/docs/code_organization.md)\n1. [数据类型](https://github.com/NVIDIA/cutlass/blob/main/media/docs/fundamental_types.md)\n1. [CuTe layout](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/01_layout.md)\n1. [efficient_gemm.md](https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md)\n1. 最全流程图\n   ![gemm-structural-components](https://github.com/NVIDIA/cutlass/blob/main/media/images/gemm-structural-components.png)\n\n### build\n\n1. [quickstart](https://github.com/NVIDIA/cutlass/blob/main/media/docs/quickstart.md)\n1. [选择 arch](https://github.com/NVIDIA/cutlass/blob/main/media/docs/quickstart.md#building-for-multiple-architectures)\n1. 下不下来 google test, 修改 cmakelists.txt, 注释掉 gtest\n\n```sh\nsudo apt-get install libgtest-dev\ngit clone https://github.com/NVIDIA/cutlass.git\nexport CUDACXX=${CUDA_INSTALL_PATH}/bin/nvcc\nexport PATH=/usr/local/cuda/bin/:$PATH\nmkdir build && cd build\ncmake .. -DCUTLASS_NVCC_ARCHS=86\nmake cutlass_profiler -j12\n./tools/profiler/cutlass_profiler --kernels=sgemm --m=4352 --n=4096 --k=4096\n```\n\n## samples\n\n1. 00 basic gemm\n   - 创建 cutlass::gemm::device::Gemm，CutlassGemm::Arguments 实例 gemm_operator 和 args\n   - gemm_operator(args) launch kernel\n1. 01 cutlass utilities\n   - HostTensor contributes management for both host and device memory. HostTensor allocates host and device memory upon construction. Basic element-wise operations on host memory synchronize device memory automatically.\n   - cutlass::reference::host::Gemm\n   - cutlass::reference::device::TensorFillRandomGaussian()\n   - cutlass::reference::host::BlockFillSequential()\n   - cutlass::reference::host::TensorEquals()\n1. [02_dump_reg_shmem](https://github.com/NVIDIA/cutlass/tree/main/examples/02_dump_reg_shmem)\n\n   - Don't use cuda-gdb to debug any heavy templated CUDA code. We just insert printf to dump the information to debug. We have some utilities to help you to do it. [link](https://github.com/NVIDIA/cutlass/issues/372#issuecomment-987532556)\n\n1. 41_fused_multi_head_attention\n1. cute/tutorial/sgemm_nt_1.cu\n   - thread layout tA, tB 是读 AB 数据时线程 layout, tC 是计算 C 时的线程 layout\n   - tAsA 搬 block tile 数据用， tCsA 计算 block tile 用\n   - tCrC 每个线程算 8x8 数据， shared memory 中 A、B 数据也为 8x8，\n   - 使用 cuda core 计算，并没使用 tensor core\n   - tAsA 表示 copy 时 thr_copy A 时 Shared memory 上的 thread 数据\n   - tCsA, tCsB, tCrC 表示 mma 计算 thr_mma 用到的 A，B shared memory thread 数据和 C 寄存器数据; 提供线程号，则获得具体线程的数据划分能力，对给定的数据块进行划分，得到线程级的数据描述。\n   - 可以顺序赋值给 A, B, 类型改为 int, 来模仿各 tile layout\n   - axpby:alpha x plus beta y `y = alpha*x + beta*y`; where x and y are vectors of n elements and alpha and beta are scalars.\n   - axpy here is simply an abbreviation of alpha times x plus y.\n\n## debug\n\n### [cutlass device dump](https://github.com/NVIDIA/cutlass/blob/main/tools/util/include/cutlass/util/device_dump.h#L131)\n\n1. cutlass::debug::dump_shmem()\n   - 每行 128B，bank: 4B x 32 = 128B\n1. cutlass::debug::dump_fragment()\n\n### [cute tensor](https://github.com/NVIDIA/cutlass/blob/main/include/cute/tensor.hpp#L985)\n\n1. cute::print(tensor)\n2. cute::print_tensor(tensor); 可以打印各种维度\n3. cute::print_tensor_os()\n4. operator<<()\n5. print 会打印 tensor 存储空间所在的位置和 shape、stride 信息，而 print_tensor 除了以上信息还会打印 Tensor 中具体的每一个数值：\n\n### [cute layout](https://github.com/NVIDIA/cutlass/blob/main/include/cute/layout.hpp#L1694)\n\n1. cute::print(layout): `print(layout.shape()); print(\":\"); print(layout.stride());`\n1. cute::print_layout(layout)\n   - NOTE: 只能打印二维 layout; 代码会做检查 `CUTE_STATIC_ASSERT_V(rank(layout) == Int<2>{});` 可以 slice 成二维的再打印\n1. cute::print_layout(tensor.layout());\n1. operator<<(): ` os << shape(layout) << \":\" << stride(layout);`\n\n## links\n\n1. [CUTLASS 3 0 Next Generation Composable and Reusable GPU Linear Algebra Library - TVMCon2023](https://www.youtube.com/watch?v=QLdUML5MCfE&t=42s)\n1. [nvidia cutlass-linear-algebra-cuda](https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/)\n1. [Graphene: An IR for Optimized Tensor Computations on GPUs](https://dl.acm.org/doi/pdf/10.1145/3582016.3582018)\n1. [CUDA 高性能编程](https://www.zhihu.com/column/c_1696937812497235968)\n1. [cutlass 源码导读（1）——API 与设计理念](https://zhuanlan.zhihu.com/p/588953452)\n"},{"title":"OpenCL","url":"/2024/01/25/OpenCL/","content":"\n## base\n\n1. `OpenCL(Open Computing Language)`\n\n## links\n\n1. [opencl](https://www.khronos.org/opencl/)\n"},{"title":"Efficient-LLM","url":"/2024/01/14/Efficient-LLM/","content":"\n## 综述\n\n### [大模型推理综述 Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems](https://arxiv.org/abs/2312.15234)\n\n1. Context or initial or prefil 阶段: input token 是并行的，可以利用 flash attention 来加速\n1. Generation or incremental or decode 阶段: input token 是 1， Autoregressive(AR)，使用 paged attention 来实现\n   - 如果是 batch 为 1， 可以填充为 8 来利用 tensor core; 或者使用优化的 cuda core GEMV\n   - 解码阶段的过程主要由 GEMV（batch size=1）或 flat GEMM（batch size > 1）操作组成\n1. [中文解读](https://mp.weixin.qq.com/s/Uue0SxH6W_tI8K4Zb0igLQ)\n1. [黄浴中文翻译](https://zhuanlan.zhihu.com/p/675101140)\n1. `低延迟和高吞吐量`是 LLM 服务系统中的双重优化目标，代表了互补但往往相互冲突的目标，需要一种平衡的策略来优化单个任务的快速响应和指定时间内最大化处理任务量之间的权衡\n1. 除了注意计算之外，对于线性投影算子，最近还出现了用`通用矩阵向量积（GEMV，general matrix-vector product）代替GEMM的趋势`，更有效地处理小批量（即大小为 1）的情况。\n\n### [Full Stack Optimization of Transformer Inference: a Survey](https://arxiv.org/abs/2302.14017)\n\n- [黄浴中文翻译](https://zhuanlan.zhihu.com/p/677595064)\n\n### [Efficient Large Language Models: A Survey](https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey)\n\n### [Efficient Deep Learning Book](https://www.jiqizhixin.com/articles/2022-05-03-4)\n\n1. [https://efficientdlbook.com](https://efficientdlbook.com/#download)\n\n## sampling\n\n1. top-K\n1. top-P\n\n## 系统优化技术\n\n1. [nvidia mastering-llm-techniques-inference-optimization](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/)\n1. [大模型推理综述 Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems](https://arxiv.org/abs/2312.15234)\n   - 量化\n   - 并行计算\n   - 内存管理\n   - 请求调度\n   - kernel 优化\n1. [tensorrt llm 高级主题](https://github.com/NVIDIA/TensorRT-LLM/tree/main?tab=readme-ov-file#advanced-topics)\n   - 量化\n   - in-flight batching\n   - attention\n   - graph rewriting\n\n## 架构\n\n1. vllm\n2. tensorrt-llm\n\n## attention 优化\n\n1. flash attention -> flash attention v2 -> flash decoding\n1. xformers\n1. paged attention\n1. token attention\n1. cublas\n1. cutlass\n\n## links\n\n1. [Awesome-Efficient-LLM](https://github.com/horseee/Awesome-Efficient-LLM)\n1. [llm-perf-leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)\n   - lantency vs memory vs score\n\n## 指标\n\n1. [PPL: perplexity](https://huggingface.co/docs/transformers/perplexity)\n1. hugging face 显示指标\n   ![指标](https://i.ibb.co/THp5xHd/Ijz-O30-Ae96.png)\n1. [llm-perf-leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)\n   - lantency vs memory vs score\n   - e2e lantency\n   - e2e throughput\n   - prefil lantency\n   - decode throughput\n1. TTFT 表示 Time To First Token，TPOT 表示 Time Per Output Token。前者由初始相位处理速度驱动，而后者直接取决于增量解码中的每次迭代执行时间。\n"},{"title":"blas","url":"/2024/01/11/blas/","content":"\n## basic\n\n1. 先选定 TileC，然后沿着 k 轴移动小块进行累加求和的策略为 sliced-k，它对于 m、n 维度较大的场景（m n 分块所需要的 block 数目足以填充所有的 SM）比较有效。`对于k比较大，而m、n比较小的场景，由于m、n较小而我们根据C来划分thread block，这时需要的thread block数目比较小，当这个数目无法填充所有的SM时，则存在很多SM无任务，而有任务的SM需却又需要循环多次的问题`，这时候可以考虑将 k 轴拆分成多段，每一段都计算一个 TileC 结果，最后再通过额外的累加过程将多段的结果进行求和，这种模式的任务划分方法成为 split-k 方法。[link](https://zhuanlan.zhihu.com/p/667521327)\n1. [Stream-K: Work-Centric Parallel Decomposition for Dense Matrix-Matrix Multiplication on the GPU](https://arxiv.org/abs/2301.03598)\n   - 矩阵乘分块方法： sliced-k, split-k, stream-k\n1. [split-k](https://blog.csdn.net/u013701860/article/details/128674224)\n   - k 维度由多个 thread block 计算， 减少单 thread block 的延迟， 增加并行性\n   - 算完之后需要同步多个TB\n1. split-k和slice-k区别：\n   - Split-K 是一种 矩阵乘法加速技术，把 C = A x B 中 K 维度拆成多个部分 并行计算，最后结果归约（加和）。提高并行度，适用于 K 非常大、单个线程块处理不完时。\n   - slice-K（或叫 tiled-K）把矩阵乘法中的 K 维度切分成多个 tile，每次只加载一小段 K 来乘， 通常用于 节省寄存器和共享内存使用，适用于单个线程块处理较大 K 的情形。\n1. [efficient_gemm.md parallelized-reductions](https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#parallelized-reductions)\n   - split-k\n1. [各种 blas 实现](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3)\n1. BLAS 是 Basic Linear Algebra Subprograms 的缩写,即基本线性代数子程序,它是一系列线性代数运算函数的标准说明和接口规范。通常情况下,BLAS 函数库会为矩阵和向量运算提供高效、经过优化的实现,这些函数被广泛使用于科学计算、机器学习、数据分析等领域需要大量数值计算的场景中。常见的 BLAS 函数标准说明有:\n   - BLAS level 1:向量-向量运算\n   - BLAS level 2:矩阵-向量运算\n   - BLAS level 3:矩阵-矩阵运算\n1. `GEMM(General Matrix-Matrix Multiplication)` 是通用矩阵乘法,表示 `C = αAB + β*C`。它计算两个矩阵的乘积,是最常见的数值密集型运算之一。\n1. `GEMV(General Matrix-Vector Multiplication)`是矩阵向量乘法,表示`y = αAx + β*y`。它计算矩阵和向量的乘积得到一个向量。 αβ 是标量，A 是矩阵，xy 是向量\n1. [how-to-optimize-gemm](https://github.com/flame/how-to-optimize-gemm)\n1. [maxas/wiki/SGEMM](https://github.com/NervanaSystems/maxas/wiki/SGEMM)\n1. blas 库命名规则\n   ![name rule](https://i.ibb.co/8MWgXfn/atg4bmz-C9l.png)\n1. sgemm: [link](https://mp.weixin.qq.com/s/AukKpQU0XDH1YrDH_U8i1g)\n   - s: single\n   - ge: general\n   - mm:matirx matrix matrix product\n1. [矩阵乘法的内积与外积](https://zhuanlan.zhihu.com/p/350470257)\n   - cuda thread tile 时用了外积，可以缓存 shared memory 到 register\n1. [向量和矩阵的各种积](https://kl66.top/2019/05/30/%E5%90%91%E9%87%8F%E5%92%8C%E7%9F%A9%E9%98%B5%E7%9A%84%E5%90%84%E7%A7%8D%E7%A7%AF/)\n1. `lda` leading dimension of two-dimensional array used to store matrix A.\n1. `batch gemm` 拼接，在 thread block tile 时只计算对角线上的数据, 优化技术跟 gemm 一样，就是 thread block tile 方式不同\n\n## optimize\n\n1. [how-to-optimize-gemm](https://github.com/flame/how-to-optimize-gemm)\n   - 列优先\n   - Optimization2: 循环展开\n   - Optimization_1x4_4: 使用内联\n   - Optimization_1x4_5: 循环合并\n   - Optimization_1x4_6: 使用寄存器：register double test;\n   - Optimization_1x4_7: 减少 for 循环变量访存\n   - Optimization_4x4_5 MMult_4x4_5.c: 可以显示 block 计算过程, 可以看出分块之后有很多访存可以合并, A 和 B 分别可以减少到 1/4\n   - Optimization_4x4_10: 使用 vector 计算 block\n1. [how-to-optimize-gemm C++ 中文版](https://github.com/tpoisonooo/how-to-optimize-gemm/blob/master/README_ZH_CN.md)\n1. [cuda_sgemm 旷视](https://github.com/njuhope/cuda_sgemm/tree/master)\n1. (good)[CUDA GEMM 理论性能分析与 kernel 优化](https://zhuanlan.zhihu.com/p/441146275)\n1. [CUDA 矩阵乘法终极优化指南](https://zhuanlan.zhihu.com/p/410278370)\n   - 如果是 float4(128bit)，一个 warp 访问 16B，32x16B 得分 4 次做，那么会把一个 warp 分成 0-7/8-15/16-23/24-31 这么 4 组，只要组内没有 bank conflict 就行了。\n   - shared memory: float4 0 和 8 线程确实读的都是 0-3bank，但是 0-7 8-15 是分别的两组，这时候一个 warp 会分成 4 组 4 次读取，在不同组是不会冲突的，只需要组内没有冲突就可以了。我之前说了 shared memory 是 32 bank，每 bank 4B，你要一个 warp 访问 float4，数据量是 32\\*16B，是 shared memory 一次性能提供的数据量的 4 倍，那么必然要分 4 次获得，然后架构设计就是我说的分组情况。如果你一个线程访问 float2，那么就会是 0-15 16-31 两组\n   - shared memory 一次不冲突只能读 32x4B， 看一次读取的数据量，多于 32x4B 需要读取多次 n/(32x4)，只要在 n/(32x4) 次内读完就是最高效的。\n1. 使用 float4 类型访存，用向量化的 LDG.128 和 STG.128 指令一次读 4 个元素，以减少指令数\n1. block tile 时，第一个矩阵在 shared memory 上要按列存储，为了连续存储，好利用 LDG.128 和 STG.128 指令\n\n## [blas](https://www.netlib.org/blas/)\n\n1. 名词出自 blas 库，例如\n   - SGEMM: single matrix matrix multiply;\n   - DGEMM: double matrix matrix multiply\n   - SGEMV: single matrix vector multiply\n\n## [NVIDIA/cutlass](https://github.com/NVIDIA/cutlass)\n\n## [cuBLAS](https://developer.nvidia.com/cublas)\n\n1. CUDA Basic Linear Algebra Subprograms\n1. cuBLAS（CUDA Basic Linear Algebra Subprograms）是由 NVIDIA 提供的官方库，它是经典 BLAS（Basic Linear Algebra Subprograms）库的 CUDA 实现。\n1. 列优先\n1. cublasSgemmBatched 支持 batch 运算\n\n## [OpenBLAS](https://github.com/OpenMathLib/OpenBLAS)\n\n1. [intel cblas interface](https://www.intel.com/content/www/us/en/docs/onemkl/developer-reference-c/2023-2/cblas-gemm-001.html#GUID-97718E5C-6E0A-44F0-B2B1-A551F0F164B2)\n1. [apple 接口文档](https://developer.apple.com/documentation/accelerate/1513264-cblas_sgemm?language=objc)\n1. `sudo apt-get install libopenblas-dev`\n1. [usage](https://github.com/OpenMathLib/OpenBLAS?tab=readme-ov-file#usage)\n\n```shell\nexport OPENBLAS_NUM_THREADS=4\nexport GOTO_NUM_THREADS=4\nexport OMP_NUM_THREADS=4\n```\n\n1. If you compile this library with USE_OPENMP=1, you should set the OMP_NUM_THREADS environment variable; OpenBLAS ignores OPENBLAS_NUM_THREADS and GOTO_NUM_THREADS when compiled with USE_OPENMP=1.\n1. For a general introduction to the BLAS routines, please refer to the extensive documentation of their reference implementation hosted at netlib: https://www.netlib.org/blas. On that site you will likewise find documentation for the reference implementation of the higher-level library `LAPACK - the Linear Algebra Package` that comes included with OpenBLAS\n1. [performance-and-runtime-behavior-related-build-options](https://github.com/OpenMathLib/OpenBLAS/blob/develop/docs/distributing.md#performance-and-runtime-behavior-related-build-options)\n1. openblas.pc 中可以看到 `USE_OPENMP=0 MAX_THREADS=24`\n1. [论文](https://arxiv.org/pdf/1609.00076.pdf)\n\n## [blis](https://github.com/flame/blis)\n\n## [hipBLAS](https://github.com/ROCm/hipBLAS)\n\n## [Intel oneMKL](https://github.com/oneapi-src/oneMKL)\n\n1. oneAPI Math Kernel Library (oneMKL)\n1. [统一多种 blas](https://github.com/oneapi-src/oneMKL?tab=readme-ov-file#supported-configurations)\n\n## [CLBlast](https://github.com/CNugteren/CLBlast)\n"},{"title":"llama.cpp","url":"/2024/01/10/llama-cpp/","content":"\n## 语言模型质量评测\n\n1. [perplexity-measuring-model-quality](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#perplexity-measuring-model-quality)\n1. `./perplexity -m models/7B/ggml-model-q4_0.gguf -f wiki.test.raw`\n   - 使用 openblas 版本\n   - `OMP_NUM_THREADS=16 OPENBLAS_NUM_THREADS=16 ./build/bin/perplexity -m ../llama/llama-2-7b/ggml-model-q4_0.gguf -f wikitext-2-raw/wiki.test.raw`\n1. 7b q4_0 result:\n\n```shell\nllama_print_timings: prompt eval time = 11159747.56 ms / 335360 tokens (   33.28 ms per token,    30.05 tokens per second)\nFinal estimate: PPL = 5.9621 +/- 0.03348\n```\n\n1. 7b fp16 result:\n\n```shell\nllama_print_timings: prompt eval time = 9850067.93 ms / 335360 tokens (   29.37 ms per token,    34.05 tokens per second)\nFinal estimate: PPL = 5.7962 +/- 0.03235\n```\n\n## run on cpu\n\n1. build\n\n```sh\ngit clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\nmkdir build\ncd build\ncmake ..\ncmake --build . --config Release -j16\n```\n\n2. convert llama2 model\n\n```sh\n## llama repo下下载了7b模型\npython convert.py ../llama/llama-2-7b/\n\n## q4_0量化\n./build/bin/quantize ../llama/llama-2-7b/ggml-model-f16.gguf ../llama/llama-2-7b/ggml-model-q4_0.gguf q4_0\n```\n\n3. run\n\n```sh\n./build/bin/main -m ../llama/llama-2-7b/ggml-model-f16.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 40 -e\n```\n\n4.result\n\n```shell\nllama_print_timings:        load time =    2123.21 ms\nllama_print_timings:      sample time =      22.01 ms /    40 runs   (    0.55 ms per token,  1817.19 tokens per second)\nllama_print_timings: prompt eval time =     820.50 ms /    19 tokens (   43.18 ms per token,    23.16 tokens per second)\nllama_print_timings:        eval time =   15164.51 ms /    39 runs   (  388.83 ms per token,     2.57 tokens per second)\nllama_print_timings:       total time =   16023.55 ms\n```\n\n## run on cpu with mpi on\n\n1. build\n\n```shell\nsudo apt install libopenmpi-dev\ncmake -S . -B build -DLLAMA_MPI=ON\ncd build\ncmake --build . --config Release -j16\n```\n\n2. result\n\n```shell\nllama_print_timings:        load time =    2092.53 ms\nllama_print_timings:      sample time =      20.81 ms /    40 runs   (    0.52 ms per token,  1922.15 tokens per second)\nllama_print_timings: prompt eval time =     668.81 ms /    19 tokens (   35.20 ms per token,    28.41 tokens per second)\nllama_print_timings:        eval time =   10150.64 ms /    39 runs   (  260.27 ms per token,     3.84 tokens per second)\nllama_print_timings:       total time =   10855.62 ms\n\n## run with mulit core\nmpirun -np 8 ./build/bin/main -m ../llama/llama-2-7b/ggml-model-f16.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 40 -e\n```\n\n## openBlas\n\n1. build\n\n```shell\ncd build\ncmake .. -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\ncmake --build . --config Release -j16\n```\n\n1. result\n\n```sh\n./build/bin/main -m ../llama/llama-2-7b/ggml-model-f16.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 40 -e\nllama_print_timings:        load time =    2008.93 ms\nllama_print_timings:      sample time =      50.88 ms /   100 runs   (    0.51 ms per token,  1965.37 tokens per second)\nllama_print_timings: prompt eval time =     647.12 ms /    19 tokens (   34.06 ms per token,    29.36 tokens per second)\nllama_print_timings:        eval time =   25045.01 ms /    99 runs   (  252.98 ms per token,     3.95 tokens per second)\nllama_print_timings:       total time =   25779.49 ms\n\n# OMP_NUM_THREADS=32 OPENBLAS_NUM_THREADS=32 都设置比较快\nOMP_NUM_THREADS=32 OPENBLAS_NUM_THREADS=32 ./build/bin/main -m ../llama/llama-2-7b/ggml-model-f16.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 40 -e\nllama_print_timings:        load time =    1977.25 ms\nllama_print_timings:      sample time =      19.27 ms /    40 runs   (    0.48 ms per token,  2076.20 tokens per second)\nllama_print_timings: prompt eval time =     622.66 ms /    19 tokens (   32.77 ms per token,    30.51 tokens per second)\nllama_print_timings:        eval time =    6629.75 ms /    39 runs   (  169.99 ms per token,     5.88 tokens per second)\nllama_print_timings:       total time =    7284.48 ms\n```\n\n## cublas\n\n1. build\n\n```shell\nexport CUDA_VISIBLE_DEVICES=0,1\nexport PATH=/usr/local/cuda/bin/:${PATH}\nmkdir build\ncd build\ncmake .. -DLLAMA_CUBLAS=ON\ncmake --build . --config Release -j16\n```\n\n1. result\n\n```shell\nllama_print_timings:        load time =    2225.07 ms\nllama_print_timings:      sample time =      21.19 ms /    40 runs   (    0.53 ms per token,  1887.33 tokens per second)\nllama_print_timings: prompt eval time =     814.74 ms /    19 tokens (   42.88 ms per token,    23.32 tokens per second)\nllama_print_timings:        eval time =   10324.64 ms /    39 runs   (  264.73 ms per token,     3.78 tokens per second)\nllama_print_timings:       total time =   11176.41 ms\n```\n\n1. q4_0 result\n\n```shell\nllama_print_timings:        load time =    1097.19 ms\nllama_print_timings:      sample time =      22.74 ms /    40 runs   (    0.57 ms per token,  1759.25 tokens per second)\nllama_print_timings: prompt eval time =     780.84 ms /    19 tokens (   41.10 ms per token,    24.33 tokens per second)\nllama_print_timings:        eval time =    6715.37 ms /    39 runs   (  172.19 ms per token,     5.81 tokens per second)\nllama_print_timings:       total time =    7536.04 ms\n```\n"},{"title":"model-compression","url":"/2024/01/10/model-compression/","content":"\n## quantization 量化\n\n## pruning 剪枝\n\n### 稀疏化\n\n1. [scipy csr_matrix 和 csc_matrix 函数详解](https://blog.csdn.net/u013010889/article/details/53305595)\n\n## knowledge distillation 蒸馏\n\n## links\n\n1. [4-popular-model-compression-techniques-explained](https://xailient.com/blog/4-popular-model-compression-techniques-explained/)\n"},{"title":"huggingface","url":"/2024/01/04/huggingface/","content":"\n## [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines)\n\n1. The pipelines are a great and easy way to use models for inference.\n1. llama2\n\n```python\n# Use a pipeline as a high-level helper\nfrom transformers import pipeline\npipe = pipeline(\"text-generation\", model=\"./Llama-2-7b-hf\")\npipe(\"how are you\")\n# 查看帮助\nhelp(pipeline)\nhelp(pipe)\n```\n\n## 查看模型信息\n\n1. [基础信息](https://github.com/saratbhargava/ai-blog-resources/blob/main/LLM/Llama_2_param_count.ipynb)\n\n```python\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"./Llama-2-7b-hf\")\nprint(model)\nfrom prettytable import PrettyTable\n\ntable = PrettyTable(['Name', 'Shape', 'Param'])\n\nfor name, param in model.named_parameters():\n    param_count = param.numel()\n    table.add_row([name, param.shape, param_count])\nprint(table)\nnum_parameters = sum(p.numel() for p in model.parameters())\nprint(num_parameters)\n```\n\n## [models](https://huggingface.co/models)\n\n1. repo 包含\n   - config.json 每个架构一个 config.json [llama config](https://huggingface.co/docs/transformers/main/model_doc/llama2#transformers.LlamaConfig)\n\n## [模型文件类型](https://www.zhihu.com/question/620641385/answer/3230090109)\n\n1. 支持 bin 或 safetensors 文件\n1. safetensors 是谷歌开发的一种 TensorFlow Lite 模型文件格式，用于在移动设备上运行模型\n1. bin 文件自存储模型的参数，不包含\n1. pytorch 两种方式\n   - 保存整个模型：保存整个模型的结构（代码）、参数 `torch.save(model, 'model.pth')`\n   - 保存模型参数：仅保存模型的参数，而不保存模型的结构（代码）。`torch.save(model.state_dict(), 'model_params.pth'`\n1. 有些模型保存未 gguf 格式，需要专门推理引擎才能使用\n1. [gguf doc](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)\n1. gguf：It is a successor file format to GGML, GGMF and GGJT, and is designed to be unambiguous by containing all the information needed to load a model. It is also designed to be extensible, so that new features can be added to GGML without breaking compatibility with older models.\n1. The .bin files that are used by llama.cpp allow users to easily share models in a single file. Except they had one big problem: lack of flexibility. You could not add additional information about the model.\n1. [hugging face models to gguf](https://github.com/ggerganov/llama.cpp/discussions/2948)\n\n## links\n\n1. [transformers 中文文档](https://huggingface.co/docs/transformers/v4.36.1/zh/index)\n1. [blog](https://huggingface.co/blog/zh/llama2)\n1. [Llama 2 来袭 - 在 Hugging Face 上玩转它](https://huggingface.co/blog/zh/llama2)\n"},{"title":"tensorrt-llm","url":"/2024/01/01/tensorrt-llm/","content":"\n## base\n\n1. `A TensorRT Toolbox for Optimized Large Language Model Inference`\n1. tensorrt-llm 使用了很多 tensorrt 插件，插件里调用 kernel [link](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture.md#plugins)\n1. 大模型优化方法：tensorrt + plugins, 耗时的算子用专有库进行替换，如矩阵乘用 cutlass\n1. TensorRT-LLM wraps TensorRT’s deep learning compiler—which includes optimized kernels from FasterTransformer, pre- and post-processing, and multi-GPU and multi-node communication—in a simple open-source Python API for defining, optimizing, and executing LLMs for inference in production.\n1. [generation](https://nvidia.github.io/TensorRT-LLM/gpt_runtime.html#generation)\n1. [gptmanager](https://nvidia.github.io/TensorRT-LLM/batch_manager.html#gptmanager-design)\n1. TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines.\n1. [doc](https://nvidia.github.io/TensorRT-LLM/)\n1. TensorRT-LLM includes an API to implement Python and C++ runtimes. The role of the runtime components is to load the TensorRT engines and drive their execution. Typically, for an auto-regressive model like GPT, the runtime is in charge of loading the engine that implements `both the processing of the input sequence as well as the body of the generation loop`.\n1. TensorRT-LLM 的运行时需要管理：\n   - 模型加载及执行\n   - 对于 AR 模型，两阶段(prompt 和 decoding)都需要处理\n1. [decode only 执行流程](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/gpt_runtime.md)\n1. [attention 流程](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/gpt_attention.md)\n   - As a quick reminder, the multihead attention is the sequence of a batched matmul, a softmax and another batched matmul described in the Attention Is All You Need article. attention 计算是 batch 的\n   - the current implementation supports two input modes: Padded and packed (non-padded). As the packed mode is always more memory-efficient and faster than the padded mode, support for padded mode may be removed in the future. 打包模式更节省内存并且更快\n1. 重算可以不用考虑 kv cache, uniad 只考虑 5 帧：前 4 帧运行时 padding 到 5 帧，重算 attention; 第六帧开始，recompute sliding window; 只考虑窗口的 5 帧，位置编码一直都是 0-5（位置改变，kv cache 需要重新计算)。 普通的 sliding windows 都是\n\n## [best practice](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/perf_best_practices.md)\n\n## memory\n\n1. [显存使用](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/memory.md)\n\n## key-features\n\n1. [key-features 可学习如何优化](https://github.com/NVIDIA/TensorRT-LLM/tree/main?tab=readme-ov-file#key-features)\n\n### in-flight batching\n\n### paged attention\n\n### MHA GQA MQA\n\n### [Quantization](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/precision.md)\n\n### Graph Rewriting\n\n1. TensorRT-LLM 在将 LLM 模型编译为 TensorRT Engines 时会对神经网络进行优化，提升执行效率。\n\n## deubg\n\n1. [TLLM_LOG_LEVEL=TRACE](https://github.com/NVIDIA/TensorRT-LLM/blob/6cc5e177ff2fb60b1aab3b03fa0534b5181cf0f1/cpp/tensorrt_llm/common/logger.cpp#L32)\n1. 打开 trace， 跟踪代码执行\n\n## docker\n\n1. [docker hubs](https://hub.docker.com/search?q=tensorrt_llm)\n\n## install\n\n1. 使用 docker [baseten/tensorrt_llm-release](https://hub.docker.com/r/baseten/tensorrt_llm-release)\n1. [fetch-the-sources](https://github.com/NVIDIA/TensorRT-LLM/blob/v0.7.1/docs/source/installation.md#fetch-the-sources) in docker\n1. [build-tensorrt-llm](https://github.com/NVIDIA/TensorRT-LLM/blob/v0.7.1/docs/source/installation.md#build-tensorrt-llm)\n1. 可能需要先卸载 `pip uninstall tensorrt_llm`， 重新安装\n\n## 数据集\n\n### [ccdv/cnn_dailymail](https://huggingface.co/datasets/ccdv/cnn_dailymail)\n\n1. gitee 镜像版本不太好使\n1. 下载之后传到服务器\n1. [clone txt](https://github.com/abisee/cnn-dailymail/tree/master/url_lists)\n1. 修改 summarize.py；从本地 load 数据集\n1. `dataset = load_dataset(\"/mnt/data-2/home/xiyang.jia/TensorRT-LLM/examples/bloom/cnn_dailymail/cnn_dailymail.py\", \"3.0.0\")` 从本地加载数据集\n\n## vscode setting\n\n1. env settings\n\n```json\n\"env\": {\n    \"PYDEVD_WARN_EVALUATION_TIMEOUT\": \"500\",\n    \"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\": \"python\",\n    \"OPAL_PREFIX\": \"/opt/hpcx/ompi\"\n},\n```\n\n1. launch.json\n\n```json\n{\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"name\": \"Python: tensorrt\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      // \"program\": \"${workspaceFolder}/examples/summarize.py\",\n      // \"cwd\": \"${workspaceFolder}/examples/bloom/\",\n      // \"program\": \"../summarize.py\",\n      // \"args\": [\n      //     \"--test_trt_llm\",\n      //     \"--hf_model_dir\",\n      //     \"./bloom/560M/\",\n      //     \"--data_type\",\n      //     \"fp16\",\n      //     \"--engine_dir\",\n      //     \"./bloom/560M/trt_engines/fp16/1-gpu/\"\n      // ],\n      // run llama test\n      \"cwd\": \"${workspaceFolder}/examples/llama/\",\n      \"program\": \"../run.py\",\n      \"args\": [\n        \"--max_output_len=50\",\n        \"--tokenizer_dir\",\n        \"/mnt/data-2/home/xiyang.jia/Llama-2-7b-hf\",\n        \"--engine_dir=./tmp/llama/7B/trt_engines/fp16/1-gpu/\"\n      ],\n      // build llama test\n      // \"cwd\": \"${workspaceFolder}/examples/llama/\",\n      // \"program\": \"./build.py\",\n      // \"args\": [\n      //     \"--model_dir\",\n      //     \"/mnt/data-2/home/xiyang.jia/Llama-2-7b-hf\",\n      //     \"--dtype\",\n      //     \"float16\",\n      //     \"--remove_input_padding\",\n      //     \"--use_gpt_attention_plugin\",\n      //     \"float16\",\n      //     \"--enable_context_fmha\",\n      //     \"--use_gemm_plugin\",\n      //     \"float16\",\n      //     \"--output_dir\",\n      //     \"./tmp/llama/7B/trt_engines/fp16/test/\"\n      // ],\n      \"env\": {\n        \"PYDEVD_WARN_EVALUATION_TIMEOUT\": \"500\",\n        \"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\": \"python\",\n        \"OPAL_PREFIX\": \"/opt/hpcx/ompi\",\n        \"CUDA_VISIBLE_DEVICES\": \"6,7\",\n        \"TLLM_LOG_LEVEL\": \"DEBUG\"\n      },\n      \"console\": \"integratedTerminal\",\n      \"justMyCode\": false\n    }\n  ]\n}\n```\n\n## examples\n\n1. `build.py` 每个 example 独占，用于构建模型和编译成 engine 模型; 需要模型配置：权重等\n1. In addition, there are two shared files in the parent folder examples for inference and evaluation:\n   - `run.py` to run the inference on an input text; 运行推理\n   - `summarize.py` to summarize the articles in the cnn_dailymail dataset. 总结故事\n1. 需要 hf model; 例:llama-7B-hf; hf 是 Huggingface 对原始 llama-7B 模型的打包版本,\n1. TensorRT-LLM LLaMA builds TensorRT engine(s) from HF checkpoint. If no checkpoint directory is specified, TensorRT-LLM will build engine(s) with dummy weights. 如果不设置 `--model_dir`, 使用随机权重\n1. llama sample 运行时需要 config.json, `--tokenizer_dir`指定\n1. [gitee.com/hf-models/Llama-2-7b-hf](https://gitee.com/hf-models/Llama-2-7b-hf) gitee 下载要快很多 `git lfs clone https://gitee.com/hf-models/Llama-2-7b-hf`\n1. run 的时候加参数`--temperature=0.6 --top_k=10`可生成不一样内容\n1. `/usr/local/tensorrt/bin/trtexec --loadEngine=tmp/llama/7B/trt_engines/fp16/1-gpu/llama_float16_tp1_rank0.engine`会出错，未解决\n   - `export LD_LIBRARY_PATH=/usr/local/tensorrt/lib/:$LD_LIBRARY_PATH`\n1. ` Error[1]: [stdArchiveReader.cpp::stdArchiveReaderInitCommon::47] Error Code 1: Serialization (Serialization assertion stdVersionRead == kSERIALIZATION_VERSION failed.Version tag does not match. Note: Current Version: 226, Serialized Engine Version: 228)` tensorrt 版本有问题\n1. [tensorrt llm llama](https://github.com/hpcaitech/SwiftInfer?tab=readme-ov-file)\n"},{"title":"gpt","url":"/2023/12/30/gpt/","content":"\n## links\n\n1. [openai/gpt-2 开源代码](https://github.com/openai/gpt-2)\n"},{"title":"jupyter","url":"/2023/12/26/jupyter/","content":"\n## base\n\n```sh\nconda create --name d2l python=3.9 -y\nconda activate d2l\npip install torch==1.12.0 torchvision==0.13.0 d2l==0.17.6 RICE\njupyter notebook # 映射端口号 注意不用使用机器自带jupyter\njupyter notebook --port 5900 # 映射端口号, 可能不好使\n```\n\n## shortkeys\n\n1. `S-Enter` 运行并到下一 cell\n1. `C-Enter` 运行 code\n"},{"title":"llama2","url":"/2023/12/24/llama2-c/","content":"\n## base\n\n### llama2 参数量计算\n\n1. 见 transformer 参数计算\n1. [LLAMA2 的参数计算](https://zhuanlan.zhihu.com/p/649125936)\n1. [Mastering Llama Math (Part-1): A Step-by-Step Guide to Counting Parameters in Llama-2](https://medium.com/@saratbhargava/mastering-llama-math-part-1-a-step-by-step-guide-to-counting-parameters-in-llama-2-b3d73bc3ae31)\n1. `窗口长度, 上下文长度(Context Length)`指的是模型在做预测时能够看到的上下文单词的数量。具体来说,在 transformer 等注意力机制的模型中,输入是一个定长的窗口,窗口中的每个词会通过自注意力机制关联到窗口中的其他词。这个窗口的长度就是上下文长度。\n1. `词向量`：embeding 层的输出就是词向量\n1. 推理时间复杂度：整体级别是 l*n*d^2+l*d*n^2，其中：d 是词向量维度，n 是窗口或序列长度，l：层数\n   - 无论是窗口长度还是词向量维度，都会让推理时间呈平方指数级上升。\n   - 层数对推理时间的影响是线性的。\n   - 而词表大小对推理时间基本没有影响。\n\n### 长文本(输入输出更长 token)\n\n1. [大模型长文本建模的难点与方案](https://zhuanlan.zhihu.com/p/657210829)\n1. [卷完参数后，大模型公司又盯上了“长文本”？](https://36kr.com/p/2470939687950470)\n\n## [llama2.c](https://github.com/karpathy/llama2.c)\n\n1. [Intro to Large Language Models](https://www.youtube.com/watch?v=zjkBMFhNj_g) ka\n1. [python 推理代码](https://github.com/facebookresearch/llama/blob/main/llama/model.py)\n1. [Coding LLaMA 2 from scratch in PyTorch - KV Cache, Grouped Query Attention, Rotary PE, RMSNorm](https://www.youtube.com/watch?v=oM4VmoabDAI)\n1. build and run\n\n```sh\ngit clone https://github.com/karpathy/llama2.c.git\ncd llama2.c\nmake run; make rundebug\nwget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin\n./run stories15M.bin\nmake runfast  # 加速运行， 80 tok/s => 180 tok/s\nmake runomp && OMP_NUM_THREADS=4 ./run out/model.bin # 多核加速， from 80 tok/s => 320 tok/s 主要加速matmul\n```\n\n### 代码理解\n\n1. 固定长 n 的 kv cache, 并且最大支持 n 的序列，使用 for 循环计算 attention，没有使用矩阵乘， 不用移动 kv cache; 矩阵乘时需要 kv cache 是矩阵，如果 kv cache 矩阵 padding, 是固定长, 需要移动 cache\n1. `Tokenizer, Transformer, Sampler` 三大部分\n1. tokenizer 也是一个模型，需要训练\n1. Tokenizer 的主要作用是将自然语言文本转换为机器学习模型可以理解的格式。这通常意味着将文本拆分成词汇、子词或字符单元（即 tokens），然后将这些 tokens 转换为数字 ID。这些 ID 对应于模型的词汇表中的索引。\n1. 当有 prompt 时，有预热过程，从第一个 prompt_tokens 开始推理，if we are still processing the input prompt, force the next prompt token， otherwise sample the next token from the logits\n1. Sampler 的主要作用是根据模型输出的 logits（未归一化的概率对数）来决定下一个生成的 token\n1. Temperature Scaling：通过温度参数调整 logits。温度较低（<1）会使 softmax 输出分布更加尖锐（更确定），温度较高（>1）则使分布更平滑（更随机），由 softmax 前的 scale 决定\n1. 采样策略对生成文本的质量和多样性有显著影响。例如，argmax 采样可能导致非常重复和可预测的文本，而合适的随机采样可以增加多样性和创造性，同时保持文本的连贯性和可读性。通过合理配置 temperature 和 topp 参数，可以在随机性和确定性之间找到平衡，生成符合预期的文本。\n1. top-p 采样：\n   - 模型输出的 logits（未归一化的概率对数），转换为 softmax 概率\n   - 创建一个结构体数组，每个元素包含 token 的索引和对应的概率。\n   - 概率小于 cutoff 过滤掉\n   - 排序\n   - 计算累计概率， 累计概率超过 topp 后不选择\n   - 随机数\\*累计概率；找到这个随机数对应的累积概率区间内的 token。\n   - 返回所选 token 的索引\n   - 控制生成质量：通过调整 p 的值，可以控制生成文本的随机性和确定性。较低的 p 值会导致更确定性的输出（更少的 token 可供选择），而较高的 p 值会增加输出的多样性。\n   - 避免不合理的 token：Top-p 采样可以有效地避免选择那些极不可能的 token，这对于生成更加连贯和自然的文本至关重要。\n1. 前馈神经网络（Feed-Forward Neural Network, `FFN`）\n1. `forward` 跟 all you need is attention 论文描述的结构非常像\n1. 相对位置编码（RoPE）：模型利用 RoPE 来给序列中的每个 token 引入位置信息。这种方式与原始 Transformer 中的绝对位置编码不同。\n1. 对于每个输入 token， 生成 q, k, v, q 用来和之前的 k 做 attention 得到 scores, scores 和 k 做加权得到多头输出; 缓存 k, v 用于之后的推理\n1. q 和之前所有 k 做 attension, 结果在和 v 做加权\n1. [grouped-query-attention](https://paperswithcode.com/method/grouped-query-attention)\n1. Config 中通过 n_heads 和 n_kv_heads 可以看出是 multi-head, group-query, multi-query； 相同时（不为 1）是 multi-head, 都为 1 时是 multi-query; n_kv_heads < n_heads 时是 group_query\n1. MQA，全称 Multi Query Attention, 而 GQA 则是前段时间 Google 提出的 MQA 变种，全称 Group-Query Attention。MHA（Multi-head Attention）是标准的多头注意力机制，h 个 Query、Key 和 Value 矩阵。\n1. 推理的过程是一个自回归的过程，也就是说前 i 次的 token 会作为第 i+1 次的预测数据送入模型，拿到第 i+1 次的推理 token。\n1. embedding 还能起到降维的作用，将 one-hot 的[s,vocab_size]大小变成了[s,d]。\n1. 在大多数基于 Transformer 的模型中，embedding 层输出的词向量维度通常与 Transformer 的隐藏层(attention layer)维度（也称为 Transformer dimension）相匹配。\n1. Config 中的 dim 和 hidden_dim:\n   - dim（Transformer Dimension）: 这个参数指的是 Transformer 模型中主要的隐藏层维度。在标准的 Transformer 模型中，这包括自注意力层（Self-Attention Layer）的输出维度和 embedding 层的维度。因此，dim 通常与 embedding 层输出的词向量的大小相匹配。\n   - hidden_dim（Feed-Forward Network Dimension）: 这个参数指的是 Transformer 模型中前馈网络（Feed-Forward Network, FFN）层的内部隐藏层的维度。FFN 是 Transformer 每个注意力层之后的一个子层，它的维度通常与主要隐藏层维度不同。这个维度通常更大，用于在模型中引入额外的非线性。\n1. Config 中的窗口长度等于 seq_len， 词向量维度等于 dim\n\n### embedding 和 tokenizer 的区别\n\n1. Tokenizer 是第一步：它将原始文本转换为一系列的 token 索引。Embedding 是第二步：利用这些索引在 embedding 层中查找或生成每个 token 的向量表示。互补关系：Tokenizer 和 embedding 层一起工作，将自然语言文本转换为机器学习模型可以有效处理的数值形式。\n1. tokenizer 负责将文本转换为一系列的 token，而 embedding 则负责将这些 token 转换为机器学习模型可以理解的语义向量。\n1. 通过 token_embedding_table 查找 token 对应的的向量\n\n### 模型基本结构\n\n1. 层的堆叠：模型由多个相同的层堆叠而成，每层包含两个主要子模块：多头自注意力（Multi-Head Self-Attention）和前馈神经网络（Feed-Forward Neural Network, FFN）。\n\n## others\n\n1. openmp 加速会获得较大加速比, 编译选项加`-fopenmp -march=native`\n\n```c++\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n```\n\n## links\n\n1. [chatgpt 解释](https://chat.openai.com/c/4e520557-e827-40c5-9357-db76b5a5e6ba)\n1. [一文看懂 llama2(原理,模型,训练)](https://zhuanlan.zhihu.com/p/651248009)\n1. [Embedding 的作用](https://zhuanlan.zhihu.com/p/83814532)\n1. [为什么现在的 LLM 都是 Decoder-only 的架构？](https://kexue.fm/archives/9529)\n1. [gguf doc](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)\n1. [pytorch llama](https://github.com/mit-han-lab/streaming-llm)\n1. [tensorrt llm llama](https://github.com/hpcaitech/SwiftInfer?tab=readme-ov-file)\n"},{"title":"deep-learning","url":"/2023/12/21/deep-learning/","content":"\n## 基础概念\n\n1. 神经网络本质上是一个计算图。计算图的节点是算子，边是参与运算的张量\n1. `Deep learning` is an approach to machine learning characterized by deep stacks of computations. This depth of computation is what has enabled deep learning models to disentangle the kinds of complex and hierarchical patterns found in the most challenging real-world datasets.\n1. `SGD`: 全称为 Stochastic Gradient Descent 即随机梯度下降,是机器学习中常用的优化算法,用于训练各种模型(如神经网络)寻找最优参数, optimizer\n1. `neuron` : the Linear Unit y = wx + b; w: weight, b: bias\n1. `layers`: Neural networks typically organize their neurons into layers. When we collect together linear units having a common set of inputs we get a `dense layer`.\n1. 一个 layer 共享一个 bias: y = w1 _ x1 + b1 + w2 _ x2 + b2 ==> y = w1 _ x1 + w2 _ x2 + b\n1. `ReLU`: rectified linear unit\n1. `Linear Unit + ReLU`: y = max(0, w \\* x + b)\n1. relu 没有参数， leakyrelu 有参数\n1. Without activation functions, neural networks can only learn linear relationships.\n1. A `loss function` that measures how good the network's predictions are.\n1. An `optimizer` that can tell the network how to change its weights.\n1. `MAE`: mean absolute error; loss function, for regression\n1. Each iteration's sample of training data is called a `minibatch` (or often just \"batch\"), while a complete round of the training data is called an `epoch`.\n1. The `learning rate` and the size of the `minibatches` are the two parameters that have the largest effect on how the SGD training proceeds.\n1. `Adam` is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is \"self tuning\", in a sense). Adam is a great general-purpose optimizer.\n1. `Underfitting the training set` is when the loss is not as low as it could be because the model hasn't learned enough signal.\n1. `Overfitting the training set` is when the loss is not as low as it could be because the model learned too much noise. The trick to training deep learning models is finding the best balance between the two.\n1. `Early Stopping`: stop the training whenever it seems the validation loss isn't decreasing anymore. Interrupting the training this way is called early stopping. Once we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured.\n1. `dropout layer` we randomly drop out some fraction of a layer's input units every step of training, making it much harder for the network to learn those spurious patterns in the training data. Instead, it has to search for broad, general patterns, whose weight patterns tend to be more robust. 可以纠正过拟合\n1. `Batch Normalization layer`\n\n   - why? Features that tend to produce activations of very different sizes can make for unstable training behavior.\n   - A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters.\n   - 做两次 normalize, 先基于输入的 batch 数据做， 后基于训练的均值和方差来做\n   - Models with batchnorm tend to need fewer epochs to complete training. Moreover, batchnorm can also fix various problems that can cause the training to get \"stuck\".\n   - get better performance if you standardize your data before using it for training\n\n1. The main difference regression and classification is in the loss function we use and in what kind of outputs we want the final layer to produce. 主要区别是损失函数和最后一层的输出类型\n1. `Accuracy` is one of the many metrics in use for measuring success on a classification problem. Accuracy is the ratio of correct predictions to total predictions: `accuracy = number_correct / total`\n1. `Cross-Entropy` 交叉熵\n   - Cross-entropy is a sort of measure for the distance from one probability distribution to another.\n   - SGD needs a loss function that changes smoothly, but accuracy, being a ratio of counts, changes in \"jumps\". So, we have to choose a substitute to act as the loss function. This substitute is the cross-entropy function.\n   - With regression, our goal was to minimize the distance between the expected outcome and the predicted outcome. We chose MAE to measure this distance.\n   - For classification, what we want instead is a distance between probabilities, and this is what cross-entropy provides.\n1. `softmax` 也是激活函数， layer to layer; not functions of a single fold x; 在 softmax 函数的实现中减去最大值是一种数值稳定性的技巧。从所有输入值中减去同一个常数不会改变函数的输出。如果 x 很大，可能导致 exp(x)溢出\n1. `relu` 是 single x 的激活函数\n1. [GEGLU](https://medium.com/@tariqanwarph/activation-function-and-glu-variants-for-transformer-models-a4fcbe85323f)\n1. `MLP, CNN, RNN, Transformer` 四大深度学习架构 Multilayer Perceptron(MLP)\n1. 样本和特征, batch 是样本\n1. `正则化(Regularization)` 指的是在训练过程中添加额外信息以防止模型过度拟合的技术。\n   - L1 正则化:在损失函数中添加模型权重参数绝对值的和,使权重 decay 到 0,从而使模型更稀疏。\n   - L2 正则化:在损失函数中添加模型权重参数平方和,惩罚大的参数值,使权重较为平均分布,避免个别权重参数过大。也称为权重衰减(weight decay)。\n   - Early Stopping:在模型测试指标不再改善时中止训练,防止过拟合。\n   - Dropout:以一定概率随机置部分节点为 0,增加模型泛化能力\n   - Data Augmentation:人工生成更多训练数据,改善模型泛化能力。\n   - Batch Normalization: 通过调整网络中间层的激活值，使其在训练时保持一个更稳定的分布。虽然其主要目的是加快训练过程，但它也有一定的正则化效果。\n1. [Batch Normalization 计算](https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html)\n   - 全连接层\n     仿射变换和激活函数之间;对 minibatch 整体做 normalization\n   - 卷积\n     卷积层之后和非线性激活函数之前; 对每个通道分别做 normalization; NCHW, 固定 C; 对于 RGB， 相当于 R, G, B 单独做 normalization\n   - 预测时：均值和方差为整个训练数据集的样本均值和方差(或者学习的均值和方差)\n1. [各种 normlization 方法， 带图](https://www.cnblogs.com/LXP-Never/p/11566064.html)\n1. [文本 normalization 图示](https://blog.tensorflow.org/2022/11/whats-new-in-tensorflow-211.html)\n   - layer norm:输入一句话直接对其输出做 norm，不用管其他句子\n1. `SiLU: f(x) = s * sigmoid(x)`\n1. 图神经网络（Graph Neural Networks，GNN)\n\n### [卷积](https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html)\n\n1. [Convolutional Layers User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#tc-usage)\n1. 每个卷积核输出一个 feature map； 代表一种特征\n1. [卷积转换为矩阵乘](https://zhuanlan.zhihu.com/p/63974249)\n   - batch conv 转化为 gemm， 注意不是 batch gemm\n   - stride 有重叠还是矩阵乘,行多输出\n1. tensor core 用 Implicit GEMM 实现的卷积不用生成中间的矩阵，直接原位计算， 不需要 im2col [link](https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#tc-usage)\n1. 因为为 Tensor Core 实现的卷积需要 NHWC 布局，并且当输入张量以 NHWC 布局时速度最快\n\n## model\n\n1. 构建深度学习模型的基本流程就是：搭建计算图，求得损失函数，然后计算损失函数对模型参数的导数，再利用梯度下降法等方法来更新参数。\n\n## backward\n\n1. 反向传播用于算梯度\n1. backward()实际上是通过 DCG 图从根张量追溯到每一个叶子节点，然后计算将计算出的梯度存入每个叶子节点的.grad 属性中\n1. 在某种程度上，反向传播只是链式法则的一个花哨的名字—— Jeremy Howard\n1. backward 不传入参数时，默认为传入 backward(torch.tensor(1.0))。\n1. [Learning PyTorch with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n1. [用 numpy、PyTorch 自动求导、torch.nn 库实现两层神经网络](https://blog.csdn.net/niexinyu0026/article/details/122262082) [link](https://www.cnblogs.com/luedong/p/14492361.html)\n1. [PyTorch：梯度计算之反向传播函数 backward()](https://blog.csdn.net/baidu_38797690/article/details/122180655)\n\n## optimizer\n\n1. 根据参数梯度来更新参数\n1. pytorch 中初始化需要注意 model 参数位置， model.to(gpu_device)之后初始化 optim\n\n## dataset\n\n1. 用于管理训练时(x_train, y_train)\n\n## dataloader\n\n1. dataset batch 管理\n\n## links\n\n1. [kaggle intro-to-deep-learning](https://www.kaggle.com/learn/intro-to-deep-learning)\n1. [sgd 动画](https://www.kaggle.com/code/ryanholbrook/deep-learning-animations-and-illustrations/notebook)\n1. [overfitting-and-underfitting](https://www.kaggle.com/code/ryanholbrook/overfitting-and-underfitting)\n1. [激活函数](https://en.wikipedia.org/wiki/Activation_function)\n\n### 术语表\n\n1. [深度学习速查词典](https://mp.weixin.qq.com/s/0evrjcivb5ArZGLQ4tGrmg)\n1. [google 机器学习术语表](https://developers.google.com/machine-learning/glossary?hl=zh-cn)\n\n## 全连接层与矩阵计算\n\n1. [图示](https://excalidraw.com/#json=EUPwP_pkPfoNDDVEC4b71,-89u61cxUzIS_dhKYsdHQQ)\n   ![图](https://i.ibb.co/bWthfyQ/o-XLOSNus4-J.png)\n1. 输出的每个神经元可以看到所有输入，提取了输入的某种特征\n1. 两个相乘的矩阵分别为 m×k 和 k×n 时，计算强度的计算略有不同。在这种情况下，矩阵乘法需要进行大约 m×n×k 次乘法和相同数量的加法操作。\n   - `浮点运算次数`：每个元素的计算涉及 k 次乘法和 k-1 次加法（对于每行和每列中的每个交叉点）。因此，总的浮点运算次数大约是 2m×n×k 次。\n   - `内存操作`：如果每个矩阵元素是单精度浮点数（4 字节），那么矩阵 A 需要 m×k×4 字节，矩阵 B 需要 k×n×4 字节，矩阵 C 需要 m×n×4 字节的内存。因此，总的内存操作大约是 (m×k + k×n + m×n)×4 字节。\n   - `计算强度（算术强度)`可以表示为： `2mnk/(4(mk + kn + mn)) = 0.5/((1/n + 1/m + 1/k))`; 计算强度与 m, k, n 成正比, 实际中，由于现代处理器和 GPU 上的内存缓存效应，以及各种数学库和编译器优化技术的应用，真实的计算强度可能会有所不同。\n1. [dl-performance-matrix-multiplication](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html)\n   - 包含 tensor core 计算\n1. [矩阵分块减少访存示例](https://github.com/flame/how-to-optimize-gemm/blob/master/src/MMult_4x4_5.c#L54C2-L78C4)\n"},{"title":"multimodal","url":"/2023/12/19/multimodal/","content":"\n## links\n\n1. [clip](https://openai.com/research/clip) [multimodal-learning 中文解析](https://imzhanghao.com/2022/10/27/multimodal-learning/)\n"},{"title":"diffusion","url":"/2023/12/19/diffusion/","content":"\n## diffusers\n\n1. [huggingface/diffusers](https://github.com/huggingface/diffusers)\n   - Diffusers: State-of-the-art diffusion models for image and audio generation in PyTorch and FLAX.\n   - 可以用 xformers 来加速推理和训练 [readme](https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/xformers.md)\n   - State-of-the-art diffusion pipelines that can be run in inference with just a few lines of code.\n1. diffusers 加速技术\n   - [accelerate-inference-of-text-to-image-diffusion-models](https://huggingface.co/docs/diffusers/tutorials/fast_diffusion#accelerate-inference-of-text-to-image-diffusion-models)\n   - [diffusers/optimization](https://huggingface.co/docs/diffusers/optimization/fp16)\n\n## [MagicDrive](https://arxiv.org/abs/2310.02601)\n\n1. MagicDrive can be used for data augmentation, facilitating the training for perception models on both BEV segmentation and 3D object detection tasks.\n1. bevfusion 用来处理原始 bev 数据得到 bev map 和 3d box\n1. 可以不自己产生 bev map 和 3d boxes, 从真实数据拿。只需要不同的 prompt 就能生成不同的场景\n1. 可以使用 diffusion model 替换真实场景中的物体\n1. Training Support for BEV Segmentation and 3D Object Detection. MagicDrive can produce augmented data with accurate annotation controls, enhancing the training for perception tasks.\n1. 生成的数据可以用于训练\n1. 用了 control-net diffusers pipeline 和 xformers 来加速\n\n## papers\n\n1. [必读的 10 篇经典论文](https://zhuanlan.zhihu.com/p/595866176)\n1. [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)\n1. [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) stable diffusion 的原型\n   - [code](https://github.com/CompVis/latent-diffusion)\n   - [stable-diffusion code](https://github.com/CompVis/stable-diffusion)\n1. [Video Diffusion Models](https://arxiv.org/abs/2204.03458)\n   - [code](https://github.com/lucidrains/video-diffusion-pytorch)\n"},{"title":"ai-papers","url":"/2023/12/18/ai-papers/","content":"\n## [alexnet](https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)\n\n1. 关键是 end2end, 直接 rgb 到结果，不用做各种专业处理\n1. CNN 关键是压缩(特征一层一层压缩)\n1. SGD: 全称为 Stochastic Gradient Descent,即随机梯度下降,是机器学习中常用的优化算法,用于训练各种模型(如神经网络)寻找最优参数\n1. dropout\n\n## [resnet](https://arxiv.org/abs/1512.03385)\n\n1. 加残差， 能训练很深，计算量未增加\n\n## [unet](https://arxiv.org/abs/1505.04597)\n\n## [transformer](https://arxiv.org/abs/1706.03762)\n\n## ViT Vision Transformer,\n\n## flash attention\n\n## [PagedAttention](https://arxiv.org/pdf/2309.06180.pdf)\n\n1. 分块 KV 缓存通过消除 KV 缓存引起的内存碎片化，增加了潜在的序列并发量，从而增加了系统吞吐量。\n1. 没有减少 KV cache\n1. 类似于现有的框架如 TRT-LLM、TGI 和 vLLM，DeepSpeed-FastGen 的目标是利用连续批处理和非连续 KV 缓存技术，以提升数据中心服务大型语言模型（LLM）的硬件利用率和响应速度。为了实现更高的性能，DeepSpeed-FastGen 提出了 SplitFuse 技术，它利用动态提示和生成分解, 统一来进一步改善连续批处理和系统吞吐量。\n\n## diffusion\n\n## stable diffusion\n\n## [UniAD](https://arxiv.org/abs/2212.10156)\n\n1. [github](https://github.com/OpenDriveLab/UniAD)\n\n## [BEVFormer](https://github.com/fundamentalvision/BEVFormer)\n\n1. [中文版](https://drive.google.com/file/d/1dKnD6gUHhBXZ8gT733cIU_A7dHEEzNTP/view)\n\n## 如何使用 arxiv\n\n## links\n\n1. [Parameter, Compute and Data Trends in Machine Learning](https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/edit#gid=0) good：包含参数，计算量, 训练数据量，论文引用\n1. [labml.ai Deep Learning Paper Implementations](https://github.com/labmlai/annotated_deep_learning_paper_implementations)\n   - colab 中有测试代码\n1. [papers 画了重点](https://github.com/labmlai/annotated_deep_learning_paper_implementations/tree/master?tab=readme-ov-file#highlighted-research-paper-pdfs)\n1. [paperwithcode](https://paperswithcode.com/)\n1. [李沐论文精度](https://www.youtube.com/playlist?list=PLFXJ6jwg0qW-7UM8iUTj3qKqdhbQULP5I)\n1. [李沐《动手学深度学习》](https://zh.d2l.ai/)\n1. [《动手学深度学习》pdf](https://zh-v2.d2l.ai/d2l-zh.pdf)\n1. [images-models-papaers](https://github.com/huggingface/pytorch-image-models)\n"},{"title":"transformer","url":"/2023/11/21/transformer/","content":"\n## base\n\n1. 硬 train 一发\n1. [attention](https://zhouyifan.net/2022/11/12/20220925-Transformer/)\n1. [What exactly are keys, queries, and values in attention mechanisms?](https://stats.stackexchange.com/a/424127)\n   - The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best matched videos (values).\n   - 搜索是 query, 每个视频的信息是 key, query 和所有视频 key 做相关， 然后推荐最相关的视频(value)\n1. 只不过解码器的输入是编码器的状态的加权和，而不再是一个简单的中间状态。每一个输出对每一个输入的权重叫做注意力，注意力的大小取决于输出和输入的相关关系\n1. 注意力机制能够无视序列的先后顺序，捕捉序列间的关系\n1. RNN 本轮的输入状态取决于上一轮的输出状态，这使 RNN 的计算必须串行执行。因此，RNN 的训练通常比较缓慢。\n1. transformer: 需要对每个输入向量做 3 次 transform, Wq, Wk, Wv，转换矩阵都是学习得到的\n1. score matrix:自相关矩阵\n1. 预测时就相当于使用编码器(输入)和之前的输出来预测下一个输出； 考虑了输入和输出信息 [Illustrated Guide to Transformers Neural Network: A step by step explanation](https://www.youtube.com/watch?v=4Bdc55j80l8) 图示非常好\n1. 多头是想模拟卷积，可以输出多个通道 由多个 feature，识别多个模式\n1. 模型训练好之后，推理时每个 token 对应的 q,k,v 都固定了，因为 token embedding 后的词向量是固定的，Q,K V 矩阵也是固定的\n1. 每一个 token 都会生成 q,k,v，用 q 去和所有 token 的 k 做相关，根据结果从 k 中提取特征; 例如： how are you How 的 q 会和每个 token 中的 k 提取出一些信息，根据这些信息决定下一个生成的字符，比如 you 中的 k 和 I 很相关（embeding 和训练 决定），从 you 中很可能提取出 I, 然后再根据 how are you I 中重复上一步动作，这就是为什么 token 要和自己做相关，所有 token 相关性是由 embeding 和训练一起决定的?\n1. kv cache 的长度分别为 `head 长度 * 每个 head 向量维度 * token 个数 * 层数`\n1. transform llama 各层 shape 参数量和计算量\n1. 推理时才自回归；训练时用相关矩阵(下三角)一步算出所有输出，推理时每次算出当前值与之前 token 注意力，相当于在下三角矩阵加一层，只输出一个预测 token;\n1. 添加 position 信息在输入之后，attention 之前\n1. decode 训练时加掩码并行，同时输出； 推理时自回归一步一步输出\n1. encode decoder 架构推理直接有输入和起始符输出，decode only 需要预热\n1. batch 指的是在模型训练或推理的时候,同时输入模型的样本数量。\n   - Transformer 可处理变长序列输入。不同的样本序列长度本来就可能不同,每个 batch 中的最大 token 长度是固定的,短的序列会 padding 补 0。对超长序列做截断,限制最大 token 数\n   - Transformer 模型的参数在不同的 batch 之间是共享的，而 KV cache 的共享则取决于具体的应用场景和模型配置。在自回归生成任务中，KV cache 通常是特定于单个样本的，不是共享的.\n   - 在 Transformer 模型中,batch size 越大,则 key-value 缓存需要占用的内存空间也越大。\n1. 每个词向量(word vectors)代表了“词空间（word space）”中的一个点，具有相似含义的词的位置会更接近彼此。例如，在向量空间中与猫最接近的词包括狗、小猫和宠物。用实数向量表示单词（相对于“C-A-T”这样的字母串）的一个主要优点是，数字能够进行字母无法进行的运算。 [link](https://www.understandingai.org/p/large-language-models-explained-with)\n1. 像 ChatGPT 这样的语言模型能够根据单词出现的上下文以不同的向量表示同一个词(前几层 transformer 自动修改)。有一个针对“bank（金融机构）”的向量，还有一个针对“bank（河岸）”的向量。\n1. 前几层专注于理解句子的语法,后面的层则致力于对整个段落的高层次理解。\n1. `注意力层`从提示的较早部分检索信息，而`前馈层`使语言模型能够“记住”未在提示中出现的信息。事实上，可以将前馈层视为模型从训练数据中学到的信息的数据库。记忆力在前馈层。靠前的前馈层更可能编码与特定单词相关的简单事实，例如“特朗普经常在唐纳德之后出现”。靠后的层则编码更复杂的关系，如“添加这个向量以将一个国家转换为其首都。\n1. LLM 的一个关键创新之处在于，它们不需要显式标记的数据。相反，它们通过尝试预测文本段落中下一个单词来学习。几乎任何书面材料都适用于训练这些模型——从维基百科页面到新闻文章再到计算机代码。\n1. multi head 可能也是一维 把一维 head 当作多维 head\n1. Perplexity (PPL) 是一个衡量语言模型预测能力的指标。当评估大型语言模型时，PPL 是一个关键指标，因为它直接关系到模型对语言的处理能力。它反映了模型对语言的理解程度，特别是在预测下一个词时的不确定性。PPL 通常是通过在测试集上计算模型的交叉熵损失（cross-entropy loss）并将其转换为 PPL 来得到的。数学上，PPL 定义为交叉熵的指数。\n   - 低 PPL：意味着模型对数据的预测更准确，对语言的理解更深入。\n   - 高 PPL：表明模型预测不准确，对语言的理解较浅。\n\n## [attention 计算](https://excalidraw.com/#json=0Es8o7IlBTF7Enynbr6Pb,yNTjZuZeNWYLdDYxy_dGzg)\n\n![矩阵计算](https://i.ibb.co/ZhBZMds/fz-L82-OZ7od.png)\n\n## [attention 种类](https://mp.weixin.qq.com/s/t4ytOIuPx0799kkjFs5lbQ)\n\n## kv cache\n\n![四种attention策略](https://i.ibb.co/XCJ85TL/p3k-B45-Mhaj.png)\n\n1. 多种 attention 策略\n   - dense attention: 计算复杂度为 O(T\\*T), 缓存 T cache; 当句子超过训练长度时，PPL 增大, 计算复杂度：因为句子长为 T, 需要计算 T 次，每次需要和前 T 个 token 做 attention\n   - window attention: 计算复杂度为 O(T\\*L), 缓存 L cache，当句子大于窗口长度时， 模型的 PPL 会急速变大, 因为前几个 token 非常重要， 被换出后影响模型性能； `绝对位置编码`\n   - sliding window attention with re-computing : 计算复杂度为 O(T*L*L), T 个字符，L 窗口内类似 dense attention, 重新计算 L kv, 相当于缓存 L cache; 每层中会为每个生成的 token 重建最近 token 的 KV 状态。这种方法虽然性能强大，但由于需要在窗口内计算 quadratic attention，因此速度明显较慢; [和 window attention 区别](https://github.com/mit-han-lab/streaming-llm/issues/33#issuecomment-1758597666) The critical distinction is that in sliding window with re-computation, some key states are treated as initial tokens, whereas in window attention, all previous tokens' KV are computed as if they were middle tokens.\n     - re-computation 重置了状态，窗口第一个 token 位置变为 0(位置改变，kv cache 需要重新计算)， 可以扩展为无限长输出，但没有记住无限长; 窗口应该会添加 BOS； `相对位置编码`\n   - sliding window attention:(和 re-computing 有较大区别) 计算复杂度为 O(T\\*L), 这种方法类似卷积， 一层一层增加感受范围，\n     - 对于 m 层的 Transformer，receptive field 的大小为 m \\* L； 增加了感知范围，但没有到无限长， decoder only 时注意有掩码，看不到之前节点的(这条不需要专门处理，多层 decoder 自动有这个特性)\n       ![类似卷积](https://i.ibb.co/r2c3DLY/p5h-Alp-Fh-TT.png)\n     - [Understanding LongFormer’s Sliding Window Attention Mechanism](https://ahelhady.medium.com/understanding-longformers-sliding-window-attention-mechanism-f5d61048a907) 有图\n   - streaming LLM: 计算复杂度为 O(T\\*L)\n1. [streaming-llm](https://github.com/mit-han-lab/streaming-llm)\n   - 这个方法并没有增加 LLM 的对上文的记忆，只是让它输入输出无限长\n   - [slides](https://github.com/mit-han-lab/streaming-llm/blob/main/assets/StreamingLLM.pdf)\n   - 介绍了多种 cache 方法\n   - 上下文窗口保持不变。只保留最近的标记和注意力汇，丢弃中间的标记。这意味着模型只能处理最新的标记。需要重新计算 position\n   - StreamingLLM 的优势在于无需刷新缓存就能从最近的标记生成流畅的文本\n   - 利用了 attention sink 现象, 由于 softmax，训练时前几个 token 对最终生成的内容非常关键，是生成内容稳定的关键 token, 如果保留前几个 token， 加上 window attention, 长文本时生成的内容就比较稳定\n   - 这文章感觉就是之前 softmax 的 bug 带来的; 如果流式推理保证 system prompt 不被换出 不就没问题了…\n   - 如何处理后面窗口位置信息的？有技巧，了解一下\n1. llama kv cache\n   ![llama-2-7B](https://i.ibb.co/88sYM2f/Cnc1i7-G0qy.png)\n1. 在基于 Transformer 的 decoder-only 模型中，包括`起始符号`在内的每个标记都有与之对应的键值（KV）缓存。\n1. decoder-only 推理过程，假设 prompt 为 how are you; 先输入 bos token, 生成对应 kv cache, 输入 how, 和 bos 做 attention, 不用管预测，将 are 作为输入，和之前做 attention， 不用管输出，将 you 作为输入， 输出为 I, 将 I 作为输入......\n   - 注意，如果有随机，每次 bos 生成的 kv cache 不同，对后面结果都有影响\n   - 只要有输出不一样， 每一层的 kv cache 也不同， 会传递\n   - 可以使用 flashattention 一次输出所有结果\n1. encoder 也要存多有\n\n#### 变长矩阵乘\n\n1. 预先申请长为 n 的 kv cache 内存, 使用 for 循环或变长矩阵乘计算 attention，\n   - kv cache 长度小于 n 时， 不用移动 kv cache; [link](https://chat.openai.com/c/405fe374-fd46-4ec2-85be-ce08e32912ce)\n   - 大于 n 时，\n     - 环形缓冲区, 先放入到最早的 cache 位置; 不用关心顺序； 然后和所有 kv cache 做 attention; 和 q 直接矩阵乘; 由于每个 KV 对都与序列中的特定位置相关联，移除操作不会改变剩余 KV 对之间的相对位置关系。\n     - 当前 kv 放到 cache 中的末尾，之前的 kv cache 需要向前移动\n\n#### 定长矩阵乘\n\n1. 矩阵乘时需要 kv cache 是固定长的矩阵，刚开始 kv cache 矩阵需要 padding, 如果分配固定长的 kv cache 矩阵\n1. padding 利用优化的定长矩阵乘法来加速, 会浪费一些计算\n\n## 推理过程：\n\n1. 有两个阶段 Prefill Phase 和 Decoding Phase（ FlexGen 中讲的比较清楚）。\n1. Prefill Phase：称为预处理/Encoding。计算并缓存每一层的 key 和 value，其他的不需要缓存。每一个请求的 prompt 需要经过这个阶段，它只计算一次，是并行计算的。这个缓存称为 KV Cache，KV Cache 是整个解码过程中最为核心关键的一块。\n1. Decoding Phase：生成新 token 阶段，它是串行的，也就是 decode one by one。它用上一步生成的 token，称为当前 token 放到 input 中，然后生成下一个 token。具体包括两步，一是 Lookup KV Cache 计算并输出当前 token 最终 embedding 用来预测下一个 token，二是缓存计算过程中得到的当前 token 在每一层的 key 和 value，update 到第一阶段 Prefill Phase 中的 KV Cache 中。\n1. 无问苍穹 [paper](https://arxiv.org/abs/2311.01282), 有介绍各种推理框架\n   ![inference](https://i.ibb.co/T1DZvdD/0pemezw-XTa.png)\n\n## decode only 推理过程: prompt（“how are you”）\n\n1. BOS Token：\n   - 开始：推理过程以输入一个特殊的开始标记（BOS, Begin Of Sentence）开始。\n   - 生成 KV 缓存：对于 BOS 标记，模型计算出对应的键（Key）和值（Value），并将它们存储在 KV 缓存中。\n2. 逐个处理 Prompt 中的词：\n   - 对于 prompt 中的每个词（如\"how\", \"are\", \"you\"），模型依次进行处理。在处理每个词时，会使用到目前为止累积的 KV 缓存来进行注意力计算。例如，处理\"how\"时，它会与 BOS 的 KV 进行注意力计算；处理\"are\"时，它会与 BOS 和\"how\"的 KV 进行注意力计算，以此类推。\n     每个新词的处理结果也会生成新的 KV 对，这些新的 KV 对被添加到缓存中。\n3. 生成响应：\n   - 当处理完 prompt 中的所有词后，模型开始生成响应。假设首个生成的词是\"I\"。生成\"I\"时，会利用到目前为止（包括 BOS 标记和 prompt 中所有词）的所有 KV 缓存。\n4. 递归生成：\n   - 随后，模型继续基于累积的 KV 缓存和已生成的词（如\"I\"）来生成下一个词。这个过程会持续进行，直到生成一个完整的响应或达到某个终止条件（如特殊的结束标记或达到最大长度限制）。\n\n## encoder only 推理过程\n\n1. 在 encoder-only 的 LLM 中，KV 缓存并不是一个主要问题，因为这些模型的设计和应用方式与需要逐步生成和维护上下文状态的 decoder-only 模型不同。在 encoder-only 模型中，更关注的是整个输入序列的一次性处理和理解。\n1. 在 encoder-only 模型中的推理过程与 decoder-only 模型有所不同。encoder-only 模型，如 BERT，通常用于理解、分析或分类文本，而不是像 decoder-only 模型那样用于生成文本。以下是 encoder-only 模型的典型推理过程\n1. `输入处理`：\n   - `完整的输入`：与 decoder-only 模型不同，encoder-only 模型在推理时接收完整的输入序列，如一个句子或段落。这个输入通常包括特殊的标记，如开始（BOS）和结束（EOS）标记。\n   - `预处理`：输入文本经过标记化（tokenization），转换成模型能够理解的标记序列。\n1. `通过Encoder层传递`：\n   - `编码`：整个输入序列被送入模型的多个 encoder 层。在每一层中，通过自注意力机制和前馈神经网络，模型学习到输入中每个标记的上下文表示。\n   - `自注意力计算`：在自注意力阶段，每个标记都考虑到序列中所有其他标记的信息，以捕捉内部的上下文关系。\n1. `输出提取`：\n   - `特定任务的输出`：根据任务的不同，模型的输出被相应地处理。例如，对于分类任务，模型可能只使用特定标记（如[CLS]）的最终隐藏状态；对于命名实体识别或问答任务，模型可能输出每个标记的特征表示。\n1. `后处理`：\n   - `映射到任务`：模型输出被映射到具体任务的要求上，如将隐藏状态映射到类别标签或其他输出格式。\n   - `生成最终结果`：模型的输出经过适当的后处理步骤（如 softmax 层，用于分类任务），以生成最终的推理结果。\n1. 推理特点\n   - `不生成文本`：encoder-only 模型通常不用于生成文本，而是用于理解或分类输入文本。\n   - `全局上下文`：模型在处理输入时同时考虑所有标记的上下文，与 decoder-only 模型逐步生成的方式不同。\n   - `特定任务适用`：这类模型通常针对特定的 NLP 任务进行训练和优化，如情感分析、文本分类、实体识别等。\n\n## 关键操作\n\n1. multi head attention\n1. feed forward\n1. layernorm\n1. softmax\n1. matmul\n1. concat\n1. linear(生成 q, k, v) Q, K, V\n\n## 参数、计算复杂度和 cache 统计\n\n1. [Model Size of Notable Machine Learning Systems Over Time](https://epochai.org/mlinputs/visualization?yAxis=Parameters) 可交互, 右上角 option 可搜索，可直接到论文\n1. [transformer-inference-arithmetic](https://kipp.ly/transformer-inference-arithmetic/)\n\n### transformer(all you need is attention)\n\n1. [harvardnlp/annotated-transformer](https://github.com/harvardnlp/annotated-transformer)\n1. 注意：模型结构定义并不决定 forward 流程，可以有多个函数，使用网络中不同的部分\n1. base model info: embeding 参数可能会共享\n\n```text\nEncoderDecoder(\n  (encoder): Encoder(\n    (layers): ModuleList(\n      (0-5): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linears): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Linear(in_features=512, out_features=512, bias=True)\n            (2): Linear(in_features=512, out_features=512, bias=True)\n            (3): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (sublayer): ModuleList(\n          (0): SublayerConnection(\n            (norm): LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): SublayerConnection(\n            (norm): LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    (norm): LayerNorm()\n  )\n  (decoder): Decoder(\n    (layers): ModuleList(\n      (0-5): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linears): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Linear(in_features=512, out_features=512, bias=True)\n            (2): Linear(in_features=512, out_features=512, bias=True)\n            (3): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linears): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Linear(in_features=512, out_features=512, bias=True)\n            (2): Linear(in_features=512, out_features=512, bias=True)\n            (3): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (sublayer): ModuleList(\n          (0): SublayerConnection(\n            (norm): LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): SublayerConnection(\n            (norm): LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): SublayerConnection(\n            (norm): LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    (norm): LayerNorm()\n  )\n  (src_embed): Sequential(\n    (0): Embeddings(\n      (lut): Embedding(32000, 512)\n    )\n    (1): PositionalEncoding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (tgt_embed): Sequential(\n    (0): Embeddings(\n      (lut): Embedding(32000, 512)\n    )\n    (1): PositionalEncoding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (generator): Generator(\n    (proj): Linear(in_features=512, out_features=32000, bias=True)\n  )\n)\n```\n\n1. 参数总量：65M,论文中有提及\n\n### [llama2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/main/config.json)\n\n1. model info\n\n```text\nLlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False) # 输入是 in_features, 输出为 out_features, 矩阵参数shape为（out_features, in_features); 因为矩阵乘法：(out_features, in_features)(in_features, 1) = (out_features, 1)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)\n```\n\n1. 参数: 见 pytorch params 计算，[link](https://medium.com/@saratbhargava/mastering-llama-math-part-1-a-step-by-step-guide-to-counting-parameters-in-llama-2-b3d73bc3ae31)\n   - total: 131072000 + 6476267520 + 4096 + 131072000 = 6738415616 = 6.7B\n     - embed_tokens: (32000, 4096) = 131072000\n     - attention block x 32 = 202383360 x 32 = 6476267520\n       - attention layer:4 x (4096, 4096) = 67108864\n       - mlp: 2 x (11008, 4096) + (4096, 11008) = 135266304 (占比最大)\n       - input layernorm: (4096) = 4096\n       - post_attention_layernorm: (4096) = 4096\n     - norm: (4096) = 4096\n     - lm_head:(32000, 4096) = 131072000\n1. 计算复杂度：\n\n   - total: O(l(n^2 x d + ndd’ + nd)) ; l 为层数; 不要考虑 kv cache 更好理解, 加掩码的矩阵乘\n     - Multi-head self-attention 层 计算复杂度主要来自注意力机制的计算。对于序列长度为 n,hidden size 为 d 的 self-attention,其复杂度为 O(n^2 x d)；（n, d)(d, n) => O(n^2 x d); 包含 cache 的也等价矩阵乘\n     - 前馈全连接层 这一层包含两个仿射变换,如果其 width 为 d’,那么复杂度为 O(n x d x d’)； (n, d)(d, d') => O(ndd')\n     - Layer normalization 对序列长度为 n, 向量维度为 d,其复杂度为 O(n x d)\n\n1. kv cache\n   - `BS * layers * kv-heads * heads-d * N * 2(kv) * sizeof(fp16) = BS * 32 * 32 * 128 * N * 2 * 2 = 0.5MB * BS * N` 注：128 = 4096 / 32\n\n## [hugging face transformers](https://github.com/huggingface/transformers)\n\n1. [doc](https://huggingface.co/docs/transformers/index)\n1. [offline-model](https://huggingface.co/docs/transformers/installation#offline-mode)\n1. 模型文件页面右侧有对应使用方法\n1. 模型里有 onnx\n1. 只需要模型和对应的 config.json\n1. sample: [llama2.c-stories15M](https://huggingface.co/Xenova/llama2.c-stories15M/tree/main)\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"/home/xiyang/d/working/transformers/models/\")\nmodel = AutoModelForCausalLM.from_pretrained(\"/home/xiyang/d/working/transformers/models/\")\nmodel # 查看模型结构\n```\n\n## links\n\n1. [transformers/model_summary](https://huggingface.co/docs/transformers/model_summary)\n1. [pytorch transformer_tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n1. [nn.labml.ai/transformers/mha](https://nn.labml.ai/transformers/mha.html)\n1. [Illustrated Guide to Transformers Neural Network: A step by step explanation](https://www.youtube.com/watch?v=4Bdc55j80l8) 图示非常好\n1. [harvardnlp/annotated-transformer](https://github.com/harvardnlp/annotated-transformer)\n1. [Transformer 论文逐段精读](https://www.youtube.com/watch?v=nzqlFIcCSWQ)\n   代码带解说 李沐](https://zh.d2l.ai/chapter_attention-mechanisms/transformer.html)\n1. [quicktour](https://huggingface.co/docs/transformers/quicktour) can run in colab\n1. [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)s\n1. [fetch-models-and-tokenizers-to-use-offline](https://huggingface.co/docs/transformers/installation#fetch-models-and-tokenizers-to-use-offline)\n1. [huggingface/transformers](https://github.com/huggingface/transformers)\n1. [Attention Is All You Need (Transformer) 论文](https://arxiv.org/pdf/1706.03762.pdf)\n1. [(good)Attention Is All You Need (Transformer) 论文精读](https://zhouyifan.net/2022/11/12/20220925-Transformer/)\n\n### [李宏毅](https://www.youtube.com/@HungyiLeeNTU/playlists)\n\n1. [Transformer](https://hackmd.io/@shaoeChen/rJlRfP7mL)\n1. [self attention](https://www.youtube.com/watch?v=hYdO9CscNes)\n1. [Self-attention 自注意力机制讲解 李宏毅版 v.s 吴恩达版](https://zhuanlan.zhihu.com/p/505105707)\n"},{"title":"LLM","url":"/2023/11/21/LLM/","content":"\n## [LLM 进化树](https://github.com/Mooler0410/LLMsPracticalGuide)\n\n![LLM 进化树](https://github.com/Mooler0410/LLMsPracticalGuide/raw/main/imgs/tree.jpg)\n\n## 语言模型质量评测\n\n1. 信息论中，困惑度度量概率分布或概率模型的预测结果与样本的契合程度，困惑度越低则契合越准确。该度量可以用于比较不同模型之优劣。\n1. [PPL: perplexity](https://huggingface.co/docs/transformers/perplexity)\n1. [perplexity-measuring-model-quality](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#perplexity-measuring-model-quality)\n\n## concept\n\n1. o1、o3、R1 都是推理模型。DeepSeek V3 是 LLM，也就是“基础大语言模型”，而这些推理模型都是在它之上微调出来的。[link](https://baoyu.io/translations/deepseek-r1-overview)\n1. \"zero-shot\", \"one-shot\", 和 \"few-shot\" 学习是指训练模型以使其能够处理它在训练过程中未直接遇到过的任务或类别的方法。\n   - zero shot: 实际应用时，它需要对它在训练时从未见过的类别做出预测或决策。\n   - one shot: 模型对每个类别仅看到一个样本就要学会识别该类别\n1. `General Matrix Multiply (GeMM)`\n   - `C = αAB + βC`\n   - GEMM 是一种更通用的矩阵乘法操作。它不仅包括两个矩阵的乘法，还可以包括对这两个矩阵进行转置或共轭转置，以及将结果乘以一个标量或与另一个矩阵的和。\n   - 在函数接口上,GEMM 通常需要传入更多的参数,包括缩放因子、转置选项等。而 matmul 接口更加简洁。\n   - 在一些深度学习框架如 TensorFlow 和 PyTorch 中,matmul 是矩阵乘法的默认操作。而 GEMM 则由于其优化,常被用在需要高性能的场景。\n1. 奇怪的话可以放到 midjouney(or dell-e) 画出图像，助于理解\n1. token(令牌?) 是某个领域中一个抽象的语法或逻辑单元的称呼。在自然语言处理中,token 指一个文本串中基本的符号。比如一个句子可以被切分为多个词(word),每个词就是一个 token。\n1. prompt 提示很重要\n1. 机器来找 prompt(提示)： hard prompt, soft prompt(adpter 放在 input), using reinforcement learning(加 generator), 让 llm 自己产生自己 prompt\n1. toolformer: 使用工具\n1. token 中文是字， 英文是 word piece, word 太多; unbreakable -> un break able\n1. model 本质是函数\n1. prompting 给 chatgpt 催眠，设置限制，让 chatgpt 回答某方便问题，如设置中文聊天\n1. neural editing 训练好模型改参数\n1. machine unlearning 忘记曾经学过的东西， 遗忘某些涉密问题\n1. hyperparameter 超参数，学习算法的参数，不是神经网络的参数\n1. 文字冒险游戏： chatgpt + midjourney + 语音\n1. 腾讯语音情感 https://arxiv.org/abs/2301.13662 https://dongchaoyang.top/InstructTTS/\n1. embeddings:在某种程度上，就是用来降维的，降维的原理就是矩阵乘法 [一文读懂 Embedding 的概念](https://zhuanlan.zhihu.com/p/164502624)\n1. [tokenizer](https://platform.openai.com/tokenizer)\n1. tokenizer 是将文本进行分词,通常分为单词(word)或子词(subword)单元。它的目标是划分语料的基本符号。embeddings 是将语料库中的词或字符映射到 dense 向量表示。通常先经过 tokenizer 得到词单元,然后再映射为稠密词向量。tokenizer 侧重文本符号的划分,主要方法是基于词典或规则进行分割。embeddings 侧重语义信息的编码,主要方法是基于语料训练词向量。\n1. AGI (Artificial General Intelligence): 人工通用智能或通用人工智能，是指在任何智能任务上都能表现得与人类智能相媲美的人工智能系统。\n1. AIGC (AI-Generated Content): 人工智能生成内容，指的是通过人工智能算法和模型自动或半自动生成的内容。这包括文本、图像、音乐、视频和其他媒体形式。\n1. 大模型（Large Language Model，LLM）\n   ![示意](https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/1_Welcome_GenerativeMeena_CL_V02_150521_v2_720_25fps.gif)\n1. 模型大小 7B(billion), 7B 表示 70 亿个可训练参数。吉比（gigabyte）是一个信息技术单位，。十亿（billion）是一个计数单位， 参数类型(float, int8)和数量(billion)决定模型的大小(gigabyte)\n1. Bard 是由 Google 开发的生成式人工智能聊天机器人，最初基于大型语言模型的 LaMDA 系列，后来基于 PaLM2。\n1. 基础架构 transformer\n1. [claude](https://claude.ai/chats) chatgpt 主要竞争对手\n1. webgpt: 产生的内容带引用网址, 先 gpt 处理文字， 得到关键字用于搜索引擎搜索，得到各网页结果，然后处理搜索结果，然后点选，处理网页中的内容，关联的收藏起来， 可以产生多个关键字， 多次搜索， 最终有多个收藏， 只处理这些收藏\n1. chatgpt 文字接龙， bert 文字填空\n1. 大模型可以帮助完成强有力的事情 \"A mouse riding on the head of an elephant, using reins to steer the giant creature.\" (powered by Midjourney )\n1. 对于大模型的期待：专才(finetune, adapter)，通才(instruction learning， in-context learning)\n1. finetune: update network parameters by gradient descent\n1. adapter(efficient fine tuning): 大模型加入插件(例如：加一层)， 只调整新添加插件参数, 优势：不用调整大模型原始参数\n1. in-context learning(示例学习): 给一些例子， 例子可能只是用于启动任务, 唤醒记忆。 更大的模型可能从例子学习到信息更多, 输入一些分类 feature 例子(直接给数字和 label)，可以将大模型变为一个分类器\n1. learing in-context learning: 学习示例学习结果更好，但数据难收集\n1. instruction learning\n1. chain-of-thought(CoT) prompting: 训练时给出推论再给结果， 结果正确率会高很多。多次推论有不同的答案再投票.。 （chatgpt 默认列出计算过程，如果不让列详细过程， 结果可能会差）\n1. 参数越多，数据越多， 效果越好\n1. emergent ability 顿悟时刻 [Emergent Abilities of Large Language Models](https://arxiv.org/pdf/2206.07682.pdf) 10B\n1. [Language Models (Mostly) Know What They Know](https://arxiv.org/abs/2207.05221)\n1. calibration: 大模型 softmax 分数越高，正确率越高，大模型知道知己是否在瞎掰\n1. 模型越大结果越差例子： [inverse-scaling/prize](https://github.com/inverse-scaling/prize), 更大的模型可能会顿悟，结果会更好; U-shaped U 型曲线\n1. [Switch Transformers](https://www.jmlr.org/papers/v23/21-0998.html) 训练时用所有参数，推理时只用部分参数，加快推理运行\n1. 从数据中学习语言：世界知识(尝试)， 语言知识\n1. data preparation: 数据处理\n   - [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446)\n   - [Deduplicating Training Data Makes Language Models Better](https://arxiv.org/abs/2107.06499)\n1. 固定算力资源情况下：模型参数(思考)， data(学习) 成反比； 大模型小数据(思而不学)， 小模型大数据(学而不思); 学思应该平衡\n   - [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) 给出算力，参数量和数据量如何确定\n   - LLaMa 使用了这个知识\n1. fintuning 和 reinforcement lerning 效果很好 [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) 小模型也可以胜大模型\n1. openai 收齐了很多问题，所以即使 chatgpt 模型不那么大，也比更大模型好\n1. `MoE(Mixture of Experts)`模型指的是专家混合模型,是一类将多个模型专家进行混合的组合结构。例如： Mixtral 8x7B\n\n## 生成内容\n\n1. 文本\n1. 语音\n1. 声音\n1. 图像\n1. 视频\n\n## 生成方式\n\n1. 各个击破 Autoregressive(AR) model\n\n   - 速度慢 无法并行\n   - 质量高\n   - 常用于文字生成\n\n1. 一次到位 Non-autoregressive(NAR) model\n\n   - 速度快\n   - 质量较差\n   - 冲用于图片生成\n\n1. 各个击破和一次到位结合\n   - 先各个击破产生中间产物(先决定大方向)，再一次到位\n   - 一次到位改成 N 次到位(先一次到位再各个击破) diffusion model\n\n### 多任务学习(Multi-Task Learning)和多模态学习(Multimodal Learning)\n\n1. 目标差异\n\n   - 多任务学习的目标是同时学习多个相关的任务,在不同任务间实现知识迁移,从而 mutually improve 模型的泛化性能。\n   - 而多模态学习是为了建模和理解包含多个模态(文本、图像、语音等)的单一任务或场景。\n\n2. 方法差异\n\n   - 多任务学习通常是共享底层特征表示,在顶层分出多个 task-specific 的输出层。\n   - 多模态学习则更关注不同模态间的交互建模、对齐、融合,学习联合的媒体表征。\n\n3. 应用差异\n\n   - 多任务学习的应用更广泛,从计算机视觉、NLP 到健康领域都有。\n   - 多模态应用更集中在人机交互、信息检索、场景理解等领域。\n\n4. 总结\n   - 多任务学习 optimize 同一模型在不同任务上的泛化性能\n   - 多模态学习 optimize 不同媒体表征的融合,用于理解复杂的多模态场景或问题。\n\n### 多模态\n\n1. [clip](https://openai.com/research/clip) [multimodal-learning 中文解析](https://imzhanghao.com/2022/10/27/multimodal-learning/)\n\n## 排名\n\n1. [stanford helm leaderboard](https://crfm.stanford.edu/helm/lite/latest/#/leaderboard)\n1. [SuperCLUE 中文通用大模型综合性基准](https://github.com/CLUEbenchmark/SuperCLUE)\n1. [open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n1. [UC 伯克利发布大模型排行榜 需要查看 leaderboard](https://arena.lmsys.org/)\n\n## 数据集\n\n1. [laion](https://laion.ai/) 5B images, 图像生成训练集\n   - [online search](https://rom1504.github.io/clip-retrieval/?back=https%3A%2F%2Fknn.laion.ai&index=laion5B-H-14&useMclip=false)\n\n## huggine face\n\n1. [models](https://huggingface.co/models)\n1. [open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n1. `dataset = load_dataset(\"/mnt/data-2/home/xiyang.jia/TensorRT-LLM/examples/bloom/cnn_dailymail/cnn_dailymail.py\", \"3.0.0\")` 从本地加载数据集\n\n## sota models\n\n1. [gpt-4](https://openai.com/gpt-4)\n1. [llama](https://ai.meta.com/llama/)\n1. [palm2](https://ai.google/discover/palm2/)\n1. [claude-2](https://www.anthropic.com/index/claude-2)\n\n## [llama](https://github.com/facebookresearch/llama)\n\n1. 读音： 拉马（西班牙语通话的意思)\n1. clone 之后执行 download.sh, 需要官网申请的 url\n1. [demo_apps](https://github.com/facebookresearch/llama-recipes/tree/main/demo_apps)\n\n## gpt\n\n1. [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n1. GPT(generative pre-trained transformer)\n1. [rags](https://github.com/run-llama/rags) RAGs is a Streamlit app that lets you create a RAG pipeline from a data source using natural language.\n1. chatgpt：文字接龙\n1. gpt 自监督学习(pre train)得到的模型(基础模型)，chatgpt 在 gpt 基础上监督学习(finetune)\n1. chatgpt 实际是分类问题， 从使用者角度是生成式学习（生成句子：多个分类问题）\n1. chatgpt 评价是增强学习\n1. chatgpt: gpt -> 监督学习 -> 增强学习\n\n## [bing copilot](https://www.bing.com/)\n\n## precision 精度\n\n1. float32\n1. float16\n1. bfloat16\n   - bfloat16 更快，是因为它保留 float32 的动态范围，能避免很多精度陷阱，硬件实现简单，且能省去很多训练时的额外处理逻辑。\n   - bfloat16 本质上就是 float32 截掉低 16 位，只保留高 16 位，`因此可以快速转换`，不需复杂舍入。\n   - 与 float32 的指数格式兼容，转换无需复杂操作, 比如 float32 → float16 需要复杂舍入，而 float32 → bfloat16 可以直接位复制高16位。\n   - 转换速度更快，编译器/硬件优化更容易。\n   - 与 float32 拥有相同的 8 位指数，因此数值范围一样大。\n   \n1. 8bit\n1. 4bit\n1. GPTQ\n\n## links\n\n1. [LLMsPracticalGuide](https://github.com/Mooler0410/LLMsPracticalGuide)\n1. [oneflow 技术年货 2023](https://z7nobhiey2.feishu.cn/file/JTa0bZ38RohzwTx9zjucuDmunVe)\n1. [李宏毅 生成式 AI](https://www.youtube.com/watch?v=yiY4nPOzJEg&list=PLJV_el3uVTsOePyfmkfivYZ7Rqr2nMk3W)\n1. [Intro to Large Language Models](https://www.youtube.com/watch?v=zjkBMFhNj_g)\n1. [李沐论文精度](https://www.youtube.com/playlist?list=PLFXJ6jwg0qW-7UM8iUTj3qKqdhbQULP5I)\n1. [openai.com/research](https://openai.com/research/clip)\n"},{"title":"shareX","url":"/2023/11/15/shareX/","content":"\n## base\n\n1. 设置上传 url\n   ![设置上传url](https://i.ibb.co/3C66vgd/UQTv-UKxj-Vi.png)\n\n## 设置自定义服务器\n\n1. [imagebb 生成 key](https://api.imgbb.com/)\n   ![生成key](https://i.ibb.co/Dz3Xkyn/Jf4m-Fd-WXg-F.png)\n1. [自定义目标设置贴入，修改 key](https://github.com/ShareX/CustomUploaders/blob/master/imgbb.com.sxcu)\n   ![修改key](https://i.ibb.co/MM2mLg5/2z4l9e-PS2x.png)\n   ![key](https://i.ibb.co/TwmGMk8/BUHj-Zwd0ke.png)\n\n## links\n\n1. [shareX](https://getsharex.com/)\n1. [图片共享](https://imgbb.com/)\n1. [imgur](https://imgur.com/)\n"},{"title":"nvidia","url":"/2023/11/10/nvidia/","content":"\n## base\n\n1. [trends-in-machine-learning-hardware](https://epochai.org/blog/trends-in-machine-learning-hardware)\n1. `export CUDA_VISIBLE_DEVICES=0,1`强制使用哪几个 GPU 上\n1. sdk manager 图像界面安装各种工具\n1. fp32 -> fp16(int8)是否加速要看芯片手册\n1. 芯片手册中有 cuda core 算力和 tensor core 算力 [link](https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf)\n   - 3090 的 cuda core int32 算力是 fp21 的一半\n   - 3090 的 cuda core fp32 和 fp16 算力一样\n   - 3090 的 fp32 cuda core 和 tensor core 算力一样\n   - 3090 的 tensor core fp16 算力是 fp32 的 2 倍或 4 倍\n   - 3070 一样的现象\n\n## 查看 GPU info\n\n1. [cuda-sample](https://github.com/nvidia/cuda-samples) 见 cuda.md\n\n   - `cd /usr/local/cuda-12/samples/1_Utilities/deviceQuery && make && ./deviceQuery` 查看 gpu 详细信息,\n     - arch: 3080， 3090 是 sm_86, orin 是 sm_87\n     - sm num\n     - cuda core num / per sm\n     - Maximum number of threads per block\n     - Maximum number of threads per multiprocessor\n     - Max dimension size of a thread block (x,y,z)\n     - Max dimension size of a grid size (x,y,z)\n\n1. 通过对比 sm 个数等信息可以估算大致性能差距\n1. 查看 driver 版本 `cat /proc/driver/nvidia/version`\n\n### server\n\n1. `nvitop`\n1. `nvtop` 有风险\n1. [gpustat](https://github.com/wookayin/gpustat)\n   - 不要用源码装：否则 NVML 版本不匹配\n1. [nvidia-smi 命令参数含义](https://www.cnblogs.com/michaelcjl/p/16657548.html)\n1. `nvidia-smi` 可以看出每个 GPU 的显存总量和使用量 还可以看到 cuda 版本\n1. `nvidia-smi -L`列出 GPU 列表, 查看 GPU 先开显卡型号\n1. `nvidia-smi -l 1` 会每秒更新一次显示信息。`watch -n 1 nvidia-smi` 类似 top\n1. `nvidia-smi -q` 查看 GPU 信息\n1. `free -h` 看内存，注意服务器上内存和显存不一样, 不共享; PCIe 接在服务器上。 服务器市场的需求与集成设备（如 Jetson 系列）不同。服务器用户通常需要高度定制化的解决方案，可以独立升级和优化各个组件，而集成设备则更注重成本效益和空间效率。\n1. 加速器：不包含 arm\n\n## edge\n\n1. SOC: 包含 arm\n1. NVIDIA Jetson 系列设备，包括像 Jetson Nano、Jetson TX1/TX2、Jetson Xavier、 Jetson Orin 等，使用的是 NVIDIA 的 `Tegra` 架构。在这种架构中，CPU 和 GPU 确实共享内存，这被称为`统一内存（Unified Memory）或共享内存`。\n1. 统一内存的优缺点：[link](https://chat.openai.com/c/c4285778-9988-4bba-80b2-0ae08689af88)\n   - 优点：简化编程模型， 提高效率\n   - 缺点：内存访问延迟，带宽限制，复杂的内存管理\n1. tegra 是 SOC 架构\n1. `jtop` [jetson_stats](https://github.com/rbonghi/jetson_stats) jetson-stats is a package for monitoring and control your NVIDIA Jetson [Orin, Xavier, Nano, TX] series.\n1. `jetson_release`\n\n## 指标\n\n1. [gpu specs](https://www.techpowerup.com/gpu-specs/)\n\n### performance\n\n1. `FLOPS = 2 x freqs * cores` fma 有 2 个 flop， f32 性能 cores 就是 f32 cuda core number\n\n### memory\n\n1. `B/s = bps * width / 8`\n\n### pcie\n\n## 产品\n\n1. 性能参数：产品 -> 规格\n1. [边缘计算产品](https://www.nvidia.cn/autonomous-machines/embedded-systems/) 查看模组对比， 可以看到各个芯片的参数\n   - [jetson roadmap](https://developer.nvidia.com/embedded/develop/roadmap)\n1. 数据中心： A, H, L, V 系列\n   - [H100 性能参数](https://www.nvidia.cn/data-center/h100/) 游戏：RTX, GTX\n   - RTX: Ray Tracing eXtreme\n   - GTX: Graphics Processor protoType eXtreme\n   - [GeForce 显卡比较](https://www.nvidia.cn/geforce/graphics-cards/compare/?section=compare-specs)\n   - 在 NVIDIA 的 GeForce 系列显卡中，Ti 是一个特殊的标识，用来表示某个型号的改进或升级版。从本意来讲，TI 属于'Titanium'的缩写，和太空金属'钛'是一个意思，在显卡中他代表着更高级版本。\n\n## 架构\n\n1. 搜 white paper 看细节\n1. [ampere-architecture 官方架构详细介绍](https://www.nvidia.com/en-us/design-visualization/ampere-architecture/)\n1. [hopper-architecture](https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/)\n   - 下方的白皮书\n   - 手册中有芯片 cuda core 算力和 tensor core 算力\n   - 利用 tensor core 才能达到最大算力\n   - int32 不一定有 float32 算力高，因为 float32 计算单元更多\n   - 多关注 SM 架构\n     ![sm](https://i.ibb.co/2c186WT/g-LTQhd-K40g.png)\n1. [Hardware Model](https://docs.nvidia.com/nsight-compute/2023.3/ProfilingGuide/index.html#metrics-hw-model)\n   - 对 sm 介绍比较好\n1. 可以从架构图上看出一个 SM 有多少个 cuda core\n1. [cuda-compiler-driver-nvcc gpu-feature-list](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list)\n1. [https://blog.csdn.net/kunhe0512/article/details/126247243](https://blog.csdn.net/kunhe0512/article/details/126247243)\n1. [NVIDIA GPU 架构梳理](https://zhuanlan.zhihu.com/p/394352476)\n1. 天梯图\n1. 架构演进\n1. tegra 是 SOC 架构, tesla 是 GPU 架构\n1. NVIDIA GeForce RTX 3090 使用的是 NVIDIA 的 Ampere 架构 sm_86\n1. NVIDIA Jetson Orin 是基于 NVIDIA 的 Ampere 架构的高性能 AI 计算平台\n1. CUDA_ARCHS 是指定 CUDA 代码编译目标 NVIDIA GPU 架构的编译标志。\n1. [为各种 NVIDIA 架构匹配 CUDA arch 和 CUDA gencode](https://zhuanlan.zhihu.com/p/631850036)\n1. 架构历史\n1. [understanding-nvidias-datacenter-gpu-line](https://www.baseten.co/blog/understanding-nvidias-datacenter-gpu-line/)\n   ![历史](https://i.ibb.co/hZVxQFd/ZEI0-D9zw-C5.png)\n\n## [JetPack](https://docs.nvidia.com/jetson/jetpack/introduction/index.html)\n\n1. NVIDIA JetPack SDK is the most comprehensive solution for building AI applications. JetPack SDK provides a full development environment for hardware-accelerated AI-at-the-edge development.JetPack SDK includes Jetson Linux Driver Package with bootloader, Linux kernel, Ubuntu desktop environment, and a complete set of libraries for acceleration of GPU computing, multimedia, graphics, and computer vision. It also includes samples, documentation, and developer tools for both host computer and developer kit, and supports higher level SDKs such as DeepStream for streaming video analytics, Isaac for robotics and Riva for conversational AI.\n\n## [vpi](https://docs.nvidia.com/vpi/algorithms.html)\n\n1. 支持多种硬件做前处理, 如 resize, convert color, remap (VIC 都支持，可以释放 GPU 资源)\n"},{"title":"package manager","url":"/2023/11/05/package-manager/","content":"\n## C++\n\n### [CPM](https://github.com/cpm-cmake/CPM.cmake)\n\n## python\n\n### [virtual-environments](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/)\n\n```sh\nsudo apt install python3-venv\npython3 -m venv .venv\nsource .venv/bin/activate\ndeactivate\npip install pip -U -i https://pypi.tuna.tsinghua.edu.cn/simple\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\npip install -r requirements.txt\npython3 -m pip install --upgrade pip\npython3 -m pip install 'requests==2.18.4'\npython3 -m pip install 'requests>=2.0.0,<3.0.0'\n```\n\n### [miniconda](https://docs.conda.io/en/latest/miniconda.html#linux-installers) 选择对应 python 版本, install 时可以选路径\n\n### [anaconda](https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/)\n\n### [mamba](https://github.com/mamba-org/mamba)\n\n## rust\n\n### cargo\n\n## multi\n\n### [pixi](https://github.com/prefix-dev/pixi)\n\n1. pixi is a cross-platform, multi-language package manager and workflow tool built on the foundation of the conda ecosystem. It provides developers with an exceptional experience similar to popular package managers like cargo or yarn, but for any language.\n"},{"title":"pixi","url":"/2023/11/03/pixi/","content":"\n## base\n\n1. 不同工程可以用软连接共享.pixi 文件夹\n1. cfonts 使得 task 更清楚\n1. `curl -fsSL https://pixi.sh/install.sh | bash` install\n1. 工程目录下`.pixi`包含各种环境依赖\n1. pixi is a cross-platform, multi-language package manager and workflow tool built on the foundation of the conda ecosystem. It provides developers with an exceptional experience similar to popular package managers like cargo or yarn, but for any language.\n1. Supports multiple languages including Python, C++, and R using Conda packages.\n1. 可用于 github actions, 只需要配置 pixi，不用手动安装各种环境\n1. 参考写 task [example](https://github.com/prefix-dev/pixi/blob/main/examples/cpp-sdl/pixi.toml)\n1. 如果下载不了 package， 可以删除~/.cache/rattler/cache/repodata 试试\n1. 执行命令在 repo 下都行\n1. `pixi global install <cmake>`在机器上安装 cmake; 安装到~/.pixi/bin， 需要需改.zshrc `export PATH=~/.pixi/bin:$PATH`\n1. `pixi shell`中可以用 pixi 安装的最新命令\n1. `pixi add gxx clangxx` install g++ and clang++； ++符号不允许\n1. 下载的时候 VPN 设置为 global， 用 rule 可能识别为本地， 下载出问题\n1. python 需要先安装 pip\n\n## [command](https://prefix.dev/docs/pixi/cli)\n\n1. `pixi init <name>` 初始化工程, 添加 pixi.toml 和 gitignore 信息\n1. `pixi add cmake ninja opencv glog`添加依赖库\n1. `pixi add \"clang-tools<=15.0.0,>13.0\"`\n1. `pixi run <task>` run task\n1. `pixi search glog` search package\n1. `pixi info` 查看各种信息， cache 位置：~/.cache/rattler/, task list\n1. `pixi global list` list global package\n1. `pixi shell` 进入 pixi shell， 执行/tmp/pixi_env_xxx.sh 脚本设置环境变量， 可以在配置文件 activate 脚本中使用\n\n## [config](https://prefix.dev/docs/pixi/configuration)\n\n1. pixi shell 可以查看环境变量\n1. channels = [\"conda-forge\", \"robostack\", \"bioconda\", \"nvidia\", \"pytorch\"]; 设置源 [link](https://prefix.dev/channels)\n1. env set, 可以设置 LD_LIBRARY_PATH\n\n```yaml\n[activation]\nscripts = [\"env_setup.sh\"]\n```\n\n## env\n\n1. `PIXI_PACKAGE_ROOT` repo 目录\n1. `CONDA_PREFIX` pixi env 目录 ${PIXI_PACKAGE_ROOT}/.pixi/env\n1. `PIXI_PACKAGE_PLATFORMS` platform 类型\n1. `PATH` 只有 PATH, 没有 LD_LIBRARY_PATH, 需要自己在配置文件 activate 脚本里设置\n\n```yaml\nexport LD_LIBRARY_PATH=${CONDA_PREFIX}/lib  # for run app\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX}/lib  # for cmake find_package\nexport PKG_CONFIG_PATH=${CONDA_PREFIX}/share/pkgconfig:${CONDA_PREFIX}/lib/pkgconfig  # for pkg-config\n```\n\n## links\n\n1. [pixi](https://github.com/prefix-dev/pixi)\n1. [What is the difference with conda, mamba, poetry, pip](https://github.com/prefix-dev/pixi/blob/main/docs/FAQ.md)\n1. [cpp using example](https://www.rerun.io/docs/howto/arrow-cpp-install)\n1. [rerun cpp-example-opencv-eigen pixi example](https://github.com/rerun-io/cpp-example-opencv-eigen/blob/main/pixi.toml)\n"},{"title":"IREE","url":"/2023/10/16/IREE/","content":"\n## links\n\n1. [iree](https://www.iree.dev/)\n","tags":["AI","IR","Compiler"],"categories":["AI","IR"]},{"title":"SPIR-V","url":"/2023/10/16/SPIR-V/","content":"\n## links\n\n1. [spir](https://www.khronos.org/spir/)\n","tags":["AI","IR","Compiler"],"categories":["AI","IR"]},{"title":"MLIR","url":"/2023/10/12/MLIR/","content":"\n\n## transform conversion translation\n1. transform: dialect内转换\n1. conversion: dialect之间转换\n1. tranlation: MLIR 和外部格式之间的转换\n\n### [DialectConversion](https://mlir.llvm.org/docs/DialectConversion/)\n1. [郑思泽](https://zhuanlan.zhihu.com/p/3453025905)\n1. dialect之间转化的时候才会用到typeconverter, 相同dialect type不变\n1. 在tollvm时可以在matchandwrite函数中通过typeconverter拿到要转换的结果，和转换的结果做比对\n1. 每个pass typeconverter 会转换所有满足要求的type\n   - to llvm时第一个pass typeconvert会先加unrealized_conversion_cast from type to llvm_struct, 后面真正lowering op的时候会pack到llvm_struct, 再添加unrealized_conversion_cast from llvm_struct to type, 最终消除unrealized_conversion_cast\n1. convertOpResultTypes(...)只改变输出type(sample: block level to thread level, remove tensor encoding) 主要完成两件事：\n   - 调用 TypeConverter::convertType 把结果类型从非法的转换为合法的\n   - 收集转换后的结果类型，供新 op 使用\n1. RewritePattern: 一般 IR 的局部优化 / 重写； 不涉及类型转换， 不需要typeconverter; 主要目的：替换一个 op → 另一个等价 op\n1. ConversionPattern：类型转换场景（type conversion）, 专门用于类型转换, 需要 TypeConverter, 主要目的：替换不合法类型 → 合法类型的 op\n   - ConversionPattern; 转换所有op, 可以配合convertOpResultTypes（）一起使用, struct ConvertAllOpResultsPattern : public mlir::ConversionPatternstruct ConvertAllOpResultsPattern : public mlir::ConversionPattern\n   - 子类 OpConversionPattern；转换一种op; sample: struct ConvertMyAdd : public OpConversionPattern<MyAddOp>\n1. typeconverter.isLegal(op); 需要转换的类型不合法\n   - 判断某个类型是否合法（能否通过 type converter 转换）:typeConverter.isLegal(type);\n   - 判断某个 op 的 operands/results/region param 类型都合法: typeConverter.isLegal(op);\n   - 配合target使用, 需要转换的标记为不合法，不用单独写判断逻辑; 在 ConversionTarget 中使用 isLegal(op) 的最大好处是：✅ 让合法性判断自动化、类型驱动、覆盖全面;\n     - 不用列举所有非法 op，只要类型非法，op 就非法; \n     - 自动遍历 op 的 operand/result/region param;\n     - 合法性逻辑与类型转换保持一致，避免重复维护；\n     - 只要你更新 TypeConverter，合法性判断逻辑自动变化; \n     - 能对整个 dialect 应用统一规则，而不是逐个 op 列表维护\n\n   ``` c++\n    addDynamicallyLegalDialect<triton::TritonDialect, tensor::TensorDialect,\n                               triton::gpu::TritonGPUDialect,\n                               arith::ArithDialect, math::MathDialect>(\n        [&](Operation *op) -> std::optional<bool> {\n          return converter.isLegal(op);\n        });\n    }\n   ```\n\n## ops\n1. zext\tZero extension\t无符号扩展，高位补0\n1. sext\tSign extension\t有符号扩展，高位补符号位\n1. trunc\tTruncate\t截断高位\n\n\n## call intrinsic\n1. 使用\n\n``` c++\nmlir::LLVM::CallIntrinsicOp\ncallLLVMIntrinsic(mlir::OpBuilder &rewriter, mlir::Location loc,\n                  mlir::TypeRange returnType, llvm::StringRef intrinsicName,\n                  llvm::SmallVector<mlir::Value> operands, bool usePoison) {\n\n  if (usePoison)\n    operands.insert(operands.begin(),\n                    rewriter.create<mlir::LLVM::PoisonOp>(loc, returnType));\n\n  return rewriter.create<mlir::LLVM::CallIntrinsicOp>(\n      loc, returnType, rewriter.getStringAttr(intrinsicName), operands,\n      mlir::LLVM::FastmathFlagsAttr());\n}    \n```\n1. vector返回需要poisonOp占位，PoisonOp 创建一个“未定义但确定非法使用会导致未定义行为”的值，常用于初始化向量或寄存器值，告诉编译器“这个值现在还没初始化”。\n\n``` c++\n      mlir::Value poison = rewriter.create<mlir::LLVM::PoisonOp>(loc, gatherResVecType);\n```\n\n\n## builder and rewriter\n1. builder 和 rewriter 都用于构建和修改 IR（中间表示），但它们的用途和设计目标不同。\n1. OpBuilder 适用于构建新的 IR，但不会管理旧 IR。\n   - builder.setInsertionPointToEnd(&block); 需要指定insert节点\n1. PatternRewriter 适用于修改 IR，可以删除、替换和优化现有操作，在 MLIR Pass 变换中更常用。\n\n``` c++\n  Operation *op = getOperation();\n  OpBuilder builder(op->getContext());\n  IRRewriter rewriter(builder);\n```\n\n## layout\n1. blocklayout (tb级别)\n1. threadlayout (thread级别)\n1. ir也分block和thread level的\n\n## [dialects](https://mlir.llvm.org/docs/Dialects/)\n1. 注意op和oprand的区别, Oprand和type的区别\n1. [What is the difference between the Tensor and Vector types? ¶](https://mlir.llvm.org/getting_started/Faq/#what-is-the-difference-between-the-tensor-and-vector-types)\n   - Tensors accept an optional “encoding” attribute, vector don’t at the moment.\n   - The set of allowed element types is different: the Tensor type isn’t limited while Vector is limited to float and integer types.\n   - You can have a memref (a buffer in memory) containing Vectors but you can’t have a memref of a tensor type.\n\n### tensor\n1. 注意tensor dialect, tensor type(oprand type), tensor op区别\n1. [tensor type](https://github.com/llvm/llvm-project/blob/51c91095ab5b1f8f0d65e6fbd551e8b991866b3f/mlir/include/mlir/IR/BuiltinTypes.h#L55)\n1. [tensor dialect](https://mlir.llvm.org/docs/Dialects/TensorOps/)\n1. tensor type是一种builtin type， 不是tensor dialect中定义的\n1. Tensor Dialect 提供了一组针对 张量操作 的通用抽象，例如 切片（slice）、拼接（concatenate）、广播（broadcast）等。\n\n### vector\n1. 当前上游的多级中间表示（MLIR，Multi-Level Intermediate Representation）中的向量方言（Vector Dialect）采用的是定长向量表示。\n1. 注意type和dialect区别\n1. [vector定义](https://github.com/llvm/llvm-project/blob/f71b83b3593588c56fd4ab3e1347ad9c7bec624f/mlir/include/mlir/IR/BuiltinTypes.td#L1241)\n   - 用llvm tablegen 生成\n1. [vector dialect](https://mlir.llvm.org/docs/Dialects/Vector/)\n\n## AsmPrinter\n1. AsmPrinter 是 MLIR（Multi-Level Intermediate Representation）中的一个 IR 序列化（Serialization）工具，用于将 MLIR 操作（Operation）、类型（Type）、属性（Attribute） 等以 可读的文本格式（assembly format） 输出。它主要用于 调试、代码生成和 IR 可视化。\n1. [别名 Customizing Assembly Behavior](https://mlir.llvm.org/docs/DefiningDialects/Assembly/)\n   - If multiple types/attributes have the same alias from getAlias, a number is appended to the alias to avoid conflicts.\n1. generic assembly format\n   - is the one shown when breaking down toy.transpose at the beginning of this chapter. \n1. custom assembly format\n   - either declaratively or imperatively via C++. Defining a custom assembly format allows for tailoring the generated IR into something a bit more readable by removing a lot of the fluff that is required by the generic format.\n\n\n## [attributes and types](https://mlir.llvm.org/docs/DefiningDialects/AttributesAndTypes/)\n1. [别名](https://mlir.llvm.org/docs/DefiningDialects/Assembly/)\n1. 允许方言为属性(Attribute)和类型(Type)定义别名标识符,在打印文本形式的 IR 时使用\n   - 使用OpAsmDialectInterface方法 AliasResult getAlias(Attribute attr, raw_ostream &os) \n   - AliasResult getAlias(Type type, raw_ostream &os)\n1. tablegen生成的类通过get()创建对象\n1. 注意: 给op加参数和给tensor加属性不一样\n1. cuda通过给tensor加layout属性来确定线程处理的数据\n1. set： `op->setAttr(\"tts.target\", StringAttr::get(op->getContext(), \"TILE\"));`\n1. tablegen op定义的时候直接带属性\n1. RankedTensorType\n   - tenosr带的属性影响整个op的验证：tt.load只改变输出tensor的属性，会报错，ptr指向的数据类型和输出数据类型的attr需要一致； tt.addi输入输出tensor的encoding(attr)需要一致, layout变换需要加转换op, 如triton::gpu::ConvertLayoutOp\n   - static get(ArrayRef<int64_t> shape, Type elementType, Attribute encoding = {}); 创建对象时设置属性,见 \n     - [vector定义](https://github.com/llvm/llvm-project/blob/f71b83b3593588c56fd4ab3e1347ad9c7bec624f/mlir/include/mlir/IR/BuiltinTypes.td#L1241)\n   - getEncoding() 获取属性\n   - 一旦创建后，tensor的属性就是不可变的(immutable)\n   - 如果需要修改属性，需要创建一个新的tensor类型\n1. VectorType\n   - 没有类似的属性获取方法,它更关注向量的形状、元素类型和可扩展性\n   - static VectorType get(ArrayRef<int64_t> shape, Type elementType, ArrayRef<bool> scalableDims = {});\n     - scalableDims参数用于指定向量类型中哪些维度是可伸缩的（scalable）\n     - 可伸缩向量（Scalable Vector）是一种其大小在运行时才能确定的向量类型, \n     - VectorType::get({2, 4}, elementType, {false, true}); //创建一个2x[4]的向量，其中第二维是可伸缩的\n     - 可伸缩向量主要用于支持SIMD（单指令多数据）等向量化操作, 这种设计使MLIR能够更好地支持现代处理器的向量化特性，特别是那些支持可变长度向量处理的架构\n\n\n## translate\n1. `mlir-translate --mlir-to-llvmir lowered_example.mlir > output.ll` mlir to llir\n\n## tools\n1. lsp server\n1. tablegen lsp server\n1. mlir-opt\n\n## note \n1. add_mlir_tool默认不编译, 需要手动指定\n   - ninja -t targets && ninja xxx\n   - make help && make xxx\n\n## [mlir-tutorial](https://github.com/KEKE046/mlir-tutorial)\n1. ex3 - ex7不能同时运行， CMakeLists.txt只能选一个\n1. 需要单独指定编译ninja toy-opt(target)\n\n\n## MLIR基本构成\n1. [Operation](https://github.com/KEKE046/mlir-tutorial/blob/main/README.md#41-op-%E7%9A%84%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2) \n   - 用户定义的 arith.addi 等等 Op，本质上都是 Operation 的指针。但与 Operation* 不同的是，AddIOp 定义了 Operation 里储存的数据的解释方式。如 AddOp，自己是一个 Operation 的指针，也定义了一个函数 getLhs 用来返回第一个值，当作 lhs。\n1. Operations can also have custom parser, printer, builder, verifier, constant folder, or canonicalizer.\n1. Operation类用于对所有操作进行一般建模。它是“不透明的”，因为它不描述特定操作或操作类型的属性。相反， Operation类为操作实例提供通用 API。另一方面，每个特定类型的操作都由Op派生类表示。   \n1. Type / Attribute: MLIR 的 Type 和 Attribute 与 Op 类似。Type 是到 TypeStorage 的指针，Attribute 也是到 AttributeStorage 的指针。\n1. Value 要么来自于 Operation 的 Result 要么来自于 BlockArgument\n   - op管理result value， result可以作为其他op的input（引用), 类似模型推理时的tensor管理\n1. Value有owner:\n   - tt.dot的输入oprand owner是tt.load, gdb打印显示  owner = 0x0000000032661040 \"tt.load\", tt.dot的输出Value owner是tt.dot op\n1. Result：这个 Op 生成的新 Value\n1. Operand：这个 Op 接受的操作数, 每个 Operation 的 Operand 都是到 Value 的指针\n1. [MLIR入门理解2-背景及演进](https://zhuanlan.zhihu.com/p/451061340)\n   - MLIR提供的解决方案是Dialect和DialectConversion。Dialect用来抽象opset，DialectConversion用来做转换。\n   - 接着为了增加对Dialect和Operation的标准化和功能扩展，MLIR增加了Constraint, Interface, Trait，方便对Operation进行限制和扩展。\n1. 每个 op（操作） 都有两类非常重要的机制：verifier 和 builder。它们的作用分别是 校验操作的合法性 和 简化操作的构造。\n### IR Verifier\n1. Verifier 是用来检查一个操作实例是否满足该操作的语义约束。它会在 IR 构建或变换后运行，用来确保这个操作合法、内部一致，而不是 “乱七八糟的” IR。\n1. 在没有注册 dialect/operation 之前，MLIR 结构上会接受任意字符串作为操作名，可以 round-trip（解析 + 打印）未定义操作。但是这种 IR 可能是语义上不合法的（比如参数数量错误、返回类型不对等），Verifier 就是负责捕捉这类问题的机制。\n1. 当你定义一个 operation（比如 toy.constant）时，可以：\n   ✔ 自动根据 Operation Definition Specification（ODS） 生成结构层面的约束检查\n   ✔ 或者自己定义一个更复杂的 verify 方法来执行额外的检查 let hasVerifier = 1;\n   \n### builder\n1. Builder 提供一种 简洁、安全 的方式来创建这个 operation。\n1. 一个 helper，让你通过抽象参数就能生成合法的操作实例\n1. 可直接用于编译器前端、Pass、Dialect 创建代码等\n1. 构造 MLIR operation 要指定：操作名称，输入 operands等参数， 如果手动组装，会异常繁琐，而且容易出错。builder 自动帮你填充 OperationState 的内容\n### Parser\n\n### Interface\n1. Interface 是一种“基于 trait 的多态机制”，允许 dialect 中的 op/type 实现一组统一的行为接口，而不需要相互依赖。\n1. Interface 定义只定义：接口名字，namesapce, 接口有哪些方法，不关心谁来实现\n   - 不关心 op 是谁; 只关心：是不是实现了接口\n1. op 在pass里使用interface\n   - 直接op.func()调用\n   - `if (auto shapeOp = dyn_cast<ShapeInferenceOpInterface>(op))`\n1. 注意traits和interface的区别\n   - interface有多态，有方法\n   - DeclareOpInterfaceMethods<CastOpInterface> 可以把interface转换为trait\n   \n### Trait\n1. Trait 是给 Op / Type / Attribute 打的“静态标签”，用于表达“这个东西满足某种结构或语义约束”，但不提供行为函数。\n1. 它本质是 编译期/结构级别的约束与标记机制。\n1. 在 MLIR 里，你经常需要表达：这个 op 没有副作用;这个 op 只有一个结果;这个 op 是 terminator;这个 op 结果类型和输入类型一致;这个 op 是 elementwise 的;\n   - 这些都是： “事实 / 性质”，不是“行为算法”\n1. Trait影响verifier, conconiacalization, folding\n\n## 思考\n1. 不好实现的先站位，从高层网低层次转， 抽象级别要对应，一次处理一小步\n1. 核心在于transform\n1. 上层的IR是一种抽象，不用具体实现，只需要规定输入输出类型，前后Op能匹配上就行\n1. 每一行都是op, 在lower到llvm ir需要具体实现，上层的op不需要具体实现，只要后面能lower到低级别的op\n\n\n## dialect conversion\n1. [docs/DialectConversion](https://mlir.llvm.org/docs/DialectConversion/)\n1. 不合法才转换\n1. 类型转换的时候，如果op输入输出类型变换，可以在定义op输入输出时允许多种类型\n\n## pass\n1. ConversionTarget里合法的不会在做matchandrewrite, 需要标记为非法才做matchandrewrite\n1. 注意op和pass的区别；op可以为load, store, convert_tile_to_vector; pass用来转换的，将其他op转换，注意：convert_tile_to_vector是op，不是pass\n1. 写pass时如果输入输出类型改变，插入转换op，后面再融合; 方便pass对op处理(使用unrealized_convertion_cast)\n   - A --t--> B   ====>  A1 --t1-> convert_t1_to_t --t-> convert_t_to_t2 --t2-> B2\n1. [Tutorial: Create an MLIR pass for triton](https://drive.google.com/file/d/1YTGfJq-ccuVPH50r8ERnCrC977-4uzly/view)\n1. 区分op和op pass; op是一种表示，pass是对op的处理(conversion or optimization); conversion指的是从其他op转换为当前op；当前op转换为其他op由后续的op处理\n1. op tablegen需要指定name, input, output及其类型\n1. Trasnform(optimize) [link](https://blog.csdn.net/2301_76434200/article/details/136205302)\n   - in dialect\n   - Transform 通常指的是在 同一 dialect 内对程序进行优化或转换。这个过程目的是通过简化、优化或改进表达式来提升程序的性能或质量，通常是通过应用特定的优化规则来消除冗余、合并操作、简化控制流等。\n   - 写pass不需要像conversion需要ilegal一些op才能转换\n   - 可以用于添加属性之类的操作\n1. Conversion\n   - between dialect\n   - Conversion 指的是将一个 Dialect 中的 IR 转换为另一个 Dialect 的 IR。这种转换通常涉及对同一程序的不同表示的转换，目的是为了适应不同的执行环境或优化策略。转换通常发生在不同的 Dialect 之间，或者从高层次的抽象操作到底层硬件相关的操作。\n   - NOTE: 注意op convertion(一种transform) 和 type convertion(只改变op输入输出类型)之间的区别\n1. translation\n   - other ir -> mlir -> other ir\n   - Translation 是指将 从其他 IR 转换到 MLIR 或从 MLIR 转换到其他 IR 的过程。它允许将一种编译表示（IR）转化为另一种表示。翻译通常发生在不同编译体系结构或编译链的之间，可以涉及从一种中间表示到另一种表示的转换，或者从源代码到中间表示的转换。\n1. CSE（Common Subexpression Elimination） \n   - 是一种常见的优化 Pass，中文称为公共子表达式消除。\n   - CSE 的主要目的是识别并消除 IR（中间表示）中重复的子表达式，从而减少冗余计算，提升运行效率。\n\n``` c++\nint a = x + y;\nint b = x + y;\n\n// cse\nint temp = x + y;\nint a = temp;\nint b = temp;\n```\n##  [Canonicalize](https://mlir.llvm.org/docs/Canonicalization/)\n1. Canonicalizer 知道清理死操作；然而，MLIR 保守地假设操作可能会产生副作用。我们可以通过向TransposeOp添加新特征Pure来解决此问题 [link](https://mlir.llvm.org/docs/Tutorials/Toy/Ch-3/)\n1. [两种方法](https://mlir.llvm.org/docs/Canonicalization/#defining-canonicalizations)\n1. 可以做类型转换消除优化\n1. 是一个标准的优化 Pass，用于对中间表示 (IR) 进行规范化（Canonicalization）。它的主要目的是通过应用一系列模式匹配和重写规则，简化 IR，去掉冗余操作，提升后续优化和代码生成的效率。\n1. 其主要作用是清理冗余、提升 IR 的可优化性和一致性，(不能有side effect)\n\n``` \n// op.td里op添加\nlet hasCanonicalizeMethod = 1;\n\n// op.cpp中完成canonicalize函数\nLogicalResult ConvertTensorToTileOp::canonicalize(ConvertTensorToTileOp op,\n                                                  PatternRewriter &rewriter) {\n  // patterns and rewrites go here.\n  Value input = op.getOperand();\n  auto tileToTensorOp =\n      input.getDefiningOp<ConvertTileToTensorOp>();\n  if (!tileToTensorOp)\n    return failure();\n\n  rewriter.replaceOp(op, tileToTensorOp.getOperand());\n  return success();\n  // return failure();\n  \n// 先replace， 再modify， 在erase《\n\n  * Pattern  : 'tts.convert_tensor_to_tile -> ()' {\nTrying to match \"\"\n    ** Replace : 'tts.convert_tensor_to_tile'(0x6a3fc50)\n    ** Modified: 'tts.tile_dot'(0x8590110)\n    ** Erase   : 'tts.convert_tensor_to_tile'(0x6a3fc50)\n\"\" result 1\n  } -> success : pattern applied successfully\n}\n```\n\n   \n   \n\n## mlir-opt\n1. `mlir-opt input.mlir -canonicalize -o output.mlir`\n1. `-debug` 选项，可以打印 Pass 的详细执行过程\n1. `-verify-each` 验证每个 Pass 运行后 IR 是否有效\n\n\n## IR表示\n1. [从零开始教你写一个MLIR Pass](https://www.cnblogs.com/BobHuang/p/18249482)\n1. [UnderstandingTheIRStructure](https://mlir.llvm.org/docs/Tutorials/UnderstandingTheIRStructure/)\n1. [MLIR_Tutorial](https://llvm.org/devmtg/2020-09/slides/MLIR_Tutorial.pdf)\n1. [mlir-tutorial](https://github.com/KEKE046/mlir-tutorial?tab=readme-ov-file#5-mlir-%E7%9A%84%E5%9B%BE%E7%BB%93%E6%9E%84)\n\n## match and rewrite\n\n1. (good)[debug](https://mlir.llvm.org/docs/PatternRewriter/#debug-names-and-labels)\n1  \n1. 内部参数不要直接使用`Value lhs = op.getOperand(0);` 获取的是转化之前的value， \n   - `auto lhs = adaptor.getOperands()[0];`\n   - `auto lhs = rewriter.getRemappedValue(op.getOperand(0));`\n   - op 是 TileDotNoaccOp 的实例，它代表 MLIR 中的原始操作（原始 IR）。在转换过程中，op 仍然保持着 MLIR 中定义的 TileDotNoaccOp 操作的结构，并可以直接访问它的操作数、属性等。\n   - adaptor 是 OpAdaptor 类型，它用于适配 MLIR 转换过程中的操作数（operands）。\n     - 在 MLIR 低级转换（如从 Triton 低级 IR 到 LLVM IR）时，操作数可能已经经过类型转换或其他调整，而 adaptor 允许获取转换后的操作数。\n     - adaptor.getOperands() 通常用于获取转换后的操作数，而不是 op.getOperands()\n   - 在实际编写重写模式时，通常会同时使用 op 和 adaptor，前者用于获取操作的上下文信息，后者用于访问已转换的操作数和属性。  \n1. Canonicalizer 知道清理死操作；然而，MLIR 保守地假设操作可能会产生副作用。我们可以通过向TransposeOp添加新特征Pure来解决此问题\n1. 当你使用 rewriter.replace 替换一个操作时，原操作会被标记为 \"无效\" 或 \"已被替换\"，但在实际的 MLIR IR 中，它的内存依然存在，直到清理阶段（如垃圾回收或优化 passes 的清理过程）才会被真正删除。\n1. `Value newvalue = rewriter.create<AddOp>(loc, input0, input1);` rewriter.create 是 MLIR 中 PatternRewriter 类提供的方法，用于创建新的操作（Operation）。\n1. `void replaceOp(Operation *op, ValueRange newValues);` rewriter.replace将op的结果用new value替换, 作用是将一个旧的操作及其结果用新的值替换，同时自动更新使用这些结果的其他操作，确保 IR 的一致性。\n\n## base\n1. populateXXXPatterns()\t是一种注册/收集多个 rewrite pattern 的函数，将 pattern 添加到 pattern list 中\n   - 用于标准 dialect lowering， patterns.add用于自己的dialect lowering\n   - 可用于to LLVM dialect\n   - RewritePattern\t是一个具体的转换规则（转换逻辑 + 匹配条件），用来定义“怎么把 A 转换成 B”\n   - pass可以一次执行所有收集到的pattern;也可以只执行一个或多个\n   - populate...只是收集的作用, 在一个pass里执行多个pattern; 一个pass执行的好处是能控制转换顺序；多个pass\n   - lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp poplate了所有op，按顺序改变op输出type; 输入是adaptor.getOperands()，因为按顺序，表明输入之前已经转换为输出type了\n``` c++\nRewritePatternSet patterns(ctx);\n\n// 1. 标准 dialect lowering\npopulateArithToLLVMConversionPatterns(converter, patterns);\npopulateFuncToLLVMConversionPatterns(converter, patterns);\n\n// 2. 自己的 dialect lowering\npatterns.add<\n  LowerMyDialectOp1,\n  LowerMyDialectOp2\n>(ctx);\n```\n   \n1. MLIR 是一个强有力的代表，但它也有非目标。我们不尝试支持低级机器代码生成算法（例如寄存器分配和指令调度）。它们更适合较低级别的优化器（例如 LLVM）。此外，我们不希望 MLIR 成为最终用户自己编写内核的源语言（类似于 CUDA C++）。另一方面，MLIR 为表示任何此类 DSL 并将其集成到生态系统中提供了支柱。\n1. 在 MLIR（多级中间表示）中，ODS 是 \"Operation Definition Specification\" 的缩写。ODS 是一种用于定义操作（operation）、类型（type）、属性（attribute）等的 DSL（领域特定语言）。它通过一种声明式的方式来描述这些元素，并生成相应的 C++ 代码，从而简化了编写和维护 MLIR 操作的过程。\n1. MLIR 即 Multi-Level Intermediate Representation，多级的中间表示。\n1. mlir 只是个编译器框架，本身并没有什么完整功能。所以可以参考一些基于 MLIR 做的开源项目。\n1. MLIR 能够很好地桥接高层 DSL 和底层编译器（如 LLVM）。\n1. we do not intend MLIR to be a source language that end-users would themselves write kernels in (analogous to CUDA C++). On the other hand, MLIR provides the backbone for representing any such DSL and integrating it in the ecosystem.\n1. `!` 在 MLIR 中，自定义类型通常由特定的方言定义，并使用 `!` 符号进行引用\n1. `#` 符号用于表示自定义属性或符号。这些属性或符号通常由特定的方言（Dialect）定义，用于描述复杂的数据结构、布局和操作。通过使用自定义属性，可以在编译过程中对特定的硬件架构和优化需求进行精确的描述和操作。\n\n## tablegen\n1. `assemblyFormat`注意有格式要求\n1. 使用mlir-tablegen，还有llvm-tablegen等其他tablegen\n1. class和def\n   - class 关键字用于定义可继承的类，而 def 关键字用于实例化具体的对象。\n   - 在 TableGen 中，def 语句创建的是一个 具体实例，而 class 语句定义的是 可继承的模板。通常，def 不能被继承，因为 def 本质上是一个实例，而不是一个类。\n1. `.td` 文件的名称来源是 \"Table Definition\" 的简称。这个命名反映了它的主要用途：定义表格式的数据结构，以支持代码生成和编译器相关的元信息处理。\n1. ODS(Operation Definition Specification) 模块用于定义operation\n   - 每个 op（操作） 都有两类非常重要的机制：verifier 和 builder。它们的作用分别是 校验操作的合法性 和 简化操作的构造。\n1. DRR(Declarative Rewriting Rule) 模块用于实现两个dialect之间的conversion\n1. .td文件定义dialect, op\n   - .h中需要include生成的.h.inc\n   - .cpp中需要include生成的.cpp.inc\n   \n\n## debug\n1. [debug tips](https://mlir.llvm.org/getting_started/Debugging/#miscellaneous-tips)\n   - For printf debugging, instead of using llvm::errs(), one can emit diagnostics. For example, using `op.emitWarning() << \"HERE: \" << myVariable`; instead of llvm::errs() << \"HERE: \" << myVariable << \"\\n\";. This prints nicely with colors, shows the op (and its location) for free,\n1. (good) lldb有问题调试，没问题直接命令行脚本执行更快\n1. vscode inlay hints可以方便看类型, 解决auto类型问题\n1. lldb\n   - `~/.lldbinit`\n   - [lldbinit](https://github.com/gdbinit/lldbinit)\n   - [pytorch .lldbinit](https://github.com/pytorch/pytorch/blob/main/.lldbinit)\n``` gdb\ncommand script import /data/liudonghui/llvm-project-86b69c3/llvm/utils/lldbDataFormatters.py\ncommand script import /data/liudonghui/llvm-project-86b69c3/mlir/utils/lldb-scripts/mlirDataFormatters.py\n#  \"command script import lldb.formatters.cpp.libcxx\",\nsettings set target.prefer-dynamic-value run-target\nsettings set target.display-runtime-support-values true\n\n# command script import  ~/lldbinit.py\n```\n1.  [codelldb](https://marketplace.visualstudio.com/items?itemName=vadimcn.vscode-lldb) 调试加速\n``` json\n        {\n            \"type\": \"lldb\",\n            \"request\": \"attach\",\n            \"name\": \"lldb\",\n            \"program\": \"/data/xiyang/miniconda3/envs/siorigin_triton_test/bin/python\",\n             \n                \"command script import /data/liudonghui/llvm-project-86b69c3/llvm/utils/lldbDataFormatters.py\",\n                \"command script import /data/liudonghui/llvm-project-86b69c3/mlir/utils/lldb-scripts/mlirDataFormatters.py\"\n            ]\n            ]\n        }\n```\n1. [lldb pretty print](https://github.com/vadimcn/codelldb/issues/574#issuecomment-1279891501)\n1. lldb debug command:\n   - `command script import /data/xiyang/llvm-project-86b69c3/llvm/utils/lldbDataFormatters.py`\n   - `call op.dump()`\n   - `type lookup xxx`\n1. `(*src).dump()` ref的时候需要先取值\n1. vscode lldb debug 调试可以把 c++: on throw勾选去掉， \n1. (very good) `-exec pt xxx` 打印type\n   - 可以查看成员函数和变量，然后调用或者打印\n1. dump(): 支持module, op, type, value的打印\n   - str(mod) python更全面\n1. 打印op; vscode debug console: `-exec call op.dump()` 没有输出再调用`-exec call fflush(stderr)`， 输出在terminal里\n   - 改launch.json `\"terminal\": \"console\",` 在debug console输出\n   - 可以在terminal里按enter, 让多次输出之间有空行，方便查看\n   - 可以在watch里调用函数\n1. op->dump()：直接输出到 llvm::errs()，适合快速调试。\n   - python代码也可以\n1. op->print(llvm::errs())：需要提供一个输出流，可以更灵活地控制输出目标（如文件或标准输出）。\n1. pattern->getDebugName()\n1. 打印pass name: vscode中：`-exec call pass.getName()`\n\n``` c++\nfor (Pass &pass : pm.getPasses())\n    if (failed(run(&pass, op, am, verifyPasses, parentInitGeneration)))\n      return failure();\n```\n1. [gdb llvm mlir pretty print](https://lists.llvm.org/pipermail/llvm-dev/2016-May/100365.html)\n\n``` gdb\nsource /data/xiyang/llvm-project-86b69c3/mlir/utils/gdb-scripts/prettyprinters.py\nsource /data/xiyang/llvm-project-86b69c3/llvm/utils/gdb-scripts/prettyprinters.py\n```\n1. gdb 里直接`source .gdbinit` and then try again the `info pretty-print`. 可以看看pretty-print信息\n\n\n## CRTP\n1. [chatgpt](https://chatgpt.com/share/677b7d82-9e7c-8004-a823-244513e3e974)\n1. CRTP 的全称是 Curiously Recurring Template Pattern，翻译为“奇异递归模板模式”。\n\n``` c++\ntemplate <typename Derived>\nclass Base {\n    void func() {\n        static_cast<Derived*>(this)->impl();\n    }\n};\n\nclass Derived : public Base<Derived> {\n    void impl() {\n        // 派生类实现\n    }\n};\n```\n1. CRTP 是 C++ 中非常强大的设计模式，虽然对初学者稍显复杂，但在性能关键场景和库开发中极具价值。\n1. CRTP 是一种模板模式，其中一个类将自己作为模板参数传递给它的基类。\n看起来有些“奇怪”，但它非常强大，尤其在以下场景中：\n    - 静态多态：在编译时实现类似于动态多态的行为。\n    - 代码复用：通过基类模板实现通用功能。\n    - 编译期优化：避免运行时的虚函数调用开销。\n1. 静态多态: 编译期确定类型，无需虚函数调用。更高效，适合性能敏感场景。\n\n``` c++\n#include <iostream>\ntemplate <typename Derived>\nclass Shape {\npublic:\n    void draw() {\n        static_cast<Derived*>(this)->drawImpl();\n    }\n};\n\nclass Circle : public Shape<Circle> {\npublic:\n    void drawImpl() {\n        std::cout << \"Drawing Circle\\n\";\n    }\n};\n\nclass Square : public Shape<Square> {\npublic:\n    void drawImpl() {\n        std::cout << \"Drawing Square\\n\";\n    }\n};\n\nint main() {\n    Circle c;\n    c.draw(); // 输出：Drawing Circle\n\n    Square s;\n    s.draw(); // 输出：Drawing Square\n}\n```\n","tags":["AI","IR","Compiler"],"categories":["AI","IR"]},{"title":"Mojo","url":"/2023/10/10/Mojo/","content":"\n## links\n\n1. [LLVM 之父 Chris Lattner：我的 AI 基础设施软件构建理念](https://mp.weixin.qq.com/s?__biz=MzU5ODY2MTk3Nw==&mid=2247492618&idx=1&sn=a20f4828b9ab3e3cee3fedfd906e0eb2&chksm=fe426a3cc935e32a8312ce9efbb4f2640787508d3e811579bbffe918685cdb07a8bd8e3ffc4b&scene=132&exptype=timeline_recommend_article_extendread_samebiz#wechat_redirect)\n1. [未来的人工智能语言，是 Rust 还是 Mojo ？](https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA%3D%3D&mid=2247597850&idx=1&sn=679ccdcaf76e8b6ab97ca42687769da1)\n1. [github mojo](https://github.com/modularml/mojo)\n"},{"title":"rerun","url":"/2023/10/08/rerun/","content":"\n## base\n\n1. [foxglove 支持 ros](https://foxglove.dev/) [Visualizing-ROS-2-Data-With-Foxglove-Studio](https://docs.ros.org/en/foxy/How-To-Guides/Visualizing-ROS-2-Data-With-Foxglove-Studio.html)\n1. 日志重放工具 log-replay tools [link](https://www.rerun.io/blog/rosbag)\n1. 观察工具 [what-is-a-seeingtool](https://www.rerun.io/blog#what-is-a-seeingtool)\n1. [Seeing Spaces 它是 Figma、Webflow、Our World in Data 等许多项目的灵感来源](https://vimeo.com/97903574)\n1. 类似 rosbag\n1. rerun 可以纪录每帧对应的 latency，fps, 画图，平均值？方便分析哪一帧耗时\n1. 不链接 viewer 也不保存，相当于不操作？方便开启和关闭 log\n1. 使用 rerun 实时或回放监测的数据\n1. `rr.set_time_sequence(\"frame_nr\", x) # frame_nr为横坐标` 横坐标可以选择按帧或时间显示\n1. 测试程序可以关闭 viewer(init spawn false)， 将结果记录到 rrd 文件里（rr.save), 使用 online demo 回放\n1. 可以显示 log 和文档 [link](https://github.com/rerun-io/rerun/tree/main/examples/python/detect_and_track_objects)\n1. `pip install rerun-sdk && python3 -m rerun_demo`安装并运行 demo\n1. `python3 -m rerun`启动客户端\n1. `python3 -m rerun --web-viewer`启动 web, `http://{ip}:9090/` to open webserver\n1. `python3 -m rerun --web-viewer --web-viewer-port 5900` wsl docker\n1. `python3 -m rerun --web-viewer --web-viewer-port 5900 --save test.rrd` 保存接收到的 log, save 时不能打开 web\n1. [4 sdk-operating-modes](https://www.rerun.io/docs/reference/sdk-operating-modes) Connnect: Connects to a remote Rerun Viewer and streams all the data via TCP.\n1. good: docker web live data\n   - `python3 -m rerun --web-viewer --web-viewer-port 5900 --ws-server-port 6080` 注意--web-viewer-port 是打开 viewer 的端口， --ws-server-port 是用于连接 server 端口\n   - `examples/python/clock/main.py --connect --addr=127.0.0.1:9876` example 用到的代码`rr.connect(127.0.0.1:9876)`; 注意 viewer 默认监听的是 9876, --port 才能修改，注意和--web-viewer-port 的区别\n   - `http://localhost:10087/?url=ws://localhost:10089` (in docker -p10087:5900 -p10089:6080) or `http://10.31.2.47:9090/?url=ws://10.31.2.47:9877` 注意两个 localhost 和端口号都要替换\n1. 测试是否收到数据\n   - `python3 -m rerun --web-viewer --web-viewer-port 5900  --test-receive`\n   - `examples/python/clock/main.py --connect --addr=127.0.0.1:9876`\n1. [connect sample](https://github.com/rerun-io/rerun/blob/0a2a594b1f9527e91400a0b1d2dbb4943080f385/rerun_py/README.md?plain=1#L50)\n1. 多次链接有多次数据\n1. python connect code\n\n```python\nimport rerun as rr  # NOTE: `rerun`, not `rerun-sdk`!\nimport numpy as np\nrr.init(\"rerun_example_my_data\", spawn=True)\n\nrr.connect(\"192.168.1.3:9876\")\n\npositions = np.zeros((10, 3))\npositions[:,0] = np.linspace(-10,10,10)\ncolors = np.zeros((10,3), dtype=np.uint8)\ncolors[:,0] = np.linspace(0,255,10)\nrr.log(\"my_points\",rr.Points3D(positions, colors=colors, radii=0.5))\n```\n\n## c++\n\n1. [cpp-example-opencv-eigen](https://github.com/rerun-io/cpp-example-opencv-eigen)\n   - `pixi run example`\n1. 测试程序可去掉`rec.spawn`, 添加\n   - `rec.save(\"test.rrd\");` 保存结果到 test.rrd\n   - `rec.connect(\"127.0.0.1:9876\");` 连接本地的 viewer, 本地的 viewer(in docker -p10087:5900 -p10089:6080) `python3 -m rerun --web-viewer --web-viewer-port 5900 --ws-server-port 6080`; 本地浏览器`http://localhost:10087/?url=ws://localhost:10089`\n\n## 环境变量\n\n1. `export RERUN=off`环境变量控制是否输出 log\n1. `rerun --help`可以查看环境变量\n\n## install\n\n1. [troubleshooting](https://www.rerun.io/docs/getting-started/troubleshooting)\n\n## [logging](https://www.rerun.io/docs/reference/types/archetypes/text_log#textlogintegration)\n\n1. code\n\n```\nimport rerun as rr\nimport logging\n\ndef main() -> None:\n    rr.init(\"rerun_example_my_data\", spawn=False)\n    rr.save(\"recording.rrd\")\n    rr.set_time_sequence(\"frame\", 0)\n    logging.getLogger().addHandler(rr.LoggingHandler(\"logs/handler\"))\n    logging.getLogger().setLevel(-1)\n    logging.info(\"This INFO log got added through the standard logging interface\")\n    rr.set_time_sequence(\"frame\", 1)\n    logging.info(\"Test logging\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## samples\n\n1. 只记录数据不显示, 用于回放 w\n\n```python\nimport rerun as rr\nimport numpy as np\nimport random\nfrom math import cos, sin, tau\ndef log_trig() -> None:\n    # Log a time series\n    for t in range(0, int(tau * 2 * 100.0)):\n        rr.set_time_sequence(\"frame_nr\", t)\n\n        sin_of_t = sin(float(t) / 100.0)\n        rr.log(\"trig/sin\", rr.TimeSeriesScalar(sin_of_t, label=\"sin(0.01t)\", color=[255, 0, 0]))\n\n        cos_of_t = cos(float(t) / 100.0)\n        rr.log(\"trig/cos\", rr.TimeSeriesScalar(cos_of_t, label=\"cos(0.01t)\", color=[0, 255, 0]))\n\ndef log_linear() -> None:\n    for x in range(0, 1000):\n        rr.set_time_sequence(\"frame_nr\", x) # frame_nr为横坐标\n        y = x\n        rr.log(\"linear/x\", rr.TimeSeriesScalar(y, label=\"y=x\", color=[255, 0, 0]))\n\ndef main() -> None:\n    rr.init(\"rerun_example_my_data\", spawn=False) # spawn设置为False表示只记录数据， 不使用viewer显示\n    rr.save(\"recording.rrd\") # 保存为rrd\n    log_trig()\n    log_linear()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n2. cool\n\n```python\nimport rerun as rr\nimport numpy as np\n\nrr.init(\"rerun_example_my_data\", spawn=False)\nrr.save(\"recording.rrd\")\n\nSIZE = 10\n\npos_grid = np.meshgrid(*[np.linspace(-10, 10, SIZE)]*3)\npositions = np.vstack([d.reshape(-1) for d in pos_grid]).T\n\ncol_grid = np.meshgrid(*[np.linspace(0, 255, SIZE)]*3)\ncolors = np.vstack([c.reshape(-1) for c in col_grid]).astype(np.uint8).T\n\nrr.log(\n            \"my_points\",\n                rr.Points3D(positions, colors=colors, radii=0.5)\n                )\n```\n\n## demos\n\n1. [plots](https://github.com/rerun-io/rerun/tree/latest/examples/python/plots)\n1. [live-depth-sensor](https://github.com/rerun-io/rerun/tree/main/examples/python/live_depth_sensor)\n\n## links\n\n1. [rerun discord 在线信息 ](https://discord.com/invite/PXtCgFBSmH)\n1. [rerun-io/rerun](https://github.com/rerun-io/rerun)\n1. [docs 示意图](https://www.rerun.io/docs)\n1. [online demo](https://app.rerun.io/)\n1. [rerun/milestones](https://github.com/rerun-io/rerun/milestones)\n","tags":["Debug","DataView"],"categories":["Tools","Debug"]},{"title":"GPU","url":"/2023/10/07/GPU/","content":"\n## DLA (Deep Learning Accelerator)。\n\n1. NVIDIA DLA 是一个专门设计的硬件加速器，用于加速深度学习推断在边缘设备上的性能。DLA 被设计为与 NVIDIA 的 Jetson 平台（例如 Jetson AGX Xavier、Jetson Xavier NX 等）一同工作，这些平台主要用于嵌入式和边缘计算应用。\n1. `专为推断优化`：与训练深度学习模型的 GPU 不同，DLA 专门针对推断进行了优化，使其能够在功耗有限的边缘设备上高效运行。\n1. `集成与Jetson平台`：DLA 被集成到了某些 NVIDIA Jetson 模块中，提供了与 GPU 和 CPU 协同工作的能力，以实现多任务处理。\n1. `低功耗`：DLA 的设计重点是在保持低功耗的同时实现高效的推断处理，这对于需要长时间运行但电源有限的嵌入式应用非常重要。\n1. `支持多种网络`：DLA 支持多种流行的深度学习模型和框架，使开发者能够灵活地部署各种应用。\n1. `TensorRT兼容`：NVIDIA 的 TensorRT 是一个用于优化深度学习模型的推断的 SDK，它可以与 DLA 结合，进一步提高推断速度和效率。\n"},{"title":"test","url":"/2023/09/21/test/","content":"\n## Fuzz test\n\n1. 模糊测试（fuzz testing, fuzzing）是一种软件测试技术。 其核心思想是将自动或半自动生成的随机数据输入到一个程序中，并监视程序异常，如崩溃，断言（assertion）失败，以发现可能的程序错误，比如内存泄漏。 模糊测试常常用于检测软件或计算机系统的安全漏洞。\n1. [LibFuzzer](https://llvm.org/docs/LibFuzzer.html) [libFuzzerTutorial](https://github.com/google/fuzzing/blob/master/tutorial/libFuzzerTutorial.md)\n1. [oss-fuzz](https://github.com/google/oss-fuzz)\n1. [clusterfuzz](https://github.com/google/clusterfuzz)\n1. [AFL++](https://github.com/AFLplusplus/AFLplusplus)\n\n## 代码覆盖率检查\n\n### [gcovr](https://github.com/gcovr/gcovr)\n\n1. 可以用来查看 repo 中函数个数\n1. [doc](https://gcovr.com/en/stable/)\n1. `-DCMAKE_CXX_FLAGS=\"--coverage -g -O0\"` 需要 O0, 否则代码对不上\n1. [交叉编译](https://www.cnblogs.com/iamkun2005/p/16502967.html) [link](https://www.cnblogs.com/vaughnhuang/p/16224069.html)\n   - `export GCOV_PREFIX_STRIP=9`\n   - `export GCOV_PREFIX=$PWD`\n   - 运行程序，生成对应的.gcno, 将该目录拷贝回主机并覆盖同级目录（即 .gcda 文件与同名 .gcno 文件应该在同一目录）\n   - `mkdir out && gcovr --html-nested out/coverage.html` html-nested 表示生成的报告是有层级的\n   - `gcovr --decisions --txt`\n   - `-f samples -e build` -f 包含， -e 排除\n\n<!-- more -->\n\n1. [codecov](https://about.codecov.io/) [glog](https://github.com/google/glog)\n1. [coveralls](https://coveralls.io/)\n1. [config file](https://gcovr.com/en/stable/guide/configuration.html#configuration) [glog config](https://github.com/google/glog/blob/master/gcovr.cfg)\n1. `--gcov-ignore-parse-errors`\n1. `gcovr --gcov-executable /opt/ti-processor-sdk-rtos-j721e-evm/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/aarch64-none-linux-gnu-gcov` 交叉编译有时需要指定 gcov\n1. [原理](http://blog.chinaunix.net/uid-24709751-id-3612976.html)\n1. [Create a GitLab Pages deployment for a static site](https://docs.gitlab.com/ee/user/project/pages/getting_started/pages_ui.html)\n1. `pip install gcovr`\n1. [合并结果](https://gcovr.com/en/stable/guide/merging.html)\n\n```sh\ncp -r outs/build_tda4/* ./build_tda4\ngcovr --gcov-executable /opt/ti-processor-sdk-rtos-j721s2-evm/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/aarch64-none-linux-gnu-gcov -r . --filter src/ --filter include/ --xml-pretty --print-summary --json segment_height.json -o tmp\nfind build_tda4/ -name \"*.gcda\" |xargs rm\ncp -r outc/build_tda4/* ./build_tda4\ngcovr --gcov-executable /opt/ti-processor-sdk-rtos-j721s2-evm/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/aarch64-none-linux-gnu-gcov -r . --filter src/ --filter include/ --xml-pretty  --print-summary --json classify_roadtype.json -o tmp\ngcovr -a segment_height.json -a classify_roadtype.json --xml-pretty --print-summary -o coverage.xml\n```\n\n1. ctrl-c 不产生数据问题\n\n```c++\n#include <stdlib.h>\n#include <signal.h>\n\nextern \"C\" void __gcov_flush(void);\n\nvoid my_signal_handler(int signum) {\n    __gcov_flush();\n    // 结束程序或进行其他操作\n    exit(signum);\n}\n\n\nint main() {\n    signal(SIGINT, my_signal_handler);\n    // 其他代码...\n}\n\n```\n\n#### gitlab gcovr\n\n1. cmake\n\n```cmake\nif(COVERAGE_TEST)\n    set(CMAKE_BUILD_TYPE Debug)\n    add_compile_options(-ftest-coverage)\n    add_compile_options(-fprofile-arcs)\n    link_libraries(gcov)\nendif()\n\n```\n\n2. ci\n\n```sh\nCoverageTest::CPU:\n  stage: test\n  image: centos7\n  tags:\n    - phigent_runner_cpu\n  script:\n    - sh script/build_cpu.sh -DCOVERAGE_TEST=ON -DBUILD_TEST=ON\n    - gcovr -r . --filter src/ --filter include/\n  coverage: '/^TOTAL.*\\s+(\\d+\\%)$/'\n```\n\n3. [可视化](https://docs.gitlab.com/ee/ci/testing/test_coverage_visualization.html)\n\n### [(备选)gcov \\+ lcov](https://blog.csdn.net/u012247418/article/details/90137291)\n\n`gcov`是 GNU 编译器集合（GCC）中的一个代码覆盖测试工具。它可以分析程序的性能，并显示源代码的每一行执行了多少次。这在测试和调试中非常有用。\n`lcov`是一个图形前端，它基于`gcov`生成更容易理解和更详细的信息。它产生 HTML 页面，可以让你看到哪些行执行了多少次，哪些没有。\n\n```fish\ngcc -fprofile-arcs -ftest-coverage -o my_program my_program.c\n./my_program\ngcov my_program.c\n```\n\n这将在当前目录生成一个名为`my_program.c.gcov`的文件，其中包含每一行代码执行次数的统计信息。\n\n```fish\nsudo apt install lcov\nlcov --capture --directory . --output-file coverage.info\ngenhtml coverage.info --output-directory out\nfirefox out/index.html\n```\n\n这些只是基础; `gcov`和`lcov`都有许多其他选项和高级用法。在实际项目中，你可能还需要结合其他工具和方法，比如持续集成（CI）系统、测试框架等，以实现更全面的代码覆盖和测试。\n\n## [gitlab testing](https://docs.gitlab.com/ee/ci/testing/)\n","tags":["Test"],"categories":["Program","Test"]},{"title":"assembly","url":"/2023/09/21/assembly/","content":"\n## examples\n\n### hello world\n\n1. code\n\n```assembly test.s\n.section .data   # 指示汇编器开始一个数据段，通常用于存储变量和常量。\nhello_message:   # 定义了一个名为 hello_message 的标签，其内容是 \"Hello, World!\\n\" 字符串。.asciz 指令告诉汇编器创建一个以零结尾的 ASCII 字符串（即 C 字符串）。\n    .asciz \"Hello, World!\\n\"\n.section .text   # 指示汇编器开始一个代码段，通常用于存储程序代码。\n.globl _start    # 告诉链接器 _start 标签是一个全局符号，意味着它可以在其他文件中被引用。对于这个程序，_start 是程序的入口点。\n\n_start:\n    # Write hello_message to stdout\n    mov x8, #64            // syscall: write\n    mov x0, #1             // file descriptor: stdout\n    ldr x1, =hello_message // pointer to hello_message\n    mov x2, #13            // message length\n    svc #0                 // invoke syscall\n\n    # Exit the program\n    mov x8, #93            // syscall: exit\n    mov x0, #0             // exit status 0\n    svc #0                 // invoke syscall\n```\n\n1. compile command\n\n```sh\nas test.s -o test.o\nld test.o -o test\n./test\nobjdump -dw test #查看生成的汇编\n```\n\n### vector sample\n\n1. code\n\n```assembly test.s\n.global _start      // 声明全局标签，这是程序的起始点\n\n.section .data      // 开始数据段，用于定义初始化的变量\n.align 3            // 确保32位对齐\na:      .int 1, 2, 3, 4    // 定义整数数组a\nb:      .int 5, 6, 7, 8    // 定义整数数组b\nresult: .int 0, 0, 0, 0    // 定义整数数组result用于存储结果\nmsg:    .asciz \"Result: [%d, %d, %d, %d]\\n\"   // 定义用于打印的字符串格式\n\n.section .text      // 开始文本段，用于定义代码\n\nvector_add:\n    ld1 {v0.4s}, [x0]   // 从x0指向的地址加载向量到v0寄存器\n    ld1 {v1.4s}, [x1]   // 从x1指向的地址加载向量到v1寄存器\n    add v2.4s, v0.4s, v1.4s  // 将v0和v1的内容相加，结果存放在v2中\n    st1 {v2.4s}, [x2]   // 将v2的内容存储到x2指向的地址\n    ret                 // 返回到调用此函数的位置\n\n_start:\n    adr x0, a           // 将数组a的地址加载到x0寄存器\n    adr x1, b           // 将数组b的地址加载到x1寄存器\n    adr x2, result      // 将数组result的地址加载到x2寄存器\n    bl vector_add       // 调用vector_add函数\n\n    adr x0, msg         // 加载字符串格式的地址到x0寄存器\n    adr x5, result      // 将数组result的地址加载到x5寄存器\n    ldr w1, [x5]        // 加载result数组的第一个元素到x1寄存器\n    ldr w2, [x5, #4]    // 加载result数组的第二个元素到x2寄存器\n    ldr w3, [x5, #8]    // 加载result数组的第三个元素到x3寄存器\n    ldr w4, [x5, #12]   // 加载result数组的第四个元素到x4寄存器\n    bl printf           // 调用printf函数打印结果\n\n    mov x8, #93         // 加载系统调用号到x8寄存器，93是exit的调用号\n    mov x0, #0          // 设置退出状态为0\n    svc 0               // 进行系统调用\n```\n\n1. compile command\n\n```sh\nas -o test.o test.s\nld -o test test.o -lc # 需要链接C库\n./test\nobjdump -dw test #查看生成的汇编\n```\n\n1. [系统调用寄存器配置](https://arm64.syscall.sh/)\n\n## basic\n\n1. ARMv8-A：\n   - 前 8 个整数参数传递给函数时，使用寄存器 x0 到 x7。如果有更多的整数参数，它们会通过堆栈传递。前 8 个浮点参数使用浮点寄存器 v0 到 v7 传递。整数或指针返回值使用寄存器 x0（和 x1，如果需要返回更大的值，例如 128 位的值）。浮点数或向量返回值通过 v0 和 v1 返回。\n   - 系统调用主要使用 x0 到 x8 这 9 个寄存器，其中 x8 存储系统调用号，x0 到 x7 用于参数传递和返回值。\n1. 指令、伪指令、寄存器都可以全部是大写或者小写，GNU 风格是小写。\n\n## [伪指令](https://zhuanlan.zhihu.com/p/533821145)\n\n1. 伪指令就是对汇编器发出的指令。它在源程序汇编期间由汇编器处理。伪指令是由汇编器预处理的指令。它可以分解为几条指令的集合。\n1. 伪指令可以实现一下功能 - 符号定义 - 数据定义和对齐 - 汇编控制 - 汇编宏 - 段描述\n\n## [WebAssembly (abbreviated Wasm)](https://webassembly.org/)\n\n1. WebAssembly 是一种运行在现代网络浏览器中的新型代码，并且提供新的性能特性和效果。它设计的目的不是为了手写代码,而是为诸如 C、C++和 Rust 等低级源语言提供一个高效的编译目标。\n\n## links\n\n1. [ARMv8-aarch64 寄存器和指令集](https://winddoing.github.io/post/7190.html)\n1. [Armv8/armv9 架构入门指南](https://armv8-doc.readthedocs.io/en/latest/02.html)\n1. [Linux System Call Table 多个平台](https://chromium.googlesource.com/chromiumos/docs/+/master/constants/syscalls.md)\n1. [不同平台系统调用寄存器配置](https://syscall.sh/)\n"},{"title":"QEMU","url":"/2023/09/20/QEMU/","content":"\n## arm simulation\n\n```sh\nsudo apt install qemu-system-aarch64\nsudo apt install qemu qemu-system qemu-kvm\n\n## download image\n## https://cloud-images.ubuntu.com/releases/\n\n\n```\n\n## arm qnx\n\n## links\n\n1. [QEMU 入门指南](https://blog.csdn.net/FontThrone/article/details/104157859)\n1. [docs](https://www.qemu.org/docs/master/)\n"},{"title":"Reverse-Engineering","url":"/2023/09/13/Reverse-Engineering/","content":"\n## [radare2](https://github.com/radareorg/radare2)\n\n1. install\n\n```\nsudo pip install -U r2env\n\ngit clone https://github.com/radareorg/radare2\nradare2/sys/install.sh\n```\n\n1. <table> show help\n\n### usage\n\n1. `r2 ./test`\n1. `r2 -A ./test` analyse or `r2 ./test; aaa`\n1. `ia` show all info\n1. `il` libraries\n1. `is` show symbols\n1. `iS` show sections\n1. `iX` show used files\n1. `ic` show classes\n1. `iE | grep FUNC` show output functions\n\n## [ida](https://hex-rays.com/ida-pro/)\n\n1. [反编译工具集合](https://www.52pojie.cn/forum.php?mod=forumdisplay&fid=4&filter=typeid&typeid=123)\n1. [IDA Pro 权威指南（第二版）.pdf](https://github.com/Coldwave96/WebSecurity/blob/master/IDA%20Pro%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89.pdf)\n1. `Tab` jump to pseudocode\n1. `Esc` jump to previous position\n1. `Space` text view and graph view 切换\n1. `C-Enter` jump to next position\n1. `C-l` jump by name + `C-F` filter\n1. `C-p` jump to function\n1. `C-s` jump to segment\n1. `M-t` +`C-t` find\n1. `text view` and `graph view`\n1. 窗口拖出来后，可以点击窗口灰色线条来 reattach, If you click on the small gray bar just below the titlebar, you can drag it back to the main window.\n1. `view -> open subviews`看各种窗口\n1. `view -> toolbars -> debug` 显示 debug 按钮\n1. 分割窗口，diassembly 和 pseudocode， 右键 synchronize with；可以高亮同步代码\n1. 新窗口会在鼠标停留的窗口打开\n\n### debug\n\n1. 可以用 gdbserver 调试\n1. 可以实时看内存, bss, data, text\n1. 下方有`gdb`命令窗口， 可以`exit`来结束远程 gdbserver\n1. `debugger -> detach from process`\n1. `debugger -> debugger windows` 可以打开 stack trace(call stack), Locals 等调试窗口\n1. `debugger -> using source level debugging` 才可以在反编译的文件加断点\n\n### 修改二进制\n\n1. `Edit -> Pach program -> change byte/assemble` 修改二进制\n1. `Edit -> Pach program -> Apply patches to input file` 报错修改\n1. `File -> Produce file -> Create DIF file` 可以列出被修改的字节\n1. [Compiler on line wandbox](https://wandbox.org) 可对比结果\n1. [汇编指令和二进制互转](https://aarmconverter.com/)\n1. [汇编指令和二进制互转 1](https://disasm.czbix.com/)\n\n### 修改 PC 值\n\n1. 能临时跳到不同分支执行\n\n#### [gdb](https://wizardforcel.gitbooks.io/100-gdb-tips/content/patch-program.html)\n\n1. `b *main+12`函数偏移来打断点\n1. `disassemble` 显示汇编\n1. `p $pc`显示 PC 值\n1. `set var $pc=0x11111`设置 PC 值\n\n### [plugins](https://github.com/onethawt/idaplugins-list)\n\n1. [Virtuailor](https://github.com/0xgalz/Virtuailor) Virtuailor is an IDAPython tool that reconstructs vtables for C++ code written for intel architecture, both 32bit and 64bit code and AArch64 (New!).\n1. [lighthouse](https://github.com/gaasedelen/lighthouse)\n\n## [ghidra](https://github.com/NationalSecurityAgency/ghidra)\n\n1. [使用说明](https://www.secrss.com/articles/8829)\n1. 需要先安装 jdk, 使用 cmd 打开， powershell 不行\n1. 开始界面有 debugger\n\n## [binary.ninja](https://binary.ninja/)\n\n1. [破解](https://down.52pojie.cn/Tools/Disassemblers/)\n   - 需要替换安装路径下的动态库\n   - 还不支持 debugger\n1. 可以远程调试\n1. 可以从不同 level 查看各个 section 内容\n1. 查看各种 level 代码\n1. 窗口划分，可同时显示不同级别代码\n1. 图形化做的不错\n\n## [cutter](https://github.com/rizinorg/cutter)\n\n1. [官网](https://cutter.re/)\n\n## x64dbg\n\n1. 只试用 windows\n\n## [cheat-engine](https://github.com/cheat-engine/cheat-engine)\n\n## base\n\n1. 关键在读懂汇编\n1. [反编译工具集合](https://www.52pojie.cn/forum.php?mod=forumdisplay&fid=4&filter=typeid&typeid=123)\n1. [合集 2](https://down.52pojie.cn/Tools/Disassemblers/)\n1. 反编译器： 从二进制到代码\n"},{"title":"go","url":"/2023/09/04/go/","content":"\n## basic\n\n1. `sudo apt install golang`\n1. `go env -w GOPROXY=https://goproxy.cn`\n1. `export PATH=$PATH:$GOPATH/bin`\n1. `go install github.com/google/pprof@latest`\n"},{"title":"计算机原理","url":"/2023/08/24/计算机原理/","content":"\n## 处理器比较\n\n### CPU\n\n1. 前后处理\n\n### GPU\n#### GPGPU\n\n1. 矩阵运算\n\n### DSP\n\n1. element wise op\n\n### FPGA\n\n1. 通信\n\n### ASIC\n\n## [netdata](https://github.com/netdata/netdata)\n\n1. [使用说明](https://getiot.tech/zh/foss/netdata)\n\n## [跑分软件](https://post.smzdm.com/p/aox560x7/)\n\n1. [windows aida64](https://www.aida64.com/downloads/YWY1MWUxM2E=)\n\n   - 工具 -> 内存缓存测试 -> 有内存类型\n   - 最下面有性能测试\n   - [破解](https://www.extremexbb.com/aida64-keys/) 使用: 6.85.6300 版本\n\n1. [aida64](https://www.aida64.com/downloads/OThhYjQxZDU=)\n\n## base\n\n1. top 出来的 core 数是逻辑内核，例如：windows 系统显示 4 个物理 core, 8 个逻辑 core， wsl 里 top 显示的 8core\n1. lscpu 可以看出逻辑 core 和物理 core\n\n## tools\n\n1. `likwid-topology`(good) 显示有关硬件拓扑的详细信息，例如 CPU、缓存、NUMA 域等 [link](https://github.com/RRZE-HPC/likwid)\n1. [cpuid](https://zh.wikipedia.org/wiki/CPUID) CPUID 指令是一条 x86 架构中的的扩展指令（此处的扩展指相对 i80386），其操作码助记符缩写于“CPU 识别”（CPU Identification），其作用是返回特定的 CPU 信息\n1. [cpuid 另一个](https://github.com/klauspost/cpuid/releases)\n1. `lscpu`\n   - `-C` 显示详细 cache 信息\n   - cores 和 sockets 区别 [tutorial1](https://blog.rwth-aachen.de/hpc_import_20210107/attachments/20056127/24117298.pdf)\n1. [cpufreq](https://sunpma.com/115.html)\n   - `sudo apt-get install cpufrequtils`\n   - `cpufreq-info | grep \"current CPU\"`查看各 core cpu 频率\n   - `for (( i = 0; i<100; i++ )); do echo $i;  cpufreq-info | grep \"current CPU\"; sleep 1; done`\n   - `for (( i = 0; i<100; i++ )); do echo -n \"$i:    \";  cpufreq-info | grep \"current CPU\" | grep -o '[0-9.]* [a-zA-Z]*Hz' | awk '{printf \"%s,\\t  \", $0} END {print \"\"}'; sleep 1; done`\n   - `for (( i = 0; i<100; i++ )); do echo -n \"$i:    \";  cpufreq-info | grep \"current CPU\" | grep -o '[0-9.]* [a-zA-Z]*Hz' | awk '{printf \"%d:%s,\\t  \", NR, $0} END {print \"\"}'; sleep 1; done`\n   - `cpufreq-set -g performance` 保持最大频率\n\n## cpu 性能测试\n\n1. [各平台各性能指标计算方法](https://github.com/RRZE-HPC/likwid/tree/master/groups)\n1. 理论浮点运行峰值 = CPU 主频 × CPU 核心数 × 每周期浮点运算次数。\n1. `Linpack`：这是最著名的浮点性能测试之一。HPL（高性能 Linpack）版本用于排名全球 TOP500 的最强大超级计算机。您可以在单个机器上运行 Linpack 来获取其浮点性能。\n1. `Flops.c`：这是一个简单的 C 程序，用于测量机器的浮点性能。它执行加法、乘法、除法和平方根操作，并报告 GFLOPS 性能。\n1. `Geekbench`：这是一个流行的跨平台基准测试工具，它提供了浮点性能的测量。\n1. `Phoronix Test Suite`：这是一个开源的、综合的测试和基准测试框架，它包括许多专门针对浮点性能的测试。\n1. [各种 benchmark 及数据](https://github.com/kreier/benchmark/tree/main)\n1. [hpl 各种测试](https://www.netlib.org/benchmark/)\n1. pidstat 1 -----> `process usage`\n   - 可以看进程分配在哪一个 cpu 核上\n1. perf stat 可以查看 CPU 真实利用率： CPUs utilized, 不包括等待时间，自旋时间等。真实利用率比 top 查看的利用率要低很多。\n1. top 查看的利用率： 如果 CPU 没有运行在内核的 idle 线程，就认为 CPU 在被使用。当 cpu 阻塞并等待内存访问时，也可能显示较高的利用率。\n\n### 指标\n\n1. `flops`\n   - [nvidia 计算方法](https://www.jianshu.com/p/b1ceaa7effa8)\n1. `ops`\n1. `DMIPS`\n1. `DMIPS/MHz`\n1. `MAC`\n1. 指令性能\n\n### likwid\n\n1. [测 flops 性能](https://github.com/RRZE-HPC/likwid/wiki/Tutorial%3A-Empirical-Roofline-Model)\n\n### [flops.c](https://github.com/AMDmi3/flops)\n\n```sh\ngit clone https://github.com/AMDmi3/flops.git\ncd flops && make\n```\n\n1.  测试的是单核性能 flops\n\n### linpack\n\n1. [使用说明](https://github.com/UNINETTSigma2/documentation/blob/main/jobs/arm-perf/linpack.md)\n2. [src test](https://elinux.org/RPi_Performance)\n\n```sh\nhttps://www.netlib.org/benchmark/linpackc.new\ngcc -O3 -o linpack  linpack.c\n```\n\n### [dhrystone](https://github.com/kreier/benchmark/tree/main/dhrystone)\n\n1. 目前找到的达到最大 ops 和最大 IPC 的程序，使用自己写的 perf monitor 监控得到.\n1. [ARM 各内核系列整型运算能力对比---DMIPS / MHz](https://blog.csdn.net/qq_21475601/article/details/106564903)\n1. MIPS: 每秒执行了多少百万条指令。DMIPS：D 是 Dhrystone 的缩写，在 MIPS 前面加上 Dhrystone （整数运算），用于测整数计算能力。进程占用 DMIPS = 总的 DMIPS \\* 进程占用 CPU 百分比\n1. I got 2898550.8 Dhrystones per Second, and I divided by 1757 to get 1649 DMIPS/core. And then I divided by 1200 to get 1.37 DMIPS/MHz/core.\n1. A55 单核算力： 2.7DMIPS/MHz\\*1.2GHz=3.24 KDMIPS, 每核没时钟周期处理 2.7 指令\n1. KDMIPS 是算力表示， DMIPS/MHz 是密度表示\n1. perf stat -ddd 显示频率\n1. [Keith-S-Thompson/dhrystone](https://github.com/Keith-S-Thompson/dhrystone/tree/master)\n1. [sbc-bench/src/dhrystone](https://github.com/fm4dd/sbc-benchmarks/tree/master/sbc-bench/src/dhrystone)\n1. use\n\n```sh\nwget https://homepages.cwi.nl/~steven/dry.c\nsh dry.c\n```\n\n### [phoronix-test-suite](https://github.com/phoronix-test-suite/phoronix-test-suite)\n\n### [MegPeak](https://github.com/MegEngine/MegPeak)\n\n1. 可以测 memory`GOPS`, 各指令性能\n1. 可以测 roofline, cpu gflops\n1. [MegPeak——让你更懂你的处理器](https://zhuanlan.zhihu.com/p/522007924)\n\n### [clpeak](https://github.com/krrishnarraj/clpeak)\n\n1. 测试 opencl 设备性能\n\n## links\n\n1. [（good)各种动画](https://www.scss.tcd.ie/Jeremy.Jones/VivioJS/)\n1. [(有动画)计算机组成原理（4）-中央处理器 CPU](https://blog.csdn.net/chen1083376511/article/details/81941318)\n1. [cpu](http://note.wcoder.com/computer/cpu.md)\n1. [plantegg.github.io/categories/CPU/](https://plantegg.github.io/categories/CPU/)\n1. [cpu_features](https://github.com/google/cpu_features) c++ 获取 cpu feature\n\n## data type\n\n1. [Floating-Point Calculator — 32-bit IEEE754](https://www.omnicalculator.com/other/floating-point)\n"},{"title":"pandas","url":"/2023/07/11/pandas/","content":"\n## [describe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html)\n\n1. 获取统计信息， 可配置\n"},{"title":"data_view","url":"/2023/07/09/data-view/","content":"\n## note\n\n1. `bar with table`\n   - [ggplot2 R language](https://stackoverflow.com/questions/71551678/how-to-turn-a-contingency-table-directly-into-a-bar-graph-when-using-the-base-ta)\n   - [example](https://mp.weixin.qq.com/s/0iAbHZ-hN6Mj2c2j2Uw03w)\n1. openai 数据分析可视化\n1. [科学可视化：Python+Matplotlib](https://github.com/rougier/scientific-visualization-book)\n\n## idea\n\n1. [cxx2flow C++代码生成流程图](https://github.com/Enter-tainer/cxx2flow)\n   - 可以生成 dot 文件\n   - [graphviz-visual-editor 在线编辑 dot](http://magjac.com/graphviz-visual-editor/)\n   - [vscode-interactive-graphviz 渲染 dot](https://github.com/tintinweb/vscode-interactive-graphviz)\n   - [cxx2flow/releases](https://github.com/Enter-tainer/cxx2flow/releases) 直接下载二进制用\n   - cxx2flow-linux-amd64 test.cpp --cpp > test.dot\n   - cxx2flow main.cpp my_custom_func | dot -Tsvg -o test.svg\n   - 注意要有 main 函数才能生成, 一个函数一个函数生成，函数名改为 main\n   - [线条格式](https://graphviz.org/docs/attrs/splines/)\n   - 写一个功能：从文件夹中获取所有 cpp, 对每个 cpp, 生成所有函数的 svg, 每个函数一个 svg\n     - ctags -x gpu/model_process_tensorrt_impl.cpp | grep \"(\" | awk '{print $1}' 获取 cpp 中函数名\n     - 提取出函数体到一个文件，再生成 svg [提取函数体](https://stackoverflow.com/a/37339591/23011500)\n     - doxygen 提取\n     - doxygen\n   - [不使用流程图](https://stackoverflow.com/a/51318005/23011500)\n1. [3σ 准则](https://zhuanlan.zhihu.com/p/572327280?utm_id=0)\n1. [可视化工具原则](https://www.rerun.io/blog#principles-for-a-computer-vision-focused-seeingtool)\n1. 使用 rerun 实时或回放监测的数据: 将可视化代码与算法代码分开\n1. 两二进制文件比较服务器(转成 float 对比)\n   - error 画图\n   - 三列, 数据 1， 数据 2， error， error 用色阶表示\n1. [pandas table 颜色设置（好用）](https://pandas.pydata.org/docs/user_guide/style.html), 类似 excel 表格色阶\n1. excel 中可以套用表格格式来美化表格, 利用好色阶，条件格式, 数据条\n1. 将 log 文件拖到浏览器中，生成报告(正则表达式先生成 pandas， 然后生成报告)\n1. csv server\n1. [提高数据可视化效果的五个原则](https://www.51cto.com/article/719697.html)\n1. [excel with python](https://techcommunity.microsoft.com/t5/excel-blog/announcing-python-in-excel-combining-the-power-of-python-and-the/ba-p/3893439)\n\n<!-- more -->\n\n## [apache/superset](https://github.com/apache/superset/tree/master)\n\n1. [online test](https://manage.app.preset.io/app/)\n\n## rerun\n\n1. 可以实时查看结果并回放，后续支持 c++;\n1. [python/plots](https://github.com/rerun-io/rerun/tree/latest/examples/python/plots)\n\n## [PlotNeuralNet](https://github.com/HarisIqbal88/PlotNeuralNet)\n\n## matplotlib\n\n1. [cheatsheets](https://github.com/matplotlib/cheatsheets)\n\n## [seaborn](https://seaborn.pydata.org/index.html)\n\n## [plotly.py](https://github.com/plotly/plotly.py)\n\n## [streamlit](https://github.com/streamlit/streamlit)\n\n1. [csv view server](https://jiaxiyang-streamlit-example-streamlit-app-t8698c.streamlit.app/)\n   - `grep \"diff time\" 1117.log | awk '{print $NF}' | awk 'BEGIN { print \"数据描述\" } { print $0 }' > test.csv` 搜索 log，创建 csv\n   - `paste $(ls *.csv)  -d \",\" > output.csv` 合并多个 csv 文件\n   - `paste $(ls *.csv)  -d \",\" > tmp.csv && tr -d '\\r' < tmp.csv > output.csv && rm tmp.csv` 处理回车符问题: ^M 符号通常表示回车符 (Carriage Return)\n1. [共享的 gallery](https://streamlit.io/gallery)\n1. [cheatsheet](https://daniellewisdl-streamlit-cheat-sheet-app-ytm9sg.streamlit.app/)\n1. [discuss.streamlit.io](https://discuss.streamlit.io/)\n1. [Streamlit-Authenticator](https://github.com/mkhorasani/Streamlit-Authenticator)\n1. NOTE: 测试文件名不能取名 steamlit.py, 否则会引起 circle import\n1. Streamlit 可让您在几分钟（而不是几周）内将数据脚本转变为可共享的 Web 应用程序。\n1. `pip install --upgrade streamlit`\n1. docker 注意端口号\n1. 网页中的 csv 数据点击表头可排序\n\n### 部署到服务器\n\n```\ndocker pull samdobson/streamlit:latest\npip install pip -U -i https://pypi.tuna.tsinghua.edu.cn/simple\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\npip install matplotlib openpyxl\nstreamlit run streamlit_csv_server.py  --server.port 12345\n```\n\n### chatgpt\n\nStreamlit 是一个开源的 Python 库，它使得创建和共享数据驱动的 web 应用程序变得非常简单和快速。它提供了一个直观的 API，可以方便地从数据生成丰富的交互式界面。\n以下是 Streamlit 的一些关键特性：\n\n1. **简单性**：Streamlit 的 API 非常简洁，只需要几行 Python 代码就可以构建一个应用程序。你可以将所有的精力集中在数据和模型上，而不是界面设计和 Web 开发。\n2. **交互性**：Streamlit 支持很多交互元素，例如滑动条、选择框、输入框等等。你可以用这些元素收集用户输入，并实时更新应用程序的输出。\n3. **快速迭代**：Streamlit 的设计哲学是\"先编码，再调整\"。你可以先构建一个基本的应用程序，然后再逐步增加新功能，或者修改现有功能。Streamlit 会自动跟踪你的代码变化，并实时更新应用程序。\n4. **灵活性**：Streamlit 支持 Python 的各种数据处理库（如 Pandas、NumPy）和可视化库（如 Matplotlib、Plotly）。你可以很容易地将这些库与 Streamlit 一起使用。\n5. **部署**：使用 Streamlit Share 或其他云服务（如 Heroku、AWS 等），你可以很容易地将你的应用程序部署到 Web 上，让其他人也可以访问和使用。\n   Streamlit 的工作流程基本上就是这样：首先，你会写一个 Python 脚本，其中包含了你的数据处理和可视化代码，然后你会添加一些 Streamlit 的特殊命令，以创建交互式的用户界面。然后，你可以运行这个脚本，Streamlit 会生成一个可交互的 Web 应用，并在本地打开一个 Web 浏览器窗口显示这个应用。你可以在这个窗口中与你的应用互动，查看不同输入条件下的结果。\n\n### compare\n\n1. Streamlit：如果你正在寻找一个轻量级的，可以快速原型设计和部署机器学习和数据科学应用的工具，Streamlit 是一个很好的选择。它的设计理念是“代码即应用”，并且有一个很直观的接口。\n2. Dash：如果你正在寻找一个更强大的工具，你的项目需要更复杂的交互和可视化，Dash 可能更适合你。Dash 为数据可视化提供了更多的控制权，但也可能需要编写更多的代码。\n3. Panel：Panel 是一个非常灵活的工具，它可以让你在 Jupyter notebook 中创建应用，然后将其转化为独立的 Web 应用。如果你已经习惯于使用 Jupyter 进行数据分析，那么 Panel 可能是一个很好的选择。\n4. Voila：如果你喜欢在 Jupyter notebook 环境中工作，并希望能够无缝地将你的工作转化为交互式 Web 应用，那么 Voila 可能是一个很好的选择。\n5. Shiny：如果你是 R 语言的用户，Shiny 是一个非常强大的工具，用于构建交互式 Web 应用。Shiny 是 R 社区中使用最广泛的工具之一。\n","tags":["DataView","CsvServer"],"categories":["DataView"]},{"title":"matplotlib","url":"/2023/07/05/matplotlib/","content":"\n## links\n\n1. [examples](https://matplotlib.org/stable/gallery/index.html)\n1. [tutorials](https://matplotlib.org/stable/tutorials/introductory/pyplot.html)\n"},{"title":"pip","url":"/2023/06/26/pip/","content":"\n## use\n1. `pip install /tmp/torch-2.8.0.dev20250323+cu128-cp312-cp312-manylinux_2_28_x86_64.whl   --cache-dir /tmp/pip` 指定cache dir, 不用重复下载\n1. `pip install viztracer -i https://pypi.tuna.tsinghua.edu.cn/simple`使用清华源安装\n1. `pip download \"torch==2.5.1+cpu.cxx11.abi\" --index-url https://download.pytorch.org/whl` pip下载包，一台机器有cache，可以下载whl包后安装到另一台机器上\n1. 注意 conda 源和 pip 源的区别，并不是共用\n1. `pip show numpy`查看 numpy 信息, 包括安装路径\n1. `pip list`\n1. `pip install torchtext --upgrade`\n1. `pip install torchtext==0.6.0`\n1. `pip freeze > requirements.txt` 导出 requirements\n1. 注意 conda 源和 pip 源的区别，并不是共用\n1. set source\n\n```\npip install pip -U -i https://pypi.tuna.tsinghua.edu.cn/simple\npip install pip -U -i https://mirrors.aliyun.com/pypi/simple/\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\npip3 install tensorrt_llm -U --extra-index-url https://pypi.nvidia.com # 可以追加source\n```\n\n1. install from source `pip install git+https://github.com/huggingface/transformers`\n\n\n## source \n1. https://mirrors.aliyun.com/pypi/simple/\n1. https://pypi.tuna.tsinghua.edu.cn/simple\n"},{"title":"conda","url":"/2023/06/26/conda/","content":"\n## NOTE\n\n1. 注意 conda 源和 pip 源的区别，并不是共用\n1. gcc 版本有要求\n1. [清华源切换](https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/)\n1. `unset all_proxy` 不能使用代理，可能出现 install 错误 [link](https://blog.csdn.net/whatday/article/details/109287343)\n1. `conda config --append channels conda-forge`\n1. `wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && sh Miniconda3-latest-Linux-x86_64.sh` install\n\n## basic use\n1. `~/miniconda3/bin/conda init`\n1. `conda config --set auto_activate_base false` 关闭自启动\n1. `conda info` 查看安装情况\n1. `conda env list` list env\n1. `conda list` list package in env\n1. `conda create --name ENVNAME` create env\n1. `conda create --name d2l python=3.9 -y`\n1. `conda activate ENVNAME` activate env\n1. `conda deactivate` deactivate env\n1. `conda install PKGNAME=3.1.4` install lib\n1. `conda uninstall PKGNAME` uninstall lib\n1. 打包conda 环境\n\n``` shell\nconda install conda-pack\nconda pack -n your_env_name -o myenv.tar.gz\n```\n\n1. 导出导入环境\n\n```sh\nconda list -e > requirements.txt\nconda install --yes --file requirements.txt\n```\n\n## ~/.condarc\n\n```sh\nauto_activate_base: false\n\nchannels:\n  - defaults\nshow_channel_urls: true\ndefault_channels:\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2\ncustom_channels:\n  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n```\n\n## links\n\n1. [user-guide/install](https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html)\n1. [miniconda](https://docs.conda.io/en/latest/miniconda.html#linux-installers) 选择对应 python 版本, install 时可以选路径\n1. [user-guide/configuration](https://docs.conda.io/projects/conda/en/latest/user-guide/configuration/index.html#)\n1. [cheatsheets/conda-4.14.pdf](https://github.com/conda/conda/blob/main/docs/source/user-guide/cheatsheets/conda-4.14.pdf)\n1. [build python from source](https://github.com/deadsnakes/docs/blob/main/Building-Deadsnakes-Packages-from-Git.rst) 编译之后需要前一级目录 `sudo dpkg -i *.deb`\n"},{"title":"VPN","url":"/2023/06/25/VPN/","content":"\n## NOTE\n\n1. `export all_proxy=\"socks5://${IP}:7890\"` linux 服务器可以设置 all_proxy 来翻墙， clash 鼠标悬浮 `Allow Lan` 可以看到对应 IP\n1. `export http_proxy=\"http://10.31.2.35:7890\" && export https_proxy=\"https://10.31.2.35:7890\"` socks5 不起作用时\n1. Proxies 选择 Rule，可以同时连接公司内网和外网，不要选 Global，不能连接内网\n\n## links\n\n1. [agentneo](https://agentneo.tech/) 使用 clash 客户端\n1. [WSL 2 配置代理 clash](https://solidspoon.xyz/2021/02/17/%E9%85%8D%E7%BD%AEWSL2%E4%BD%BF%E7%94%A8Windows%E4%BB%A3%E7%90%86%E4%B8%8A%E7%BD%91/) 配置 WSL2 使用 Windows 代理上网 有用\n1. [docker build 以及 docker run 时使用 host 网络的方法](http://www.debugself.com/2018/01/17/docker_network/)\n1. [vscode proxy setting](https://device.harmonyos.com/cn/docs/documentation/guide/vscode_proxy-0000001074231144)\n   - 注意本地 proxy 和远程 proxy 都要设置对\n"},{"title":"onnxruntime","url":"/2023/06/12/onnxruntime/","content":"\n## concept\n\n![ONNX 运行时高级架构](https://developer-blogs.nvidia.com/wp-content/uploads/2022/12/image3-2.png)\nONNX Runtime 是一个性能优秀的跨平台推理（inference）引擎，用于 ONNX（Open Neural Network Exchange）模型。它具有灵活的支持和高效的性能，可用于各种硬件设备（包括在边缘设备上）和操作系统。\n\n以下是关于 ONNX Runtime 的一些基本概念：\n\n1. **ONNX 模型执行**：ONNX Runtime 提供了执行 ONNX 模型的能力。你可以加载一个 ONNX 模型，然后使用 ONNX Runtime 来进行推理。\n2. **硬件优化**：ONNX Runtime 被设计为能够充分利用不同的硬件能力。它支持 CPU，GPU，以及更专用的硬件加速器（如 Microsoft 的 DirectML 和 NVIDIA 的 TensorRT）。\n3. **跨平台**：ONNX Runtime 可以在多种操作系统（包括 Windows，Linux，和 Mac OS）上运行，并且支持多种硬件设备，包括在边缘设备上。\n4. **语言绑定**：ONNX Runtime 提供了多种语言的 API，包括 C，C++，Python，C#，Java，和 JavaScript，使得开发者可以在他们选择的语言中使用 ONNX Runtime。\n5. **会话（Session）**：在 ONNX Runtime 中，一次模型的推理被称为一个会话。你可以创建一个会话，然后通过这个会话来执行模型。\n6. **提供者（Providers）**：ONNX Runtime 支持通过不同的“提供者”来执行模型。这些提供者可以是 CPU，CUDA（NVIDIA GPUs），TensorRT（NVIDIA GPUs），DirectML（Windows GPUs），OpenVINO（Intel GPUs）等。\n   通过 ONNX Runtime，开发者可以将 ONNX 模型部署到各种平台和设备上，同时保持良好的性能和灵活性。\n\n### [Providers](https://onnxruntime.ai/docs/execution-providers/)\n\nONNX Runtime 的\"提供者\"（Providers）是执行 ONNX 模型运算的后端引擎。每种提供者都是为特定的硬件或者软件平台优化的。使用正确的提供者可以大大提高模型的执行效率。\n以下是一些主要的 ONNX Runtime 提供者：\n\n1. **CPU Execution Provider**: CPU 提供者是 ONNX Runtime 的默认提供者，它在 CPU 上执行模型运算。CPU 提供者在所有系统上都可用，不需要任何额外的依赖。\n2. **CUDA Execution Provider**: CUDA 提供者是为 NVIDIA 的 GPU 优化的，它使用 CUDA 和 cuDNN 库来在 GPU 上执行模型运算。使用 CUDA 提供者需要安装 CUDA 和 cuDNN。\n3. **TensorRT Execution Provider**: TensorRT 提供者也是为 NVIDIA 的 GPU 优化的，但是它使用 NVIDIA 的 TensorRT 库来执行模型运算。TensorRT 提供者可以提供比 CUDA 提供者更高的性能，但是需要更复杂的设置。\n4. **DirectML Execution Provider**: DirectML 提供者是为 Windows 系统上的 GPU 优化的，它使用 Microsoft 的 DirectML 库来执行模型运算。DirectML 提供者可以在任何支持 DirectX 12 的 Windows 系统上使用。\n5. **OpenVINO Execution Provider**: OpenVINO 提供者是为 Intel 的硬件优化的，包括 CPU，GPU，VPU，和 FPGA。它使用 Intel 的 OpenVINO 库来执行模型运算。\n6. **Nuphar Execution Provider**: Nuphar 是一个为 CPU 优化的 JIT 编译器，主要用于对模型中的循环结构进行优化。\n7. **VitisAI Execution Provider**: VitisAI 提供者是为 Xilinx FPGA 硬件优化的，使用了 Xilinx 的 Vitis AI 库。\n   当你创建一个 ONNX Runtime 会话时，你可以指定用于执行模型运算的提供者。如果你没有指定提供者，ONNX Runtime 会使用默认的 CPU 提供者。如果你在一个支持 GPU 的系统上运行 ONNX Runtime，并且你已经安装了相应的依赖，你可以选择使用 CUDA，TensorRT，DirectML，或者 OpenVINO 提供者来提高模型的执行效率。\n   Note: provider 在 onnxruntime repo 里\n\n## TVM and onnxruntime\n\nTVM 是一个开源的机器学习编译器堆栈，它可以将机器学习模型从各种框架（例如 TensorFlow、PyTorch、ONNX、Keras 等）优化编译到各种硬件（例如 CPU、GPU、FPGA、ASIC 等）。\nONNX Runtime 是一个用于运行和推理 ONNX 模型的高性能跨平台推理引擎。然而，TVM 的关键优势在于它的自动调度程序和编译器栈，能够生成优化的计算内核，而 ONNX Runtime 的优势在于它对 ONNX 模型的广泛支持以及一系列优化技术。ONNX Runtime 支持多种硬件平台，包括 CPU、GPU 和专用加速器。它可以在不同硬件上运行，无需重新编译模型。\nTVM 和 ONNX Runtime 的结合可以在两者之间提供一个桥梁，使得开发者可以利用 TVM 的优化能力，同时使用 ONNX Runtime 的灵活性和易用性。\nONNX Runtime 和 TVM 结合的一种方式是使用 TVM 作为 ONNX Runtime 的一个执行提供者。TVM 有一个 ONNX 编译器，可以将 ONNX 模型编译成 TVM 模块，然后在 ONNX Runtime 中注册这个模块作为一个提供者，这样 ONNX Runtime 就可以使用 TVM 来执行模型。\n另一种方式是使用 TVM 来优化 ONNX 模型，然后在 ONNX Runtime 中执行优化后的模型。这种方法的优点是可以使用 TVM 的自动调度程序和编译器栈来优化模型，然后使用 ONNX Runtime 的高效运行时来执行优化后的模型。\n这两种方法都需要一些设置和配置，并且可能需要修改 ONNX Runtime 或者 TVM 的代码。然而，它们都可以提供更好的性能和更高的灵活性，使得开发者可以更好地利用他们的硬件资源。\n\nTVM 和 ONNX Runtime 都是用于机器学习模型推理的工具，但它们各自有着不同的优势和设计目标。\n**TVM**是一个开源的深度学习编译器和优化器，它的主要目标是提供一种灵活的方式来优化和部署深度学习模型到各种硬件平台，包括 CPU、GPU、FPGA 和 ASIC 等。TVM 的优势在于：\n\n1. **硬件无关的优化**：TVM 的自动调度功能可以生成针对特定硬件优化的代码，无论这个硬件是 CPU、GPU 还是其他类型的硬件。\n2. **端到端的编译优化**：TVM 包括了从高层图优化到底层代码生成的全流程优化。\n3. **支持多种深度学习框架**：TVM 可以接受多种框架的模型，包括 TensorFlow、PyTorch、MXNet、Keras、ONNX 等。\n   而**ONNX Runtime**是一个用于运行和推理 ONNX 模型的跨平台高性能推理引擎，它的主要目标是提供一种高效、灵活且易于使用的方式来部署和执行 ONNX 模型。ONNX Runtime 的优势在于：\n4. **广泛的 ONNX 模型支持**：ONNX Runtime 支持 ONNX 模型中的所有运算符和特性。\n5. **性能优化**：ONNX Runtime 包含了一系列优化技术，包括图优化、运算符融合、内存优化等，以提高模型的执行性能。\n6. **硬件加速**：通过不同的执行提供者（如 CUDA、TensorRT、DirectML 等），ONNX Runtime 可以利用硬件加速器来提高模型的执行速度。\n   两者之间并非完全的竞争关系，它们可以相互结合，例如使用 TVM 作为 ONNX Runtime 的一个执行提供者，使得 ONNX Runtime 能够利用 TVM 的优化能力。\n\n## compare results with pytorch\n\n1. [Compare the PyTorch results with the ones from the ONNX Runtime](https://pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html#compare-the-pytorch-results-with-the-ones-from-the-onnx-runtime)\n1. [Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)\n\n## compare results with tensorrt\n\n1. [polygraph](https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy/examples/cli/inspect/)\n   - `polygraphy inspect model tensorrt/resnet50/model.onnx`\n   - `polygraphy inspect capability model.onnx` Inspecting TensorRT ONNX Support\n   - `polygraphy inspect model op16_iter7_refine_filter_fb.trt --model-type=engine --show layers`\n   - `polygraphy run dynamic_identity.onnx --trt --onnxrt` Comparing TensorRT And ONNX-Runtime Outputs\n   - `polygraphy run dynamic_identity.onnx --trt --fp16 --onnxrt --input-shapes X:[1,2,4,4]` Comparing TensorRT Precisions\n   - [run/05_comparing_with_custom_input_data](https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy/examples/cli/run/05_comparing_with_custom_input_data)\n   - `polygraphy surgeon sanitize model.onnx --fold-constants -o folded.onnx` 可以 fold constant, 作为 op 参数, 不用作为 input\n\n## install\n\n1. `pip install onnxruntime`\n1. [c++直接下载编译好的库](https://github.com/microsoft/onnxruntime/releases)\n\n## [sample](https://github.com/microsoft/onnxruntime-inference-examples/tree/main/c_cxx)\n\n1. 测试\n\n```sh\ngit clone --depth=1 https://github.com/microsoft/onnxruntime-inference-examples.git\ncd onnxruntime-inference-examples/c_cxx/\nmake -p build\ncd build\ncmake -DONNXRUNTIME_ROOTDIR=/xxx/onnxruntime-linux-x64-1.15.1 ..\nmake -j4\ncurl https://media.githubusercontent.com/media/onnx/models/main/vision/classification/squeezenet/model/squeezenet1.0-7.onnx --output squeezenet.onnx\n./build/model-explorer/model-explorer squeezenet.onnx\n```\n\n1. [c\\+\\+ sample code](https://github.com/microsoft/onnxruntime-inference-examples/blob/main/c_cxx/model-explorer/model-explorer.cpp)\n\n```c++\n#include \"onnxruntime_cxx_api.h\"\n\n// Load the model and create InferenceSession\nOrt::Env env;\nstd::string model_path = \"path/to/your/onnx/model\";\nOrt::Session session(env, model_path, Ort::SessionOptions{ nullptr });\n\n// Load and preprocess the input image to\n// inputTensor, inputNames, and outputNames\n...\n\n// Run inference\nstd::vector outputTensors =\n session.Run(Ort::RunOptions{nullptr},\n \t\t\tinputNames.data(),\n\t\t\t&inputTensor,\n\t\t\tinputNames.size(),\n\t\t\toutputNames.data(),\n\t\t\toutputNames.size());\n\nconst float* outputDataPtr = outputTensors[0].GetTensorMutableData();\nstd::cout << outputDataPtr[0] << std::endl;\n```\n\n1. sample 解析\n   - Session 处理各种环境信息，比如模型信息， 环境变量等，同时也进行调度， 不负责管理模型输入输出数据\n   - 由 Ort::Value::CreateTensor 申请模型输入输出的内存， 所有权归上层应用\n\n## 模型优化\n\n### sample\n\n## links\n\n1. [支持的平台选择](https://onnxruntime.ai/index.html#getStartedTable)\n1. [onnxruntime.ai](https://onnxruntime.ai/docs/execution-providers/)\n1. [docs](https://onnxruntime.ai/docs/execution-providers/)\n1. [onnxruntime/core/providers](https://github.com/microsoft/onnxruntime/tree/eed02a3f782407e569c29a8a86c58a4d398d0b0e/onnxruntime/core/providers)\n1. [vitis ai onnxruntime samples](https://github.com/Xilinx/Vitis-AI/tree/c55b7565bde608dd65dda94abea154ad7db4d594/examples/vai_library/samples_onnx)\n1. [onnxruntime vitis support](https://github.com/search?q=repo%3Amicrosoft%2Fonnxruntime%20USE_VITISAI&type=code)\n1. [tda4 onnx runtime](https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/07_03_00_07/exports/docs/tidl_j7_02_00_00_07/ti_dl/docs/user_guide_html/md_tidl_osr_onnxrt_tidl.html)\n1. [doxygen](https://onnxruntime.ai/docs/api/c/struct_ort_1_1_session.html)\n1. [推理模型部署(一)：ONNX runtime 实践](https://zhuanlan.zhihu.com/p/582974246)\n"},{"title":"Quantization","url":"/2023/06/12/Quantization/","content":"\n## 量化（定浮点转换）\n\n1. If you are using reduced precision, run the network in FP32. If it produces the correct result, it is possible that lower precision has an insufficient dynamic range for the network.\n   - 如果您使用降低的精度，请以 FP32 运行网络。如果它产生正确的结果，则较低的精度可能导致网络的动态范围不足。\n1. 也有人称量化为`定点化`，但是严格来讲所表示的范围是缩小的。定点化特指 scale 为 2 的幂次的线性量化，是一种更加实用的量化方法。\n1. 由于线性量化引入的额外量化/反量化计算都是标准的向量操作，也可以使用 SIMD 进行加速，带来的额外计算耗时不大。\n1. [A Practical Guide to Neural Network Quantization](https://cms.tinyml.org/wp-content/uploads/industry-news/tinyML_Talks-_Marios_Fournarakis_210929.pdf)\n1. 量化的是输入和算子的参数\n1. https://blog.csdn.net/niaolianjiulin/article/details/82764511\n1. NVIDIA’s Turing architecture introduced INT4 precision\n1. 不是所有的 nvidia gpu 都支持 4bit 量化， Turing 架构之前的 Pascal、Volta 等架构就不提供对 4-bit 定点数的硬件加速支持。\n1. 是的,绝大多数 Nvidia GPU 都原生支持 8-bit 整数(INT8)定点数运算。\n1. 如果处理器不支持 4bit 量化； 那么 4bit 量化只能减少内存使用\n1. 模型量化还有一个潜在的好处是降低运行时内存占用，这个特性无论是在移动端还是云端都是具有现实意义的。\n   - 降低内存占用与内存读写\n1. 运行时内存：参数 weight 只占很少一部分， 大部分内存占用来自激活值 activation。如何才能用量化降低内存占用，只有一个方式: 将尽可能多的 layer 的激活值都进行量化 。\n1. 注意 weight, activation 和 op 之间的关系，如果 weight 和 activation 都是 fp32, 需要使用 fp32 op 实现版本，如果都是 int8, 需要使用 int8 op 实现版本。\n1. 为什么权重不能是 pre-tensor 呢？这个对精度的影响太大了，所以一般不用。那输入就可以 pre-tensor？当然可以，也经过测试了，对精度的影响不是很大，完全可以用。\n1. 这就是 pre-channel 或者详细点就是 per-output-channel 也就是卷积输出通道\n1. Explicit vs Implicit Quantization\n   - 显示量化：能控制在何处进行量化，例如：pytorch_quantization\n   - 隐私量化：不能控制, 例如：python onnx 转 trt\n   - [tensorrt 官方 int8 量化方法汇总](https://zhuanlan.zhihu.com/p/648877516)\n\n### PTQ\n\n1. qat vs ptq\n   ![qat vs ptq](https://i.ibb.co/XJnYcYY/i-Rl-Ilz-S8-XJ.png)\n1. 交叉熵或者 percentile 的方式进行校准\n\n### [QAT](https://oldpan.me/archives/quantize-in-action-tensorrt-8)\n\n1. 量化后，通常需要调整神经网络(NN)中的参数。这可以通过 retraining 模型来执行，该过程称为量化感知训练（QAT）\n1. QAT 中需要 QDQ 算子，QuantizeLiner 和 DequantizeLiner\n1. QAT 量化中最重要的就是 fake 量化算子，fake 算子负责将输入该算子的参数和输入先量化后反量化，然后记录这个 scale，FQ(fake-quan)算子会将 FP32 精度的输入和权重转化为 INT8 再转回 FP32，记住转换过程中的尺度信息。这些 fake-quan 算子在 ONNX 中可以表示为 QDQ 算子\n\n## LLM\n\n1. 可量化的参数包括: 权重和激活值（Weight and Activation），对于矩阵乘法 Y = WX，W 为权重，X 就是激活值（输入）。\n\n## papers\n\n### 综述\n\n1. [A Survey on Model Compression for Large Language Models](https://arxiv.org/abs/2308.07633)\n\n### [SmoothQuant](https://arxiv.org/abs/2211.10438)\n\n1. SmoothQuant 观察到不同的 token 在它们的通道上展示出类似的变化，引入了逐通道缩放变换，有效地平滑了幅度，使得模型更易于量化。\n1. INT8 SmoothQuant (W8A8)\n1. 量化模式:Given a matrix (2D tensor) of shape M x N (M rows and N columns) where M is the number of tokens and N is the number of channels. TensorRT-LLM has the three following modes to quantize and dequantize the elements of the tensor:\n   - Per-tensor: It uses a single scaling factor for all the elements,\n   - Per-token: It uses a different scaling factor for each token. There are M scaling factors in that case, 激活和权重都可以\n   - Per-channel: It uses a different scaling factor for each channel. There are N scaling factors in that case， 激活和权重都可以\n1. 可以分别进行 per-tensor, per-token, per-channel 量化\n\n### [GPTQ](https://arxiv.org/abs/2210.17323)\n\n1. W4A16\n\n### [AWQ](https://arxiv.org/abs/2306.00978) 激活感知权重量化\n\n1. weight int4; Weight-Only 只量化权重, 激活不量化\n1. The INT4 and INT8 Weight-Only techniques consist in quantizing the weights of a model and dequantizing those weights on-the-fly in linear layers (Matmuls). The activations are encoded using floating-point values (FP16 or BF16). To use INT4/INT8 Weight-Only methods, the user must determine the scaling factors to use to quantize and dequantize the weights of the model.\n1. W4A16\n1. 权重并不是同等重要的，通过保留 1%的显著权重可以大大减少量化误差。\n1. per-channel(针对 activation) 对权重做量化，权重矩阵的列, 每个 d 一个 scale,如果 tensor 中有几列为 fp16, 其他列为 int8，那么对硬件不友好。\n1. per-channel 在对权重量化前先求出权重 channel 对应的激活 channel 的平均值， 对权重做量化前，每个 channel 先乘以对应的平均值\n\n## links\n\n1. [Pytorch 模型量化](https://www.cnblogs.com/LXP-Never/p/16822727.html)\n","tags":["AI","Quantization"]},{"title":"pytorch","url":"/2023/06/09/pytorch/","content":"\n## compile\n\n1. 直接用 torch.compile 编译一个函数(都是用 torch 实现的，可以生成 triton 函数)来加速，[getting start](https://pytorch.org/docs/stable/torch.compiler_get_started.html)\n1. CPython 的 Frame Evaluation API（框架评估 API）是一个高级特性，允许开发者定制 Python 解释器执行代码的方式。这个 API 提供了一种方法，能够在 Python 解释器运行时动态插入和替换代码执行的框架，从而可以进行代码插桩、动态优化或其他高级操作。\n1. torch.compile is a PyTorch function introduced in PyTorch 2.x that aims to solve the problem of accurate graph capturing in PyTorch and ultimately enable software engineers to run their PyTorch programs faster.\n1. [torch dynamo 加速性能例子](https://github.com/pytorch/pytorch/issues/93794)\n1. [torch dynamo 后端](https://pytorch.org/docs/stable/torch.compiler.html)\n1. In 2.0, if you wrap your model in model = torch.compile(model), your model goes through 3 steps before execution:\n   - Graph acquisition: first the model is rewritten as blocks of subgraphs. Subgraphs which can be compiled by TorchDynamo are “flattened” and the other subgraphs (which might contain control-flow code or other unsupported Python constructs) will fall back to Eager-Mode.\n   - Graph lowering: all the PyTorch operations are decomposed into their constituent kernels specific to the chosen backend.\n   - Graph compilation, where the kernels call their corresponding low-level device-specific operations.\n1. For instance, something innocuous as a print statement in your model’s forward triggers a graph break. 模型中打印会中断 graph 执行\n1. `torch.compile()` We expect this one line code change to provide you with between 30%-2x training time speedups on the vast majority of models that you’re already running.\n1. 芯片商可以集成到 dynamo 后端（和 inductor 并列)或 inductor 后端(和 triton 并列)或混合后端\n   - 目前 Inductor 有两个后端：(1) 生成多线程 CPU 代码的 C++，(2) 生成高性能 GPU 代码的 Triton\n1. 编译过程\n   ![编译过程](https://pytorch.org/assets/images/pytorch-2.0-img4.jpg)\n1. 堆栈\n   ![堆栈](https://pytorch.org/assets/images/pytorch-2.0-img12.png)\n1. Our philosophy on PyTorch has always been to keep flexibility and hackability our top priority, and performance as a close second.\n1. In the past 5 years, we built `torch.jit.trace, TorchScript, FX tracing, Lazy Tensors`. But none of them felt like they gave us everything we wanted. Some were flexible but not fast, some were fast but not flexible and some were neither fast nor flexible. Some had bad user-experience (like being silently wrong). While TorchScript was promising, it needed substantial changes to your code and the code that your code depended on. This need for substantial change in code made it a non-starter for a lot of PyTorch users. 之前的都不行\n1. `TorchDynamo` TorchDynamo acquired the graph 99% of the time, correctly, safely and with negligible overhead – without needing any changes to the original code. This is when we knew that we finally broke through the barrier that we were struggling with for many years in terms of flexibility and speed.\n1. [pytorch 2.0](https://pytorch.org/get-started/pytorch-2.0/)\n1. [eager mode vs graph mode:](https://pytorch.org/blog/optimizing-production-pytorch-performance-with-graph-transformations)\n   - 在 PyTorch 中，\"Eager Execution\"（即即时执行模式）是指一种动态图计算模式，其中每个操作都立即被执行，而不是被先放入计算图中。这与静态图计算框架（如 TensorFlow 的早期版本）的工作方式不同。在即时执行模式中，你可以像使用 NumPy 一样进行操作，逐步构建计算图，方便调试和交互。\n   - 开发用 eager 模型，部署用 torchscript 来过渡到 graph mode(会做融合)\n   - With TorchScript, PyTorch provides ease-of-use and flexibility in eager mode, while seamlessly transitioning to graph mode for speed, optimization, and functionality in C++ runtime environments.\n1. torch.jit.trace 基于字节码， torch.jit.script 基于 AST\n1. torch inductor\n   - 作为 torch.compile 的基础技术，配备 Nvidia 和 AMD GPU 的 TorchInductor 将依靠 OpenAI Triton 深度学习编译器来生成高性能代码并隐藏底层硬件细节。OpenAI Triton 生成的内核可实现与手写内核和专用 cuda 库(如 cublas)相当的性能。\n1. torch.compile 的基础是新技术——TorchDynamo、AOTAutograd、PrimTorch 和 TorchInductor\n1. TorchInductor 是一种深度学习编译器，可为多个加速器和后端生成快速代码。对于 NVIDIA 和 AMD GPU，它使用 OpenAI Triton 作为关键构建块。对于 intel CPU，我们使用多线程、向量化指令生成 C++ 代码，并在可能的情况下将适当的操作卸载到 mkldnn。\n\n## base\n\n1. `from torch.utils.cpp_extension import load_inline`可以方便的在 pytorch 中调用 cuda\n1. `torch.cuda.current_stream().synchronize()` 只同步当前 CUDA 流\n1. [nvdia docker](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch)\n1. 比较两个 tensor 是否相近 [torch.isclose](https://pytorch.org/docs/stable/generated/torch.isclose.html)\n   - [numpy.isclose](https://numpy.org/doc/stable/reference/generated/numpy.isclose.html)\n1. 收集 pytorch 环境相关信息：[link](https://github.com/pytorch/pytorch/issues/44299#issue-695606076)\n   - [collect_env.py](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\n1. 导出 onnx 时如果遇见 cuda 算子有问题，可以写个 fake 算子, 直接 return out, 不用计算\n1. `python -m torch.utils.collect_env`\n1. 静态图：先编译，如果增加一个计算，需要重新编译, 改变网络意味着重新开始，类似 C++工程的重头编译\n1. 动态图：实时构图，增加一个计算不用编译，直接在原来的图上添加计算节点，类似 C++工程的增量编译\n1. 深度学习模型实际上就是一个计算图。模型部署时通常把模型转换成静态的计算图，即没有控制流（`分支语句、循环语句`）的计算图。\n1. `device = \"cuda\" if torch.cuda.is_available() else \"cpu\"`\n1. pytorch 导出模型时会显示 ONNX IR, 类似三段式\n   - [Functions to export models into the ONNX IR format.](https://github.com/pytorch/pytorch/blob/main/torch/onnx/utils.py#L190)\n   - verbose = True\n1. pytorch 导出 onnx 问题\n   - 先跑通 model(x)\n   - Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DataContainer\n   - 需要处理 dataloader DataContainer 到 list\n1. pytorch 模型结构定义之后, 有些算子不一定会使用，导出 onnx 模型时不使用的算子不会导出，应为导出模型进行一次模型推理，在推理的过程中记录所有经过的计算，将这些记录整合成计算图\n1. pytorch 为什么不直接用 numpy?\n   - `GPU 支持`：PyTorch 设计之初就考虑到了与 GPU 的兼容性，允许其在 GPU 上直接执行张量运算，大大加快了深度学习模型的训练和推理速度。相比之下，NumPy 主要是为 CPU 设计的，不支持 GPU 或其他类型的加速硬件。\n   - `自动微分`：PyTorch 提供了自动微分功能，这对于深度学习至关重要。通过它的 `autograd` 系统，PyTorch 能够自动计算模型参数的梯度，这对于训练神经网络来说是必需的。NumPy 没有内置这样的功能。\n   - `深度学习特定的操作`：PyTorch 提供了许多专为深度学习设计的操作和函数，如卷积、池化等，这些在 NumPy 中不是直接可用的。\n   - `动态计算图`：PyTorch 使用动态计算图（也称为即时执行），这意味着计算图在运行时动态构建，从而提供了更灵活的编程模式，特别是对于复杂的模型和动态输入。而 NumPy 没有这样的概念。\n   - `可扩展性和生态系统`：虽然 NumPy 在科学计算方面非常强大，但 PyTorch 提供了更适合于大规模、复杂的深度学习模型和应用的工具和库。\n1. 在使用动态图（Dynamic Graph）框架（如 PyTorch 或 TensorFlow 的 Eager Execution 模式）进行单步调试时，并不是每一步操作都会完全重新构建整个计算图。相反，每一步操作通常对应计算图的一部分，这个部分在执行时被动态创建和执行。在单步调试时，整个模型的计算图不会在每一步都被重新构建。只有实际执行的操作会被动态添加到图中。\n1. 在使用动态图框架（如 PyTorch 或 TensorFlow 的 Eager Execution 模式）进行单步调试时，整个模型的计算图并不会在每一步都被重新构建。动态图的特点是在运行时动态构建和执行计算图的一部分，而非整个图。这种方法与静态图框架（如 TensorFlow 的传统模式）形成对比，后者在执行任何计算前需要先构建完整的计算图并对其进行优化。\n1. 循环：\n   - 不固定：动态图\n   - 固定：可以被展开，构成静态图\n1. `torch==1.11.0+cu113`\n1. `pip install torch==1.11.0+cu113 --extra-index-url https://download.pytorch.org/whl/cu113`\n1. `pip freeze | grep torch`: 查看库版本\n1. `pip show torch`: 查看库版本\n1. `python3 -c \"import torch; print(torch.__version__)\"`\n1. pytorch tensor to binary file: `tensor.cpu().numpy().astype(np.float32).tofile(\"test.bin\")`; c++ read binary file\n1. tensor 中取单个元素会降维；例如从二维 tensor 取单行或者单列结果会变为一维 tensor\n1. `help(torch.ones)` 显示函数 help\n1. `print(dir(torch.distributions))` 显示 torch 的 distributions\n\n## [extending](https://pytorch-cn.readthedocs.io/zh/latest/notes/extending/)\n\n### Autograd\n\n1. Autograd Profiler 可以统计 autograd 性能\n1. `c = a.detach().clone()` c 不计算 grad, requires_grad=False\n1. 通过 watch model[0].weight.data 和 model[0].weight.grad 看 weight 值和 grad 变化， [sample](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-optim)\n1. 见 deep_learning.md 下的 backward\n1. 通过 loss 函数求各个 module weights 的 grad，存在 weights tensor.grad 里，中间的 activation 没有 grad, 只有叶子节点有\n1. Operation 对 tensor 求 grad\n1. 自定义 OP 需要继承 torch.autograd.Fuction [pytorch-defining-new-autograd-functions](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-defining-new-autograd-functions)\n   - forward 输入参数个数是 backward 输出参数个数\n   - backward 输入参数个数是 forward 输出参数个数\n   - 通过 ctx 在 forward 和 backward 中传递 tensor, 用于计算梯度\n1. 每个原始的 Autograd 运算符实际上都是在 tensor 上运行的两个函数。 正向函数从输入 tensor 计算输出 tensor。 反向函数接收相对于某个标量值的输出 tensor 的梯度，并计算相对于相同标量值的输入 tensor 的梯度。\n1. 反向传播用于算梯度\n1. backward()实际上是通过 DCG 图从根张量追溯到每一个叶子节点，然后计算将计算出的梯度存入每个叶子节点的.grad 属性中\n1. 在某种程度上，反向传播只是链式法则的一个花哨的名字—— Jeremy Howard\n1. backward 不传入参数时，默认为传入 backward(torch.tensor(1.0))。\n1. [Learning PyTorch with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n1. [用 numpy、PyTorch 自动求导、torch.nn 库实现两层神经网络](https://blog.csdn.net/niexinyu0026/article/details/122262082) [link](https://www.cnblogs.com/luedong/p/14492361.html)\n1. [PyTorch：梯度计算之反向传播函数 backward()](https://blog.csdn.net/baidu_38797690/article/details/122180655)\n\n### Module\n\n1. print(model)只是打印 self 定义的 layer，并不是计算图\n1. [pytorchviz](https://github.com/szagoruyko/pytorchviz) 在 pytorch 中画计算图\n\n## tensor\n\n1. [基础运算](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\n1. [高级](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html)\n1. [d2l ndarray](https://zh.d2l.ai/chapter_preliminaries/ndarray.html)\n1. `x = torch.arange(10)` tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n1. `x = torch.arange(10.0)` tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n1. `type(x)` 打印 x 类型\n1. `torch.randn(2)` tensor([ 0.6872, -0.3433]); 返回一个填充随机正态分布的张量(mean=0, std=1)。即,生成的值大概符合平均数为 0,标准差为 1 的正态分布。\n1. `X = torch.rand(2,20)` 返回一个填充随机均匀分布的张量,即在[0,1)区间内均匀随机。\n1. `x = x.reshape(2, 5)`\n1. `x.shape` torch.Size([2, 5])\n1. `a = torch.tensor(3.4); a.shape` torch.Size([]) 标量\n1. `a = torch.tensor([3.4]);a.shape` torch.Size([1]) 向量\n1. `x.numel()` 10; element number\n1. `len(x)` 2; len()为 python 内置函数， 用于 tensor 时是指 tesnor 的维度（dimension）\n1. `torch.ones(2, 4)`\n1. `torch.zeros(2, 4)`\n1. `X.reshape(-1)` 展平为一维\n1. `//`向下取整除法\n1. `%` 求模，取余\n1. `math.ceil(x)`向上取整数\n1. `math.floor(x)`向下取整\n1. `round(x)` 四舍六入五成双（例如 round(2.5)=2, round(3.5)=4, round(4.5)=4, round(5.5)=6) 小数部分为 0.5 向偶数\n1. tensor 基本运算\n\n```python\nx = torch.tensor([1.0, 2, 4, 8])  # 1.0 mean float\ny = torch.tensor([2, 2, 2, 2])\nx + y, x - y, x * y, x / y, x ** y  # **运算符是求幂运算\n```\n\n1. tensor 矩阵运算\n\n```python\nX = torch.arange(9).reshape(3,3)\nY = torch.arange(9).reshape(3,3)\nX.t() # 转置\nX @ Y # 矩阵乘\ntorch.matmul(X, Y) # 矩阵乘\nX * Y # 元素分别相乘\nX + 5 # 广播：分别加5\n\n```\n\n1. `torch.exp(x)` tensor 求指数\n1. concat and condition\n\n```python\nX = torch.arange(12, dtype=torch.float32).reshape((3,4))\nY = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\ntorch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)  # dim=0按行拼接， dim=1按列拼接， 0代表最里面一个维度\nX == Y # shape: torch.Size([3, 4])\nX < Y\nX > Y\nX.sum() # 求和\nX.mean() # 求均值\n```\n\n1. 原位操作 下划线\n\n```python\na = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\nprint('a:')\nprint(a)\nprint(torch.sin(a))   # this operation creates a new tensor in memory\nprint(a)              # a has not changed\n\nb = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\nprint('\\nb:')\nprint(b)\nprint(torch.sin_(b))  # note the underscore\nprint(b)\n```\n\n1. 索引和切片\n\n```python\nX[-1] # 取最后一个元素\nX[1:3] # 取第二个和第三个元素，不包含X[3]\nX[1,2] = 9 # 赋值\nX[0:2, :] = 12 # 前两行赋值为12\nX = torch.arange(12, dtype=torch.float32).reshape((3,4))\nX[1:3, 2:4] 取右下角两行两列\n```\n\n1. 节省内存: 注意 Y = Y + X 与 X += Y 效果不一致\n\n```python\nbefore = id(Y)\nY = Y + X\nid(Y) == before # False\n\nZ = torch.zeros_like(Y)\nprint('id(Z):', id(Z))\nZ[:] = X + Y\nprint('id(Z):', id(Z))\n\nbefore = id(X)\nX += Y\nid(X) == before # True\n```\n\n1. 和 numpy 转换\n\n```python\nA = X.numpy()\nB = torch.tensor(A)\ntype(A), type(B)\n\na = torch.tensor([3.5])\na, a.item(), float(a), int(a) # (tensor([3.5000]), 3.5, 3.5, 3)  tuple\n```\n\n1. type 转换 [type](https://pytorch.org/docs/stable/tensors.html)\n\n```python\nX = torch.arange(12).reshape(3, 4)\nX.dtype # torch.int64\nX.to(torch.float32)\ntorch.tensor([1.2]).type() # torch.FloatTensor\ntorch.tensor([1.2]).dtype # torch.float32\n```\n\n1. 判断是否有 gpu\n\n```python\nif torch.cuda.is_available():\n    print('We have a GPU!')\nelse:\n    print('Sorry, CPU only.')\n\nif torch.cuda.is_available():\n    my_device = torch.device('cuda')\nelse:\n    my_device = torch.device('cpu')\nprint('Device: {}'.format(my_device))\n\nx = torch.rand(2, 2, device=my_device)\nprint(x)\n```\n\n1. cpu cuda\n\n```python\ntensor.device #查看在gpu还是cpu\nX.cpu()\nX.cuda() # 默认cuda:0\nX.to(\"cpu\")\nX.to(\"cuda:0\")\n\ndevice = torch.device(\"cuda\")\nY = X.to(device)\n\n# PyTorch的GPU端对tensor数据类型的支持是有限的,很多运算只实现了float/double类型的GPU支持。\nX = torch.arange(9).reshape(3,3).cuda().to(torch.float32)\nY = torch.arange(9).reshape(3,3).cuda().to(torch.float32)\ntorch.matmul(X, Y)\nX @ Y\n```\n\n1. model cpu cuda\n1. model.to(\"cuda\")会将 model 参数放在显存中\n\n## [torchscript](https://cloud.tencent.com/developer/article/2010575)\n\n1. TorchScript 是 PyTorch 模型推理部署的中间表示，可以在高性能环境 libtorch（C ++）中直接加载，实现模型推理，而无需 Pytorch 训练框架依赖。torch.jit 是 torchscript Python 语言包支持，支持 pytorch 模型快速，高效，无缝对接到 libtorch 运行时，实现高效推理。\n1. torchscript 主要包含权重和计算过程(IR; 类似.text; 各种函数，有一个入口)\n1. trace 指的是进行一次模型推理，在推理的过程中记录所有经过的计算，将这些记录整合成计算图\n   - for 循环被展开\n1. script 会直接解析网络定义的 python 代码，生成抽象语法树 AST，因此这种方法可以解决一些 trace 无法解决的问题，比如对 branch/loop 等数据流控制语句的建图。\n   - for 循环编程子图\n\n## model\n\n1. [PyTorch 模型定义的方式](https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%BA%94%E7%AB%A0/5.1%20PyTorch%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E7%9A%84%E6%96%B9%E5%BC%8F.html)\n   - Sequential 适用于快速验证结果\n   - ModuleList 和 ModuleDict 在某个完全相同的层需要重复出现多次时，非常方便实现，可以一行顶多行；\n1. 定义模型时可以直接初始化参数，也可以后期加载\n   - `self.lin1.weight = nn.Parameter(torch.arange(-4.0, 5.0).view(3, 3))`\n1. basic\n\n```python\nimport torch\nfrom torch import nn\nmodel = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\nX = torch.rand(2, 20)\nmodel(X)\nhelp(model) #可以看帮助\n\noutput = x\nprint('输入:', output)\n\n# 查看每层输出\nfor name, layer in model.named_children():\n    output = layer(output)\n    print('层:', name, ',', '输出:', output)\n\n# 查看每层参数\nfor name, param in model.named_parameters():\n    print(f\"Name: {name}\")\n    print(f\"Size: {param.size()}\")\n    print(f\"Values: \\n{param.data}\")\n```\n\n1. model parmas\n\n```python\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"./Llama-2-7b-hf\")\nprint(model)\nfrom prettytable import PrettyTable\n\ntable = PrettyTable(['Name', 'Shape', 'Param'])\n\nfor name, param in model.named_parameters():\n    param_count = param.numel()\n    table.add_row([name, param.shape, param_count])\nprint(table)\n\nnum_parameters = sum(p.numel() for p in model.parameters())\nprint(num_parameters)\n```\n\n1. 打印 model parameters [autograd_tutorial](https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html) [model turorial](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html)\n   - `list(model.parameters())`\n   - `list(model.named_parameters())`\n   - `print(model.layer2.weight[0][0:10])`\n   - `print(model[0].weight)` sequnce\n   - `print(model[0].bias)`\n   - `print([param for name,param in model.named_parameters()][0])`\n1. [torch save and load](https://pytorch.org/tutorials/beginner/saving_loading_models.html#save)\n\n```python\ntorch.save(model, PATH)\n\nmodel = torch.load(PATH)\nmodel.eval()\n```\n\n## model export\n\n1. Expected all tensors to be on the same device\n   - vsocde 断点到\\_jit_pass_onnx_constant_fold， 查看 graph， 会看到每个 op 所在 device 和代码位置\n1. 注意 pytorch 模型在转出 onnx 时会做融合或拆分，不是一对一的关系\n1. nonzero: B=A[b > c], b > c 是 bool, B 取 b > c 的值; tensorrt8.6 之前不支持， 可用 topk + mask 替代\n1. nonzero: Returns the indices of the elements that are non-zero\n1. squeeze: 如果某一维是 1，把它删掉。需要判断， 也会导致图里面有 If\n1. update: a[100] = 1 不会产生新 tensor, tensorrt 不支持，导出的图会有问题， 用 scatter 替换,scatter 会生成新的 tensor\n1. a[b>c]会产生 nonzero, 有 nonzero 就会有 if 分支，就是动态图\n1. export 加 verbose = True, # onnx op 显示代码位置; pytorch1.10 还不支持，需要搜 log\n1. 当我们使用了 Pytorch 里面的[]索引操作或者其它需要判断的情况，ONNX 模型会多出一些 if OP，这个时候这个 if OP 的输入已经是一个确定的 True，因为我们已经介绍过为 False 那部分的子图会被丢掉。[link](http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/AI%20%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%85%B6%E5%AE%83%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/ONNX%E5%86%8D%E6%8E%A2/)\n1. [不要用 ONNX_FALLTHROUGH](https://glaringlee.github.io/onnx.html#id17)\n   - 此模式可用于导出未在 ONNX 中注册和支持的任何运算符（ATen 或非 ATen）。导出失败并按原样导出操作符，作为自定义操作。\n1. copy.deepcopy() 可能导致 export 出问题\n1. export 有问题需要从最外层一步步定位到内部看哪里出问题了，提前 return,多层次上都提前返回, 一步步定位哪里导出的问题, 注意 export 函数中 model_output 不要填，否则会强制输数个数报错\n   - 不要的代码先注释掉\n1. export 出问题可以先定位具体哪个 module 出的问题\n1. Function 类有一个很好的性质：如果它定义了 symbolic 静态方法，该 Function 在执行 torch.onnx.export() 时就可以根据 symbolic 中定义的规则转换成 ONNX 算子。\n1. 导出 onnx 模型时不用 pytorch 自定义算子不用定义 backward, trace 只运行 forward\n1. ONNX 是一套标准，本身并不包括实现。导出为 onnx 时我们就简略地定义一个 ONNX 可变形卷积算子，而不去写它在某个推理引擎上的实现。\n1. symbolic 符号函数，可以看成是 PyTorch 算子类的一个静态方法。在把 PyTorch 模型转换成 ONNX 模型时，各个 PyTorch 算子的符号函数 symbolic 会被依次调用，以完成 PyTorch 算子到 ONNX 算子的转换。\n   - 第一个参数就固定叫 g，它表示和计算图相关的内容。g 有一个方法 op。在把 PyTorch 算子转换成 ONNX 算子时，需要在符号函数中调用此方法来为最终的计算图添加一个 ONNX 算子。\n   - g.op(\"Asinh\", input)则完成了 ONNX 算子的定义。其中，第一个参数\"Asinh\"是算子在 ONNX 中的名称。\n1. (good)PyTorch 转 ONNX 的跟踪导出法是不是万能的。如果我们在模型中做了一些很“出格”的操作，跟踪法会把某些取决于输入的中间结果变成常量，从而使导出的 ONNX 模型和原来的模型有出入。 [link](https://mmdeploy.readthedocs.io/zh-cn/v1.2.0/tutorial/03_pytorch2onnx.html#id4)\n   - 涉及张量与普通变量转换的逻辑都会导致最终的 ONNX 模型不太正确, 例如 64 要用 torch.tensor(64)\n   - 我们也可以利用这个性质，在保证正确性的前提下令模型的中间结果变成常量。这个技巧常常用于模型的静态化上，即令模型中所有的张量形状都变成常量; shape to constant\n1. [Saving and loading a general checkpoint in PyTorch](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html)\n1. PyTorch 模型在导出到 ONNX 模型时，模型的输入参数的类型必须全部是 torch.Tensor\n1. [Export a PyTorch model to ONNX](https://pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html)\n   - pytorch model to onnx(导出为 batch 1 时需要设置输入数据第一维度为 1,)\n   - [Compare the PyTorch results with the ones from the ONNX Runtime](https://pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html#compare-the-pytorch-results-with-the-ones-from-the-onnx-runtime)\n   - [Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)\n\n```python\nimport torch\nimport torchvision.models as models\nimport torch.onnx as onnx\n\n# 加载预训练模型\nmodel = models.resnet18(pretrained=True) ## 有网络结构\n\n# 创建一个输入张量作为示例; 注意数据和模型要么都在cuda，要么都在cpu\ninput_data = torch.randn(1, 3, 224, 224)\ninput_data = torch.randn(1, 3, 224, 224).cuda()\n\n# 设置模型为推理模式\nmodel.eval() # 只影响, 不启用 Batch Normalization 和 Dropout\n\n# 将模型和输入张量转换为ONNX格式\nonnx_path = \"model.onnx\"\nonnx.export(model, input_data, onnx_path) # 有参数可以做常量折叠\n\nprint(\"模型已成功转换为ONNX格式并保存在:\", onnx_path)\n\n# Input to the model\nx = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\ntorch_out = torch_model(x)\n\n# Export the model\ntorch.onnx.export(torch_model,               # model being run\n                  x,                         # model input (or a tuple for multiple inputs)\n                  \"super_resolution.onnx\",   # where to save the model (can be a file or file-like object)\n                  export_params=True,        # store the trained parameter weights inside the model file\n                  opset_version=10,          # the ONNX version to export the model to\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\n                  input_names = ['input'],   # the model's input names\n                  output_names = ['output'], # the model's output names\n                  verbose = True,            # onnx op 显示代码位置\n                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n                                'output' : {0 : 'batch_size'}})\n```\n\n## model info\n\n1. 需要 model.eval()； 不会打印 dropout 层, 不启用 Batch Normalization 和 Dropout\n1. [torchinfo](https://github.com/TylerYep/torchinfo)\n1. `summary(model, [(1, 1, 32000), (1,1,32000), (1, 1, 32000), (1,1,32000)], dtypes=[torch.long, torch.long, torch.long, torch.long])`\n\n## tools\n\n1. [captum](https://github.com/pytorch/captum) Model interpretability and understanding for PyTorch\n1. [tensorboard_tutorial](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)\n\n## samples\n\n2. 量化模型\n\n```python\n\n```\n\n## links\n\n1. [tutorials](https://github.com/pytorch/tutorials/tree/main)\n1. [分布式训练](https://pytorch.org/tutorials/distributed/home.html)\n1. [tutorials](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\n1. [docs](https://pytorch.org/docs/stable/index.html)\n1. [中文](https://yiyibooks.cn/yiyibooks/pytorch_131/index.html)\n"},{"title":"colab","url":"/2023/06/08/colab/","content":"\n## link\n\n1. [importing_libraries](https://colab.research.google.com/notebooks/snippets/importing_libraries.ipynb)\n"},{"title":"roadmap","url":"/2023/06/08/roadmap/","content":"\n## 如何成为资深 AI 推理软件工程师(chatgpt)\n\n成为资深 AI 推理软件工程师需要具备以下关键步骤和能力：\n\n1. 建立扎实的 AI 基础：首先，建立扎实的人工智能基础知识。深入了解机器学习和深度学习的基本概念、算法和原理，包括神经网络、优化算法、模型评估等。\n2. 学习推理引擎和框架：研究和学习不同的 AI 推理引擎和框架，如 TensorRT、OpenVINO、TVM 等。了解它们的特点、优化技术和部署方法，熟悉使用它们进行模型推理的流程和工具。\n3. 实践推理项目：通过实践推理项目来应用所学的知识。参与真实的 AI 推理项目，涉及模型优化、推理性能调优、模型部署等方面。通过实践，积累经验并提升自己在 AI 推理软件工程方面的能力。\n4. 熟悉硬件加速和优化技术：AI 推理软件工程师需要了解硬件加速和优化技术，如 GPU、FPGA 等。学习并熟悉如何利用硬件加速器进行模型推理的优化，包括模型转换、量化、融合等技术。\n5. 深入了解模型优化和压缩：掌握模型优化和压缩的技术，以减小模型的大小和计算量，提高推理性能。了解剪枝、量化、知识蒸馏等技术，并能够应用到实际项目中。\n6. 掌握性能分析和调优工具：熟悉性能分析和调优工具，如 TensorBoard、NVIDIA Nsight 等。能够使用这些工具来分析和调优模型推理的性能瓶颈，并做出相应的优化策略。\n7. 深入理解模型部署和集成：了解模型部署和集成的流程和工具。掌握常见的部署方式，如嵌入式设备、边缘计算平台、云平台等，能够将优化后的模型成功部署到目标平台上。\n8. 持续学习和关注最新技术：AI 领域发展迅速，持续学习和关注最新的技术和研究成果至关重要。阅读相关的学术论文、技术博客、研讨会等，保持对新技术和方法的敏感度\n\n要成为 AI 推理软件专家，可以采取以下步骤：\n\n1. 建立扎实的 AI 基础知识：深入学习机器学习和深度学习的基本理论、算法和模型。了解常见的神经网络结构和优化算法，如卷积神经网络、循环神经网络、注意力机制等。\n2. 学习推理引擎和框架：熟悉不同的 AI 推理引擎和框架，如 TensorRT、OpenVINO、ONNX Runtime 等。了解它们的特点、优化技术和部署方法，掌握使用它们进行模型推理和优化的技巧。\n3. 实践推理项目：参与实际的 AI 推理项目，从数据预处理、模型选择、推理性能优化等方面积累实践经验。通过实践中的挑战和解决方案，提升在 AI 推理软件工程中的能力。\n4. 研究模型优化和压缩技术：深入研究模型优化和压缩技术，如剪枝、量化、蒸馏等。了解如何减小模型的大小和计算量，提高推理性能，并能够应用这些技术来优化和压缩模型。\n5. 掌握性能分析和调优工具：熟悉性能分析和调优工具，如 TensorBoard、Profiler 等。学会使用这些工具来分析和评估模型的推理性能，并针对性地进行调优。\n6. 深入了解硬件加速和优化技术：了解常见的硬件加速器，如 GPU、FPGA 等，并学习如何充分利用硬件加速器提高模型推理的效率和速度。掌握硬件相关的优化技术和工具。\n7. 持续学习和追踪最新技术：AI 领域发展迅速，保持学习和追踪最新技术和研究成果非常重要。阅读学术论文、参加研讨会和会议，关注业界领先公司的技术博客和公开项目，与其他专家进行交流和讨论。\n8. 实践和贡献开源项目：参与开源项目，为 AI 推理软件的开发和优化做出贡献。通过与开源社区的合作和分享，不断提升自己的技能和知识，并展示自己在 AI 推理软件方面的专业能力。\n9. 与领域专家合作：寻求与领域内的专家合作和交流，参与相关的研究团队或实验室，共同探讨和解决 AI 推理软件领域的问题。与其他专家的合作将加速个人成长和知识积累。\n\n通过持续的学习、实践和不断提升自身能力，成为 AI 推理软件专家需要一定的时间和经验积累，但坚持不懈和热爱学习将助您在这个领域取得成功。\n\n## 如何学习推理优化技术\n\n要学习推理优化技术，可以按照以下步骤进行：\n\n1. 建立基础知识：首先，建立对深度学习模型推理的基本理解。了解常用的深度学习框架，如 TensorFlow、PyTorch 等，以及它们的推理过程和优化方法。掌握深度学习模型的基本结构和常用的推理算法。\n2. 学习推理优化技术的理论：深入学习推理优化技术的理论知识。这包括模型剪枝、量化、蒸馏、深度压缩等技术。了解这些技术的原理、优势和适用场景。阅读相关的学术论文、研究报告和专业书籍，掌握推理优化技术的最新研究成果。\n3. 掌握推理优化工具和框架：学习使用推理优化工具和框架，如 TVM、ONNX Runtime、NVIDIA TensorRT 等。了解它们的使用方法、API 接口和优化技术支持。通过实际操作和示例代码，掌握如何在具体的环境中应用推理优化技术。\n4. 实践项目和案例研究：选择一些实际的项目和案例，应用推理优化技术进行实践研究。可以使用开源数据集和模型，尝试不同的推理优化方法，并进行性能对比和评估。通过实践项目，加深对推理优化技术的理解和掌握。\n5. 参加培训和研讨会：参加相关的培训课程、研讨会和工作坊，了解推理优化技术的最新进展和应用实践。这些培训活动通常由专业机构、学术界和工业界组织举办，提供专业指导和实际操作经验。\n6. 加入社区和论坛：加入相关的学术社区、开发者论坛和技术交流平台，与其他学习者和专家进行交流和讨论。这些社区和论坛提供了学习资源、问题解答和经验分享，可以获取更多的学习资料和实践建议。\n7. 持续学习和实践：推理优化技术是一个不断发展和演进的领域，需要持续学习和实践。跟踪最新的研究成果和技术进展，参与相关的竞赛和挑战，与行业内的专家保持联系，不断提升自己的技能和知识水平。\n   总之，学习推理优化技术需要坚实的基础知识、实践项目的经验和与专家的交流。通过不断学习和实践，逐步提升自己的技能，成为一名熟练的推理优化技术专家。\n\n## 如何学习推理框架\n\n学习推理框架的步骤如下：\n\n1. 确定学习目标：明确你想学习的推理框架，如 TensorRT、ONNX Runtime、OpenVINO 等。了解每个框架的特点、适用场景和优势，选择符合你学习目标和需求的框架。\n2. 学习基础知识：掌握深度学习的基础知识，包括神经网络的结构、训练和推理过程等。了解常见的深度学习框架，如 TensorFlow、PyTorch 等，并学习它们的基本使用方法和 API。\n3. 框架官方文档：查阅所选推理框架的官方文档。官方文档是学习和使用框架的最重要资源，其中包含框架的详细说明、API 文档、示例代码和教程。仔细阅读官方文档，了解框架的功能、接口和使用方法。\n4. 在线教程和视频：寻找相关的在线教程和视频资源。许多推理框架提供官方的在线教程，可以帮助你快速入门和掌握基本操作。此外，还可以搜索其他人分享的教程和视频，如博客文章、YouTube 视频等。\n5. 实践项目：选择一些实际的项目来应用所学的推理框架。可以使用开源的深度学习模型和数据集，将它们加载到推理框架中，进行模型推理和结果分析。通过实践项目，加深对框架的理解和掌握。\n6. 社区交流和讨论：参与相关的开发者社区和技术论坛，与其他学习者和专家进行交流和讨论。这些社区和论坛提供了宝贵的学习资源、问题解答和经验分享，可以从中获得更多的学习资料和实践建议。\n7. 持续学习和更新：推理框架和相关技术在不断发展和更新，保持学习的持续性非常重要。关注最新的技术进展、研究成果和开发者社区的动态，了解最新的框架版本和功能，不断更新自己的知识和技能。\n\n## 如何学习 python\n\n成为 Python 专家需要付出一定的努力和持续的学习。以下是一些建议：\n\n1. 扎实的基础知识：掌握 Python 的基础知识，包括语法、数据类型、流程控制、函数、模块等。理解 Python 的核心概念和特性，建立坚实的基础。\n2. 深入学习核心库：熟悉 Python 的核心库，如 NumPy、Pandas、Matplotlib 等。这些库是 Python 数据分析和科学计算的基石，深入学习它们的使用方法和高级特性。\n3. 探索常用库和框架：学习并熟悉 Python 的常用库和框架，如 Django、Flask、TensorFlow、PyTorch 等。了解它们的特点、用途和 API，能够使用它们构建实际项目。\n4. 阅读优秀的 Python 代码：阅读优秀的 Python 代码可以帮助你学习和理解高效、优雅的编程风格和技巧。参考开源项目、博客文章、书籍等资源，学习其他 Python 专家的实际应用。\n5. 实践项目：选择一些实际的项目来应用 Python 技术。通过实践项目，深入理解 Python 在不同领域的应用，掌握解决问题的实际方法和技巧。\n6. 参与社区和开源项目：积极参与 Python 开发者社区和开源项目。与其他 Python 专家交流和讨论，分享自己的经验和知识。通过参与开源项目，提升自己的编码能力和项目管理能力。\n7. 持续学习和更新：Python 生态系统在不断发展和更新，保持学习的持续性非常重要。关注最新的 Python 版本、库和框架的发布，了解最新的技术趋势和最佳实践。\n8. 掌握调试和性能优化技巧：学会使用调试工具和技巧来解决代码中的问题。了解 Python 的性能优化技巧，提高代码的执行效率和资源利用率。\n9. 教学和分享经验：通过教学和分享经验，巩固自己的知识，并帮助其他人学习 Python。可以编写博客文章、教程，参与技术分享活动等。\n   最重要的是保持对 Python 的热情和持续的学习态度。不断学习新知识，探索新技术，并将其应用到实际项目中，才能逐步成为 Python 专家。\n\n## AI 推理框架\n\nAI 推理框架是用于部署和执行训练好的深度学习模型进行推理（预测）的软件工具。这些框架提供了一套接口和工具，使得将训练好的模型部署到生产环境中变得更加容易和高效。\n以下是一些常见的 AI 推理框架：\n\n1. TensorFlow：由 Google 开发的开源深度学习框架，提供了高度灵活的图计算模型和丰富的工具和库。TensorFlow 的推理部分包括 TensorFlow Serving 和 TensorFlow Lite，用于在服务器和嵌入式设备上进行推理。\n2. PyTorch：由 Facebook 开发的开源深度学习框架，提供了动态图计算模型和易用性。PyTorch 的推理部分包括 TorchScript 和 TorchServe，用于在生产环境中进行高性能推理。\n3. ONNX：开放神经网络交换格式（Open Neural Network Exchange），是一种开放的中间表示格式，可用于在不同的深度学习框架之间转换和共享模型。ONNX 定义了一套通用的推理规范，使得模型能够在支持 ONNX 的框架中进行部署和执行。\n4. TensorRT：英伟达（NVIDIA）推出的推理加速引擎，针对英伟达 GPU 进行了优化。TensorRT 通过网络优化、低精度推理和并行计算等技术，提供高性能和低延迟的深度学习推理能力。\n5. OpenVINO：英特尔（Intel）推出的开放式视觉推理和神经网络优化工具包，用于在英特尔硬件上进行高效的深度学习推理。OpenVINO 支持多种模型优化技术，包括量化、剪枝和硬件加速等。\n   这些 AI 推理框架都有各自的特点和优势，选择合适的框架取决于你的应用需求、硬件平台和编程偏好。学习和掌握这些框架可以帮助你有效地进行深度学习模型的推理部署，并实现高性能和高效能的预测任务。\n\n## collect\n\n1. 模型部署\n   - [(good)mmdeploy 文档](https://mmdeploy.readthedocs.io/zh-cn/latest/tutorial/01_introduction_to_model_deployment.html)\n   - Transform DNNs to Low Level Code: `model -> graph -> kernel -> device` [triton next](https://www.jokeren.tech/slides/triton_next.pdf)\n1. [部署优化](https://microsoft.github.io/AI-System/SystemforAI-8-Inference.pdf)\n   - 延迟优化\n     - 量化\n     - 剪枝\n     - layer or tensor fusion\n     - 内存优化\n     - 调度优化\n     - cache\n   - 吞吐优化\n     - batch\n1. [(very good)神经网络编译器优化](https://microsoft.github.io/AI-System/SystemforAI-9-Compilation%20and%20Optimization.pdf)\n   - 计算图优化(graph)\n   - 内存优化\n   - 调度优化\n   - kernel 优化 - 算子表示与调度逻辑分离 - 自动调度搜索与代码生成\n     ![架构图](https://i.ibb.co/mHM8NPV/pl-Uhf-F2-HIN.png)\n1. 深度学习基础, 熟悉常见的模型架构，先不用管精度，专注在推理和性能\n   - [llm-course](https://github.com/mlabonne/llm-course)\n   - [李沐《动手学深度学习》](https://zh.d2l.ai/)\n   - 李宏毅\n   - [kaggle learn](https://www.kaggle.com/learn)\n   - [DeepLearningSystem](https://github.com/chenzomi12/DeepLearningSystem)\n   - [microsoft/generative-ai-for-beginners](https://github.com/microsoft/generative-ai-for-beginners) 注意仓库里有中文翻译\n   - [AI-For-Beginners](https://github.com/microsoft/AI-For-Beginners)\n   - [statquest](https://www.youtube.com/@statquest)\n1. 大模型推理\n   - [llm.c](https://github.com/karpathy/llm.c)\n   - [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) faster transformer 后续\n   - [mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm)\n   - [llama2.c](https://github.com/karpathy/llama2.c) 可以放到 chatgpt 中解释基本流程\n   - [llama.cpp](https://github.com/ggerganov/llama.cpp)\n   - [DeepSpeed](https://github.com/microsoft/DeepSpeed)\n   - [ggerganov/ggml](https://github.com/ggerganov/ggml)\n   - [nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all)\n   - [vllm](https://github.com/vllm-project/vllm)\n     - [slides](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit#slide=id.g24ad94a0065_0_209)\n     - In vLLM, we identify that the performance of LLM serving is bottlenecked by memory.\n     - [blog](https://blog.vllm.ai/2023/06/20/vllm.html)\n   - [TGI: huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference)\n   - [Mozilla-Ocho/llamafile](https://github.com/Mozilla-Ocho/llamafile)\n   - [GGML - AI at the edge](https://ggml.ai/)\n1. 大模型推理优化\n   - xformers, flash attention\n   - [LLM Inference Performance Engineering: Best Practices](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)\n     - Our goal? `The fastest time to first token, the highest throughput, and the quickest time per output token`. In other words, we want our models to generate text as fast as possible for as many users as we can support.\n     - Optimizing LLM inference benefits from general techniques such as: Operator Fusion, Quantization, Compression, Parallelization, KV caching\n     - Identify your optimization target: Do you care about interactive performance? Maximizing throughput? Minimizing cost? There are predictable trade-offs here.\n   - [韩松大模型优化介绍](https://www.youtube.com/watch?v=7WeraZ0LLlg)\n   - [TensorRT-LLM key-features](https://github.com/NVIDIA/TensorRT-LLM/tree/main?tab=readme-ov-file#key-features)\n   - [一文总结当下常用的大型 transformer 效率优化方案](https://www.cvmart.net/community/detail/7069)\n   - [mit-han-lab/streaming-llm](https://github.com/mit-han-lab/streaming-llm) 韩松论文\n   - [LLM 推理优化技术综述：KVCache、PageAttention、FlashAttention、MQA、GQA](https://zhuanlan.zhihu.com/p/655325832)\n   - [DeepSpeed-FastGen：通过 MII 和 DeepSpeed-Inference 实现 LLM 高吞吐量文本生成](https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-fastgen/chinese/README.md)\n   - 推理性能关键在： 矩阵乘法， kv cache 索引， embedding 索引（数据库技术）\n   - NCCL is a communication framework used by PyTorch to do distributed training/inference. text-generation-inference make use of NCCL to enable Tensor Parallelism to dramatically speed up inference for large language models.\n   - [大模型推理妙招—投机采样（Speculative Decoding）](https://zhuanlan.zhihu.com/p/651359908)\n     - 大小模型：小模型运行 n 次，大模型运行一次(输入为 n 次的拼接，利用 batch，多 batch 的延迟和单 batch 相差不大)，如果两个结果相差不多，接收，如果相差较大, 从相差较大的 token 重新生成，大模型的输出作为该位置的输出\n   - 矩阵运算：\n     - GEMM\n     - [cutlass](https://github.com/NVIDIA/cutlass)\n   - token merging\n   - diffusers 加速技术\n     - [accelerate-inference-of-text-to-image-diffusion-models](https://huggingface.co/docs/diffusers/tutorials/fast_diffusion#accelerate-inference-of-text-to-image-diffusion-models)\n     - [diffusers/optimization](https://huggingface.co/docs/diffusers/optimization/fp16)\n1. 大模型量化\n   - 韩松视频\n1. 大模型训练（选）\n   - finetune\n   - [NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM)\n1. onnx\n   - 熟悉规范\n1. onnx runtime\n   - 推理\n   - 模型优化， 量化\n1. TVM\n1. MLIR IREE\n1. TensorRT\n   - 各种教程\n   - c++ 推理接口, sample, plugin\n1. pytorch\n   - [tutorials](https://github.com/pytorch/tutorials/tree/main)\n1. 模型优化和压缩技术(剪枝、量化、 蒸馏)\n1. [ncnn](https://github.com/Tencent/ncnn)\n1. Modular vs OctoML (MLIR vs TVM)\n   - OctoML: 是一个真正的“输入模型，自动化输出硬件和软件”，而且随时可以部署\n1. LLVM\n   - 学习模块化\n1. 汇编\n1. neon\n1. dsp\n1. gpu cuda 加速\n1. 计算机体系结构\n1. 熟悉常用的算子\n1. chatgpt 使用: vscode ...\n1. 线性代数\n1. python\n1. 性能优化\n1. 推理两大主题： 内存管理(onnx runtime tensor) + 调度(onnx runtime session)\n1. 内存管理\n   - [memory-allocators-101-write-a-simple-memory](https://arjunsreedharan.org/post/148675821737/memory-allocators-101-write-a-simple-memory)\n   - malloc 源码\n1. 调度\n\n- dag\n\n## links\n\n1. [深度学习推理引擎的一些思考](https://www.zhihu.com/column/frozengene)\n1. [developer-roadmap](https://github.com/kamranahmedse/developer-roadmap)\n1. [LLVM 之父 Chris Lattner：我的 AI 基础设施软件构建理念](https://mp.weixin.qq.com/s?__biz=MzU5ODY2MTk3Nw==&mid=2247492618&idx=1&sn=a20f4828b9ab3e3cee3fedfd906e0eb2&chksm=fe426a3cc935e32a8312ce9efbb4f2640787508d3e811579bbffe918685cdb07a8bd8e3ffc4b&scene=132&exptype=timeline_recommend_article_extendread_samebiz#wechat_redirect)\n1. [TVM：成为深度学习领域的“Linux”](https://mp.weixin.qq.com/s?__biz=MzU5ODY2MTk3Nw==&mid=2247487015&idx=1&sn=04282e2d15eca05eb56062062b46e781&chksm=fe418011c9360907048966af43299fa55b570c9d634c4bcdb3702c6e4d8101a72ef07d5ec77f&scene=21#wechat_redirect)\n","tags":["Roadmap"],"categories":["Others","Roadmap"]},{"title":"AI-ops","url":"/2023/06/08/AI-ops/","content":"\n## NOTE\n\n1. [Operators](https://github.com/onnx/onnx/blob/main/docs/Operators.md)\n1. [torch.nn](https://pytorch.org/docs/stable/nn.html)\n1. [pytorch2.0 ops](https://pytorch.org/get-started/pytorch-2.0/#primtorch-stable-primitive-operators)\n1. [netron 在线打开 onnx](https://netron.app/)\n1. [arm 官方加速库](https://github.com/ARM-software/ComputeLibrary)\n\n## 常见的算子\n\n深度学习中常见的算子包括：\n\n1. 卷积层 (Convolutional Layer)：用于提取输入数据的特征，通过卷积操作在输入数据上滑动卷积核，并对每个位置进行乘法累加操作。\n2. 池化层 (Pooling Layer)：用于降低特征图的空间维度，常见的池化操作包括最大池化和平均池化，通过选取窗口中的最大值或平均值来减小特征图的大小。\n3. 全连接层 (Fully Connected Layer)：将输入数据的每个元素与权重相乘并加上偏置，得到输出向量，常用于将卷积层或池化层的输出展平并连接到最终的分类或回归层。\n4. 激活函数 (Activation Function)：引入非线性变换，增加模型的表达能力。常见的激活函数包括 ReLU (Rectified Linear Unit)、Sigmoid、Tanh、Leaky ReLU 等。\n5. 批归一化 (Batch Normalization)：用于提高网络的训练速度和稳定性，通过对每个小批量数据进行归一化操作，减少内部协变量偏移。\n6. 残差连接 (Residual Connection)：用于解决深层网络的梯度消失和梯度爆炸问题，通过将输入直接与输出相加，使得网络可以学习残差信息。\n7. 循环神经网络 (Recurrent Neural Network, RNN)：用于处理序列数据的神经网络，通过引入循环结构，使得网络可以对先前的信息进行记忆和处理。\n8. 长短期记忆网络 (Long Short-Term Memory, LSTM)：一种特殊的循环神经网络，通过引入门控机制，有效地处理长序列依赖关系。\n9. 注意力机制 (Attention Mechanism)：用于对输入数据中的不同部分分配不同的注意力权重，以便于网络更加关注重要的信息。\n10. 卷积转置层 (Transpose Convolutional Layer)：用于进行上采样操作，通过反卷积操作将低分辨率特征图转换为高分辨率特征图。\n\n以上只是深度学习中常见的一些算子，实际上还有很多其他的算子和结构，不同的模型和任务可能会使用不同的算子组合。\n除了上述提到的常见算子外，深度学习中还有其他一些常用的算子，包括但不限于以下几种：\n\n1. 扩张卷积 (Dilated Convolution)：通过在卷积核中引入间隔，使得卷积核在输入上以不同的空洞率进行卷积操作，从而增加感受野大小。\n2. 反向卷积 (Deconvolution)：也称为转置卷积或分数步长卷积，用于进行上采样操作，将低分辨率特征图转换为高分辨率特征图。\n3. 转移学习 (Transfer Learning)：通过将预训练模型的权重作为初始权重进行微调，从而加快训练速度和提升模型性能。\n4. 高斯滤波 (Gaussian Filtering)：一种平滑图像的操作，通过对每个像素及其周围像素进行加权平均，减小图像中的噪声和细节。\n5. 非极大值抑制 (Non-Maximum Suppression)：用于边缘检测和目标检测等任务中，通过对局部最大值进行抑制，提取出稀疏且准确的边缘或目标位置。\n6. 随机失活 (Dropout)：用于减少过拟合的正则化技术，通过随机将部分神经元的输出设置为零，降低神经元之间的依赖关系。\n7. 权重衰减 (Weight Decay)：一种正则化技术，通过在损失函数中引入权重的 L2 范数惩罚项，降低权重的绝对值，减小模型的复杂度。\n8. 梯度裁剪 (Gradient Clipping)：用于解决梯度爆炸问题，通过限制梯度的范数阈值，防止梯度值过大导致训练不稳定。\n9. 自注意力机制 (Self-Attention Mechanism)：一种注意力机制的变种，用于对序列或图像中不同位置之间的关系进行建模，通过计算位置之间的相对权重来加强或抑制不同位置的表示。\n10. 生成对抗网络 (Generative Adversarial Network, GAN)：由生成器和判别器组成的对抗性模型，通过博弈训练的方式，使生成器逐渐生成逼真的样本。\n\n这些算子在不同的深度学习任务和模型中发挥着重要的作用，可以根据具体的问题和需求选择适合的算子进行使用。\n\n## 其他算子\n\n1. clip 夹子：np.clip(x, min_val, max_val), 限制输入范围\n1. nozero: B=A[b > c], b > c 是 bool, B 取 b > c 的值; tensorrt8.6 之前不支持， 可用 topk + mask 替代\n1. update: a[100] = 1 不会产生新 tensor, tensorrt 不支持，用 scatter 替换\n1. scatter：根据输入、index, update 生成一个新 tensor\n1. gatther: 求子集\n1. inverse:求矩阵逆\n\n### [激活函数](https://www.jiqizhixin.com/articles/2021-02-24-7)\n\n#### relu\n\n1. `max(0,x)`\n1. [可视化](https://www.desmos.com/calculator/hi1hxgezri)\n1. code\n\n```c++\ndouble relu(double x) {\n    return (x > 0) ? x : 0;\n}\n```\n\n#### sigmoid\n\n1. `1/(1+exp(-x))`\n1. [可视化](https://www.desmos.com/calculator/hp9f98wxrh)\n1. code\n\n```c++\ndouble sigmoid(double x) {\n    return 1.0 / (1.0 + std::exp(-x));\n}\n```\n\n1. 优化： 指数运算很耗时， 查表法\n\n```c++\n#include <iostream>\n#include <cmath>\n#include <vector>\n\n// 查表法实现的Sigmoid函数\ndouble sigmoid(double x, const std::vector<double>& sigmoidTable) {\n    // 查表获取结果\n    double index = (x + 5.0) / 10.0 * sigmoidTable.size();\n    int lowerIndex = static_cast<int>(std::floor(index));\n    int upperIndex = lowerIndex + 1;\n    double lowerValue = sigmoidTable[lowerIndex];\n    double upperValue = sigmoidTable[upperIndex];\n    double fraction = index - lowerIndex;\n\n    return lowerValue + (upperValue - lowerValue) * fraction;\n}\n\nint main() {\n    // 构建查表\n    const int tableSize = 1000;\n    std::vector<double> sigmoidTable(tableSize + 1);\n    for (int i = 0; i <= tableSize; ++i) {\n        double value = static_cast<double>(i) / tableSize * 10.0 - 5.0;\n        sigmoidTable[i] = 1.0 / (1.0 + std::exp(-value));\n    }\n\n    double x = 0.0;\n    double result = sigmoid(x, sigmoidTable);\n\n    std::cout << \"Input: \" << x << std::endl;\n    std::cout << \"Output: \" << result << std::endl;\n\n\n    return 0;\n}\n```\n\n#### tanh\n\n1. 双曲正切函数： `(exp(x) - exp(-x)) / (exp(x) + exp(-x))`\n1. [可视化](https://www.desmos.com/calculator/8xqzjeujqw)\n\n## links\n\n1. [Operators](https://github.com/onnx/onnx/blob/main/docs/Operators.md)\n1. [tensorrt onnx operators.md](https://github.com/onnx/onnx-tensorrt/blob/main/docs/operators.md)\n1. [数学公式可视化](https://www.desmos.com/calculator?lang=zh-CN)\n1. [深度学习编译器 - 算子的高效实现](https://zhuanlan.zhihu.com/p/511043383)\n   - 自动生成算子代码\n1. [onnxruntime/contrib_ops](https://github.com/microsoft/onnxruntime/tree/main/onnxruntime/contrib_ops)\n"},{"title":"chatgpt","url":"/2023/06/01/chatgpt/","content":"\n## 用处\n\n1. 将自己记录输入到 gpt 中，看看自己理解对不对\n1. (very good)`梳理`： 输入关键理解，让 gpt 帮助梳理, 写 blog 很有用\n1. 生成文字，可以用作文档、PPT 和简历\n1. python 转换 c++\n1. 优化 c++函数\n1. 深度学习常见的算子？\n1. c++测试配合 [wandbox](https://wandbox.org/) 在线编译\n1. python 测试配合 [google colab](https://colab.research.google.com/) or [codespaces](https://github.com/codespaces)\n1. 解释代码\n1. 添加注释\n\n## samples\n\n1.  如何成为资深 AI 推理软件工程师\n1.  先创建一个大小为 100 的二进制文件， 读入一个二进制文件，输出各元素，float 类型\n1.  如何使用 openmp 优化 c++?\n1.  如何研究推理优化技术\n1.  cuda 核函数测试程序\n1.  pytorch 基本概念\n1.  pytorch to onnx sample\n1.  量化上述模型\n\n## Note\n\n1. vpn 选美国\n1. 注册邮箱后可能需要清理缓存，或者换个浏览器\n\n## links\n\n1. [tutorial](https://mp.weixin.qq.com/s/7k5IlgxKMirED653oJfDgg)\n1. [google bard](https://bard.google.com/)\n1. [LLaMa](https://labs.perplexity.ai/?utm_content=first_codellama&s=u&utm_source=twitter&utm_campaign=labs)\n1. [claude](https://claude.ai/) 可用于分析 csv 文件, 生成文字，用于汇报\n1. [bot-on-anything](https://github.com/zhayujie/bot-on-anything)\n"},{"title":"RISC-V","url":"/2023/05/10/RISC-V/","content":"\n## sync, atomic, mbarrier/memory fence\n1. [gpt](https://chatgpt.com/share/6848df65-bf90-8004-bb23-ddb78a7f8f9a)\n\n## NOTE\n\n1. GCS(global control system): 是硬件编程的主要接口，通过GCS对硬件做配置， 发送命令，做同步等\n1. 片内互联：NOC（Network on Chip，片上网络）：NOC 是一种在芯片内部多个处理单元或功能模块之间实现通信的网络结构。传统的总线或交叉开关（crossbar）在面对大规模多核系统时效率和可扩展性有限，而 NOC 通过引入网络的概念（如包交换、路由等），解决了这些问题。\n1. DNOC 通常指的是 Distributed Network on Chip（分布式片上网络），是对传统 NOC（Network on Chip） 的一种增强和扩展形式，主要目的是在 大规模多核系统（如AI芯片、处理器阵列、Chiplet架构） 中提高通信效率、带宽利用率和系统可扩展性。\n   - DNOC 是一种 去中心化 的片上网络架构，它将网络功能（如路由、流控、调度等）分布式部署在芯片的多个区域，而不是集中由某个模块控制。每个计算单元或子系统可能都有其独立的网络管理单元，通信更加自组织、自适应。\n   - 在某个 AI 芯片中，假设有 1024 个小计算单元（PE），这些单元通过 DNOC 互联。每个 PE 可以根据自己当前的任务负载、数据流方向、邻居拥塞情况来动态选择数据传输路径，而不是由一个中心节点统一控制路由和调度。这种结构可以显著提升整体吞吐量，并降低热点区域的拥塞。\n   - DNOC 是为了解决传统 NOC 在大规模异构/并行系统中面临的通信瓶颈和扩展性问题所提出的分布式互连架构。\n   - PE访存其他PEC存储的通路，包括DRAM和shared memory\n   - PE，on-chip-DRAM以及Serdes，GTE/PCIE之间的互联组成片内互联NoC。\n   - 分层级：PE间， PEC间， PEG间, 分别用到crossbar, xbar(crossbar switch), ring互联技术\n1. XBAR 是 “Crossbar Switch（交叉开关）” 的缩写，是一种用于模块间高速互连的通信结构，广泛应用于片上系统（SoC）、多核处理器、存储控制器等场景中。\n   - XBAR 就像一个“十字路口的交换机”，输入输出可以任意连接，不互相干扰。\n1. 片间互联：C2C（Chip-to-Chip，芯片间通信）: C2C 指的是不同芯片之间的通信接口或协议，用于在物理上分离的芯片之间传输数据。例如 CPU 和 GPU、CPU 和外设、AI 加速芯片之间的通信。\n1. SerDes 是 Serializer/Deserializer（串并转换器） 的缩写，是现代高速芯片和通信系统中非常关键的电路组件，用于在芯片之间以高速传输大量数据。\n   - Serializer（串行器）：将多个并行数据位打包转换为一个高速串行数据流。\n   - Deserializer（反串行器）：将接收到的高速串行数据流重新还原为并行数据。\n   - SerDes 是一种将宽并行数据转换为高速串行数据并进行还原的电路，用于提高芯片通信速率、降低引脚和布线开销，是高速数字通信系统的核心技术之一。\n   - 在芯片通信中，如果用并行总线传输 64-bit 数据，每根线的速率可能受限，而且布线复杂、功耗高、信号干扰大。SerDes 可以将 64-bit 并行数据压缩成一条高速串行链路传输，显著减少 IO 引脚和布线压力，还能支持更高的传输速率。\n1. sync和mbarrier: 同步（线程屏障）\t内存屏障（内存栅栏）\n1. AI 芯片的数据流架构（Dataflow Architecture）是一种针对深度学习等数据密集型计算任务优化的硬件架构。与传统的控制流（Control Flow）架构（如 CPU）和 SIMD/SIMT 架构（如 GPU）不同，数据流架构专注于最大化数据重用、提高带宽利用率和降低能耗。\n1. 数据流架构是一种计算范式，强调数据在计算单元之间的流动，计算任务在数据到达时自动触发，不需要中央控制器指令来驱动每一步操作。\n   - 数据流架构是一种基于数据依赖触发计算的体系，与传统的冯·诺依曼（von Neumann）“指令驱动”模型不同——没有程序计数器，节点只有当所有输入准备好时才执行\n   - 其核心是将计算流程建模为“有向无环图”（DAG）：节点表示运算，边表示数据依赖。计算由数据流动主动驱动\n1. Gpgpu是用编程框架和硬件去找应用场景，找算法。Dsa是用应用场景和算法去设计硬件和编程框架。在长期稳定的方向，dsa一定比gpgpu好。但现在的ai领域是找场景找算法，dsa总是会慢一拍，甚至编程框架再加半拍。\n1. 第一代TPU准确的说是一个ASIC，专针对卷积矩阵计算加速，很多难以表达的计算都在Host CPU上，而这种跨PCIE的通信对性能的损害是非常大的。[link](https://mp.weixin.qq.com/s/zy1SyXjbnH3Ix6GigArD0w)\n   - 引入RISC-V 后直接在device的cpu上算\n   - 微架构一大变化是引入了RISC-V，将TPU进一步演进成一种GPDSA for AI，\n1. 地平线的device BPU上没有cpu core, 只有host有cpu core   \n1. 仿真器只需要模拟指令集里的指令就能运行生成的程序\n1. 指令分为：\n   - 机器码指令\n   - 汇编指令（和机器码指令并不是一一对应)\n1. RVV支持动态向量长度（VL)，在RVV指令集中，vl（Vector Length）是一个关键的控制寄存器，用于指定向量指令操作的数据元素数量，vl值可以根据不同的循环迭代或不同的数据集大小来动态调整，使用起来很方便。\n1. ​在 LLVM 中，可扩展向量类型（Scalable Vector Type）是一种特殊的向量类型，旨在支持那些在编译时无法确定其长度的向量架构，如 Arm 的 SVE（Scalable Vector Extension）和 RISC-V 的 V 扩展。​这类类型通过引入 vscale 概念，使得编译器生成的代码能够适应不同硬件平台的向量长度，从而实现更高的可移植性和性能。\n1. vscale 是一个在程序运行期间由硬件确定的常数，用于表示向量长度的可扩展因子。​它使得编译器生成的代码能够在不同的硬件平台上自动适应向量长度，无需重新编译。​例如，在一个支持 128 位向量的硬件上，vscale 可能为 1；而在支持 512 位向量的硬件上，vscale 可能为 4。\n1. LLVM IR中 <vscale x n x ty> vscale表示一个向量有多少个ty的元素\n1. 在计算机体系结构和芯片设计领域，PE（Processing Element，处理单元） 是并行计算系统中的基本计算单元。它通常包括算术逻辑单元（ALU）、寄存器文件、控制逻辑和本地存储器。多个 PE 可以组成阵列，协同处理大量数据，实现高效的并行计算。\n1. RISC-V 64位架构（RV64）\n   - 基础指令长度是 32位（4字节）。这称为 定长指令格式（Fixed-Length），简化了硬件解码。\n   - 扩展：16位压缩指令 RV64C 是 RV64 的压缩指令扩展，通过引入 16 位短指令，在不牺牲功能的前提下，显著减少了代码体积和取指开销，是现代 RISC-V 设计中不可或缺的一部分。\n1. 64 位架构 相对于 32 位架构，最大的优势是支持更大的内存、更强的数据处理能力和更高的性能扩展性，适合现代计算需求；而 32 位架构更适合低功耗、资源受限的嵌入式系统。   \n\n## RVV\n1. [PPT(great)](https://eupilot.eu/wp-content/uploads/2022/11/RISC-V-VectorExtension-1-1.pdf)\n1. [LLVM RISCVVectorExtension](https://llvm.org/docs/RISCV/RISCVVectorExtension.html)\n1. [RVV SPEC](https://github.com/riscvarchive/riscv-v-spec/blob/v1.0/v-spec.adoc#3-vector-extension-programmers-model)\n1. [RVV intrinsic](https://github.com/riscv-non-isa/rvv-intrinsic-doc/releases/download/v1.0-ratified/v-intrinsic-spec.pdf)\n1. 1个RV core 可以有1个VPU vector core, 不是多个RV core共享一个vector core\n1. RISC-V将使用Vector扩展而不是SIMD扩展是非常明智的举动。\n   - RVV 是现代向量处理架构，强调可扩展性、可移植性和灵活性，而传统 SIMD 更强调针对特定硬件优化的定长指令集。\n   - RVV 支持以下高级访问模式：Strided Load/Store（步长访问）Indexed Load/Store（索引访问）Segmented Load/Store（段式结构体访问）Masking（带掩码的条件执行）\n   - \n\nSIMD 通常只支持简单的连续加载（load/store），不适合不规则内存模式。\n1. 注意向量寄存器和标量寄存器的区别，向量寄存器倾向于一次处理一个向量\n1. `ELEN`: The maximum size in bits of a vector element that any operation can produce or consume, ELEN ≥ 8, which must be a power of 2.\n1. `VLEN`: The number of bits in a single vector register, VLEN ≥ ELEN, which must be a power of 2, and must be no greater than 216.\n1. 处理器改变向量长度： RVV 可以像arm SVE 一样，同一个应用程序可以在支持不同向量长度的机器上运行，而不需要重新编译代码，这对RVV生态大有好处。\n   - 使用 VLA 模型，向量长度在运行时由 vsetvl 设置，自动适配硬件能力，比如你写 vsetvl(16)，但硬件只支持 8 个元素，它会自动返回实际可处理的数量。\n\n``` c++\n// 需要程序员手动处理分批, 只要n个元素 > 128bit, 就需要for循环，RVV最小128bit, 为了代码可移植性强, 支持各种处理器，需要分批来做 \nvoid vec_add(float* a, float* b, float* c, int n) {\n  for (int i = 0; i < n;) {\n    size_t vl = vsetvl_e32m1(n - i);\n    vfloat32m1_t va = vle32_v_f32m1(&a[i], vl);\n    vfloat32m1_t vb = vle32_v_f32m1(&b[i], vl);\n    vfloat32m1_t vc = vfadd_vv_f32m1(va, vb, vl);\n    vse32_v_f32m1(&c[i], vc, vl);\n    i += vl;\n  }\n}\n```\n\n1. 数据长度改变：RVV支持动态向量长度（VL)，在RVV指令集中，vl（Vector Length）是一个关键的控制寄存器，用于指定向量指令操作的数据元素数量，vl值可以根据不同的循环迭代或不同的数据集大小来动态调整，使用起来很方便。\n   - 支持小于向量长度的运算， vl指定\n1. vl describes how many elements of the vector (starting from the element zero) we\nare going to operate\n1. vl: Vector Length (not to be confused with VLEN!)\n   - vl: 处理的数据长度\n   - VLEN：处理器的vector regitster长度： VLEN is not a great name so read it as “vector register size (in bits)”\n1. ELEN is a constant parameter chosen by the implementor\n1. 两个register协助计算\n   - vtype\n     - sew: Standard Element Width. Size in bits of the elements being operated\n       - 8 ≤ sew ≤ ELEN\n     - lmul: Length Multiplier. Allows grouping registers\n   - vl(register, and instruction parameter)\n     - 0 ≤ vl ≤ (VLEN / sew) × lmul\n   - tail agnostic (VTA)\n   - mask agnostic (VMA)\n1. vtype 和 vl 是控制“怎么操作向量”的寄存器，而 v0~v31 是用来“装向量数据、参与实际计算”的寄存器。\n   - vtype, vl是普通寄存器\n1. [Control and Status Registers](https://github.com/riscvarchive/riscv-v-spec/blob/v1.0/v-spec.adoc#3-vector-extension-programmers-model)   \n1. vsetvli rd, rs, eN,mX,tP,mP ` vsetvli t0, a0, e32, m4, tu, ma   # Tail undisturbed, mask agnostic`\n   - rs是application vector length(AVL), 表示程序输入的长度参数，c++ intrinsic里设置的vl参数\n   - eN: sew\n   - mX: lmul\n   - tP: tail policy\n   - mP: mask policy\n   - eN,mX,tP,mP先更新vtype寄存器, tp, mP不用于计算vlmax，后面计算会用到\n   - 根据vtype计算vlmax, vlmax不存下来\n   - rd = min(rs, vlmax)\n1. passthru: 如果 mask[i] == true，那么 res[i] = op1[i] + op2[i] else  res[i] = passthru[i]， \n   - 也可以用posion来替代passthru，在 LLVM 中，poison 是一种特殊的“无效值”：A poison value is a value that, if used in a computation, results in undefined behavior.\n   - passthru可以用来实现triton的load other功能\n\n``` c++\n%res = call <vscale x 4 x i32> @llvm.riscv.vadd.vv.mask(\n    <vscale x 4 x i32> %passthru,\n    <vscale x 4 x i32> %op1,\n    <vscale x 4 x i32> %op2,\n    <vscale x 4 x i1> %mask,\n    i32 %vl)\n```\n1. often have a passthru or mask operand\n1. masked instructions must have the mask register in v0,\n1. undisturbed and agnostic polic(当vl < vlmax时, > vl如何填充，当mask为fail时如何填充， udisturbed表示不改变，agnostic表示行为未定义)\n   - tail undisturbed. Tail elements in the destination register are left unmodified.\n   - tail agnostic. Can behave like tail undisturbed or, alternatively, all the bits of the tail\nelements of the destination register are set to 1\n   - mask undisturbed. The corresponding element of the destination register is left\nunmodified.\n   - mask agnostic. The corresponding element of the destination register is either left\nunmodified or all its bits are set to 1.\n\n### scalar和vector相互转换\n1. 单个scalar到vector广播：vmv\n1. 将多个scalar拼成vector:\n   - 先申请内存，将多个scalar填充，从内存加载 scalar 数组到vector\n   - 初始化全0，vmv.s.x：将一个整数标量写入向量寄存器的第一个元素; vslideup.vx：将已有向量元素上移，为插入新元素腾出空间\n1. 取vector第一个元素到scalar: vmv\n1. 取任意位置的元素（需使用滑动指令或拷贝) vslide1down 配合 vmv.x.s\n\n### mask\n1. RISC-V 向量架构中，布尔向量寄存在 v0 ~ v7 之间。注意不是布尔向量寄存器\n1. 每个布尔向量最多可以包含 VL 个 bit（比如 256 bit），只占 一部分寄存器宽度。即使你只用了 8 bit 的布尔掩码，也必须占用一个布尔寄存器（v0~v7 之一）。\n1. vbool32_t 占用一个布尔寄存器（v0~v7），但该寄存器中只用到 VL / 32 个 bit。VL是向量寄存器bit数， 32是element位宽, 表明向量中有多少个element\n1. RVV 支持不同精度的布尔类型，称为 vboolM，M 表示每个布尔元素占用位数\n1. 指令中有表示mask的位，vm字段,  vm=0 表示使用掩码（Masked），vm=1 表示不使用掩码\n\n## links\n\n1. [riscv](https://github.com/riscv)\n1. [「硅仙人」吉姆 · 凯勒：我在特斯拉是最闲的员工，却要在英特尔管一万人](https://www.pingwest.com/a/244625)\n1. [不可一世的 Arm，要遭遇 20 年来的最大危机](https://36kr.com/p/2109040627812483)\n   - 吉姆·凯勒在离开英特尔不久之后，于 2021 年加入了一家加拿大 AI 芯片初创公司 Tenstorrent。随后，该公司就宣布将基于 RISC-V 开发自研架构，以此为基础开发高性能 AI 芯片，预计可用于各种应用，包括同时需要 AI 和 HPC 能力的下一代超级计算机。吉姆·凯勒曾经任职于英特尔、DEC、AMD、博通、苹果、特斯拉等公司，担任工程副总裁或首席架构师等工作。过去几十年，他是一手打造苹果 A 系列芯片的设计师，也是帮助 AMD 翻身的「Zen 之父」，还是特斯拉自动驾驶芯片的缔造者。此外，他还是 x86-64 指令集的作者之一。\n"},{"title":"benchmark","url":"/2023/05/10/benchmark/","content":"\n## basic\n\n## links\n\n1. [geekbench 芯片性能统计](https://browser.geekbench.com/v6/cpu)\n1. [抛弃 ARM，高通第五代汽车芯片](https://mp.weixin.qq.com/s/uTp9DXsG0pzTbyux1JgHMA)\n"},{"title":"hightway","url":"/2023/05/09/hightway/","content":"\n## links\n\n1. [highway](https://github.com/google/highway/tree/master)\n1. [getting-started](https://google.github.io/highway/en/master/faq.html#getting-started)\n1. [libjxl reference implementation of JPEG XL (encoder and decoder)](https://github.com/libjxl/libjxl)\n1. [hello_world](https://github.com/chromium/chromium/blob/22b51215052e84b7c4ee35ba2d4d21638b906b35/third_party/distributed_point_functions/code/dpf/internal/get_hwy_mode.cc)\n1. [adding-a-new-op](https://google.github.io/highway/en/master/impl_details.html#adding-a-new-op)\n1. [print simd](https://github.com/google/highway/blob/6c285d64ae50e0f48866072ed3a476fc12df5ab6/hwy/print-inl.h#L59-L59)\n\n## basic\n\n1. [profiler 使用 RAII 可以设置 zone](https://github.com/google/highway/blob/master/hwy/profiler.h)\n   - 生命周期结束做一些操作\n   - add_definitions(-DPROFILER_ENABLED=1)\n1. 可以打印 V.raw\n1. 源码编译 build 目录下工具`hwy_list_targets`可以显示 cpu 支持的加速硬件 [src](https://github.com/google/highway/blob/master/hwy/tests/list_targets.cc); HWY_STATIC_TARGET: SSE2 显示默认支持\n1. 具体运行程序见 cpp_sandbox/highway\n1. Advanced Encryption Standard (AES); NEON_WITHOUT_AES\n1. [获取支持的加速硬件](https://github.com/google/highway/blob/f86369577e9f884f9156dddbf03ee786493c67f9/hwy/examples/benchmark.cc#LL245C5-L245C31)\n   - `cat /proc/cpuinfo` 看 flags\n1. [获取函数运行 cycle](https://github.com/google/highway/blob/33d7d0a6651bd783f97534c936bc9733a3117478/hwy/nanobenchmark.cc#L736)\n\n## [simd library](https://www.reddit.com/r/cpp/comments/106ivke/simd_intrinsics_and_the_possibility_of_a_standard/)\n\n1. [highway cpu 向量加速库 SIMD](https://github.com/google/highway)\n1. [eve](https://github.com/jfalcou/eve)\n1. [xsimd](https://github.com/xtensor-stack/xsimd)\n1. [图像处理与机器学习 Simd](https://github.com/ermig1979/Simd)\n1. [parallelism TS v2](https://en.cppreference.com/w/cpp/experimental/simd)\n1. [simde](https://github.com/simd-everywhere/simde) 如果平台不存在硬件加速，使用模拟函数，如在 x86 上运行 neon\n"},{"title":"neon","url":"/2023/03/23/neon/","content":"\n## base\n\n1. 新的 Armv8a 架构有 32 个 128bit 向量寄存器，老的 ArmV7a 架构有 32 个 64bit（可当作 16 个 128bit）向量寄存器，编码时记得数一下占用多少个专用寄存器（例如 1 个 float32x4 就占用 1 个 128bit 寄存器），别用过量了，避免寄存器溢出 (Register Spilling）(https://blog.csdn.net/qq_41112170/article/details/90286091) 导致的负优化。\n1. armv8: 一条 Neon 指令最多可以计算 4 个 float32，或者 8 个 float16，或者 16 个 int8。\n1. `uint8x16_t` 表示 16 个 uint8\n1. `float32x4_t` 表示 4 个 float32\n\n## [intrinsics](https://developer.arm.com/architectures/instruction-sets/intrinsics/)\n\n1.\n\n## 要使用 NEON 加速步骤：\n\n1. `load` 将相关数组数据加载到 NEON 寄存器。\n1. `calculate` 使用 NEON 指令对多个数据元素进行并行操作。\n1. `store` 将结果从 NEON 寄存器存储回主存储器。\n\n## 编译选项\n\n1. j5 需要 `-ftree-vectorize -funroll-loops` 组合才能优化，单独不行\n1. `-mfpu=neon` 此选项告诉编译器要为 NEON 浮点单元生成代码。这适用于较老的 ARM 架构和编译器版本。\n1. `-march` 使用此选项指定目标架构，例如`-march=armv8-a`。这可以确保编译器针对特定的 ARM 版本生成优化代码。\n1. `-mfloat-abi` 该选项定义了浮点数应该如何在函数调用中传递。有三个选项可以选择：soft、softfp 和 hard。使用 NEON 时，通常建议使用-mfloat-abi=hard。\n1. `-ftree-vectorize` 这是一个优化选项，允许编译器自动将循环转换为使用向量指令。虽然这不是直接与 NEON 相关的，但它可以帮助自动利用 NEON 功能。\n1. `-funroll-loops`这也是一个优化选项，用于循环展开，可能有助于提高 NEON 代码的性能。\n\n## SVE(Scalable Vector Extension)\n\n1. 是 arm AArch64 架构下的下一代 SIMD 指令集，旨在加速高性能计算，SVE 引入了很多新的架构特点\n1. SVE（可扩展矢量指令 Scalable Vector Extension）是针对高性能计算（HPC）和机器学习等领域开发的一套全新的矢量指令集，它是下一代 SIMD 指令集实现，而不是 NEON 指令集的简单扩展。SVE 指令集中有很多概念与 NEON 指令集类似，例如矢量、通道、数据元素等。SVE 指令集也提出了一个全新的概念：可变矢量长度编程模型(VectorLength Agnostic，VLA)。\n1. SVE 指令集的矢量寄存器的长度最小支持 128 位，最大可以支持 2048 位，以 128 位为增量。\n1. SVE 指令集是在 A64 指令集的基础上新增的一组指令集，而 SVE2 是在 ARMv9 架构上发布的，它是 SVE 指令集的一个超集和扩充。\n1. [ARMv9 的 SVE/SVE2 入门教程](https://zhuanlan.zhihu.com/p/399686472)\n\n## links\n\n1. [hello world](http://www.armadeus.org/wiki/index.php?title=NEON_HelloWorld)\n1. [ARM_NEON_CNN 编程](https://github.com/Ewenwan/MVision/blob/master/CNN/HighPerformanceComputing/ARM_NEON_CNN%E7%BC%96%E7%A8%8B.md)\n1. [CPU 优化技术-NEON 指令介绍](https://zhuanlan.zhihu.com/p/441686632)\n"},{"title":"OpenXLA","url":"/2023/03/13/OpenXLA/","content":"\n## links\n\n1. [openxla/xla](https://github.com/openxla/xla)\n1. [tensorflow/xla](https://www.tensorflow.org/xla?hl=zh-cn)\n1. [iree](https://openxla.github.io/iree/#support-matrix)\n1. [简介](https://www.sohu.com/a/653633664_129720)\n1. [机器学习访存密集计算编译优化框架 AStitch，大幅提升任务执行效率](https://www.sohu.com/a/578161914_612370)\n1. [TVM 中文网站](https://tvm.hyper.ai/docs/tutorial/intro/)\n"},{"title":"cross-compile","url":"/2023/03/06/cross-compile/","content":"\n## 作用\n\n## [基本组成](https://blog.csdn.net/u012503639/article/details/104159375)\n\n### binutils\n\n### gcc\n\n### glibc\n"},{"title":"TVM","url":"/2023/02/17/TVM/","content":"\n## base\n\n1. Apache TVM 是一个用于 CPU、GPU 和机器学习加速器的开源机器学习编译器框架，旨在让机器学习工程师能够在任何硬件后端上高效地优化和运行计算。\n1. 可以搜索优化模型，提供了 runtime\n1. 模型相当于语言， TVM 相当于编译器，将模型编译到各种硬件上\n1. TVM: Tensor Virtual Machine\n1. 利用 TVM，机器学习算法可以被自动编译成可供下层硬件执行的机器语言，从而可以利用多种类型的算力。\n1. 我们希望创建一个简洁的抽象统一解决这些问题，希望用一个干净、统一的管理平台把所有的系统和硬件都清晰地抽象出来，以便能够部署模型，并最大限度地利用好目标硬件。\n1. 他们致力于打造一个可扩展、开放、中立的端到端栈，用于深度学习模型的`优化和部署`，目标是减少企业为特定硬件开发和深度学习软件部署所花费的成本和时间。\n1. 按照官方的定义，TVM 是一套完整的 stack，包括神经网络图优化（比如 op fusion）和单个 operation 优化等部分。我习惯于将图优化的部分归类做 Relay 项目，而仅仅把单个 operation 优化看做 TVM，因此文章之后提到的 TVM 基本是指单个算子优化这部分。\n1. 做自动代码优化的优秀项目除了 AutoTVM，还有 Halide(https://halide-lang.org/papers/autoscheduler2019.html)，个人认为目前Halide做代码自动优化做的更好。TVM的基本思路参考自Halide，\n1. TVM 中的调优是指，在给定 target 上优化模型，使其运行得更快。与训练或微调不同，它不会影响模型的准确性，而只会影响 runtime 性能。作为调优过程的一部分，TVM `实现并运行许多不同算子的变体，以查看哪个性能最佳`。这些运行的结果存储在调优记录文件（tune 命令的最终输出）中。\n1. 使用 auto-tuning 模块 AutoTVM 或 AutoScheduler 搜索最佳 schedule\n\n   - scheduler 调度器， schedule 具体调度方式\n\n1. [初识 TVM](https://zhuanlan.zhihu.com/p/88188955)\n\n## build\n\n1.  [docker](https://tvm.hyper.ai/docs/install/docker/)\n1.  使用自己 docker\n1.  [wadewuu/tvm-test/v2](https://hub.docker.com/layers/wadewuu/tvm-test/v2/images/sha256-58a2d0f3f9aefbf1c8d681a32f72ee3ffd2b05971461973ee632f03eaa1fd529?context=explore)\n1.  要先现在源码，编译出 docker，再编译 tvm\n1.  替换源\n1.  build 时 llvm 打开\n\n```\nRUN sed -i s@/archive.ubuntu.com/@/mirrors.tuna.tsinghua.edu.cn/@g /etc/apt/sources.list\nRUN apt-get clean\n```\n\n## python\n\n1.  `python3 -m tvm.driver.tvmc --help`\n1.  `alias tvmc='python3 -m tvm.driver.tvmc'`\n\n## [sample](https://tvm.hyper.ai/docs/tutorial/compile/)\n\n1. `wget https://github.com/onnx/models/raw/b9a54e89508f101a1611cd64f4ef56b9cb62c7cf/vision/classification/resnet/model/resnet50-v2-7.onnx`\n1. [Error when trying to tune the ResNet Model](https://discuss.tvm.apache.org/t/error-when-trying-to-tune-the-resnet-model/13431) model input should frozen\n\n## links\n\n1. [手把手带你遨游 TVM](https://zhuanlan.zhihu.com/p/50529704)\n1. [TVM](https://tvm.apache.org/#about)\n1. [TVM 中文网站](https://tvm.hyper.ai/docs/tutorial/intro/)\n1. [sample](https://tvm.hyper.ai/docs/tutorial/compile/)\n1. [Vitis AI 集成](https://tvm.hyper.ai/docs/how_to/deploy/vitis_ai)\n1. [TI TVM](https://software-dl.ti.com/codegen/docs/tvm/tvm_tidl_users_guide/index.html)\n1. [机器学习编译器的前世今生](https://www.zhihu.com/question/401766029/answer/2733700454)\n1. [TVM：成为深度学习领域的“Linux”](https://mp.weixin.qq.com/s?__biz=MzU5ODY2MTk3Nw==&mid=2247487015&idx=1&sn=04282e2d15eca05eb56062062b46e781&chksm=fe418011c9360907048966af43299fa55b570c9d634c4bcdb3702c6e4d8101a72ef07d5ec77f&scene=21#wechat_redirect)\n1. [LLVM 之父 Chris Lattner：我的 AI 基础设施软件构建理念](https://mp.weixin.qq.com/s?__biz=MzU5ODY2MTk3Nw==&mid=2247492618&idx=1&sn=a20f4828b9ab3e3cee3fedfd906e0eb2&chksm=fe426a3cc935e32a8312ce9efbb4f2640787508d3e811579bbffe918685cdb07a8bd8e3ffc4b&scene=132&exptype=timeline_recommend_article_extendread_samebiz#wechat_redirect)\n"},{"title":"JFrog","url":"/2023/01/16/JFrog/","content":"\n## [Artifactory](https://zhuanlan.zhihu.com/p/274996585)\n\n1. 上传最好不要用 sudo 用户，否则容易覆盖之前文件，不好管理\n1. guest 用户可以禁止 ui 登录，防止用户被锁\n1. 可以通过界面上传文件\n1. 作为一个二进制包管理工具，可以用于管理构建产物：即把生成的 jar 包存放到 JFrog，当在生产环境进行部署时，Jenkins 直接从 JFrog 中拉取对应的 jar 包即可。\n1. 官方的介绍：JFrog Artifactory 是一个可扩展的通用二进制存储库管理器，可在整个应用程序开发和交付过程中自动管理你的工件和依赖项。一句话概括：Artifactory 是一个存放制品（Artifacts）的工具。\n1. 左下角 getting started\n1. 允许匿名用户，否则 wget curl 下载不下来， [link](https://juejin.cn/post/7080707817778184205)\n1. 先创建 repo，再上传\n1. repo 分为：local, remote 和 virtual\n1. monitoring -> storage 可以看各 repo 存储大小\n1. Application -> Artifactory -> artifacts 右上角 Set me up 可以查看 deploy 和 resolve 命令\n1. set me up 中可以设置 configue 来隐藏密码\n1. artifacts 中的文件可以单独下载，右上角下载符号\n1. 有些文件可以预览，右上角...展开, 或者选中文件，右击\n1. 可以查看下载次数和被谁下载\n1. [artifacts 布局 layouts](https://www.jfrog.com/confluence/display/JFROG/Repository+Layouts) 可以查看版本, 可以生成各种工具依赖的版本信息\n1. MD5 check：`echo 99 > test.txt && MD5=$(md5sum test.txt | awk '{print $1}') && curl -uxiyang.jia:APAQR97hg1swsSNZ -T test.txt -H \"X-Checksum-MD5:${MD5}\" -L \"http://artifacts.phigent.io/artifactory/xiyang-test/test/test/test-1.0.3.txt\" -v`\n1. 可以通过 name, checksum, property 搜索 artifacts\n1. `wget -O test.tar.gz --no-check-certificate  https://guest:Guest.123@xxx/artifactory/test.tar.gz` 下载, 覆盖原来文件\n1. `curl -sSfLO https://guest:Guest.123@xxx/artifactory/test.tar.gz` 下载\n\n## [with gilab](https://jfrog.com/blog/gitlab-and-artifactory-on-your-mark-get-set-build/)\n\n### NOTE\n\n1. curl -L http:xxx 不加 -L 会自动转换为 https\n1. curl -v 查看中间过程\n1. shell\n\n```sh\n#!/bin/bash\n\nJFROG_USER=xiyang.jia\nJFROG_PASSWD=APAQR97hg1swsSNZ\n\npush_to_jfrog() {\n        FILE=$1\n        JFROG_PATH=http://artifacts.phigent.io/artifactory/$2\n        MD5=$(md5sum $FILE | awk '{print $1}')\n        curl -u\"${JFROG_USER}:${JFROG_PASSWD}\" -T $FILE -H \"X-Checksum-MD5:${MD5}\" -L \"${JFROG_PATH}\"\n}\n\npush_dir_to_jfrog(){\n        for file in $(find $1 -type f); do\n                push_to_jfrog ${file} $2/${file}\n        done\n}\n\nif [ -d $1 ]; then\n        push_dir_to_jfrog $1 $2\nelse\n        push_to_jfrog $1 $2\nfi\n```\n"},{"title":"Cpp20","url":"/2023/01/13/Cpp20/","content":"\n## module\n\n## [coroutine](https://zhuanlan.zhihu.com/p/172471249)\n\n1. 协程并没有增加线程数量，只是在线程的基础之上通过分时复用的方式运行多个协程，而且协程的切换在用户态完成，切换的代价比线程从用户态到内核态的代价小很多。\n1. 协程对计算密集型的任务也没有太大的好处，计算密集型的任务本身不需要大量的线程切换，因此协程的作用也十分有限，反而还增加了协程切换的开销。\n1. 因此在协程中不能调用导致线程阻塞的操作。也就是说，协程只有和异步 IO 结合起来，才能发挥最大的威力。\n1. 协程只有和异步 IO 结合起来才能发挥出最大的威力。\n1. 协程可以通过 yield（取其“让步”之义而非“出产”）来调用其它协程，接下来的每次协程被调用时，从协程上次 yield，返回的位置接着执行，通过 yield 方式转移执行权的协程之间不是调用者与被调用者的关系，而是彼此对称、平等的。\n1. 协程非常类似于线程。但是协程是协作式多任务的，而线程典型是抢占式多任务的。这意味着协程提供并发性而非并行性。协程超过线程的好处是它们可以用于硬性实时的语境（在协程之间的切换不需要涉及任何系统调用或任何阻塞调用），这里不需要用来守卫关键区块的同步性原语（primitive）比如互斥锁、信号量等，并且不需要来自操作系统的支持。有可能以一种对调用代码透明的方式，使用抢占式调度的线程实现协程，但是会失去某些利益（特别是对硬性实时操作的适合性和相对廉价的相互之间切换）。\n1. 协程的核心竞争力在于：它能简化异步并发任务，以同步方式写异步代码, 这也是为什么要引入协程的原因了：简化异步并发任务\n1. 线程是操作系统层面的概念，协程是语言层面的概念, 线程与协程最大的区别在于：线程是被动挂起恢复，协程是主动挂起恢复\n\n### link\n\n1. [wikipedia](https://zh.wikipedia.org/wiki/%E5%8D%8F%E7%A8%8B)\n1. [进程、线程和协程的区别](https://juejin.cn/post/6973650934664527885)\n"},{"title":"BPF","url":"/2022/12/26/BPF/","content":"\n## tools\n\n1. [各种 tools](https://blog.csdn.net/qq_23662505/article/details/125427689?spm=1001.2101.3001.6650.7&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-7-125427689-blog-119563743.pc_relevant_aa2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-7-125427689-blog-119563743.pc_relevant_aa2&utm_relevant_index=8)\n\n## BCC\n\n1. 需要 sudo 来执行命令\n1. 各命令有 man 手册\n1. `sudo apt-get install bpfcc-tools`\n1. ` './include/linux/kconfig.h' file not found` => `cd /lib/modules/5.10.104-tegra/build/ && ln -s /lib/modules/5.10.104-tegra/build/kernel-5.10/include .`\n1.\n\n### install\n\n1. [docker](https://github.com/iovisor/bcc/blob/master/QUICKSTART.md) 需要用宿主机上的内核源码\n\n## bpftrace\n\n1. `mount -t debugfs none /sys/kernel/debug/`\n\n### install\n\n1. `sudo apt-get install -y bpftrace`\n1. 需要内核开启一些选项\n1. [ubuntu](https://github.com/iovisor/bpftrace/blob/master/INSTALL.md#ubuntu-packages)\n\n## links\n\n1. [bpf-developer-tutorial](https://github.com/eunomia-bpf/bpf-developer-tutorial)\n"},{"title":"unix","url":"/2022/12/02/unix/","content":"\n## [UNIX 环境高级编程](http://www.apuebook.com/apue3e.html)\n\n1. ID\n\n   - 文件(文件描述符)\n   - 进程\n   - 错误处理(errno)\n   - 用户标识(用户 ID, 组 ID)\n   - 信号(SIG)\n\n1. `stat`: 获取文件各种信息\n   - `stat -c %s file` 查看文件长度\n1. `strace - trace system calls and signals`\n\n## 文件\n\n## IO\n\n## 信号\n\n1. 信号是软件中断。\n1. 终端键盘上有两种信号方法：\n   - SIGINT 中断键：C-c\n   - SIGQUIT 退出键：C-\\\n1. 使用 system 而不是直接用 fork 和 exec 的优点是： system 有各种错误处理和各种信号处理\n\n### 进程\n\n1. 进程 ID 用 pid_t 数据类型来表示，是一个非负整数。 线程 ID 是用 pthread_t 数据类型来表示， 是一个结构体， 不同系统类型不同。\n1. 如果进程中的任意线程调用了 exit, \\_Exit 或者\\_exit, 那么整个进程就会终止。\n1. `nm -C test | grep GLIBC_`可以看出 c++程序用 thread 和 mutex 最终调用了 pthread_create 和 pthread_mutex\\*\n1. 守护进程出错记录使用 syslog，log 位置 /var/log/syslog\n\n#### 进程间通信 InterProcess Communication IPC\n\n1. 同一机器：管道，FIFO, 消息队列，信号量，共享内存。\n2. 不同机器：socket\n\n#### 线程间通信\n\n1. mutex, 读写锁，自旋锁， 条件变量\n1. mutex 保护的是 lock 和 unlock 之间的代码\n1. 如果线程对一个互斥量加锁两次，那么它自身就会陷入死锁。\n1. 线程间循环依赖锁也会死锁，解决办法：1. 控制互斥量加锁的顺序避免死锁。2. try_lock 如果获得不了想要的锁， 先释放已占有的锁，过一段时间再试试。\n1. 自旋锁与互斥量类似， 但自旋锁获取锁之前线程是非阻塞状态。适用于锁持有时间段，线程不希望在重新调度上花费太多成本。自旋锁在非抢占式内核中非常有用，在用户层，自旋锁并不是很有用。\n1. 很多互斥量实现非常高效，有些互斥量的实现在试图获取互斥量的时候会自选一小段时间，只有自旋计数达到某一阈值的时候才会休眠。\n\n### [linux 下源码编译问题](https://blog.csdn.net/qq_29116917/article/details/103786194)\n\n1. minor 找不到： filedir/devrdev.c `#include <sys/sysmacros.h>`\n1. `sudo apt install libbsd-dev`\n1. err_sys 找不到问题， error.c 源码复制到 apue.h 中\n"},{"title":"version","url":"/2022/11/15/version/","content":"\n## Linux 动态库版本管理\n\n1. Linux 通过版本号来管理动态库的版本，版本号最多有 3 级，其格式为 libname.so.x.y.z\n   - x: major release，非兼容修改，可能对接口做了大改动，比如重命名、增加或减少参数等。\n   - y: minor release，不改变兼容性，但是增加了新接口\n   - z: patch release，不改变兼容性，仅仅是修复 bug、或者优化代码实现、优化性能等。\n"},{"title":"github-ci","url":"/2022/10/30/github-ci/","content":"\n## actions\n\n1. fork 的 repo 需要手动在 actions 里 enable\n1. [disabling-and-enabling-a-workflow](https://docs.github.com/en/actions/managing-workflow-runs/disabling-and-enabling-a-workflow)\n1. [actions doc](https://docs.github.com/cn/actions)\n1. [test-reporter](https://github.com/dorny/test-reporter)\n1. [test-summary](https://github.com/test-summary/)\n\n## sample\n\n1. use 可以连续用\n\n```yaml\nname: Ubuntu\non: push\njobs:\n  release-test:\n    name: dataflow build on ubuntu\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4 ## checkout到分支\n      - uses: prefix-dev/setup-pixi@v0.3.0 ## 安装pixi\n      - name: install deps\n        run: apt install -y libc6-dbg\n      - name: run all test\n        run: pixi run all\n      - name: \"Upload Artifact\"\n        uses: actions/upload-artifact@v3\n        with:\n          name: my-artifact\n          path: .gitignore\n```\n\n## [upload-artifact](https://github.com/actions/upload-artifact#zipped-artifact-downloads)\n\n1. config\n\n```yaml\n- name: \"Upload Artifact\"\n  uses: actions/upload-artifact@v3\n  with:\n    name: my-artifact\n    path: .gitignore\n```\n\n1. ci job 左侧 summary 可以下载 artifacts\n\n## [upload-release-asset](https://github.com/actions/upload-release-asset)\n\n1. [push 失败问题： 需要设置 repo read_and_write 权限](https://stackoverflow.com/questions/75995802/resource-not-accessible-by-integration-github-action-fails-for-pushed-commit)\n1. 参考 cpp_sandbox 和 python_sandbox\n\n## links\n\n1. [官方文档](https://docs.github.com/zh/actions/quickstart)\n1. [action samples](https://github.com/marketplace?type=actions)\n1. [readme show build status](https://github.com/microsoft/onnxruntime/blob/main/README.md)\n1. [taskflow samples](https://github.com/taskflow/taskflow/tree/master/.github/workflows)\n1. [基于 Github Action 的 CI/CD 流程](https://zhuanlan.zhihu.com/p/250534172)\n"},{"title":"Coroutine","url":"/2022/10/25/Coroutine/","content":"\n## base\n\n1. 协程并没有增加线程数量，只是在线程的基础之上通过分时复用的方式运行多个协程，而且协程的切换在用户态完成，切换的代价比线程从用户态到内核态的代价小很多。\n1. 协程对计算密集型的任务也没有太大的好处，计算密集型的任务本身不需要大量的线程切换，因此协程的作用也十分有限，反而还增加了协程切换的开销。\n1. 因此在协程中不能调用导致线程阻塞的操作。也就是说，协程只有和异步 IO 结合起来，才能发挥最大的威力。\n1. 协程只有和异步 IO 结合起来才能发挥出最大的威力。\n\n## links\n\n1. [什么是协程？](https://zhuanlan.zhihu.com/p/172471249)\n"},{"title":"开发方法","url":"/2022/10/21/开发方法/","content":"\n## base\n\n1. 瀑布流方法（Waterfall）\n1. V 型方法（V-model）\n1. 迭代式开发（Iterative and incremental development, IID）\n1. 螺旋开发（Spiral）\n1. 敏捷开发（Scrum）\n1. 极限编程（Extreme programming, XP）\n"},{"title":"Compiler","url":"/2022/10/21/Compiler/","content":"\n## base\n\n1. [LLVM 中的 Pass 和 PassManager](https://blog.csdn.net/2301_76434200/article/details/136205302)\n1. LLVM（Low-Level Virtual Machine)\n1. [llvm VS gcc](https://mp.weixin.qq.com/s/OB7JJHgknJgDFjXKI3jOcA)\n1. [解释， 编译， 字节码(JIT)](https://mp.weixin.qq.com/s/AH7tdFR_QGTMg2wxaj0sog)\n1. 传统的编译器通常分为三个部分，前端(frontEnd)，优化器(Optimizer)和后端(backEnd)。在编译过程中，前端主要负责词法和语法分析，将源代码转化为抽象语法树；优化器则是在前端的基础上，对得到的中间代码进行优化，使代码更加高效；后端则是将已经优化的中间代码转化为针对各自平台的机器代码\n1. llvm 和 clang 关系\n   ![llvm和clang关系](https://img-blog.csdn.net/20160718211639694?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)\n1. IR(intermediate representation): 中间语言，最常见的中间语言表述形式，是三位址码（Three address code）\n1. LLVM 是一种基础设施，是有用的编译器技术的集合，可以用于解决特定问题（例如构建 C 编译器或特效管道中的优化器）\n1. [bitcode 和 bytecode 的区别是什么](https://worktile.com/kb/p/37934)\n1. 注意`import` and `export` symbols\n1. `echo | xt-clang -E -Wp,-v -` -E 选项告诉编译器只进行预处理，-Wp,-v 选项告诉编译器打印出它在预处理阶段使用的头文件路径。\n1. [动态库全局符号覆盖的大坑 double free](https://gaomf.cn/2020/06/03/shared_library_global_symbol_override/)\n   - `nm libmylib1.so libmylib2.so | grep -P \"^[^\\\\s]+ T \" | cut -d' ' -f3 | sort | uniq -d | grep -v '_fini' | grep -v '_init' | c++filt` 查看是否有相同全局符号\n1. [Symbols 与 gcc -g 产生的 debug info 并不是一个东西](http://arthurchiao.art/blog/linux-tracing-basis-zh/)\n1. function tracing 只需要 symbols，不需要 debug symbols（gcc -g）。strip 之后不能跟踪\n1. 动态符号（.dynsym）vs. 局部符号（.symtab）， 可以用命令 strip 来手动去掉局部符号表， strip 前后用`readelf -s ./test`看, strip 只去除了.symtab\n1. [查看依赖动态库的一些方法](https://unix.stackexchange.com/questions/120015/how-to-find-out-the-dynamic-libraries-executables-loads-when-run)\n   - `sudo awk '/\\.so/{print $6}' /proc/1/maps | sort -u` 查看进行 1 链接的动态库\n   - `LD_DEBUG=files ./xx` 查看运行时加载的库\n   - `readelf -d build/dog/dog_test1 | grep 'NEEDED'` 查看加载用到的库\n   - `sudo lsof -p 1| grep mem | grep so` 查看进程用到的库\n\n<!-- more -->\n\n1. 解释型语言解释器是一个运行的交互程序，接收输入命令，执行对应操作并将结果输出\n1. ld 编译选项`--whole-archive`, 告诉链接器，将后面库中所有符号都链接进来，参数`-no-whole-archive`则是重置，以避免后面库的所有符号被链接进来。要通 gcc 传递到 ld，需要在他们前面加 -Wl\n1. `set(CMAKE_SHARED_LINKER_FLAGS \"-s -Wl,--version-script=${PROJECT_SOURCE_DIR}/symbol.lst\")` 跟据 symbol.lit 来 strip， 可以只暴露需要的 symbol, `nm -C`看不到符号， `readelf -s`可以\n   `target_link_options(tensorflowlite_c PRIVATE \"-Wl,--version-script,${TENSORFLOW_SOURCE_DIR}/tensorflow/lite/c/version_script.lds\")`\n1. [c++ 全局变量初始化](https://www.cnblogs.com/catch/p/4314256.html)\n   根据 C++ 标准，全局变量的初始化要在 main 函数执行前完成，常识无疑，但是这个说法有点含糊，main 函数执行前到底具体是什么时候呢？是编译时还是运行时？答案是既有编译时，也可能会有运行时(seriously), 从语言的层面来说，全局变量的初始化可以划分为以下两个阶段（c++11 N3690 3.6.2)：\n   - static initialization: 静态初始化指的是用常量来对变量进行初始化,主要包括 zero initialization 和 const initialization，静态初始化在程序加载的过程中完成，对简单类型(内建类型，POD 等)来说，从具体实现上看，zero initialization 的变量会被保存在 bss 段，const initialization 的变量则放在 data 段内，程序加载即可完成初始化，这和 c 语言里的全局变量初始化基本是一致的。\n   - dynamic initialization：动态初始化主要是指需要经过函数调用才能完成的初始化，比如说：int a = foo()，或者是复杂类型（类）的初始化（需要调用构造函数）等。这些变量的初始化会在 main 函数执行前由运行时调用相应的代码从而得以进行(函数内的 static 变量除外)。\n     需要明确的是：静态初始化执行先于动态初始化！ 只有当所有静态初始化执行完毕，动态初始化才会执行。显然，这样的设计是很直观的，能静态初始化的变量，它的初始值都是在编译时就能确定，因此可以直接 hard code 到生成的代码里，而动态初始化需要在运行时执行相应的动作才能进行，因此，静态初始化先于动态初始化是必然的。\n1. 注册：(`库中符号没有被使用，库不会被初始化(具有全局声明周期的变量和所有函数)` `具有side-effect的变量会被初始化，肯定用到了所在库`)\n\n   - 全局不初始化是没链接接上，没链接是因为没加载， 不是 NEEDED lib\n   - `readelf -d build/dog/dog_test1 | grep 'NEEDED'` 查看加载用到的库\n   - [注册全局变量未初始化初始化](http://www.cppblog.com/kevinlynx/archive/2010/01/17/105885.aspx)\n   - [Static variable initialization over a library](https://stackoverflow.com/questions/5202142/static-variable-initialization-over-a-library)\n   - 未依赖的全局变量可能并不初始化。main 之前也可以打印。\n   - dlopen 打开 lib，运行时注册(需要知道库名字)。\n   - My guess - when linking against a library, the linker only brings over symbols that are actually used by the executable. Object files whose exported symbols are not referenced from anywhere are simply ignored; so registered variable is dropped and its initializer doesn't run. Normally, that's a good thing - you wouldn't usually want unused parts of a large library to nevertheless wind up in your binary.\n   - [TensorFlow c++ SessionFactory 注册与 No session factory registered 错误](https://blog.csdn.net/gaussrieman123/article/details/106384853)\n     在上面我们已经分析清楚，这个报错的根本原因是 TensorFlow 动态库中的这两个源文件中的全局静态变量 registrar 没有初始化，导致后面在创建 Session 的时候 GetFactory 失败。\n     有一篇文章\"Integrating TensorFlow libraries\"也分析是这个原因。因为 TF 组件化的设计思路，一个很小的 core+注册的方式来完成包括 SessionFactory、Op_Kernel、Op 等的加载，在编译的时候，如果这部分代码被编译器忽略了，就会导致部分代码注册失败，进而导致报错。\n     为什么不初始化呢？\n     动态库的加载分为`显示加载和隐式加载`，我们后面验证了显示加载，即显示通过 dlopen 来调用这些动态库，是可以正常运行的，说明这种情况下，所有的注册都完成了。问题出在隐式加载上。\n   - `A non-local variable with static storage duration having initialization with side-effects must be initialized even if it is not odr-used ` side-effects 引起其他地方改变，被其他地方使用就算引起改变\n   - [One_Definition_Rule(ODR)](https://en.wikipedia.org/wiki/One_Definition_Rule) 在整个程序中，对象或非内联函数不能有多个定义；如果使用一个对象或函数，它必须有一个定义。您可以声明一个从未使用过的对象或函数，在这种情况下，您不必提供定义。在任何情况下都不可能有多个定义\n   - [Is static object guaranteed to be initialized](https://stackoverflow.com/questions/18600034/is-static-object-guaranteed-to-be-initialized)\n   - [与 flow 加载问题相似例子](https://stackoverflow.com/a/18601535)\n   - `the first odr-use of any function or variable deﬁned in the same translation unit as the variable to be initialized` 同一个编译单元不管是否用到都会初始化\n   - ` If the initialization is deferred to some point in time after the ﬁrst statement of main, it shall occur before the ﬁrst odr-use (3.2) of any function or variable deﬁned in the same translation unit as the variable to be initialized.` 如果某全局变量初始化推迟到 main 之后，那么使用该全局变量所在 TU 内的任何变量和函数之前需要初始化该全局变量，如果该全局变量与 main 在同一 TU 内，那么使用 main 中任何变量前，都要初始化改全局变量，等效 main 之前初始化, 因为用到了当前库中的符号。(做实验，同一 UT 内(没使用也会被初始化，注册成功)，随便使用另一个 so 中的函数(随便引用了一个毫无相关的内容，注册进工厂))\n   - `The compiler is not allowed to optimiza away global objects. Even if they are never used.` 全局变量不会被优化掉，静态库可能不链接；动态库整个 TU 可能不被加载, 动态库在磁盘上，如果没有用到，不会被加载?。 [link](https://stackoverflow.com/a/1229542) `\"It seems that having a file purely consisting of these global objects is what's causing the problem. It's as if that translation unit was entirely ignored\". The translation unit is being ignored in the sense that it is being omitted by the linker because it's in a library. If an object is in a library and isn't called by anything in the app, it's omitted, that's the whole idea of libaries! `\n   - `they were not setup at startup if no symbol of the DLL were used in the main.` 库中符号没有被使用，库不会被初始化。\n   - [SELF REGISTERING CLASSES](https://accu.org/journals/overload/6/27/bellingham_597/) (多研究下怎们用 extern)\n   - 解决方法 1： 运行时加载 dlopen 打开 so 时， 会初始化全局变量\n   - 解决方法 2： 显示使用符号，测试程序中使用库中的任意符号`extern int xxx; int test(){return xxx;}` 为什么能工作？`当前TU中的函数会被初始化， 函数里使用到了xxx， xxx所在库全局声明周期变量会被初始化`\n   - 库和测试程序都定义同一个内联函数不解决问题。\n   - 由于内存太小，所以真的动态执行，只有用到了才加载到内存，执行完就释放掉了，\n\n1. 动态链接库的加载方式有两种：隐式加载(静态链接或动态加载时链接)和显示加载(动态运行时链接)。 显示加载会加载所有符号，初始化所有变量, 可能是因为不确定要调用哪些符号，所以都初始化了。\n\n1. `flow单例注册问题`: 单例不只一份 static\n   - 隐藏单例的 Instance 函数 symbol 会出问题, 会有多个 static 变量\n   - `链接静态库要用PRIVATE保护`： 可能被链接两次，如单例模式注册问题， 链接了两份静态， 导致注册到不同地方。\n   - [C++单例模式跨 DLL 是不是就是会出问题?](https://www.zhihu.com/question/425920019/answer/2254967131)\n   - linux 上的 GNU GCC 编译器默认给函数的属性是`attribute`((visibility(\"default\")))，如果没有指定编译选项 visilibility 的话默认会导出所有的符号 `-fvisibility=hidden`\n   - [控制库符号导出](https://www.cnblogs.com/lsgxeva/p/12510201.html)\n   - 加了-fvisibility=hidden 后 static 符号(Instance 函数)被隐藏，有多份 static 局部变量，会有多个实例\n   - [自测 sample](https://github.com/jiaxiyang/cpp_sandbox/tree/master/design_pattern/singleton)\n   - 还有可能测试程序链接的静态库，只链接了用到的静态变量，导致未用到的没注册；链接整个静态库`-Wl,--whole-archive dcv_predict -Wl,-no-whole-archive`\n\n## LLVM\n\n1. `.c --frontend--> AST --frontend--> LLVM IR --LLVM opt--> LLVM IR --LLVM llc--> .s Assembly --OS Assembler--> .o --OS Linker--> executable`\n\n### MLIR（Multi-Level Intermediate Representation）和 LLVM IR（LLVM Intermediate Representation）区别：\n\n1. `多层次表示 vs 单层次表示`:\n   - `MLIR` 是多层次的中间表示，它允许表示不同层次的抽象，从高级语言特定领域语言（DSL）到底层硬件描述。这使得 MLIR 在各种领域和应用中非常灵活。\n   - `LLVM IR` 是单层次的中间表示，更专注于从高级源代码到目标机器代码的转换。它不支持多层次抽象或领域特定语言的内置支持。\n2. `领域特定语言支持`:\n   - `MLIR` 提供了内置支持领域特定语言（DSLs）的机制，这使得开发者可以更容易地在 MLIR 中定义和优化特定领域的代码。\n   - `LLVM IR` 不提供直接的 DSL 支持，尽管可以通过 LLVM 的扩展来实现某些领域特定的优化。\n3. `生态系统和用途`:\n   - `MLIR` 被设计为通用的多领域中间表示，适用于编译器、机器学习、图形处理器编程等多个领域。\n   - `LLVM IR` 在开始时更专注于编译器优化和代码生成，但它的生态系统也在不断扩展，包括用于 WebAssembly、GPU 等不同领域的扩展。\n4. `设计目标`:\n   - `MLIR` 的设计目标之一是提供灵活性和可扩展性，以适应不同领域的编译和代码生成需求，包括优化和分析。\n   - `LLVM IR` 更侧重于提供优化编译器的中间表示，以便在各种目标平台上生成高性能代码。\n5. `历史和成熟度`:\n   - `LLVM IR` 已经存在更长时间，拥有更成熟的生态系统和广泛的应用。它是众多编程语言的编译器后端的首选中间表示。\n   - `MLIR` 是相对较新的项目，尚未在广泛的生产环境中获得与 LLVM IR 相似的广泛应用。\n     总之，MLIR 和 LLVM IR 都是强大的中间表示语言，用于编译器和代码生成领域，但它们的设计和用途有所不同。MLIR 更注重灵活性和多领域支持，而 LLVM IR 则更注重编译器优化和生成高性能代码。选择使用哪个中间表示取决于项目的需求和目标。有些项目甚至可以同时使用两者，以充分利用它们的不同特性。\n\n## Just-in-Time (JiT) 和 Ahead-of-Time (AoT)\n\n### Just-in-Time (JiT) 编译：\n\n1. `定义`: JiT 编译是在运行时，即代码执行时进行的编译。一个常见的例子是 Java 虚拟机(JVM)，它将字节码在运行时转换为机器代码。\n1. `优点`:\n   - `跨平台`: 代码可以为不同的硬件/操作系统平台提供同一种中间表示形式，然后在每个目标平台上进行 JiT 编译。\n   - `优化`: 由于编译发生在运行时，JiT 编译器可以利用特定于当前机器的优化，例如考虑当前的 CPU 和内存状况。\n   - `适应性`: 可以针对正在执行的代码的实际使用模式进行优化。\n1. `缺点`:\n   - `启动延迟`: 在应用程序开始执行时可能需要额外的时间进行编译，从而导致启动延迟。\n   - `内存使用`: JiT 编译需要额外的内存来存储中间表示形式和机器代码。\n   - `调试难度高`：JIT 代码是在运行时生成的，调试比 AOT 编译代码更困难。\n\n### Ahead-of-Time (AoT) 编译：\n\n1. `定义`: AoT 编译是在代码执行之前进行的编译。传统的 C/C++编译器（例如 gcc、clang）就使用了这种方法，它们将源代码直接编译为机器代码。\n1. `优点`:\n   - `快速启动`: 由于代码已经被编译，应用程序通常可以更快地启动。\n   - `确定性`: 编译的输出是静态的，它不会因为运行时的条件或环境而改变。\n   - `资源`: 通常使用较少的内存，因为不需要存储中间表示形式或进行运行时编译。\n1. `缺点`:\n   - `跨平台问题`: 每个目标平台都需要一个特定的编译版本。\n   - `失去运行时优化`: 由于编译是在运行时之前完成的，AoT 编译器不能基于实际的运行时数据或环境进行优化。\n\n### 总结：\n\n1.  `JiT` 更适合需要跨平台支持和能够利用运行时数据进行优化的场景。\n1.  `AoT` 更适合对启动时间有严格要求、需要确定的性能或在资源受限的环境中运行的应用。\n\n## [FFI](https://cloud.tencent.com/developer/news/607788)\n\n1. FFI（Foreign Function Interface）是这样一种机制：用一种编程语言写的程序能调用另一种编程语言写的函数（routines）。\n\n## ABI 和指令集\n\n1. CPU--vendor--OS 这三者决定了一个平台，只要这三者一致，我们生成的二进制程序往往就可以确定了。这三者就被称为一个「目标三元组」（Target Triple）。x86_64-unknown-linux-gnu，这里 x86_64 指的是 CPU 架构，unknown 是 vendor，对于 Linux 环境，往往不太重要\n1. ABI (Application Binary Interface) 和指令集是两个在计算领域中经常被提及的概念，它们是相关的，但各自有其定义和用途。下面是对两者的简要描述和区别：\n\n### ABI (Application Binary Interface)\n\n1. 为什么 ABI 很重要？\n   - 如果你有一个预编译的库，并想在不同的环境中使用它，那么 ABI 的兼容性就非常重要。如果编译应用程序的编译器产生与库不兼容的 ABI，那么该应用程序可能无法正确地链接或运行。\n   - 当更新系统的库或操作系统时，保持 ABI 的稳定性是很重要的，否则之前编译的应用程序可能会因为新的库或操作系统的 ABI 改变而崩溃。\n1. `定义`：ABI 是程序模块（通常是一个可执行文件和一个共享库）之间在二进制级别进行交互所需要遵循的协议。换句话说，ABI 是软件之间的\"约定\"，这些软件在二进制层面上交互。\n1. `内容`：ABI 通常涉及到以下方面：\n   - `函数调用约定`：如何传递参数，如何返回结果，以及在调用中如何使用堆栈。\n   - `数据类型的大小和对齐`：例如，一个 `int` 类型在某个平台上可能是 4 字节，而在另一个平台上可能是 8 字节。\n   - `系统调用的约定`：这决定了应用程序如何进行系统调用。\n   - `目标文件格式`：例如 ELF (Executable and Linkable Format)。\n   - `其他方面`：如名称修饰和 C++的函数重载。\n1. `重要性`：不同的编译器，或同一编译器的不同版本，可能会有不同的 ABI。如果两个模块有不同的 ABI 并且它们尝试相互交互，那么可能会出现不可预知的行为。\n\n#### ABI 例子\n\nABI（应用程序二进制接口）是特定于架构和操作系统的，用于确保在二进制层面上的软件组件可以正确地相互交互。以下是一些常见的 ABI 的例子：\n\n1. `System V ABI`：这是 Unix 系统上最常见的 ABI，被多个 Unix-like 操作系统所采用，包括 Solaris、HP-UX、AIX 以及其他的系统。\n2. `Microsoft x64 calling convention`：这是 Windows 在 x64 架构上使用的 ABI。\n3. `PowerOpen ABI (PABI)`：这是 PowerPC 上的 ABI。\n4. `ARM ABI (AAPCS, ARM Architecture Procedure Call Standard)`：这是 ARM 架构上的 ABI。它也有多个变种，例如 AAPCS-VFP 用于硬浮点单元。\n5. `MIPS O32 ABI`：MIPS 架构上的一种常见 ABI。\n6. `Itanium C++ ABI`：这是为 Itanium 架构 (IA-64) 设计的，但其他架构上的 C++ 实现也采用了这个 ABI，如 x86 和 x86-64 的 GCC。\n7. `x86_64 psABI`：为 x86_64 架构设计的 ABI。\n8. `AArch64 ABI`：为 ARM 的 64 位版本设计的 ABI。 [arm abi github](https://github.com/ARM-software/abi-aa/)\n9. `EABI (Embedded ABI)`：适用于嵌入式处理器，特别是 ARM 和 MIPS。\n10. `Wasm ABI`：针对 WebAssembly 设计的 ABI。\n\n#### linux ABI 和 ARM ABI 关系\n\nLinux ABI (Application Binary Interface) 和 ARM ABI 都是关于应用程序二进制接口的规范，但它们的关注点和应用场景有所不同。\nLinux ABI 和 ARM ABI 是为不同的目的和在不同的层次上定义的规范，但在一个 ARM Linux 系统中，它们共同确保了软件组件的二进制兼容性和互操作性。\n\n1. `readelf -a test1 |grep ABI` 查看 elf ABI\n1. `.note.ABI-tag` section 中有信息\n1. `file ./test`也有 ABI 信息\n1. `Linux ABI`:\n   - Linux ABI 特指 Linux 操作系统内核与用户空间应用程序之间的接口。更具体地说，它描述了应用程序如何发起系统调用，以及这些调用如何传递参数、返回值等。\n   - 它定义了系统调用的接口，即应用程序如何与 Linux 内核交互。\n   - 它涉及到数据类型、系统调用编号以及如何将参数传递给这些系统调用等。\n   - Linux ABI 是跨平台的，意味着它旨在为多种体系结构（如 x86、ARM、MIPS 等）提供一致的接口。\n   - Linux ABI 可以确保，`只要应用和内核之间的 ABI 保持一致，就可以在不重新编译应用程序的情况下升级或更换 Linux 内核`。\n1. `ARM ABI`: - 这是特定于 ARM 架构的规范。\n   - ARM ABI 定义了在 ARM 架构上的软件如何布局数据和如何进行函数调用。这包括调用约定、数据类型的大小和对齐、处理器寄存器的使用，以及异常处理等。\n   - ARM 公司定义了多个 ABI。例如，有针对 ARM 架构的基础 ABI，也有针对特定指令集如 AArch64 的 ABI。\n   - 当我们谈论 ARM ABI 时，常见的是\"EABI\" (Embedded ABI)，它是为嵌入式系统设计的。\n1. `关系`:\n   - ARM ABI 确保应用程序的正确执行，而 Linux ABI 则确保应用程序与内核的正确交互。\n   - `Linux 在 ARM 平台上运行时，用户空间的应用程序和库会遵循 ARM ABI 的规范。这确保了应用程序和库在 ARM 硬件上的正确执行。同时，这些应用程序和库与 Linux 内核交互时会使用 Linux ABI 定义的系统调用`。\n   - 当一个应用程序运行在这样的系统上时，它与操作系统内核通信时使用 Linux ABI。而应用程序内部，或者它与其他库和组件通信时，可能遵循 ARM ABI。\n   - 实际上，应用程序或库在被编译和链接为 ARM Linux 系统上的二进制时，编译器和链接器会确保它们遵循这两种 ABI。\n\n### 指令集 (Instruction Set Architecture, ISA)\n\n1. `定义`：指令集是计算机架构的一部分，描述了 CPU 可以执行的低级操作和它们在机器代码级别的表示。\n2. `内容`：指令集定义了如下方面：\n   - `指令`：如加法、乘法、跳转等。\n   - `寄存器集`：描述 CPU 中可用的寄存器。\n   - `寻址模式`：决定如何访问数据和指令。\n   - `数据类型`：例如整数、浮点数等。\n3. `示例`：常见的指令集包括 x86, x86_64, ARM, MIPS 等。\n\n### 区别\n\n- `级别`：ABI 在更高的层次上定义了如何在二进制级别进行交互，而指令集是关于 CPU 可以执行哪些基本操作的描述。\n- `目标`：`ABI 主要关注软件之间的互操作性，而指令集关注硬件（CPU）如何执行特定的操作`\n- `应用场景`：`开发者在为特定平台编写程序或编译代码时需要关心 ABI。而在设计或实现 CPU 时，会关心指令集`\n\n## link\n\n1. 链接就像拼积木，有凹槽，也有凸槽，有一定的规则，满足规则凹凸部分才能拼合。\n1. 数据段段和代码段为什么分开？\n   - 程序运行多次可以共享只读内容， 包括代码段，只读数据段等，可以节省大量内存。\n   - 权限可以不同：代码自读， 数据可以读写\n   - 提高 cache 命中率\n1. `/usr/include/elf.h`看 elf 文件结构体\n\n### 环境变量\n\n1. `man ld.so` 查看各种 linker/loader 环境变量\n1. [Linux 环境变量之 LD_PRELOAD & LD_LIBRARY_PATH & LD_DEBUG](https://blog.csdn.net/llm_hao/article/details/115493516)\n\n#### LD_PRELOAD\n\n1. `LD_PRELOAD=\"/path/to/library.so\" valgrind my-executable` valgrind 定位动态库内存泄漏可以用到 [link](https://stackoverflow.com/a/13136609)\n1. gdb 调试时打不上断点，要使用 LD_PRELOAD 先加载动态库，找不到文件所在动态库，使用 speedscope\n\n#### LD_LIBRARY_PATH\n\n#### LD_DEBUG\n\n1. `LD_DEBUG=help ls` 查看帮助\n1. `LD_DEBUG=libs ./xxx` 查看加载库时详细信息\n1. `LD_DEBUG=libs LD_DEBUG_OUTPUT=lib ./xxx` 信息输出文件中\n\n   - 搜索 so -> 初始化 so -> 初始化 elf -> 控制转移到 elf -> 运行 elf -> 结束 elf -> 反序结束 so\n   - 可以看到搜索顺序; LD_LIBRARY_PATH -> RUNPATH from elf -> /etc/ld.so.cache\n   - 可以看到库的初始化顺序与完成顺序相反， 先初始化的库后完成\n   - 可以看到一次对动态库进行初始化，然后再对可执行程序进行初始化\n   - 可以指定 so 初始化和析构函数, 可用于注册机制?\n\n   ```c++\n       void __attribute__((constructor)) init_function();\n       void __attribute__((destructor))  fini_function();\n   ```\n\n1. info\n   - “files”，显示整个装载过程；\n   - “libs”，显示共享库查找过程；\n   - “symbols”，显示符号的查找过程；\n   - “bindings”，显示动态链接的符号绑定过程；\n   - “versions”，显示符号的版本依赖关系；\n   - “reloc”，显示重定位信息；\n\n## Binary Utils\n\n### Google bloaty(https://github.com/google/bloaty)\n\n1. 查看二进制大的原因\n1. [guide](https://github.com/google/bloaty/blob/main/doc/using.md)\n1. `bloaty -d symbols ./test`\n1. `bloaty -d compileunits ./test`\n1. `bloaty -v -d sections ./test`\n\n### [ghidra](https://github.com/NationalSecurityAgency/ghidra)\n\n1. 可以查看各个段内容\n\n### [elfutils](https://sourceware.org/elfutils/)\n\n1. eu-stack 可以跟踪 on-cpu and off-cpu, perf 只能跟踪 off-cpu\n\n### [GNU binutils](https://www.gnu.org/software/binutils/)\n\n1. reverse enginerring 各种工具查看段内容\n1. `ar` : 创建静态库，插入删除，列出和提取成员。\n1. `strings` : 列出一个目标文件中所有可打印的字符串。\n   - 找一个唯一字符串， 可以用来看程序是否为最新版本\n   - `strings build/out/osqp_demo | grep include  |  xargs  -t -I {} ls {} | grep signal.h`查看头文件\n1. `strip` : 从目标文件中删除符号表信息。 `file`可以查看是否 strip\n1. `nm` : 列出一个目标文件的符号表中定义的符号。\n\n   - `nm -A -l build/base` 显示文件名和行号(需要 debug 模式编译)\n   - `nm --undefined-only -C ./test` 查看 import symbols\n   - `nm --defined-only -C ./test` 查看 export symbols, export 可以看到 symbol 前面有地址\n   - `nm -S xxx`查看 defined symbols size\n   - `nm -S --size-sort test` 按 size 排序\n   - `man nm` 查看用法\n   - 大写全局，小写局部, 局部变量不会记录到符号表中（static 除外）\n   - b:bss, t:text, d:data, r:readonly\n   - `nm -C build/base` decode [Name_mangling](https://en.wikipedia.org/wiki/Name_mangling) this makes C++ function names readable.\n   - `extern \"C\" {}` 作用是阻止 C++ name mangling, 生成 C 格式符号表\n   - `c++filt _ZN9NS_QZSOCK10CTcpClient11SendAndRecvEPciRiRjd` c++filt Name De-Mangling\n   - 符号详解\n     - `T`: 定义：函数定义在当前模块中， `nm -C libxxx | grep \" T \"`查看当前模块定义的函数\n     - `U`: 声明：使用定义在其他模块中的函数\n     - `D`: 初始化的 data (静态初始化的变量)\n     - `B`: 未初始化的 data(动态初始化的变量)\n     - `N`: debug 符号\n\n1. `size` : 列出目标文件中的节的名字和大小。\n   - `size -A ./test` 列出 section 详细信息\n1. `readelf` : 显示一个目标文件的完整结构，包含 size 和 nm 的功能。\n   - `readelf --debug-dump=decodedline  build/cmake.linux-x86_64-cpython-3.10/bin/triton-tensor-layout | grep -oE '/[^ ]+\\.cpp:' | sort -u` 获取文件中debug信息包含的C++文件   \n   - `readelf -a -W -C bin2float|less` -W wide 显示， -C demangle(最新版本才支持)\n   - `readelf -d build/dog/dog_test1 | grep 'NEEDED'` 查看加载用到的库\n   - readelf -a test 先显示 section 信息， 然后显示 segment(Program Headers)信息，接着显示 Section to Segment mapping 信息\n   - `readelf -a test1 |grep ABI` 查看 elf ABI\n1. `objdump` : 所有二进制文件之母。能够显示一个目标文件中所有的信息。它最大作用是反汇编.text 节中的二进制指令。\n   - `objdump -p test | grep NEEDED` 查看加载用到的库\n   - `objdump -g libxxx.so | grep /data0/....` 动态库调试信息中找文件路径\n   - `objdump --dwarf=decodedline test` 显示内存地址映射到具体某行源代码\n   - `objdump -d -C test` 反汇编，demangle\n1. `ldd` : 列出一个可执行文件在运行时所需要的共享库。\n   - `ldd -v xxx` 列出依赖详细关系，包括依赖的依赖\n1. !notice : use tldr to show the basic usage.\n1. `cat /proc/pid/maps 或 pmap pid` 看进程映射区域\n\n## glibc 初始化\n\n1. `_start -> __libc_start_main -> __libc_csu_init -> _init -> __do_global_ctors_aux(c++构造函数) -> main -> __do_global_dtors_aux -> _fini -> __libc_csu_fini`\n\n## GCC 编译选项\n\n1. 给 gcc 加上`-fno-common`选项, 这个选项会告诉链接器，在遇到多重定义的全局符号时，触发一个错误。或者使用 -Werror 选项，它会把所有的警告都变为错误。`set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fno-common\")`\n1. `-Wl,--whole-archive target -Wl,-no-whole-archive` 静态链接时把 target 都打包， 正常情况下只会将使用到的符号相关代码数据打包\n\n## other\n\n1. [编译期间获取类型名](https://zhuanlan.zhihu.com/p/268462046)\n\n```\ntemplate <typename T>\nconstexpr auto type_name() noexcept\n{\n    std::string_view name, prefix, suffix;\n#ifdef __clang__\n    name = __PRETTY_FUNCTION__;\n    prefix = \"auto type_name() [T = \";\n    suffix = \"]\";\n#elif defined(__GNUC__)\n    name = __PRETTY_FUNCTION__;\n    prefix = \"constexpr auto type_name() [with T = \";\n    suffix = \"]\";\n#elif defined(_MSC_VER)\n    name = __FUNCSIG__;\n    prefix = \"auto __cdecl type_name<\";\n    suffix = \">(void) noexcept\";\n#endif\n    name.remove_prefix(prefix.size());\n    name.remove_suffix(suffix.size());\n    return name;\n}\n```\n\n1. [运行时获取类名字](https://blog.csdn.net/K346K346/article/details/51698184)\n\n```c++\nclass Object\n{\npublic:\n    //用于在运行时获取对象的真实类型名称\n    virtual string getClassName() const\n    {\n#ifdef _MSC_VER\n        return typeid(*this).name() + 6;\n#else\n        const char* name = typeid(*this).name();\n\n        while (*name >= '0' && *name <= '9') name++;\n\n        return name;\n#endif\n    }\n};\n```\n\n1. 防止编译器优化\n\n```c++\n#pragma optimize off\nint globalVar\n#pragma optimize on\n```\n\n## buildid\n\n1. This allows two builds of the same program on the same host to always produce consistent build-ids and binary content.\n1. `readelf -n /bin/bash` or `file /bin/bash`\n1. [读取自己的 buildid](https://blog.csdn.net/flurry_rain/article/details/122544969)\n\n## links\n\n1. [cling online](https://cppcli.net/)\n1. [cling the-interactive-c++-interpreter](https://github.com/root-project/cling/)\n1. [程序的编译、装载与链接](https://zhuanlan.zhihu.com/p/139026433)\n1. [llvm 论坛](https://discourse.llvm.org/)\n1. [llvm beginner-resources-documentation](https://discourse.llvm.org/t/beginner-resources-documentation/5872)\n1. [llvm discord](https://discord.com/invite/xS7Z362) 312567150\n1. [My First Language Frontend with LLVM Tutorial](https://llvm.org/docs/tutorial/MyFirstLanguageFrontend/index.html)\n1. [llvm blog](https://blog.llvm.org/)\n1. [compiler-research](https://compiler-research.org/)\n1. [BuildYourOwnLispCn](https://github.com/ksco/BuildYourOwnLispCn)\n1. [Writing a C Compiler](https://norasandler.com/archive/)\n1. [craftinginterpreters](https://www.craftinginterpreters.com/)\n1. [llvm-ir-tutorial](https://github.com/Evian-Zhang/llvm-ir-tutorial)\n1. [机器学习编译器的前世今生](https://www.zhihu.com/question/401766029/answer/2733700454)\n\n## books\n\n1. 深入理解计算机系统\n1. 程序员的自我修养\n1. 程序是怎样跑起来的\n"},{"title":"KataOS","url":"/2022/10/20/KataOS/","content":"\n## Links\n\n1. [sparrow-manifest](https://github.com/AmbiML/sparrow-manifest)\n1. [announcing-kataos-and-sparrow](https://opensource.googleblog.com/2022/10/announcing-kataos-and-sparrow.html)\n1. [Running Rust programs in seL4 using the sel4-sys crate](https://antmicro.com/blog/2022/08/running-rust-programs-in-sel4/)\n"},{"title":"explore","url":"/2022/10/18/explore/","content":"\n## idea\n\n1. 新功能用 sourcegraph 探索大家都怎么使用的， 如探索 mermaid 使用\n\n## links\n\n1. [star-history](https://star-history.com/) 可以多个 repo 同时显示, 左侧有 Monthly Pick\n1. [github trending](https://github.com/trending)\n1. [github next](https://githubnext.com/)\n1. [github star sort](https://github.com/search?o=desc&q=stars%3A%3E0&s=stars&type=Repositories)\n1. [HelloGitHub](https://github.com/521xueweihan/HelloGitHub/tree/master)\n1. [productivity-tools](https://github.com/collections/productivity-tools)\n1. [awesome-selfhosted 可以部署到本地的服务](https://github.com/awesome-selfhosted/awesome-selfhosted)\n1. [freeCodeCamp](https://github.com/freeCodeCamp/freeCodeCamp)\n1. [build-your-own-x](https://github.com/codecrafters-io/build-your-own-x)\n1. [project-based-learning like build-your-own-x](https://github.com/practical-tutorials/project-based-learning#cc)\n1. [system-design-primer](https://github.com/donnemartin/system-design-primer)\n1. [developer-roadmap](https://github.com/kamranahmedse/developer-roadmap)\n1. [llvm discord](https://discord.com/invite/xS7Z362) 312567150\n1. [topics/profiling](https://github.com/topics/profiling)\n1. [Open Source Awards](https://osawards.com)\n1. [osawards.com/javascript](https://osawards.com/javascript/2022)\n1. [opensource.google](https://opensource.google/projects)\n1. [DIYgod](https://github.com/DIYgod)\n1. [together c & c++ discord](https://discord.gg/tccpp)\n1. [better c++ discord](https://discord.gg/JAEgDRbxtq)\n1. [Awesome cpp](https://github.com/fffaraz/awesome-cpp#readme)\n1. [weekly](https://github.com/ruanyf/weekly?tab=readme-ov-file)\n1. [5 大平台编程语言排行榜](https://blog.csdn.net/blueskybluesoul/article/details/128593539)\n\n### AI\n\n1. [huggingface blog](https://huggingface.co/blog/zh)\n1. [paperwithcode](https://paperswithcode.com/)\n1. [DeepLearningSystem](https://github.com/chenzomi12/DeepLearningSystem)\n1. [microsoft/generative-ai-for-beginners](https://github.com/microsoft/generative-ai-for-beginners) 注意仓库里有中文翻译\n1. [AI-For-Beginners](https://github.com/microsoft/AI-For-Beginners)\n1. [Llama 2 and Ecosystem Edition](https://star-history.com/blog/llama2)\n1. [oneflow AI 基础设施](https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzU5ODY2MTk3Nw==&action=getalbum&album_id=2467151539284246528&scene=173&from_msgid=2247492618&from_itemidx=1&count=3&nolastread=1#wechat_redirect)\n1. [Andrej Karpathy](https://www.youtube.com/@AndrejKarpathy/videos)\n1. [statquest](https://www.youtube.com/@statquest)\n\n## blog\n\n1. [diygod](https://diygod.cc/)\n1. [brendangregg](https://www.brendangregg.com/overview.html)\n1. [arthurchiao](http://arthurchiao.art/articles-zh/)\n1. [码农半亩地](https://gaomf.cn/2020/06/03/shared_library_global_symbol_override/)\n1. [freelancer-leon/notes](https://github.com/freelancer-leon/notes/tree/master)\n"},{"title":"color","url":"/2022/10/15/color/","content":"\n## basic\n\n1. `GLOG_colorlogtostderr=1 ./test` glog 输出彩色\n1. [ANSI_escape_code](https://en.wikipedia.org/wiki/ANSI_escape_code#Colors) 全面；包含各种颜色值\n1. [tip_colors_and_formatting](https://misc.flogisoft.com/bash/tip_colors_and_formatting)\n1. [Linux 使用 echo 命令打印输出特定颜色的字符](https://zhuanlan.zhihu.com/p/363287904)\n   - `echo -e \"\\033[43;31;5m我是黄底红色的闪烁字符串\\033[0m\"` 输出字符串的的前部\"\\033[43;31m\"和后部\\033[0m 是固定用法，被称为`ANSI控制码`\n   - 前部的 43 是背景设置, 31 是字符颜色设置，5 是闪烁\n1. `dircolors` ls 显示颜色 env: `LS_COLORS` [在 Linux 命令行中自定义文本颜色](https://zhuanlan.zhihu.com/p/39600129)\n1. `#f12` 等效 `#ff1122`\n1. 开发板命令显示颜色：`alias ll=\"ls -al --color=auto\" alias ls=\"ls --color=auto\"`\n1. [PS1 set](https://www.cyberciti.biz/faq/bash-shell-change-the-color-of-my-shell-prompt-under-linux-or-unix/)\n\n```sh\nexport PS1=\"\\033[01;32m\\](docker)\\u@10.10.129.7\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ \"\nexport PS1=\"\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ \"\nexport PS1=\"\\e[0;31m\\u@\\h:\\W% \\e[m\"\nexport PS1=\"\\e[0;32m\\u@\\h:\\W% \\e[m\"\nexport PS1=\"\\e[0;33m\\u@\\h:\\W% \\e[m\"\n\n```\n1. [icons](https://emojipedia.org/fire) 找chatgpt让它来给出\n1. zsh配置PROMPT\n   - ~/.oh-my-zsh/themes/robbyrussell.zsh-theme 修改PROMPT\n   - jonathan theme\n```   \nPROMPT='⚡ %n@219.135.228.244%f:%~%f%B%f ⚡ \n%(?:🔥:❌) '\nPROMPT='%B%F{green}%n@219.135.228.244%f:%F{blue}%~%f%B%f \n%(?:%{$fg_bold[red]%}%1{➜%} :%{$fg_bold[yellow]%}%1{➜%} ) '\n\n# PROMPT='⚡ %B%F{green}%n@219.135.228.244%f:%F{blue}%~%f%B%f ⚡ \n# %(?:%{$fg_bold[red]%}%1{➜%} :%{$fg_bold[yellow]%}%1{➜%} ) '\nPROMPT='⚡ %n@219.135.228.244%f:%~%f%B%f ⚡ \n%(?:%{$fg_bold[yellow]%}%1{➜%} :%{$fg_bold[red]%}%1{➜%} ) '\n\nPROMPT='⚡ %n@219.135.228.244%f:%~%f%B%f ⚡ \n%(?:✅ :❌ ) '\n🔥 👻 🎃 ❄️  ☃️  🥶\n\n\nPROMPT='%B%F{green}(docker)%n@10.10.129.7%f:%F{blue}%~%f%B%f\\$ '\n\n```\n\n1. `script -q -c \"bat symbol.json\" color.log > /dev/null` 将彩色输出保存到到文件中 [write stdout to file with colors](https://stackoverflow.com/a/27399198)\n1. ninja color output\n\n```cmake\nif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n    add_compile_options (-fdiagnostics-color=always)\nelseif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\")\n    add_compile_options (-fcolor-diagnostics)\nendif ()\n```\n\n## Emacs\n\n1. `list-face-display and list-colors-display` 列出颜色\n1. `rainbow-mode` buffer 里显示颜色\n1. color-196 大红\n1. [emacs 正则](https://www.emacswiki.org/emacs/RegularExpression)\n1. 正则需要`\\\\`来转义\n1. minor mode: deffac 创建 face, font-lock-add-keywords 和 font-lock-remove-keywords 添加规则, define-minor-mode 添加 minor mode\n\n```emacs-lisp\n;; log color\n(defgroup log-group nil\n  \"Group for customization\"\n  :prefix \"log-\")\n\n(defface log-highlight-numbers-face\n  '((t :foreground \"yellow\"))\n  \"Face for numbers\"\n  :group 'log-group )\n\n(defface log-highlight-file-face\n  '((t :foreground \"magenta\"))\n  \"Face for file\"\n  :group 'log-group )\n\n(defface log-highlight-success-face\n  '((t :foreground \"black\"\n       :background \"green\"))\n  \"Face for pass success\"\n  :group 'log-group )\n\n(defface log-highlight-warning-face\n  '((t :foreground \"black\"\n       :background \"yellow\"))\n  \"Face for warning\"\n  :group 'log-group )\n\n(defface log-highlight-fail-face\n  '((t :foreground \"black\"\n       :background \"red\"))\n  ;; :bold t))\n  \"Face for fail, error\"\n  :group 'log-group )\n\n(defface log-highlight-normal-face\n  '((t :foreground \"black\"\n       :background \"cyan\"))\n  ;; :bold t))\n  \"Face for normol\"\n  :group 'log-group )\n\n\n(defvar log-keywords '(\n                       (\"[[:space:]]\\\\(\\\\+\\\\|-\\\\)?[0-9]+\\\\(\\\\.[0-9]+\\\\)?[[:space:]]\" . 'log-highlight-numbers-face)\n                       (\"[[:space:]][0-9a-zA-Z\\-\\_\\/]+\\\\.\\\\(jpg\\\\|jpeg\\\\|yuv\\\\|yuyv\\\\|png\\\\|json\\\\|bin\\\\|hbm\\\\|engine\\\\|txt\\\\)\" . 'log-highlight-file-face)\n                       (\"\\\\<\\\\([Ff]ps\\\\|FPS\\\\)\" . 'log-highlight-normal-face)\n                       (\"\\\\<\\\\([Ss]uccess\\\\|SUCCESS\\\\|[Pp]ass\\\\|PASS\\\\)\" . 'log-highlight-success-face)\n                       (\"^E\\\\([0-9]+\\\\)\" . 'log-highlight-fail-face)\n                       (\"\\\\<\\\\([Ff]ail\\\\|FAIL\\\\|[Ee]rror\\\\|ERROR\\\\|[Aa]bort\\\\|ABORT\\\\|[Ff]ault\\\\|FAULT\\\\)\" . 'log-highlight-fail-face)\n                       (\"^W\\\\([0-9]+\\\\)\" . 'log-highlight-warning-face)\n                       (\"\\\\<\\\\([Ww]arning\\\\|WARNING\\\\|[Ww]arn\\\\|WARN\\\\)\" . 'log-highlight-warning-face)\n                       )\n  \"Keywords for log-minor-mode highlighting\")\n\n(define-minor-mode log-minor-mode\n  \"Minor mode for customization\"\n  :init-value nil\n  :lighter \" LogMode\"\n  :group 'log-group\n  (when (bound-and-true-p log-minor-mode)\n    (font-lock-add-keywords nil log-keywords)\n    (font-lock-fontify-buffer))\n  (when (not (bound-and-true-p log-minor-mode))\n    (font-lock-remove-keywords nil log-keywords)\n    (font-lock-fontify-buffer)))\n(add-to-list 'auto-mode-alist '(\"\\\\.log\\\\'\" . log-minor-mode))\n```\n\n## tools\n\n1. PowerToys: windows color pick: `win + shift + c`\n1. [chrome plugin colorzilla](https://chrome.google.com/webstore/detail/colorzilla/bhlhnicpbhignbdhedgjhgdocnmhomnp?hl=zh-CN) 可以用来显示 value\n\n### bat\n\n1. [adding-new-themes](https://github.com/sharkdp/bat#adding-new-themes)\n\n### [colout](https://github.com/nojhan/colout)\n\n1. `pip install colout` install\n1. `cat yolox.log | colout fps white.red` colout 当 grep 用\n1. `ls -l | colout \".\" blue`\n   - [theme](https://github.com/nojhan/colout/tree/master/colout)\n\n```python\n## colout_log.py\ndef theme(context):\n    return context, [\n        # Pass and success\n        [\n            \"([Ff][Pp][Ss]|[Pp][Aa][Ss][Ss])(ing|ING)?|(success|SUCCESS|Success)(ful|FUL)?\",\n            \"white.green\", \"bold\"\n        ],\n\n        # warning\n        [\"^W[0-9]+\\d\", \"white.yellow\", \"bold\"],\n        [\"([Ww][Aa][Rr][Nn])(ing|ING)?\", \"white.yellow\", \"bold\"],\n\n        # error fail\n        [\"^E[0-9]+\\d\", \"white.red\", \"bold\"],\n        [\n            \"([Ee][Rr][Rr][Oo][Rr]|[Ff][Aa][Ii][Ll]|[Aa][Bb][Oo][Rr][Tt])(ing|ING|ure|URE)?\",\n            \"white.red\", \"bold\"\n        ],\n\n        # file\n        [\n            \"(\\s[0-9a-zA-Z\\/\\_\\-]+.)(jpg|jpeg|yuv|yuyv|png|json|bin|hbm|engine|txt)\\s\",\n            \"magenta\", \"bold\"\n        ],\n\n        # []\n        # [\"(\\[[^\\]]+\\])\", \"cyan\", \"bold\"],\n\n        # num\n        [\"\\s\\d+\\s\", \"yellow\", \"bold\"],\n        [\"\\s\\d+[uUmMnN]?[sS]\", \"red\", \"bold\"],\n    ]\n```\n\n1. .bashrc 添加 function\n\n```sh\ncl() {\n\tset -o pipefail\n\tcat \"$@\" 2>&1 | colout -t log -T ~/\n}\ncless() {\n\tset -o pipefail\n\tcat \"$@\" 2>&1 | colout -t log -T ~/ | less\n}\n\n```\n\n### [TxtStyle](https://github.com/armandino/TxtStyle) 自定义输出颜色\n\n1.  [style](https://github.com/armandino/TxtStyle/blob/6fd9028587bedd67a99e1bf7e950ede2c8ef35fe/txtstyle/txtsconf.py#L31)\n1.  [sample](https://stackoverflow.com/a/62337029)\n1.  color log TxtStyle `cl test.log | less` or `ls -al | cl` or `bat test.log`\n\n```sh\ncl() {\n\tset -o pipefail\n\tcat \"$@\" 2>&1 | txts -n example --color-always\n}\n\n# .txts.conf\n[Style=\"example\"]\n# Print in red any line containing the word 'error'\n# !red: regex(\"error\")\n# !red: regex(\"ERROR\")\n# !red: regex(\"Error\")\n# !yellow bold: regex(\"warning\")\n# !yellow bold: regex(\"WARNING\")\n# !yellow bold: regex(\"Warning\")\nwhite on-yellow bold: regex(\"[Ww][Aa][Rr][Nn](ing|ING)?\")\nwhite on-red bold: regex(\"([Ee][Rr][Rr][Oo][Rr]|[Ff][Aa][Ii][Ll](ing|ING|ure|URE)?)\")\nwhite on-green bold: regex(\"([Ff][Pp][Ss]|[Pp][Aa][Ss][Ss](ing|ING)?)|(success|SUCCESS|Success)(ful|FUL)?\")\nwhite on-red bold: regex(\"^E[0-9]+\\d\")\nwhite on-yellow bold : regex(\"^W[0-9]+\\d\")\n# white on-green bold: regex(\"[Ff][Pp][Ss]\")\n\nyellow: regex(\"\\s[0-9]+\\s\")\nred: regex(\"\\s[0-9]+[uUmM][sS]\")\n# magenta: regex(\"\\s[^\\]]+.(jpg|jpeg|yuv|yuyv|png|json|bin|hbm|engine|txt)\")\nmagenta: regex(\"\\s[0-9a-zA-Z\\/\\_\\-]+.(jpg|jpeg|yuv|yuyv|png|json|bin|hbm|engine|txt)\")\n\n# Date\ngreen: regex(\"\\d{4}-\\d\\d-\\d\\d\")\n# Time\ngreen bold: regex(\"\\d\\d:\\d\\d:\\d\\d\")\n# IP address (and port number if present)\nyellow underline: regex(\"\\d+\\.\\d+\\.\\d+\\.\\d+(:\\d+)?\")\nmagenta: regex(\"\\[samplesession\\]\")\n# Catch-all for anything else inside [square brackets]\nblue: regex(\"\\[[^\\]]+\\]\")\n# Catch-all for any remaining standalone numbers\ncyan bold: regex(\"\b\\d+\b\")\n\n```\n\n### [lolcat](https://github.com/busyloop/lolcat)\n\n1. `pip install logcat`\n1. 彩色输出 `lolcat phigent.txt -F 0.02`\n\n### [logtool](http://xjack.org/logtool/)\n\n1. `sudo apt install logtool`\n1. `cat no_bug_car.txt | logtool > test.txt` 输出颜色\n\n## link\n\n1. [code style 主题：programmingfonts](https://www.programmingfonts.org/#ubuntu)\n1. [color palettes](https://coolors.co/palettes/palettes)\n1. [正则表达测试](https://regex101.com/)\n1. [正则表达式解释](https://tool.oschina.net/uploads/apidocs/jquery/regexp.html)\n"},{"title":"Markdown","url":"/2022/10/14/Markdown/","content":"\n## basic\n\n1. [color](https://stackoverflow.com/a/73613628)\n1. page break: `<div style=\"page-break-after: always; break-after: page;\"></div>`\n1. check box `- [x]`: 好好利用，写好 worklog\n1. `>` 注意符号\n1. diff\n\n```diff\npublic class Hello1\n{\n   public static void Main()\n   {\n-      System.Console.WriteLine(\"Hello, World!\");\n+      System.Console.WriteLine(\"Rock all night long!\");\n   }\n}\n```\n\n## emoji\n\n1. 直接复制就能用\n1. [webfx](https://www.webfx.com/tools/emoji-cheat-sheet/)\n1. [emojipedia](https://emojipedia.org/)\n1. [总结](https://gist.github.com/rxaviers/7360908)\n1. [cheatsheet](https://github.com/ikatyang/emoji-cheat-sheet#table-of-contents)\n1. ✅ 🔥 😀\n\n## Keybindings(emacs)\n\n1. stying test: C-c C-s\n2. toggle: C-c C-x\n3. help: C-c C-h\n4. outline navigation: same as org-mode\n5. running: C-c C-c\n\n### Links and Images\n\n1. C-c C-l: insert link\n2. C-c C-i: insert image\n\n### Text Styles C-c C-s\n\n1. C-c C-s i: _italic_\n2. C-c C-s b: **bold**\n3. C-c C-s c: `inline code`\n4. C-c C-s C: code block\n\n### Headings C-c C-s\n\n### Horizontal Rules C-c C-s -\n\n### Footnodes: C-c C-s f\n\n### Wiki Links\n\n### ...TODO\n\n## Reference\n\n1. [(good)markdown 教程](https://eblog.gitee.io/posts/hexo/hexo-writing-skills.html)\n1. [写的好的 markdown, 供参考](https://eblog.gitee.io/posts/hexo/hexo-theme-beautify.html)\n1. [markdown mode tutorial](https://github.com/jrblevin/markdown-mode)\n"},{"title":"v4l2","url":"/2022/10/09/v4l2/","content":"\n## usb 双目\n\n1. 只能打开一个原因 [link](https://www.21xrx.com/Articles/read_article/259051) [link2](https://blog.csdn.net/qq_32900237/article/details/102392445)\n1. 保存到同一画面上的优势：解决同步问题, 通过板载芯片来进行同步\n\n## [v4l2](http://zhaoxuhui.top/blog/2021/09/23/v4l2-introduction-and-usb-camera-bayer-raw-data.html)\n\n1. `sudo v4l2-ctl --list-devices`获取相机列表\n1. `media-ctl -p`\n1. `v4l2-ctl --all -d /dev/video`\n1. `v4l2-ctl -d /dev/video0 --list-formats-ext` 列出/dev/video0 支持的格式\n1. `yavta /dev/video0 -n3 -c10 -f YUYV -s1920x1282 --skip 9 -Fvideo0_#.yuv &` 从/dev/video0 拿图, 保存未.yuv\n1. `v4l2-ctl --set-fmt-video=width=1920,height=1282,pixelformat=YUYV --stream-mmap --stream-count=1 -d /dev/video0 --stream-to=camera_0_1920_1282_0.yuv` 获取一张图片\n1. [send video to /dev/video0](https://askubuntu.com/a/881341)\n1. `ffmpeg -y -t 15 -f video4linux2 -video_size 1920x1080 -input_format nv12 -timestamps abs -use_libv4l2 1 -i /dev/video0 out.mov` ffmpeg [link](https://superuser.com/q/1449252)\n\n## sample\n\n1. [wuhanstudio/capturev4l2](https://github.com/wuhanstudio/capturev4l2)\n1. [C\\+\\+ sample](https://www.kernel.org/doc/html/v4.9/media/uapi/v4l/capture.c.html) `gcc -o v4l2_test v4l2_test.c`\n1. simple opencv sample\n\n```c++\n#include \"opencv2/opencv.hpp\"\n#include \"iostream\"\n\nint main(int, char**) {\n    // open the first webcam plugged in the computer\n    cv::VideoCapture camera(0);\n    if (!camera.isOpened()) {\n        std::cerr << \"ERROR: Could not open camera\" << std::endl;\n        return 1;\n    }\n\n    // create a window to display the images from the webcam\n    cv::namedWindow(\"Webcam\", CV_WINDOW_AUTOSIZE);\n\n    // this will contain the image from the webcam\n    cv::Mat frame;\n\n    // capture the next frame from the webcam\n    camera >> frame;\n\n    // display the frame until you press a key\n    while (1) {\n        // show the image on the window\n        cv::imshow(\"Webcam\", frame);\n        // wait (10ms) for a key to be pressed\n        if (cv::waitKey(10) >= 0)\n            break;\n    }\n    return 0;\n}\n```\n\n## issue\n\n1. `OPENCV_LOG_LEVEL=DEBUG ./demo` 打开 opencv debug 信息\n1. 注意`/dev/video*` 权限\n1. 双目摄像头可能需要同时打开 `/dev/video0` 和 `/dev/video1`\n"},{"title":"bling","url":"/2022/10/09/bling/","content":"\n## zh\n\n1. 路径切换\n   ![cpp profiling](/images/gif/zh.gif)\n\n## emacs and tmux\n\n1. git 一键提交\n   ![emacs magit](/images/gif/emacs_magit.gif)\n\n1. 统计 log 中数值均值, 方差......\n   ![emacs statistics](/images/gif/mean.gif)\n\n1. emacs 中通过 tmux 将命令发送到另外 tmux panel，省去多次复制操作\n   ![emacs tmux](/images/gif/emacs_tmux.gif)\n\n1. 一键启动多进程程序\n   ![tmux send](/images/gif/tmux.gif)\n\n<!-- more -->\n\n## profiling\n\n1. c++ profiling 库， 一条语句来进行流程控制，fps 打印，各模块耗时统计表\n   ![cpp profiling](/images/gif/profiling.gif)\n","tags":["ideas"],"categories":["Others"]},{"title":"Life-experience","url":"/2022/10/09/Life-experience/","content":"\n## base\n\n1. `找到内心的召唤`， 做事要培养兴趣，不然很难做好 `Hackers write better code when it's for their own satisfaction instead of for pay.`\n1. `多尝试`，别怕出错\n1. `不要用反问` 老板要是换个说法，说“这个文件需要打印两份”。你觉得哪种说法更容易接受呢？明显后者听起来舒服多了。 比如你打印文件，老板跟你说“你怎么只打印一份啊？老板要是换个说法，说“这个文件需要打印两份”。你觉得哪种说法更容易接受呢？明显后者听起来舒服多了。老板要是换个说法，说“这个文件需要打印两份”。你觉得哪种说法更容易接受呢？明显后者听起来舒服多了。\n1. `能够轻易得到答案的简单问题不要问出口`，那并不会显得你很好学，反而会显得你无知又懒惰。\n1. `不经过思考后的答案不要脱口而出`，控制住嘴巴是你学会成熟的第一步。\n1. `说话尽量不要带“吧”字`，好和好吧，行和行吧，听你的和听你的吧，前者是直接坦率的，后者是犹豫而无奈的。要做就做，干脆一点，不然为何答应\n1. `要把自己当作一个公司去运营`，要有战略方向（你的人生目标），要有运营路径（你的行动方案），要有产品竞争力（你的个人能力维度），要有营销（你的公开表达），要有财务思维（你的收支平衡）\n1. `职场没有功劳就没有苦劳` 只有建立在功劳基础上的苦劳，才有价值。\n1. `远离抱怨的人`\n1. `生气时，学会闭嘴` 生气时，情绪完全被动物大脑控制，很容易得罪人。像钉子钉墙一样，就算把钉子拔出来，墙依然会有孔 最好的办法就是深呼吸，闭嘴。\n1. `如果找不到热爱的事业，先做最擅长的工作`\n1. `别糊弄`\n1. 别人找你帮忙的时候，不要果断答应。如果对方是爽快之人，你可以同样爽快交往。但对大部分人，不要急于表露自己的热情好客，你的直爽只会把你的付出变得廉价，在他眼里你办这事没有任何难度，不会从心里感激你，只是把你当工具人。\n"},{"title":"log","url":"/2022/09/30/log/","content":"\n## glog\n\n1. `GLOG_colorlogtostderr=1 ./test` 输出彩色\n1. `GLOG_alsologtostderr=false` 关闭 log\n1. export GLOG_log_dir=./log_tmp 输入 log 到 log_tmp 目录下\n\n## logrotate\n\n1. 使用 logrotate 实现 log 循环覆盖功能，将 log 文件限制在一定大小。由于 logrotate 程序不能解决多文件未知变量名的情况，讨论了一下解决办法，设置环境变量 GLOG_max_log_size=1，使得程序输出的 log 被分片，每片最大为 1M,然后使用脚本保留最新的几片 log，从而控制 log 的大小在一定范围内。\n\nhttps://www.cnblogs.com/kevingrace/p/6307298.html 配置\n\nhttp://blog.csdn.net/hxh129/article/details/8178647 指定文件大小，超过就转存\n\n```sh\nsize=`ll /var/log/messages | cut -d \" \" -f 5`\nif [ $size -ge xxxxx ];\nthen\nlogrotate  /xxx/xxx.conf;\nfi\n```\n\n## logrotate 功能\n\n```sh\ncd /tmp\nwhile sleep 15; do\n        file_num=`ls -l |grep \"^-\"|wc -l`\n        if [ \"$file_num\" -gt 8 ]; then\n                rm_num=$(($file_num - 8))\n                for i in `ls -t /tmp | tail -n $rm_num`; do\n                        rm \"$i\"\n                done\n        fi\ndone\n\ncd /tmp\nwhile sleep 15; do\n        file_num=`ls -l |grep \"^-\"|wc -l`\n        echo \"$file_num\"\n        if [ \"$file_num\" -gt 8 ]; then\n                rm_num=$(($file_num - 8))\n                echo \"$rm_num\"\n                ls -lt\n                for i in `ls -t /tmp | tail -n $rm_num`; do\n                        echo \"clear log file $i\";\n                        rm \"$i\" || echo ok\n                done\n        fi\ndone\n\nalive_counter=0\nprog=run_smartsv\ncd /tmp\nwhile sleep 5; do\n    if [ $((alive_counter % 5 )) -eq 0 ]; then\n        echo test\n        file_num=`ls -l |grep \"^-\"|wc -l`\n        echo \"$file_num\"\n        if [ \"$file_num\" -gt 6 ]; then\n                rm_num=$(($file_num - 6))\n                echo \"$rm_num\"\n                ls -lt\n                for i in `ls -t /tmp | tail -n $rm_num`; do\n                        echo \"clear log file $i\";\n                        rm \"$i\" || echo ok\n                done\n        fi\n    fi\ndone\n```\n"},{"title":"wallpaper","url":"/2022/09/29/wallpaper/","content":"\n## 桌面背景制作\n\n1. [colorfu](https://github.com/pearmini/colorfu)\n1. color value : `1E1E1E`\n1. [express.adobe](https://express.adobe.com/zh-Hans-CN/sp/design/post/urn:aaid:sc:VA6C2:a245b511-f975-4454-94d9-cdb73067395b) 先抠图，纯色背景，调整透明度\n1. [canva](https://www.canva.com)\n1. [think twice 模板链接](https://www.canva.com/design/DAFNl8MDuMs/Og2VZlsR1bnZXNcsOhPOYg/view?utm_content=DAFNl8MDuMs&utm_campaign=designshare&utm_medium=link&utm_source=publishsharelink&mode=preview)\n1. [在线抠图](https://picwish.cn/upload)\n\n## links\n\n1. [canva 搜索壁纸](https://www.canva.com/search/templates?q=%E5%A3%81%E7%BA%B8%E6%B1%BD%E8%BD%A6&fCategory=tACZCp5JRWg&filters=eyJBIjoiTUBmcFVZX0dNQlhrM19idyIsIkE%2FIjoiQyIsIkQiOiIwMDAwMDAifQ%3D%3D)\n1. [口袋妖怪适合 zenburn 背景的壁纸](https://wallpaperaccess.com/full/1644661.jpg)\n1. [口袋妖怪壁纸](https://wallpaperaccess.com/awesome-gengar)\n1. [口袋妖怪壁纸黑](https://wallpaperaccess.com/black-pokemon)\n1. [ubuntu 每一版壁纸汇总](http://ubuntu.ecchi.ca/wallpapers/)\n1. [ubuntu 壁纸](https://wallpaperaccess.com/ubuntu)\n1. [windows 壁纸](https://wallpaperhub.app/creators/genrolecaspe)\n1. [windows 壁纸 2](https://wallpaperhub.app/wallpapers/?tags=windows)\n1. [close scroll bar](https://github.com/microsoft/terminal/issues/3878#issuecomment-735253417)\n1. [themes](https://windowsterminalthemes.dev/)\n1. [programmer backgroud](https://wallpapers.com/wallpapers/4k-programming-codes-in-black-screen-cl72lv0j7uvh4bmy.html)\n1. [cool car](https://wallpapers.com/wallpapers/cool-car-neon-blue-j3a9pqeowr7ywja3.html)\n1. [cool](https://express.adobe.com/zh-Hans-CN/sp/design/post/urn:aaid:sc:VA6C2:928899ca-b02d-442f-9132-8607d4a4fada?workflow=quicktask)\n1. [car](https://m.media-amazon.com/images/S/aplus-media-library-service-media/d64dd730-5df0-4bd0-a30c-c78d168d48bd.__CR0,0,1464,625_PT0_SX1464_V1___.jpg)\n1. [cool-sports-car](https://wallpaperaccess.com/cool-sports-car)\n"},{"title":"Pandoc","url":"/2022/09/27/Pandoc/","content":"\n## base\n\n1. Pandoc is a Haskell library for converting from one markup format to another, and a command-line tool that uses this library.\n1. [pandoc](https://github.com/jgm/pandoc) (The universal markup converter 可以转换各种标记语言)\n\n## usage\n\n1. [demos](https://pandoc.org/demos.html)\n1. [online 转换](https://pandoc.org/try/)\n1. markdown to pdf `pandoc MANUAL.md --pdf-engine=xelatex -o example13.pdf`\n"},{"title":"Document","url":"/2022/09/26/Document/","content":"\n## 文档\n\n1. [参考 bazel](https://bazel.build/?hl=zh-cn)\n1. are several main types of documents that software engineers often need to write:\n\n   - Reference documentation, including code comments 参考文档，包括代码注释\n   - Design documents 设计文档\n   - Tutorials 教程\n   - Conceptual documentation 概念文档\n   - Landing pages 着陆页\n   - quickstart, Getting Started\n   - introduction, overview\n\n1. [Software-Engineering-at-Google](https://qiangmzsx.github.io/Software-Engineering-at-Google/#/zh-cn/Chapter-10_Documentation/Chapter-10_Documentatio)\n\n## tools\n\n1. [pandoc](https://github.com/jgm/pandoc) (The universal markup converter 可以转换各种标记语言)\n1. doxygen\n1. [sphinx-doc/sphinx](https://github.com/sphinx-doc/sphinx)\n1. Latex [pandoc-latex-template](https://github.com/Wandmalfarbe/pandoc-latex-template)\n1. [readthedocs](https://readthedocs.org/)\n"},{"title":"github","url":"/2022/09/26/github/","content":"\n## note\n\n1. markdown需要打开文件才能更好的显示outline 大纲\n1. github clone/push 问题\n   - [enabling-ssh-connections-over-https](https://docs.github.com/en/authentication/troubleshooting-ssh/using-ssh-over-the-https-port#enabling-ssh-connections-over-https)\n\n```sh\nHost github.com\n    Hostname ssh.github.com\n    Port 443\n    User git\n```\n\n## explore\n\n1. [star-history](https://star-history.com/) 可以多个 repo 同时显示\n1. [starchart.cc](https://starchart.cc/)\n1. [github-readme-stats](https://github.com/anuraghazra/github-readme-stats)\n1. [start sort](https://github.com/search?o=desc&q=stars%3A%3E0&s=stars&type=Repositories)\n1. [octobox.io 管理 github 通知](https://octobox.io/)\n   - 直接登录 github 就能显示\n\n## gist\n\n## base\n\n1. [badges/shields](https://shields.io/badges) 左上角按要求输入变量，会生成 URL, 点击 execute 会预览结果\n   - [任意格式](https://shields.io/badges/static-badge) [sample](https://img.shields.io/badge/try%20it-online-orange)\n1. [badge example](https://github.com/doctest/doctest)\n1. [build-badge](https://dev.to/azure/github-how-to-display-the-status-badge-for-a-github-action-5449)\n1. [repostatus](https://www.repostatus.org/)\n1. [issuse badge](https://isitmaintained.com/)\n1. [自动生成蛇的行走路径，一口气吃光所有“绿块”](https://github.com/Platane/snk)\n1. 访问 github 有问题，访问 gitlab 无问题， 可以先用 gitlab 去 mirrors github repo， 再访问 gitlab\n1. 点击`go to file`， 搜索文件\n"},{"title":"LaTeX","url":"/2022/09/25/LaTeX/","content":"\n## base\n\n1. LaTeX: Lamport TeX\n1. 读音：莱泰克\n1. [调整字体大小](https://www.overleaf.com/learn/latex/Font_sizes%2C_families%2C_and_styles)\n1. 中文问题 `\\begin{CJK}{UTF8}` 换成 `\\begin{CJK}{gbsn}` [link](https://www.cnblogs.com/zyl910/archive/2013/06/02/doxygen_pdf_chinese.html)\n1. 换页问题： `\\doxysection` 之前加 `\\newpage`\n1. [fancy Headers_and_footers](https://www.overleaf.com/learn/latex/Headers_and_footers)\n   - `O` or `E`: to specify Odd or Even pages\n   - `H` or `F`: to indicate Header or Footer\n   - `L`, `C` or `R`: for the Left, Centre and Right “zone” of the header or footer\n1. `\\pagestyle{fancyplain}` doxygen refman.tex 删除之后页眉页脚不再有横杠\n1. pagestyle `\\pagestyle{⟨style⟩}` [link](https://www.overleaf.com/learn/latex/Headers_and_footers)\n   - empty: no headers or footers on pages\n   - plain: no page headers, footers consist of a centered page number\n   - headings: no footers, headers contains class-specific information and page number\n   - myheadings: no footers, headers contains page number and user-supplied information\n1. picture in header [link](https://tex.stackexchange.com/a/122936) picture 下可以添加官网网址，并加上点击链接 `\\fancyhead[L]{\\includegraphics[width=3cm]{logo1} \\\\\\href{https://www.wikibooks.org}{Wikibooks home}}`\n1. oneside or twoside [link](https://www.overleaf.com/learn/latex/Single_sided_and_double_sided_documents#Difference_between_one-side_and_two-side_documents)\n1. chapter style [link](https://tex.stackexchange.com/a/18605)\n1. [list of features and tables](https://www.overleaf.com/learn/latex/Lists_of_tables_and_figures)\n\n```sh\n% Headers & footers\n\\usepackage{fancyhdr}\n\\pagestyle{fancy}\n\\setlength\\headheight{32pt}\n\\fancyhead{}\n\\fancyhead[L]{\\includegraphics[width=3cm]{logo1}}\n\\fancyhead[R]{\\text{dcv-cpp 开发手册} \\\\[4pt] \\\\\\text{\\fancyplain{}{\\rightmark}}}\n% \\renewcommand{\\headrulewidth}{0pt}\n% \\renewcommand{\\footrulewidth}{0pt}\n\\renewcommand{\\headruleskip}{2.5pt}\n% \\renewcommand{\\footruleskip}{0pt}\n```\n\n1. 首页设置\n\n```sh\n\\begin{titlepage}\n\\vspace*{7cm}\n\\begin{center}%\n\\includegraphics[width=4cm]{logo1}\n\n\\textbf{\\Huge heimdallr-\\/sdk 用户手册 \\\\[3ex]\\Large v1.\\+0.\\+0 }\\\\\n\\vfill\n\\vspace*{1cm}\n{\\large 制作者 PhiGent Robotics}\\\\\n\\end{center}\n\\end{titlepage}\n```\n\n## editer\n\n1. [vscode](https://github.com/James-Yu/LaTeX-Workshop)\n1. [online editer overleaf](https://www.overleaf.com/)\n   - 点击可以到对应代码\n   - 可以修改主题， 编译器, 字体大小，style 等\n   - 添加 package [link](https://tex.stackexchange.com/a/536730)\n   - doxygen 需要的 sty: tabu_doxygen.sty, ongtable_doxygen.sty, doxygen.sty\n\n## Tutorial\n\n1. [overleaf Learn_LaTeX_in_30_minutes](https://www.overleaf.com/learn/latex/Learn_LaTeX_in_30_minutes)\n1. [Begin-Latex-in-minutes](https://github.com/luong-komorebi/Begin-Latex-in-minutes)\n1. [Begin-Latex-in-minutes Chinese](https://github.com/luong-komorebi/Begin-Latex-in-minutes/blob/master/Translation-Chinese.md)\n\n## 简历模板\n\n1. The CV presents a full history of your academic credentials, so the length of the document is variable. In contrast, a resume presents a concise picture of your skills and qualifications for a specific position, so length tends to be shorter and dictated by years of experience (generally 1-2 pages).\n1. CV 是履历，Resume 是简历。\n\n   - Resume，简述于求职相关的教育背景和工作经历，其目的在于说服用人单位雇用自己；\n   - CV，Curriculum Vitae 事集中说明学术工作，不重视与文化程度和学习成绩无直接关系的资料。\n\n1. [Awesome-CV](https://github.com/posquit0/Awesome-CV) CV: Curriculum Vitae 个人简历\n1. [overleaf awesome-cv template](https://www.overleaf.com/latex/templates/awesome-cv/dfnvtnhzhhbm)\n1. [latexcv](https://github.com/jankapunkt/latexcv/tree/master/sidebar)\n1. [awesome-resume-for-chinese](https://github.com/dyweb/awesome-resume-for-chinese)\n\n## links\n\n1. [ drawing neural networks for reports and presentation](https://github.com/HarisIqbal88/PlotNeuralNet)\n1. [pandoc-latex-template](https://github.com/Wandmalfarbe/pandoc-latex-template)\n1. [TI 文档参考](https://www.ti.com/lit/an/spracz2/spracz2.pdf?ts=1664353778597&ref_url=https%253A%252F%252Fwww.google.com%252F)\n"},{"title":"DataStructure","url":"/2022/09/15/DataStructure/","content":"\n## CPP\n\n### map\n\n1. unordered_map 排序：使用 map `std::map<int, int> ordered(unordered.begin(), unordered.end());` [link](https://stackoverflow.com/a/6212709)\n1.\n"},{"title":"J5","url":"/2022/09/14/J5/","content":"\n## base\n\n1. [多模型优先级调度](https://developer.horizon.cc/forumDetail/185446371330059350)\n\n## software development\n\n1. 功能\n1. 指标\n1. 性能\n\n## docker\n\n1. [ai_toolchain_ubuntu_20_j5_cpu](https://hub.docker.com/r/openexplorer/ai_toolchain_ubuntu_20_j5_cpu/tags)\n1. [ai_toolchain_ubuntu_20_j5_gpu](https://hub.docker.com/r/openexplorer/ai_toolchain_ubuntu_20_j5_gpu/tags)\n\n## Model Convert\n\n1. PTQ 根据算法计算 scale, QAT 训练的时候学习 scale\n1. `PTQ` 后量化（即：Post-training Quantization，浮点定点转换方案 ）：先训练浮点模型，然后使用校准图片计算量化参数，将浮点模型转为量化模型。该方法简单、快捷，但将浮点模型直接转为量化模型难免会有一些量化损失，地平线浮点转换工具链中提供的后量化工具能做到 80%以上的模型量化误差小于 1%。\n1. `QAT` 量化训练 （Quantization Aware Training）通过在训练过程中融入量化和反量化过程，来实现量化模型的精度恢复, 由于量化是每层独立进行的，所以每层的量化是不依赖于前一层量化的结果的，这就导致了在实际的 inference 过程中会出现误差累积的情况，我们需要一种可学习的 scale。QAT 就是在做这样一件事情\n1. 浮点转定点工具链生成物是异构混合模型 bin 文件，与量化训练方案中最后的定点模型 hbm 文件有概念上的差异。\n\n## Runtime 部署\n\n1. `format`: NV12, NV12_SEPERATE, BGR, YUV\n1. `layout`: NCHW or NHWC\n1. `type`: float, S32, int8; quant and dequant 定浮点转换\n1. `memory`: alloc, free, sync\n1. `inference`: setinput, do, getoutput\n1. `align`: model output align\n\n## hardware acceleration\n\n1. VPS 硬件：提供了缩放，镜像，旋转，矫正，流控，金字塔等图像处理能力。硬件上由单个 IPU，单个金字塔 PYM，两个 GDC 组成。软件上则采用分组来实现分时复用硬件，最大支持 8 个组的分时复用。\n1. VENC/VDEC 模块：编解码\n\n## 金字塔配置\n\n1. 参考《MU-3020-12-J5-视频通路配置指南》vpm 配置\n1. 1920 _ 1080 -> 896 _ 512（需要 16 的倍数)\n   - bl 线性变换到 960 \\* 540\n   - ds roi 设置 960 \\* 540 roi 0, 0, 960, 540\n   - ds 设置 output 为 896 \\* 512\n\n## Tools\n\n1. 交叉编译: OE(OpenExplorer 天工开物开发平台)\n1. 系统升级\n1. 查看模型信息\n1. profiling\n1. dump 对数\n1. `hrut_ddr -t all -p 1000000 -n 1` 查看 ddr 读写带宽\n1. `hrut_bpuprofile -b 2 -r 0`查看 bpu 使用率\n\n## Links\n\n1. [一文了解模型量化中的 QAT 和 PTQ](https://blog.csdn.net/m0_38043555/article/details/118278641)\n1. [j5 开发板信息](https://developer.horizon.ai/api/v1/fileData/horizon_j5_open_explorer_v1_1_40c_py38cn_doc/oe_mapper/source/appendix/hardware/dev_board_kit.html)\n1. [horizon_j5_open_explorer_v1_1_40c_py38cn_doc](https://developer.horizon.ai/api/v1/fileData/horizon_j5_open_explorer_v1_1_40c_py38cn_doc/vdsp/dsp_rpc_api_doc/source/index.html)\n1. [Cadence NatureDSP Library for MathX KP6 DSP cores](https://github.com/cad-rlc/ndsp-kp6)\n1. [x3 模型官方性能](https://developer.horizon.ai/api/v1/fileData/doc/ddk_doc/navigation/ai_toolchain/docs_cn/horizon_ai_toolchain_user_guide/chapter_6_appendix.html)\n1. [pointpillar dps 加速](https://developer.horizon.cc/api/v1/fileData/horizon_j5_open_explorer_cn_doc/vdsp/dsp_example_doc/source/dsp_example.html#pointpillars)\n"},{"title":"SOA","url":"/2022/09/07/SOA/","content":"\n## Basic\n\n1. SOA 是一种架构风格\n1. 实际上 SOA 只是一种架构设计模式，而 SOAP、REST、RPC 就是为了实践这种设计模式而设计的数据通讯方式，其中 SOAP 通俗理解就是服务间通过 http+xml 的形式完成数据交换，REST 就是 http+json 的形式，RPC 是基于 socket 的形式。CXF 框架就是典型的 SOAP/REST 框架，Dubbo 就是典型的 RPC 框架，而 SpringCloud 就是遵守 REST 规范的微服务生态系统。\n   1 .后来又有了微服务的概念，个人理解微服务和 SOA 就是孪生子。如今生产实践中提到的微服务，在 SOA 的基础上更进一层，引入了很多新的东西如服务治理、链路跟踪、配置管理等等可以帮助企业构建高可用高并发高性能的系统的组件。\n1. 为什么服务总是和以太网，DDS 或 SOME/IP 绑定的呢？CAN 网络上是否可以实现完整的 SOA 架构？\n   - 首先，以 SOME/IP 举例，因为 SOME/IP 的完整名称其实能回答上面的大多数问题，SOME/IP = Scalable service-Oriented Middleware over IP。即“运行于 IP 之上的可伸缩的面向服务的中间件”。可见，并不是 SOA 必须和 SOME/IP 绑定，而是 SOME/IP 天生就是为了 SOA 架构而生。\n   - SOME/IP 的精华在于“Middleware 中间件”，这是一种独立的系统软件或服务程序，分布式应用软件可借助 Middleware 在不同的技术之间共享资源。分布式应用软件，在这里指的就是“服务”；不同的技术之间，在这里指的就是不同的平台或操作系统，比如 Linux 系统或 AUTOSAR OS OSEK 等。\n   - Scalable Middleware，顾名思义，则是“可伸缩中间件”，指的是该中间件能够适配于不同的平台及操作系统，其支撑的平台可大可小。\n   - 简单来说，SOA 是软件架构的一种设计理念；SOME/IP 是一种将软件接口进行打包的打包方式，是一种中间件。而汽车行业通常所指的\"以太网\"是泛化之后的概念，涵盖了基于以太网技术所实现的各种相关技术手段，包括 TCP/IP 协议、DoIP 协议、SOME/IP 协议等。当然若通过其他非以太网的通信方式来实现 SOA 也是可行的，但通常大家不那么用。比如基于 CAN 总线的架构，由于其基础的架构和通信协议栈里不存在 Middleware 中间层的概念，所以要实现 SOA 的代价是非常巨大的。\n1. 以服务导向的软件系统构架（SOA）将成为主流，以 SOA/SOME-IP 为脉络支撑起汽车以服务为出发点和差异化竞争的整车 E/E 架构。\n1. `SOA`（Service-Oriented Architecture，面向服务的架构）\n1. SOA 的软件架构\n   ![SOA的软件架构](https://mmbiz.qpic.cn/mmbiz_png/dgmES0HFW0utCXbf5mDOFepsQSA2wVFPBqzgGf38bkxsJOlMEkjvHG3H5ibs5ZW9Mw968jLxgOOpVk8iaKNRdABg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n1. ![SOA汽车分层模型](https://mmbiz.qpic.cn/mmbiz_png/dgmES0HFW0utCXbf5mDOFepsQSA2wVFPBqzgGf38bkxsJOlMEkjvHG3H5ibs5ZW9Mw968jLxgOOpVk8iaKNRdABg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n1. [SOA 所解决的核心问题](https://segmentfault.com/a/1190000040618603)\n   - 系统集成：站在系统的角度，解决企业系统间的通信问 题，把原先散乱、无规划的系统间的网状结构，梳理成规整、可治理的系统间星形结构，这一步往往需要引入 一些产品，比如 ESB、以及技术规范、服务管理规范；这一步解决的核心问题是【有序】\n   - 系统的服务化：站在功能的角度，把业务逻辑抽象成可复用、可组装的服务，通过服务的编排实现业务的快速再生。目的：把原先固有的业务功能转变为通用的业务服务，实现业务逻辑的快速复用；这一步解决的核心问题是【复用】\n   - 业务的服务化：站在企业的角度，把企业职能抽象成可复用、可组装的服务；把原先职能化的企业架构转变为服务化的企业架构，进一步提升企业的对外服务能力；前面两步都是从技术层面来解决系统调用、系统功能复用的问题。第三步，则是以业务驱动把一个 业务单元封装成一项服务。这一步解决的核心问题是 【高效】\n\n## Links\n\n1. [车载 SOA 软件架构技术规范 1.0](https://www.kdocs.cn/l/sbWWKnW352t2)\n1. [\"域\"测未来 \"域\"见 SOA](https://www.3cst.cn/Information/info/RHcujT3VVDQD11ea8d6300163e0473d8)\n1. [什么是 SOA？什么是微服务？SOA 和微服务有什么区别？](https://bbs.huaweicloud.com/blogs/315752)\n1. [面向 SOA 服务的智能汽车高性能计算平台布局与测试方案](https://www.sohu.com/a/523519503_560178)\n1. [SOA 中的软件架构设计及软硬件解耦方法论](https://www.3cst.cn/Information/info/ALwsiHASP99L11ea8d6300163e0473d8)\n1. [MVC、 RPC、SOA 和 微服务架构异同](https://www.jianshu.com/p/48839b7d0050)\n"},{"title":"软件架构图","url":"/2022/09/07/软件架构图/","content":"\n## Basic\n\n1. 画图工具\n\n   - [excalidraw](https://excalidraw.com/)\n\n1. 黑色背景图更清晰显眼 [test](https://excalidraw.com/#json=QQSsjTtgSH00X3SatzWVM,fCNSETDLrfuaDv7-iRMwAA)\n1. 黑白灰就可以让框图显示的清晰\n1. 动态流程图 [link](https://www.bilibili.com/video/BV1by4y167EA/)\n1. 交互式流程图 [link](http://www.jtopo.com/)\n1. ProcessOn 数据流图动图\n1. [用 JointJs 实现数据流图](https://juejin.cn/post/7111894700952387621)\n1. [如何用 drawio 让自己的流程图线条产生动起来的效果](https://www.drawzh.com/1457.html)\n1. [figma motion](https://babkin-pavel.medium.com/how-to-animate-objects-along-the-path-in-figma-motion-plugin-370f3bf95e7b)\n1. [figma flow arrow](<https://www.figma.com/file/35Xwhb4WtXZdRGQlWDdewn/Motion---UI-and-Games-animation-(Community)?node-id=0%3A1>)\n\n## 层次图\n\n### sample\n\n1. [微软程序建构指南](<https://docs.microsoft.com/en-us/previous-versions/msp-n-p/ee658124(v=pandp.10)>)\n   ![Common application architecture](<https://docs.microsoft.com/en-us/previous-versions/msp-n-p/images/ee658124.b8220f0d-f76a-40d6-8b1b-5279f7cdcee9(en-us,pandp.10).png>)\n1. appolo\n   ![Apollo 7.0](https://github.com/ApolloAuto/apollo/raw/master/docs/demo_guide/images/Apollo_7_0.png)\n1. vitis-ai-library\n   ![vitis-ai-library](https://github.com/Xilinx/Vitis-AI/raw/master/src/Vitis-AI-Library/ai_library_diagram.png)\n1. dcv-cpp\n   ![dcv-cpp](/images/software-diagram/dcv-cpp.jpg)\n1. [理想的软件自研规划](https://inews.gtimg.com/newsapp_bt/0/14916534758/1000)\n   ![理想的软件自研规划](/images/software-diagram/lixiang.png)\n1. ![SOA汽车分层模型](https://mmbiz.qpic.cn/mmbiz_png/dgmES0HFW0utCXbf5mDOFepsQSA2wVFPBqzgGf38bkxsJOlMEkjvHG3H5ibs5ZW9Mw968jLxgOOpVk8iaKNRdABg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n1. ![华为MDC智能驾驶计算平台](https://img-blog.csdnimg.cn/20210414055550879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1amlhbmluZ18xMTAxMTc=,size_16,color_FFFFFF,t_70)\n1. [华玉通软 Greenstone 智能驾驶基础软件平台](https://nwzimg.wezhan.cn/contents/sitefiles2049/10245019/images/23519503.png)\n   ![华玉通软Greenstone 智能驾驶基础软件平台](/images/software-diagram/huayu.png)\n1. ![华玉通软应用](https://nwzimg.wezhan.cn/contents/sitefiles2049/10245019/images/25100339.gif)\n1. ![nvidia](http://nvidia.zhidx.com/uploadfile/2018/10/5bce021d3364a.jpg)\n"},{"title":"AUTOSAR","url":"/2022/09/06/AUTOSAR/","content":"\n## basic\n\n1. `AUTOmotive Open System ARchitecture`\n1. 开发标准化：interfaces, exchange formats, methodology\n1. 代码分为静态代码和动态代码，静态是写死的，动态是根据配置自动生成的\n1. 配置通过[axml](https://developer.cisco.com/docs/axl/#!what-is-axl)\n1. 有三层：application, runtime environment(rte) 和 basic Software(bsw), 每一层通过标准接口通信。(没有增加中间层解决不了的问题)\n1. `Any problem in computer science can be solved by another layer of indirection` 计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决\n1. application 由 software component(swc)组成\n1. rte 均是动态代码，类似事件总线, 起到交换机的作用\n1. runnables 类似 graph 概念, 是一个静态概念，tasks 是动态概念, 有 map 机制，将静态概念映射到动态\n1. 区分好静态概念和动态概念：如 graph 是静态概念，runner 是动态概念， 静态是一种表示(程序)， 动态运行静态概念(进程)\n\n## Links\n\n1. [release](https://www.autosar.org/nc/document-search/?tx_sysgsearch_pi1%5Bquery%5D=&tx_sysgsearch_pi1%5Bcategory%5D%5B150%5D=150)\n1. [AUTOSAR Introduction: The AUTOSAR Partnership and Standardization(官网)](https://www.autosar.org/fileadmin/user_upload/AUTOSAR_EXP_Introduction_Part1.pdf)\n1. [AUTOSAR Introduction: Current Features in a Nutshell(官网)](https://www.autosar.org/fileadmin/user_upload/AUTOSAR_EXP_Introduction_Part2.pdf)\n1. [Adaptive Autosar 整体架构理解](https://blog.csdn.net/AgingMoon/article/details/102813280)\n1. [AUTOSAR 与 ROS2 区别](https://www.3cst.cn/Information/info/1uwAF7ySxL8111ea8d6300163e0473d8)\n"},{"title":"域控制器","url":"/2022/09/06/域控制器/","content":"\n## Basic\n\n1. 在今天软件定义汽车和汽车智能化、网联化的发展趋势下，原来基于 ECU 的分布式 EEA 也日益暴露诸多问题和挑战。为了解决分布式 EEA 的这些问题，人们开始逐渐把很多功能相似、分离的 ECU 功能集成整合到一个比 ECU 性能更强的处理器硬件平台上，这就是汽车“域控制器（Domain Control Unit，DCU）”。\n1. ECU（Electronic Control Unit）电子控制单元，又称“行车电脑”、“车载电脑”等。它和普通的电脑一样,由微控制器（MCU）、存储器（ROM、RAM）、输入/输出接口（I/O）、模数转换器（A/D）以及整形、驱动等大规模集成电路组成。\n1. 经典的五域集中式 EEA ：\n   - 动力域（Power Train）\n   - 底盘域（Chassis）\n   - 车身域（Body/Comfort）\n   - 座舱域（Cockpit/Infotainment）\n   - 自动驾驶域（ADAS）\n1. 在五域集中式架构基础上进一步融合，把原本的动力域、底盘域和车身域融合为整车控制域，从而形成了三域集中式 EEA，也即：\n\n   - 车控域控制器（VDC，Vehicle Domain Controller）\n   - 智能驾驶域控制器（ADC，ADAS\\AD Domain Controller）\n   - 智能座舱域控制器（CDC，Cockpit Domain Controller）\n\n1. 在分布式 ECU 时代，计算和控制的核心是 MCU 芯片，传输的基础核心是基于传统的 CAN、LIN 和 FlexRay 等低速总线。\n1. 在域控制器时代，高性能、高集成度的异构 SoC 芯片作为域的主控处理器，将成为域控制器的计算与控制的核心芯片。而汽车 `TSN（Time-Sensitive Network）`以太网因为具有高带宽、实时和可靠的数据通信能力等特点，必将成为整车通信的核心基础设施，尤其是域主控处理器之间的通信主干网。\n1. 分布式 => （跨）域集中式 => 中央+区域集中式\n\n## Links\n\n1. [国内主机整车 EEA 架构汇总](https://zhuanlan.zhihu.com/p/561699068)\n1. [深度分析汽车芯片的现状与发展（一）汽车电子电气架构的变革](https://zhuanlan.zhihu.com/p/460683077)\n1. [深度分析汽车芯片的现状与发展（二）域控制器时代](https://zhuanlan.zhihu.com/p/461714692)\n1. [深度分析汽车芯片的现状与发展（三）ADAS/AD 域控制器及芯片平台分析](https://zhuanlan.zhihu.com/p/462675111)\n1. [深度分析汽车芯片的现状与发展（四）整车控制域](https://zhuanlan.zhihu.com/p/464930132)\n1. [2022 年自动驾驶域控制器行业概览](https://pdf.dfcfw.com/pdf/H3_AP202205181566386836_1.pdf?1652907375000.pdf)\n"},{"title":"ROS2","url":"/2022/09/06/ROS2/","content":"\n## Basic\n\n1. [ROS2 节点介绍](https://fishros.com/d2lros2foxy/#/chapt3/3.1ROS2%E8%8A%82%E7%82%B9%E4%BB%8B%E7%BB%8D)\n\n## Links\n\n1. [ROS1 与 ROS2、AUTOSAR 与 ROS2 区别](https://www.3cst.cn/Information/info/1uwAF7ySxL8111ea8d6300163e0473d8)\n"},{"title":"CANBUS","url":"/2022/09/06/CANBUS/","content":"\n## Basic\n\n## Links\n\n1. [CAN FD 协议实用指南](https://zhuanlan.zhihu.com/p/178274029)\n1. [CAN FD 总线协议深度解析-简单易懂协议详解](https://blog.csdn.net/u013606261/article/details/112529205)\n1. [DBC 文件：](https://zhuanlan.zhihu.com/p/141638034) DBC 是 Database Can 的缩写，其代表的是 CAN 的数据库文件，在这个文件中把 CAN 通讯的信息定义的非常完整清楚，而 CAN 网络的通讯就是依据这个文件的描述进行的，所以 DBC 文件的作用非常强大，正是因为有了它才可以使得整个 CAN 网路的节点控制器无差错的协同同步开发。\n1. [CANoe 软件介绍](https://zhuanlan.zhihu.com/p/343635976)\n"},{"title":"IDL","url":"/2022/09/06/IDL/","content":"\n## Basic\n\n1. [接口描述语言（Interface description language，缩写 IDL）](https://zh.m.wikipedia.org/zh-hans/%E6%8E%A5%E5%8F%A3%E6%8F%8F%E8%BF%B0%E8%AF%AD%E8%A8%80)\n1. 是用来描述软件组件介面的一种计算机语言。IDL 通过一种独立于编程语言的方式来描述接口，使得在不同平台上运行的对象和用不同语言编写的程序可以相互通信交流；比如，一个组件用 C++写成，另一个组件用 Java 写成。\n   IDL 通常用于远程调用软件。在这种情况下，一般是由远程客户终端调用不同操作系统上的对象组件，并且这些对象组件可能是由不同计算机语言编写的。IDL 建立起了两个不同操作系统间通信的桥梁。\n1. [idl 开发示意图](https://p1.itc.cn/q_70/images03/20220217/2bd3a649a51c4cd08a37ac5c0e864e97.png)\n"},{"title":"消息中间件","url":"/2022/09/05/消息中间件/","content":"\n## 中间件\n\n1. [自动驾驶汽车的软件连接解决方案](https://www.rti.com/rti-blog-cn/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E6%B1%BD%E8%BD%A6%E7%9A%84%E8%BD%AF%E4%BB%B6%E8%BF%9E%E6%8E%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88)\n   - 第一种选择是利用为大规模系统开发的云和企业技术\n   - 第二种选择是从头开始构建解决方案。\n   - 第三种方法是选择一种新的汽车平台，例如 AUTOSAR Adaptive、ROS 2、Apollo 等，并围绕它们构建系统。\n   - 第四种方法是使用基于标准的关键技术中已证明的基础技术。上面提到的几种汽车平台都使用 Data Distribution Service™（DDS）标准作为其核心。\n1. 中间件——一种应用于分布式系统的基础软件，自上世纪 80 年代诞生以来，在分布式环境中低调地发挥着重要作用。基于中间件，系统软件与应用软件之间实现了高效连接与沟通，应用开发得以提速。\n1. 消息中间件和计算中间件\n\n## Basic\n\n1. RPC 和消息中间件区别？ [link](https://blog.csdn.net/u012562943/article/details/98586854)\n   - 主要的区别就是消息队列适用于异步场景，而 rpc 是远程同步调用 [[消息队列 RPC 之间的区别与联系](https://www.cnblogs.com/lizhensheng/p/11117407.html)]\n   - rpc 没有 broker, 而消息队列是需要管理消息的存储的，rpc 没有存储，只有通信。\n   - 不管是消息队列还是 rpc 调用都是 分布式下面的 通信方式。\n   - 消息队列最容易理解的方式就是生产者消费者模式，使两个应用解耦。mq 等框架就是对这的具体实现。\n   - MQ 是生产者消费者模式。\n   - RPC 是请求响应模式。\n   - MQ 是面向数据的。\n   - RPC 是面向动作的。\n1. Broker: 消息服务器，作为 server 提供消息核心服务\n1. Producer: 消息生产者，业务的发起方，负责生产消息传输给 broker，\n1. Consumer: 消息消费者，业务的处理方，负责从 broker 获取消息并进行业务逻辑处理\n1. Topic: 主题，发布订阅模式下的消息统一汇集地，不同生产者向 topic 发送消息，由 MQ 服务器分发到不同的订阅者，实现消息的广播\n1. Queue: 队列，PTP 模式下，特定生产者向特定 queue 发送消息，消费者订阅特定的 queue 完成指定消息的接收\n1. Message: 消息体，根据不同通信协议定义的固定格式进行编码的数据包，来封装业务数据，实现消息的传输\n\n## baidu cyber RT\n\n1. [Apollo6.0 学习 002：Cyber RT 框架](https://zhuanlan.zhihu.com/p/397607823)\n\n## ZeroMQ\n\n1. 号称最快的消息队列系统，专门为高吞吐量/低延迟的场景开发，在金融界的应用中经常使用，偏重于实时数据通信场景。ZMQ 能够实现 RabbitMQ 不擅长的高级/复杂的队列，但是开发人员需要自己组合多种技术框架，开发成本高。\n1. 无锁的队列模型：对于跨线程间的交互（用户端和 session）之间的数据交换通道 pipe，采用无锁的队列算法 CAS；在 pipe 的两端注册有异步事件，在读或者写消息到 pipe 的时，会自动触发读写事件。\n1. [无锁队列的实现](https://blog.csdn.net/armlinuxww/article/details/103988198)\n\n## Links\n\n1. [中间件](https://zh.wikipedia.org/wiki/%E4%B8%AD%E9%97%B4%E4%BB%B6)\n1. [中间件是什么,常用的中间件有哪些](https://blog.csdn.net/weixin_40210977/article/details/109729464)\n1. [消息队列](https://www.jianshu.com/p/79ca08116d57)\n1. [消息中间件（一）MQ 详解及四大 MQ 比较](https://www.cnblogs.com/insane-Mr-Li/p/10684536.html)\n"},{"title":"DDS","url":"/2022/09/05/DDS/","content":"\n## Basic\n\n### what\n\n1. [what-is-dds](https://www.dds-foundation.org/what-is-dds-3/)\n1. `DDS(Data Distribution Service,数据分发服务)` 是一种以数据为中心的通信协议，用于分布式软件应用程序通信。\n1. RTPS 是 Real-Time Publish Subscribe 的缩写\n1. [分布式实时通信——DDS 技术简洁](https://zhuanlan.zhihu.com/p/192981171)\n1. ![示意图](https://pic1.zhimg.com/80/v2-ff84b83ecac6b84f96d3631827080188_720w.jpg)\n1. [简介 2](https://www.zhihu.com/question/56652257)\n   ![DDS模型架构图](https://picx.zhimg.com/80/v2-daee8186bd378ce224b0f032a735f6c6_720w.jpg?source=1940ef5c)\n\n### why\n\n1. [why-choose-dds](https://www.dds-foundation.org/why-choose-dds/)\n1. [dds 与 some/ip 及 opendds 与 fastdds 区别](https://icode.best/i/86439147785337)\n1. dds, mq , rpc 区别：\n   - dds： 事件总线模式， mq: 代理模式？ [事件总线（发布订阅模式）](https://juejin.cn/post/6949929918138417165)\n   - dds 是（主要是）基于组播的“无中心”的端到端发布订阅，kafka 等主要是基于代理节点的发布订阅，二者在体系结构上差异明显\n   - 抽象程度不一样。DDS 的很多高级功能如果在 ZMQ 基础上增加成本很高。 ZMQ 更底层\n1. [why-would-anyone-use-dds-over-zeromq](https://www.rti.com/blog/2017/04/20/why-would-anyone-use-dds-over-zeromq)\n   - In DDS you just publish/subscribe to topic names. You never have to provide IP addresses, computer names or ports. It is all handled by the built-in discovery. And it does it automatically without additional services. This means that applications can be re-deployed and integrated without recompilation or reconfiguration.\n   - In comparison, ZeroMQ is lower level. You must specify ports, IP addresses, etc.\n1. [IIoT Solutions | 6 Industrial IoT Communication Solutions](https://www.rti.com/blog/industrial-iot-communication-solutions-which-ones-for-you-comparison)\n1. [比较图表](https://info.rti.com/hubfs/blog-files/iiot_comsol_pngaspdf.pdf)\n1. ![Architectural diagrams for the various IoT protocols.](https://marvel-b1-cdn.bc0a.com/f00000000264121/www.rti.com/hs-fs/hubfs/Assets%202017/communicationsevolution.jpg?width=720&height=405&name=communicationsevolution.jpg)\n\n### how\n\n1. [how-dds-works](https://www.dds-foundation.org/how-dds-works/)\n\n### others\n\n1. [FastDDS、OpenDDS、ConnextDDS 三者之间是能够实现通信的](https://blog.csdn.net/Tom942067059/article/details/113382934)\n\n## Fast-DDS\n\n1. DCPS(以数据为中心的发布订阅模型)\n\n### sample\n\n```sh\n% docker pull iceoryx/fastdds\n% docker run -id --name fastdds iceoryx/fastdds\n% docker exec -it fastdds bash\n% tmux new-session \"./HelloWorldExample publisher 0 1000\" \\; \\\nsplit-window \"./HelloWorldExample subscriber\" \\; \\\nselect-layout even-vertical\n\n```\n\n## Links\n\n1. [通信中间件 Fast DDS 基础概念简述与通信示例](https://blog.csdn.net/briblue/article/details/124081170)\n"},{"title":"VSCode","url":"/2022/09/05/VSCode/","content":"\n## 手动安装插件\n1. 下载vsix插件\n2. code --install-extension vscode-lldb-1.10.0.vsix\n\n\n## vscode 性能\n1. `Developer: Startup Performance`\n1. \n\n## problems\n1. 关闭clangd各种烦人warnings\n   - `\"problems.visibility\": false,`\n\n## [基本操作](https://code.visualstudio.com/docs/editor/codebasics)\n1. `C-M-up`选择多行\n1. `M-S + cursor`按住 M-S, 鼠标选择列\n1. `M-S-n/p`像上下选择\n1. `M-S-l/r`像左右选择\n1. `M-;` shrink选择\n1. [在选中的文本中搜索](https://code.visualstudio.com/docs/editor/codebasics#_find-in-selection)\n1. `C-tab`跳转到之前的窗口\n1. `\"workbench.editor.wrapTabs\": true,` 展开tab\n1. `\"workbench.editor.enablePreview\": false,` 打开文件不覆盖\n\n## 配色\n1. Inspect Editor Tokens and scopes查看颜色所属域, 配合起来\n1. setting\n\n``` json\n\"editor.tokenColorCustomizations\": {\n  \"textMateRules\": [\n    {\n      \"scope\": \"comment\",\n      \"settings\": {\n        \"foreground\": \"#6A9955\" // 修改注释颜色\n      }\n    },\n    {\n      \"scope\": \"keyword\",\n      \"settings\": {\n        \"foreground\": \"#569CD6\" // 修改关键字颜色\n      }\n    },\n  ]\n}\n```\n\n## gitlens\n1. 窗口可以detach\n\n## [terminal](https://code.visualstudio.com/docs/terminal/getting-started)\n1. `C-g` go to recent directory\n1. (good)可以拖拽到editor, 可以调试代码\n1. `alias open=/data/xiyang/.cursor-server/code` open file\n\n## copilot\n1. copilot generate this: 在代码处写个注释，生成代码\n1. 自动生成commit message\n\n## select\n1. shift + ->\n\n## Links\n\n1. [Bito](https://marketplace.visualstudio.com/items?itemName=Bito.Bito)\n1. [codegpt.co](https://codegpt.co/)\n1. [ai 助手](https://marketplace.visualstudio.com/items?itemName=Blackboxapp.blackbox)\n1. (good)[inlay-hints](https://ash12rai-weblearning.medium.com/visual-studio-code-vs-code-inlay-hints-32fee837e80)\n   - font 从 windows 下查找\n   - font: Lucida Calligraphy, Lucida Bright\n   - toggle: `alt+'`\n1. [从 VSCode 看大型 IDE 技术架构 j](https://zhuanlan.zhihu.com/p/96041706)\n1. [vscode proxy setting](https://device.harmonyos.com/cn/docs/documentation/guide/vscode_proxy-0000001074231144)\n1. [c++ Call Hierarchy](https://devblogs.microsoft.com/cppblog/c-extension-in-vs-code-1-16-release-call-hierarchy-more/)\n\n## [sync config](https://code.visualstudio.com/docs/editor/settings-sync)\n\n1. 不好用\n\n## task\n\n1. `Task: configure task` 设置 task 选择 other task\n1. `Task: run task`\n\n## 自动跳转配置\n\n1. `M-x C/C++ Edit configurations(JSON)` 打开工程配置 c_cpp_properties.json\n1. [c_cpp_properties.json reference](https://code.visualstudio.com/docs/cpp/c-cpp-properties-schema-reference)\n   - \"compileCommands\": \"/path/to/compile_commands.json\", 设置 compile_commands.json， 方便代码补全\n   - compile_commands.json 软连接到工程目录下\n1. 关闭 cmake tools 自动 build 功能\n\n## format\n\n1. `format toggle`\n1. `M-S-f`\n1. 自动 format `C-,` 打开设置，搜索 format, 选中 format on save\n1. `File: Save withourt Formatting` 命令\n\n## base\n1. `C_Cpp: Dim Inactive Regions` C++宏定义预处理颜色问题\n1. `\"workbench.editor.enablePreview\": true` 禁用预览模式，直接打开文件而不是在预览中查看, 双击打开\n1. opened editor\n1. fontsize不同部分可以不一样, 如可以单独设置terminal字体大小\n1. `\"terminal.integrated.defaultProfile.linux\": \"zsh\",` 默认为zsh\n1. (good)写代码时用bookmark来打开相关文件\n1. `\"editor.quickSuggestionsDelay\": 5000,` 设置延迟，防止与copilot有冲突\n1. `\"git.untrackedChanges\": \"separate\"`\n   - view as list/tree\n1. copy ternminal输出\n   - shift + right click： select all + copy \n   - Terminal: copy last command and output\n1. [mlir高亮设置](https://blog.csdn.net/comway_Li/article/details/121387049) 需要先安装mlir插件\n   - change language mode可以选择mode\n1. (good)`fold all` and `unfold all`\n1. `Open Log File` 可以看各种 log, window 可以看经常断开原因\n1. `Terminal: Select All`复制到文件，保存 log\n1. `watch`可能引起 debug 崩溃 [link](https://www.cnblogs.com/Baiyug/p/16243436.html)\n1. `ps uxa | grep .vscode-server | awk '{print $2}' | xargs kill -9` kill vscode server\n1. debug console 中可以输入命令：`-exec info registers`\n1. 鼠标左键是 expand select\n1. `ctrl+g`多用\n1. `dev contatiner show all logs`\n1. `output`窗口可以选择看各种 log\n1. `\"stopAtEntry\": false` c++ 调试默认会有个断点，但不会开启（launch.json)\n1. `Debug Visualizer`可以可视化常用数据结构，便于调试\n1. debug 可以使用 `Hex Editor` 查看内存, 点击\n1. `remote-containers.openDevContainerFile` 打开配置设置 `\"remoteUser\": \"xiyang\"` docker 可以使用 xiyang 用户登录, 每个 container 有一个配置\n1. dev-container 可以访问远程机器上的 docker，不用开端口\n1. windows 配置路径在当前用户.vscode 下面 例如:C:\\Users\\86158\\.vscode (extensions) 注意和 C:\\Users\\86158\\AppData\\Roaming\\Code\\User(settings) 数据做区分\n1. `toggleDevTools` 用来调试插件\n1. 注意配置文件区分:\n   - host: `keybindings.json settings.json`\n   - target: `.vscode-server/data/User/extensions.json`\n   - target project: `launch.json settings.json task.json c_cpp_properties.json`\n1. 右侧放 baseline， 代码导航下面\n1. `alt`有时候会误触，按 esc 或 alt\n1. 使用 vim 插件，使用 vim 快捷键 如： `*` 把当前的字段作为查找内容，找下一处。可以配合 `n` or `N`使用\n1. zen mode 模式下一遍写代码， 一边画图， `M-l or M-h`切换\n1. [zen mode 显示 tabs](https://stackoverflow.com/a/49196147) 可以设置 zen mode 显示\n\n## [short keys](https://code.visualstudio.com/docs/getstarted/keybindings)\n1. `C-c` or `M-w` 直接复制当前行，无需选中\n1. `C-M-w` copy当前单词\n1. `C-M-h` toggle hover\n1. `C-S-\\` Go to branket 跳转到配对符 M-r\n1. 可以打开默认shortkey配置\n1. 可以像emacs一样设置key\n``` json\n  {\n    \"key\": \"ctrl+l g\",\n    \"command\": \"-extension.openInGitHub\"\n  },\n```\n1. (good)`C-uparrow` terminal下找到上条命令 `C-home` 到其实地方\n1. `rename symbol`文件中字符串替换\n1. `M-e` go to next problem (build with task)\n1. [basic editing](https://code.visualstudio.com/docs/editor/codebasics)\n1. [cheetsheat](https://code.visualstudio.com/shortcuts/keyboard-shortcuts-windows.pdf)\n1. [open-symbol-by-name](https://code.visualstudio.com/docs/editor/editingevolved#_open-symbol-by-name)\n1. `Navigate Edits History - Goto Last Edit`安装插件， 设置 loop true, `move to previous edit` -> `, g g`\n1. `Terminal: run recent command` 执行最近 command 的命令 或者安装插件`Rerun last command in termnalxs`\n1. `Terminal: go to recent directory` 进入最近使用的文件夹\n1. `S-M downarrow` vim 模式下对列操作\n1. `C-space` 补全的时候用, 能显示更多信息，如 c++能显示文件路径, 也可以直接按\n1. `Developer: Toggle Screencast Mode` 对应 emacs `C-h k` 显示键对应的命令\n1. 最左侧显示 `\"workbench.activityBar.visible\": true`\n1. `view: toggle menu bar / status bar / activate bar / breadcrumbs`\n1. `Terminal: focuse find` C-f 快捷键删除，想用的时候使用命令\n1. 罗技鼠标 master 可以前进后退\n1. 多用命令\n1. File > Preferences > Keyboard Shortcuts 查看所有快捷键\n1. 可以点击测试 key 绑定到什么命令上\n1. 可以复制键设置到配置文件，然后修改键绑定\n1. `alt`给禁止了 设置为 emacs key [link](https://stackoverflow.com/a/65163917)\n1. `C-p ?`查看简写帮助 like emacs `C-h h`\n1. `C-k z` zen mode 非常好用\n1. `C-1` 聚焦到 editor\n1. `C-S o` + `C n/p` goto symbol in editor 可用于跳转函数, 很好用\n1. `Ctrl+T` goto symbol in workspace\n1. `Rename symbol`\n1. `C-,` 打开设置\n1. `M-x open settings (json)` 打开配置文件\n1. `C-b` toggle left side bar\n1. `C-j` toggle down side bar\n1. C-\\` toggle termianl\n1. C-S-\\` new terminal\n1. `C-w` 关闭 tab\n1. `C-+` 扩大字体 `view room in`\n1. `C--` 减小字体 `view room out`\n1. `M-x editor font size in` 只放大 editor size(emacs buffer)\n1. `F12` goto-defination 改为 `M-.`\n1. `M-leftarrow` go-back 改为 `M-,`\n1. vim `C-o and C-i` 前后跳转\n1. `C-pagedown` next tab/terminal 改为 `M-l`\n1. `C-pageup` previous tab/terminal 改为 `M-h`\n1. `M-1` first tab\n1. `C-p` find file 删除`C-e`\n1. `C-S-p` 命令搜索\n1. Esc 取消\n1. F8 next error\n1. `C-S-y` debug console window\n1. `C-S-u` output window\n1. `C-S-m` problems window\n1. `Focus on opened editors` 设置到 `M-b`, 配合`C-S-e`使用\n1. `show all editors by appearance by recent use` 设置到 `M-b`, emacs `,bb`\n1. `View: Focus Next Editor Group` -> `alt+o`\n1. `View: duplicate edior right` -> `alt-3`\n1. `View: duplicate edior down` -> `alt-2`\n1. `go to next change` git 修改 next\n1. `C-S-t` reopen closed editor\n1. `M-p toggle editor v/h layerout` 改变横向纵向分割\n1. `reload window`\n1. `设置alt-n导航` [link](https://stackoverflow.com/a/58082645)\n1. [emacs 快捷键设置](https://gist.github.com/cschleiden/4dbc22a5711f069199a0cc730ac42cb5)\n1. [快速切换 window](https://stackoverflow.com/a/54403042)\n1. `M-S-c` copy file path\n1. `Alt + Click`：添加新的光标, 多行操作\n1. `Alt + Shift + Click`：多行操作\n1. `M-S p` project manager: list project to open 配合 `C-M q` quick swich windwos\n\n## 免密\n\n1. 公钥放到服务器配置文件中\n1. `IdentityFile \"C:\\Users\\xxx/xxx\\id_rsa\"` config 配置里添加私钥位置\n1. `chmod 400 ~/.ssh/id_rsa` Permissions are too open\n1. wsl 里权限有问题，可以把 id_rsa 拷贝出来，换个指定位置。\n1. 将本地`~/.ssh/id_rsa.pub`复制到`~/.ssh/authorized_keys`中\n\n## debug\n\n\n1. power mode\n\n``` json\n    \"powermode.enabled\": true,\n    \"powermode.combo.counterEnabled\": \"hide\",\n    \"powermode.combo.timerEnabled\": \"hide\",\n    \"powermode.combo.counterSize\": 0,\n    \"powermode.shake.enabled\": false,\n    \"powermode.explosions.size\": 1,\n    \"powermode.presets\": \"exploding-rift\",\n    \"powermode.explosions.offset\": 0.6,\n    \"powermode.explosions.maxExplosions\": 3,\n```\n\n1. [docs/editor/debugging](https://code.visualstudio.com/docs/editor/debugging) 非常好\n1. conditional breakpoints\n   - expression\n   - hit count\n1. logpoints\n\n## plugins\n1. [comment translate](https://marketplace.visualstudio.com/items?itemName=intellsmi.comment-translate)\n   - 配置成按ctrol才翻译\n1. [output-colorizer](https://marketplace.visualstudio.com/items?itemName=IBM.output-colorizer) log color\n1. [git graph](https://marketplace.visualstudio.com/items?itemName=mhutchie.git-graph)\n1. [clangd](https://marketplace.visualstudio.com/items?itemName=llvm-vs-code-extensions.vscode-clangd)\n   - 关闭hover\n   - 需要关闭remote，设置到remote settings才有效\n1. [terminal-keeper](https://marketplace.visualstudio.com/items?itemName=nguyenngoclong.terminal-keeper)\n   - 在repo里的.vscode/session.json里配置\n   - 可以执行一些配置\n1. [python c++ debuger](https://marketplace.visualstudio.com/items?itemName=benjamin-simmonds.pythoncpp-debug)\n   - [gdb需要sudo权限](https://stackoverflow.com/a/69296878)\n   - `echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope`\n   - lauch.json配置可选 python c++ debugger custom/default\n   - [c++ 默认配置](https://github.com/benibenj/vscode-pythonCpp/blob/3cb6c95181d4ed1ea96a919b29ca5a1f8390ab2f/src/pythonCppDebug.ts#L196)\n   - [python默认配置](https://github.com/benibenj/vscode-pythonCpp/blob/3cb6c95181d4ed1ea96a919b29ca5a1f8390ab2f/src/pythonCppDebug.ts#L149)\n   - [gdb llvm mlir pretty print](https://lists.llvm.org/pipermail/llvm-dev/2016-May/100365.html)\n   - [codelldb](https://marketplace.visualstudio.com/items?itemName=vadimcn.vscode-lldb) 调试加速\n\n   ``` json\n        {\n            \"type\": \"lldb\",\n            \"request\": \"attach\",\n            \"name\": \"lldb\",\n            \"program\": \"/data/xiyang/miniconda3/envs/siorigin_triton_test/bin/python\"\n        }\n   ```\n\n1. [taskexplore](https://marketplace.visualstudio.com/items?itemName=spmeesseman.vscode-taskexplorer)\n   - 拖拽到左边框\n   - 可以执行last task\n1. [vscode-pydata-viewer](https://marketplace.visualstudio.com/items?itemName=Percy.vscode-pydata-viewer) 查看 pkl, pth 文件\n1. [call-graph](https://marketplace.visualstudio.com/items?itemName=LuoZhihao.call-graph)\n   - 生成函数调用关系图\n1. (good)[inlay-hints](https://ash12rai-weblearning.medium.com/visual-studio-code-vs-code-inlay-hints-32fee837e80)\n   - 需要 install toggle 插件\n1. [vscode-netron](https://marketplace.visualstudio.com/items?itemName=vincent-templier.vscode-netron)\n1. [intellicode-api-usage-examples](https://marketplace.visualstudio.com/items?itemName=VisualStudioExptTeam.intellicode-api-usage-examples) 帮助文档里有 sample\n1. [Vscode 离线安装插件的方法](https://www.cnblogs.com/hwy6/p/15930217.html)\n1. [JSON Crack json 可视化](https://marketplace.visualstudio.com/items?itemName=AykutSarac.jsoncrack-vscode)\n1. jupyter 不用安装图形界面就能显示\n   - [教程](https://code.visualstudio.com/docs/datascience/jupyter-notebooks)\n   - C-Enter 运行 code\n   - C-M-S-Enter 进入 cell debug 模式\n1. [toggle 设置](https://marketplace.visualstudio.com/items?itemName=rebornix.toggle)\n1. [dabbrev-expand](https://marketplace.visualstudio.com/items?itemName=kiwanamie.dabbrev-expand) alt+/\n1. [format-toggle](https://marketplace.visualstudio.com/items?itemName=tombonnike.vscode-status-bar-format-toggle) status bar 右下角有 status, 可以点击切换状态\n1. [toogle shortcut settings](https://marketplace.visualstudio.com/items?itemName=hoovercj.vscode-settings-cycler)\n1. [line-num-toggle](https://marketplace.visualstudio.com/items?itemName=jfacoustic.line-num-toggle)\n1. [doxdocgen](https://marketplace.visualstudio.com/items?itemName=cschlosser.doxdocgen)\n1. [Bookmarks](https://marketplace.visualstudio.com/items?itemName=alefragnani.Bookmarks)\n1. [python-extension-pack](https://marketplace.visualstudio.com/items?itemName=donjayamanne.python-extension-pack)\n1. [vscode-pylance lsp server 好用， 智能助手](https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-pylance)\n1. vscode 里选 most popular\n1. Laravel goto view ctrl 点击跳转, 配合 mater3 鼠标后退键，很高效\n1. [marketplace](https://marketplace.visualstudio.com/vscode)\n1. [StatusBar Debugger](https://marketplace.visualstudio.com/items?itemName=fabiospampinato.vscode-statusbar-debugger)\n   - 关闭 floating debug toolbar: setting: debug -> tool bar location -> hiden\n1. magit\n1. sourcegraph\n1. shell-format\n1. Prettier format\n1. cmake format\n1. [hungry-delete](https://marketplace.visualstudio.com/items?itemName=jasonlhy.hungry-delete)\n1. [vscode-debug-visualizer](https://github.com/hediet/vscode-debug-visualizer)\n1. project manager (status bar 有图标， 可用于切换)\n   - C-M-q\n1. gitlens(不负责提交 commit)\n   - 关闭hover, 不然显示太多东西   \n   - 关闭自动显示配置\n   - 编辑右上角有 toggle changes/blame/heatmaps 图标\n   - 设置 git diff tool [link](https://code.visualstudio.com/docs/sourcecontrol/overview#_vs-code-as-git-difftool-and-mergetool)\n   - `C-w`关闭 diff\n   - `open remote` 可以打开远程 commit branch file\n   - status bar 可以选择 zen or review mode， review mode 能显示多种信息; 可以显示关闭提交历史 `\"gitlens.mode.active\": \"zen\",`\n   - `GitLens:Toggle Zen Mode`\n1. vscode-icons\n1. [Duplicate selection or line](https://marketplace.visualstudio.com/items?itemName=geeebe.duplicate)\n1. [duplicate file](https://marketplace.visualstudio.com/items?itemName=mrmlnc.vscode-duplicate)\n1. [vscode-filesize](https://marketplace.visualstudio.com/items?itemName=mkxml.vscode-filesize)\n1. wsl\n1. json\n1. code runner\n1. Material Icon Theme\n1. remote ssh\n1. C/C++ Extension Pack\n1. Better C++ Syntax\n1. C/C++ Compile Run\n1. Path Intellisense\n1. Remote Development\n1. Rainbow Brackets\n1. [highlight](https://github.com/debugpig/vscode-extension-highlight) 选择高亮字符, 配合 vim `*` `#` 使用\n   - [plugin](https://marketplace.visualstudio.com/items?itemName=debugpig.highlight)\n1. [colorize](https://marketplace.visualstudio.com/items?itemName=kamikillerto.vscode-colorize) 显示值颜色\n1. [highlight-words](https://marketplace.visualstudio.com/items?itemName=rsbondi.highlight-words)\n1. todo highlight\n1. [vscode-highlight](https://github.com/fabiospampinato/vscode-highlight) 直接高亮字符\n1. Svg Preview\n1. better comment\n1. next console\n1. gdb-syntax\n1. excel view 查看 csv 文件\n1. doxygen document 生成 doxygen 注释\n1. Manpages\n1. [dimmer](https://marketplace.visualstudio.com/items?itemName=imagio.vscode-dimmer-block)\n1. [back 图标](https://marketplace.visualstudio.com/items?itemName=nick-rudenko.back-n-forth)\n1. [path-intellisense](https://marketplace.visualstudio.com/items?itemName=christian-kohler.path-intellisense)\n1. [stats-bar: show cpu and mem usage](https://marketplace.visualstudio.com/items?itemName=njzy.stats-bar)\n1. [vscode-fileutils](https://marketplace.visualstudio.com/items?itemName=sleistner.vscode-fileutils)\n1. [vscode 中打开网页，用于调试 server](https://marketplace.visualstudio.com/items?itemName=antfu.browse-lite)\n1. [rainbow-csv](https://marketplace.visualstudio.com/items?itemName=mechatroner.rainbow-csv)\n1. [excelviewer](https://marketplace.visualstudio.com/items?itemName=GrapeCity.gc-excelviewer)\n1. [csv-to-table](https://marketplace.visualstudio.com/items?itemName=phplasma.csv-to-table)\n1. [path-autocomplete](https://marketplace.visualstudio.com/items?itemName=ionutvmi.path-autocomplete)\n\n## [vim](https://github.com/VSCodeVim/Vim)\n1. `f` + 字符， 快速跳转\n1. [设置 learder key](https://cnodejs.org/topic/5b9e696037a6965f59051a09)\n1. `toggleDevTools` 用来调试插件 vim loglevel 设置为 debug\n1. 打开 easymotion\n1. 修复 fold [link](https://stackoverflow.com/a/50892839)\n1. 可以设置 search color\n\n## [theme](https://marketplace.visualstudio.com/search?target=VSCode&category=Themes&sortBy=Installs)\n1. [one dark pro](https://marketplace.visualstudio.com/items?itemName=zhuangtongfa.Material-theme)\n1. [vscode-theme-onedark](https://marketplace.visualstudio.com/items?itemName=akamud.vscode-theme-onedark)\n2. pe\n\n## todo\n\n1. fold and unfold tab to toggle fold\n1. show command using key\n1. 对比 emacs 看有哪些好用的功能\n\n## issues\n\n1. [关闭 vscode 烦人的提示框](https://codeantenna.com/a/ZOXgWzApJO)\n   - hover\n\n## python\n\n1. 执行命令前source file(用task更好, 不影响其他程序执行)\n\n``` json settings.json\n\"terminal.integrated.profiles.linux\": {\n    \"bash_with_source\": {\n        \"path\": \"/bin/bash\",\n        \"args\": [\"--rcfile\", \"/path/to/your/script.sh\"]\n    }\n},\n\"terminal.integrated.defaultProfile.linux\": \"bash_with_source\"\n```\n\n1. ctrl 显示下划线，单击会跳转\n\n1. [不跳转问题修复](https://blog.csdn.net/singxsy/article/details/123730161)\n1. lsp 使用 pylance\n1. `\"justMyCode\":False` 可以跳转到三方库源代码\n1. set python3 path: Python: Select Interpreter\n1. config: \"cwd\": \"${fileDirname}\"\n\n```json\n{\n  // Use IntelliSense to learn about possible attributes.\n  // Hover to view descriptions of existing attributes.\n  // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"name\": \"Python: Current File\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"program\": \"${file}\",\n      // \"cwd\": \"${fileDirname}\",\n      \"cwd\": \"${fileDirname}\",\n      \"console\": \"integratedTerminal\",\n      \"justMyCode\": true # 是否跳转到其他库\n    }\n  ]\n}\n```\n\n## task and launch\n1. task\n\n``` json\n{\n    \"version\": \"2.0.0\",\n    \"tasks\": [\n        {\n            \"label\": \"Run script\",\n            \"type\": \"shell\",\n            \"command\": \"source test.sh\",\n            \"problemMatcher\": [],\n            \"group\": {\n                \"kind\": \"build\",\n                \"isDefault\": true\n            }\n        }, \n        {\n            \"label\": \"build siorigin triton\",\n            \"type\": \"shell\",\n            \"command\": \"cd /data/xiyang/siorigin_triton && bash build.sh\",\n            // \"problemMatcher\": [],\n            \"group\": {\n                \"kind\": \"build\",\n                \"isDefault\": true\n            },\n            \"problemMatcher\": \"$gcc\", // 使用内置的 GCC Problem Matcher\n            \"presentation\": {\n                \"clear\": true, //每次输出清理log\n                // \"reveal\": \"always\",\n                // \"panel\": \"shared\"\n            },\n        }\n    ]\n}\n```\n2. launch添加 \"preLaunchTask\" 配置\n\n``` json\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Run C++ Debug\",\n            \"type\": \"cppdbg\",\n            \"request\": \"launch\",\n            \"program\": \"${workspaceFolder}/your_program\", // 替换为你的可执行文件路径\n            \"args\": [],\n            \"cwd\": \"${workspaceFolder}\",\n            \"preLaunchTask\": \"Run script\", // 引用 tasks.json 中的任务\n            \"environment\": [],\n            \"externalConsole\": false,\n            \"stopAtEntry\": false,\n            \"MIMode\": \"gdb\", // 或者 lldb，取决于你的调试器\n            \"miDebuggerPath\": \"/usr/bin/gdb\", // 根据你的系统调整路径\n            \"setupCommands\": [\n                {\n                    \"description\": \"Enable pretty-printing for gdb\",\n                    \"text\": \"-enable-pretty-printing\",\n                    \"ignoreFailures\": true\n                }\n            ]\n        }\n    ]\n}\n\n```\n\n1. 改launch.json `\"terminal\": \"console\",` 在debug console输出\n"},{"title":"Jenkins","url":"/2022/08/19/Jenkins/","content":"\n## Jenkins\n\n1. [Doc](https://www.jenkins.io/doc/)\n1.\n\n## install\n\n```sh\n% wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add -\n% sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list'\n% sudo apt-get update\n% sudo apt-get install jenkins\n% sudo vi /etc/default/jenkins ## change port\n% sudo /etc/init.d/jenkins start\n## http://localhost:port\n```\n\n## Plugins\n\n1. [插件下载量排名](https://plugins.jenkins.io/ui/search?sort=installed&categories=&labels=&view=List&page=1&query=)\n1. 在 Jenkins 界面，系统管理->管理插件->安装 SSH Plugin [ssh](https://plugins.jenkins.io/ssh/) [link](https://fanlychie.github.io/post/jenkins-remote-ssh.html)\n1. [gitlab-plugin](https://plugins.jenkins.io/gitlab-plugin/)\n1. [git parameter](https://plugins.jenkins.io/git-parameter/)\n1. [build timeout](https://plugins.jenkins.io/build-timeout/)\n1. [jobConfigHistory](https://plugins.jenkins.io/jobConfigHistory/)\n1. [rebuild](https://plugins.jenkins.io/rebuild/)\n1. [material-theme](https://plugins.jenkins.io/material-theme/) Indigo is good\n1. [ansicolor](https://plugins.jenkins.io/ansicolor/)\n1. [display-console-output](https://plugins.jenkins.io/display-console-output/)\n\n## basic\n\n1. [jenkins 状态反馈给 gitlab](https://www.cnblogs.com/mingerlcm/p/12728873.html)\n1. [gitlab hook](https://cloud.tencent.com/developer/article/1599050)\n   - 使用 webhook, webhook 与异步编程中\"订阅-发布模型\"非常类似，一端触发事件，一端监听执行。\n     ![数据更新后通过 webhook主动推送](https://segmentfault.com/img/remote/1460000020249992?w=1000&h=690)\n1. 匿名用户查看权限 Manager Jenkins ---> Configure Global Security ---> 授权策略\n1. 修改记录里有 git commit 历史\n1. 可以看工作空间\n1. log rotation Job Configuration -->Discard Old Builds:(丢弃旧的构建), job 描述正下方\n1. Add timestamps to the Console Output\n1. 归档时记录所有归档成品的指纹\n1. 工作区 job 目录下的 config.xml 文件比较重要，可以 git 管理起来，能恢复配置\n1. 下载.war 包 https://jenkins.io/download/\n1. 使用命令： java -jar jenkins.war\n1. 参考文档： https://jenkins.io/doc/book/\n1. 远程部署：安装插件 [Publish Over FTP Plugin](https://wiki.jenkins-ci.org/display/JENKINS/Publish+Over+FTP+Plugin) publish over ssh FTP 用于传文件 SSH 用于在远程机器上执行命令\n1. 定时构建： https://www.cnblogs.com/caoj/p/7815820.html\n1. 存档： https://blog.csdn.net/liqiangeastsun/article/details/79062806 文件位置 : ./jobs/test/builds/57/archive ， deephi\\*\n1. 文件命名：日期 + git 号 。gitnum=`git log --pretty=oneline -1 | cut -b 1-8` date*temp=`date +%m%d` mv app.c app*${date_temp}_${gitnum}.c\n1. 文件 md5 值： http://ju.outofmemory.cn/entry/117335\n1. 邮箱：管理员地址 https://blog.csdn.net/songjiaping/article/details/51496977 126 开启 smtp https://www.cnblogs.com/yangxia-test/p/4366172.html https://www.jianshu.com/p/2afb099f2a79 http://skyfar666.iteye.com/blog/1973434\n1. 本地文件管理，源码管理不用 git 仓库，选择 none。就能在其他目录下工作，可以把要保留的文件传回到工作区，保存。\n1. 如何获取远程机器的 log?\n1. Timestamper 控制台输出显示\n1. Workspace Cleanup Plugin\n1. blue ocean 插件\n1. 增加项目描述 https://www.cnblogs.com/chongyou/p/7365351.html\n1. 配置变更记录 https://blog.csdn.net/j3T9Z7H/article/details/78422565\n1. 配置\n\n## Note\n\n1. jenkins ssh 登录找不到 cmake, 因为 jenkins 会从/usr/bin 下找，和登录环境不一样。解决办法：修改客户端 ssh 配置，允许 PermitUserEnvironment [link](https://docs.cloudbees.com/docs/cloudbees-ci-kb/latest/client-and-managed-masters/how-to-set-up-a-jenkins-agent-to-have-the-same-path-as-the-user-when-connected-via-remote-desktop)\n"},{"title":"OpenCV","url":"/2022/08/18/OpenCV/","content":"\n## doc\n\n1. 注意 doc 中函数说明下面有 examples\n1. [Mat](https://docs.opencv.org/4.6.0/d3/d63/classcv_1_1Mat.html)\n\n## Note\n\n1. debug opencv:`OPENCV_LOG_LEVEL=DEBUG`\n1. nv12 type 是 `CV_8UC1， h = real_h * 3 / 2`\n1. opencv 不支持 nv12 resize， 需要 `to rgb -> resize -> to nv12` 或 [libyuv](https://github.com/lemenkov/libyuv) 来 resize\n\n## [Mat](https://docs.opencv.org/4.6.0/d3/d63/classcv_1_1Mat.html)\n\n1. `mata.mul(matb)` 矩阵相乘\n1. `cv::Mat Z_c2 = Z_w * std::cos(phi) + hc * std::sin(phi);` Z_w 为 Mat，直接做运算\n1. [fossies 依赖关系](https://fossies.org/dox/opencv-4.6.0/classcv_1_1Mat.html) 点图右上角能展开图片\n1. (代理模式？) opencv 接口使用代理类 _InputArray，This is the proxy class for passing read-only input arrays into OpenCV functions. \\_InputArray is a class that can be constructed from Mat, Mat_<T>, Matx<T, m, n>, std::vector<T>, std::vector<std::vector<T> >, std::vector<Mat>, std::vector<Mat\\_<T> >, UMat, std::vector<UMat> or double. It can also be constructed from a matrix expression.\n1. [\\_InputArray](https://fossies.org/dox/opencv-4.6.0/classcv_1_1__InputArray.html)\n1. cv::resize 等接口用的都是(InputArray..) `cv::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR)`\n\n### 属性\n\n1. [Opencv Mat 矩阵中 data、size、depth、elemSize、step 等属性的理解](https://blog.csdn.net/dcrmg/article/details/52294259)\n1. `flags`\n1. `dims` Mat 矩阵的维度，若 Mat 是一个二维矩阵，则 dims=2，三维则 dims=3\n1. `rows` Mat 矩阵的行数。\n1. `cols` Mat 矩阵的列数。\n1. `data` uchar 类型的指针，指向 Mat 数据矩阵的首地址。\n1. `size` 首先 size 是一个结构体，定义了 Mat 矩阵内数据的分布形式，数值上有关系式：image.size().width==image.cols; image.size().height==image.rows\n1. `step` 可以理解为 Mat 矩阵中每一行的“步长”，以字节为基本单位，每一行中所有元素的字节总量，是累计了一行中所有元素、所有通道、所有通道的 elemSize1 之后的值；\n1. `channels()`：Mat 矩阵元素拥有的通道数。例如常见的 RGB 彩色图像，channels==3；而灰度图像只有一个灰度分量信息 channels==1。\n1. `type()` Mat 矩阵的类型\n1. `depth()` 用来度量每一个像素中每一个通道的精度，但它本身与图像的通道数无关！depth 数值越大，精度越高。Opencv 中，Mat.depth()得到的是一个 0~6 的数字，分别代表不同的位数，对应关系如下：enum{CV_8U=0,CV_8S=1,CV_16U=2,CV_16S=3,CV_32S=4,CV_32F=5,CV_64F=6}\n1. `elemSize()` elem 是 element(元素)的缩写，表示矩阵中每一个元素的数据大小，如果 Mat 中的数据类型是 CV_8UC1，那么 elemSize==1；如果是 CV_8UC3 或 CV_8SC3，那么 elemSize==3；如果是 CV_16UC3 或者 CV_16SC3，那么 elemSize==6；即 elemSize 是以 8 位（一个字节）为一个单位，乘以通道数和 8 位的整数倍；\n1. `total()`: the total number of array elements. `mat data size: total() * elemSize()`\n1. `isContinuous()`数据是否连续\n\n## functions\n\n### 构造\n\n1. Mat array = (Mat\\_<double>(3, 3) << 0, -1, 5, -1, 5, -1, 0, -1, 0);\n\n### imread\n\n1. `Mat cv::imread(const String &filename, int flags = IMREAD_COLOR);`\n1. [flags list](https://fossies.org/dox/opencv-4.6.0/group__imgcodecs__flags.html#ga61d9b0126a3e57d9277ac48327799c80)\n1. IMREAD_COLOR(value is 1): If set, always convert image to the 3 channel BGR color image.\n1. IMREAD_GRAYSCALE(vaule is 0): If set, always convert image to the single channel grayscale image (codec internal conversion).\n\n### imwrite\n\n1. `bool cv::imwrite (const String& filename, InputArray img, const std::vector<int> & params = std::vector<int>())`\n1. `params` Format-specific parameters encoded as pairs (paramId_1, paramValue_1, paramId_2, paramValue_2, ... .) see [cv::ImwriteFlags](https://fossies.org/dox/opencv-4.6.0/group__imgcodecs__flags.html#ga292d81be8d76901bff7988d18d2b42ac)\n\n### resize\n\n1. 不支持 nv12 resize [libyuv](https://github.com/lemenkov/libyuv)来做 yuv resize\n\n### crop\n\n1. 使用 cv::Rect (x, y, w, h)\n\n```c++\ncv::Rect crop_region(250, 10,400, 400);\nauto mat2=mat1(crop_region);\n```\n\n1. [libyuv crop](https://github.com/lemenkov/libyuv/blob/6900494d90ae095d44405cd4cc3f346971fa69c9/include/libyuv/convert.h#L900)\n\n### cvtColor\n\n1. [列表](https://docs.opencv.org/4.6.0/d8/d01/group__imgproc__color__conversions.html)\n1. `cv::cvtColor(src, dst, cv::COLOR_YUV2BGR_YVYU)`\n1. `cv::cvtColor(src, dst, cv::COLOR_BGR2GRAY)`\n1. `cv::cvtColor(src, dst, cv::COLOR_GRAY2BGR)`\n1. `cv::cvtColor(origin_image, image_rgb, cv::COLOR_YUV2RGB_NV12);`\n1. 不支持 yuv 转 rbg, 可以参考[代码](https://github.com/mhomola/AutonomousFlight/blob/422de6ccbbd3ec290a16c8d919fd23d4503ad62f/CV_scripts/opencv-4.x/modules/gapi/test/streaming/gapi_streaming_tests.cpp#L208-L208)\n\n### permute / transpose 通道转换\n\n1. [transposeND](https://docs.opencv.org/4.x/d2/de8/group__core__array.html#gab1b1274b4a563be34cdfa55b8919a4ec)： Transpose for n-dimensional matrices.\n\n### cv::FileStorage 序列化与反序列化\n\n1. `cv2.FileStorage(\"output.yml\", cv2.FILE_STORAGE_WRITE).write(\"mat\", mat)`\n1. test\n\n```c++\n    cv::FileStorage file(\"test.yml\", cv::FileStorage::READ);\n    cv::Mat left_image, right_image;\n    int out_hei, out_wid;\n    cv::Mat Q;\n    float phi, hc, traj_center, traj_wid, traj_near, traj_far;\n    bool affine;\n\n    file[\"left_image\"] >> left_image;\n    file[\"right_image\"] >> right_image;\n    out_hei = (int)file[\"out_hei\"];\n    out_wid = (int)file[\"out_wid\"];\n    file[\"Q\"] >> Q;\n    phi = (float)file[\"phi\"];\n    hc = (float)file[\"hc\"];\n    int int_affine = (int)file[\"affine\"];\n    affine = int_affine == 0 ? false : true;\n```\n\n## links\n\n1. [Mat 常见属性](https://blog.51cto.com/SpaceVision/5071910)\n"},{"title":"Vim","url":"/2022/08/13/Vim/","content":"\n## keybind [link](https://blog.csdn.net/RunBoying/article/details/7695164)\n\n1. [] 调上一个 func\n1. `:e filename` 打开 file\n1. `za` fold and unfold\n1. `zM` fold all\n1. `zO` unfold all\n1. `C-^` 切换 buffer\n1. `c-d` 向下滚屏, 左手单手操作\n1. `c-u` 向上滚屏\n1. `ZZ`: 普通模式下保存并退出\n1. `:e!` 可以不保存文件并重新载入这个文件\n1. `*` 把当前的字段作为查找内容，找下一处。可以配合 `n` or `N`使用\n1. `%` 跳转到相配对的括号\n1. `v %` 选中括号里内容\n1. `^` 跳转至行第一个非空字符\n1. `:set hlsearch` 可以将查找的内容设置成高亮。`:set nohlsearc` 取消高亮或 `/xxxx`\n1. `mm` and \\`m : mark and jump back\n1. `C-o` 命令则跳到一个 \"较老\" 的地方 (提示： O 表示 older)。`C-i` 则跳到一个 \"较新\" 的地方.\n1. `f` 向后搜索, `F` 向前搜索, `;` next\n1. H:将光标移动到当前可见页面的顶部（High）\n1. M:将光标移到当前可见页面的中间（Middle）\n1. L:将光标移动到当前可见页面的底部（Low）\n1. `:buffers` or `:ls` 列出可用的缓冲区\n1. `:bn` 下一个缓冲区。类似地，我们可以通过键入 b 数字或名称（在上面的第一列）或名称来切换到缓冲区。\n1. `:bd` 删除当前 buffer\n1. `p and S-p` 当前字符前后复制\n1. `I` go to start line and insert\n1. `A` go to end line and insert\n\n## [easymotion](https://github.com/easymotion/vim-easymotion)\n\n1. Set a custom color for the easymotion search n-character (default <leader><leader>/).\n1. `<leader><leader>f` char\n\n1. like emacs avy-jump\n\n## Plugins\n\n1. [native package](https://shapeshed.com/vim-packages/)\n\n### [vim-slime](https://github.com/jpalardy/vim-slime)\n\n1. install\n\n```sh\nmkdir -p ~/.vim/pack/plugins/start\ncd ~/.vim/pack/plugins/start\ngit clone https://github.com/jpalardy/vim-slime.git\n```\n\n1. vimrc config\n\n```sh\nlet g:slime_target = \"tmux\"\nnmap <c-c><c-l> <Plug>SlimeLineSend\nnmap <c-a> <Plug>SlimeLineSend\n```\n\n1. [try in docker](https://blog.jpalardy.com/posts/trying-vim-slime-in-docker/)\n"},{"title":"系统架构师","url":"/2022/07/30/系统架构师/","content":"\n## Links\n\n1. [网上报名平台](https://bm.ruankao.org.cn/sign/welcome)\n1. [资料](https://github.com/gitlfc163/SystemArchitect)\n1. [高质量、最全面的 2022 年系统架构设计师（软考高级）备考资源库](https://github.com/xxlllq/system_architect)\n1. [中国计算机技术职业资格网](https://www.ruankao.org.cn/)\n1. [报考指南](https://www.ruankao.org.cn/platform)\n1. [考试安排](https://www.ruankao.org.cn/arrange)\n1. [知乎分析](https://www.zhihu.com/question/264585896)\n"},{"title":"software-features","url":"/2022/07/27/software-features/","content":"\n## 软件质量\n\n1. [各种标准介绍](https://bbs.huaweicloud.com/blogs/261499)\n1. [ISO/IEC 25010](https://iso25000.com/index.php/en/iso-25000-standards/iso-25010)\n   ![ISO/IEC 25010](https://iso25000.com/images/figures/en/iso25010.png)\n\n<!-- more -->\n\n1. [ISO/IEC 5055](https://www.encora.com/insights/software-quality-standards-iso-5055-overview)\n   - [视频](https://www.youtube.com/watch?v=J8Wz0m1mkn4)\n1. ISO 5055 is intended to supplement the levels of measurement in the ISO/IEC 25000 series. Also known as SQaRE, the ISO 25000 series uses a set of eight software quality characteristics, including our four from earlier, to measure a software’s quality at a behavioral level. This is different from ISO 5055, which is looking at software at a code level. ISO/IEC 25000 and ISO 5055 are designed to complement each other.\n   ![软件质量](http://www.itcast.cn/files/image/202010/20201009143141577.jpg)\n\n1. ISO/IEC 9126:1901 标准所包含的 6 大特性的具体含义如下。\n   - `功能性`:在指定条件下,软件满足用户显式需求和隐式需求的能力。\n   - `可靠性`:在指定条件下使用时,软件产品维持规定的性能级别的能力。\n   - `可使用性`:在指定条件下,软件产品被使用、理解、学习的能力。\n   - `效率`:在指定条件下,相对于所有资源的数量,软件产品可提供适当性能的能力。\n   - `可维护性`:指软件产品被修改的能力。修改包括修正、优化和功能规格变更的说明。\n   - `可移植性`:指软件产品从一个环境迁移到另一个环境的能力。\n\n## 优点\n\n1. Functionality\n1. Usability (User-friendly) 易用性\n1. Efficiency 高效\n1. Flexibility\n1. Reliability 可靠性\n1. Maintainability\n1. Portability\n1. Integrity\n1. 可复用性\n1. 可用性\n1. 兼容性\n1. 可扩展性\n1. 可移植性\n1. 安全性\n\n## 缺点\n\n## 指标\n\n1. 延迟\n1. 吞吐\n1. 利用率\n"},{"title":"CPM","url":"/2022/07/16/CPM/","content":"\n## basic\n\n1. [usage](https://github.com/cpm-cmake/CPM.cmake?tab=readme-ov-file#usage)\n   - <dependency>\\_SOURCE_DIR is the path to the source of the dependency.\n   - <dependency>\\_BINARY_DIR is the path to the build directory of the dependency.\n   - <dependency>\\_ADDED is set to YES if the dependency has not been added before, otherwise it is set to NO.\n1. CPMAddPackage 是 CMake 的一个模块，它是 CPM.cmake 的一部分，用于管理项目依赖项。当你使用 CPMAddPackage 下载 .tgz 或 .tar.gz 归档文件时，该模块会自动解压归档文件,会去除一层\n1. [samples](https://github.com/cpm-cmake/CPM.cmake/wiki/More-Snippets#sol2)\n1. 导入外部库 `INTERFACE IMPORTED` [link](https://hsf-training.github.io/hsf-training-cmake-webpage/04-targets/index.html)\n1. 全局导入外部库 `INTERFACE IMPORTED GLOBAL`， 解决 cpm local scope include_directories 不工作问题\n1. ? add_definitions()传递， 后引入的库在作用域更小， 先引入的库作用域大，与顺序有关\n1. download\n\n```cmake\nset(CPM_DOWNLOAD_VERSION 0.27.2)\nset(CPM_DOWNLOAD_LOCATION \"${CMAKE_BINARY_DIR}/cmake/CPM_${CPM_DOWNLOAD_VERSION}.cmake\")\n\nif(NOT (EXISTS ${CPM_DOWNLOAD_LOCATION}))\n    message(STATUS \"Downloading CPM.cmake\")\n    file(DOWNLOAD https://github.com/TheLartians/CPM.cmake/releases/download/v${CPM_DOWNLOAD_VERSION}/CPM.cmake ${CPM_DOWNLOAD_LOCATION})\nendif()\n\ninclude(${CPM_DOWNLOAD_LOCATION})\n```\n\n1. cpm url lib export， [cmake 导入外部链接库](https://github.com/cpm-cmake/CPM.cmake/wiki/More-Snippets#sol2)\n\n```cmake\ncpmaddpackage(\n  NAME\n  gflags\n  URL\n  https://mirrors.xxxxxx/nfs/pkgs/gflags_2.2.2/$gflags_2.2.2.tar\n)\n\n# if(gflags_ADDED AND NOT TARGET gflags)\n## 错误\nif(gflags_ADDED)\n  add_library(gflags INTERFACE)\n  target_include_directories(gflags INTERFACE \"${gflags_SOURCE_DIR}/include\")\n  target_link_directories(gflags INTERFACE \"${gflags_SOURCE_DIR}/lib\")\nendif()\n\n## 正确(官网的例子没有set properties，是因为会编译出.so，不用指定so位置)\nif(gflags_ADDED)\n  message(STATUS \"gflags test2**********************\")\n  add_library(gflags INTERFACE IMPORTED GLOBAL) ## global导入， 解决cpm local scope include_directories不工作问题\n  add_library(gflags::gflags ALIAS gflags)\n  set_target_properties(\n    gflags PROPERTIES IMPORTED_LOCATION \"${gflags_SOURCE_DIR}/lib/libgflags.so\")\n  target_include_directories(gflags INTERFACE \"${gflags_SOURCE_DIR}/include\")\n  # target_link_directories(gflags INTERFACE \"${gflags_SOURCE_DIR}/lib\")\nendif()\n\n\n```\n\n1. 多个库 if(xxx_ADDED)只有一个库会调用，可以在 if 里面打印消息测试。不用`if(gflags_ADDED AND NOT TARGET gflags)`\n1. 注意区分`add_library(gflags SHARED IMPORTED GLOBAL)`和`libgflags.so`的区别，前者是 target, 后者是 so。为了区分两者， 可以使用`add_library(gflags SHARED IMPORTED GLOBAL) add_library(gflags::gflags ALIAS gflags)` target::target 来区分。\n\n1. [如果依赖的库有冲突， CPM 会有 WARNING](https://github.com/cpm-cmake/CPM.cmake/issues/175)\n\n```cmake\nCMake Warning at cmake/CPM.cmake:269 (message):\n  CPM: cpm-test2: requires a newer version of cpm-test1 (0.0.4) than\n  currently included ().\n```\n\n1. [解决库冲突方法](https://github.com/cpm-cmake/CPM.cmake#limitations)\n\n- 最外层添加依赖（因为 first version used)\n- lock file\n\n1. cpm funtion\n\n```cmake\nfunction(pg_add_lib name)\n  set(options \"\")\n  set(oneValueArgs PACK)\n  set(multiValueArgs INCLUDE_DIRS LIB_DIRS)\n  cmake_parse_arguments(ARG \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\"\n                        ${ARGN})\n  if(ARG_PACK)\n    set(PACK_NAME ${ARG_PACK})\n  else()\n    set(PACK_NAME ${name}.tar)\n  endif()\n  cpmaddpackage(\n    NAME ${name} URL\n    https://xxxxxxx/nfs/pkgs/${name}/${ARCH_PLAT_VENDOR}/${PACK_NAME}\n  )\n  if(${name}_ADDED)\n    if(ARG_INCLUDE_DIRS)\n      message(STATUS, \"ARG_INCLUDE_DIRS=\\\"${ARG_INCLUDE_DIRS}\\\"\")\n      foreach(INCLUDE_DIR ${ARG_INCLUDE_DIRS})\n        message(STATUS, \"INCLUDE_DIR=\\\"${INCLUDE_DIR}\\\"\")\n        list(APPEND INCLUDE_DIRS ${${name}_SOURCE_DIR}/${INCLUDE_DIR})\n      endforeach()\n    else()\n      list(APPEND INCLUDE_DIRS ${${name}_SOURCE_DIR}/include)\n    endif()\n    message(STATUS, \"INCLUDE_DIRS=\\\"${INCLUDE_DIRS}\\\"\")\n\n    if(ARG_LIB_DIRS)\n      message(STATUS, \"ARG_LIB_DIRS=\\\"${ARG_LIB_DIRS}\\\"\")\n      foreach(LIB_DIR ${ARG_LIB_DIRS})\n        message(STATUS, \"LIB_DIR=\\\"${LIB_DIR}\\\"\")\n        list(APPEND LIB_DIRS ${${name}_SOURCE_DIR}/${LIB_DIR})\n      endforeach()\n    else()\n      list(APPEND LIB_DIRS ${${name}_SOURCE_DIR}/lib)\n    endif()\n    message(STATUS, \"LIB_DIRS=\\\"${LIB_DIRS}\\\"\")\n\n    foreach(LIB_DIR ${LIB_DIRS})\n      execute_process(\n        COMMAND ls\n        COMMAND sh -c \"awk '/.so\\$|.a\\$/{print \\$0}'\"\n        COMMAND sh -c \"awk -F '.' '{print substr($1,4)}'\"\n        # COMMAND sh -c \"sort\" COMMAND sh -c \"uniq\"\n        WORKING_DIRECTORY ${LIB_DIR}\n        OUTPUT_VARIABLE LIBS\n        OUTPUT_STRIP_TRAILING_WHITESPACE)\n      message(STATUS, \"LIBS=${LIBS}\")\n      string(REPLACE \"\\n\" \";\" LIBS \"${LIBS}\")\n      foreach(LIB ${LIBS})\n        message(STATUS, \"LIB=\\\"${LIB}\\\"\")\n        if(NOT TARGET ${LIB})\n          find_library(\n            ${LIB}_PATH\n            NAMES ${LIB}\n            PATHS \"${LIB_DIR}\"\n            NO_DEFAULT_PATH)\n          message(STATUS, \"LIB_PATH=\\\"${${LIB}_PATH}\\\"\")\n          add_library(${LIB} UNKNOWN IMPORTED GLOBAL)\n          add_library(${name}::${LIB} ALIAS ${LIB})\n          set_target_properties(${LIB} PROPERTIES IMPORTED_LOCATION\n                                                  \"${${LIB}_PATH}\")\n          target_include_directories(${LIB} INTERFACE ${INCLUDE_DIRS})\n        endif()\n      endforeach()\n    endforeach()\n  endif()\nendfunction(pg_add_lib)\n```\n\n1. 适应 windows, 去除 awk 依赖\n\n```cmake\nfunction(pg_add_third_party_lib name)\n  set(options \"\")\n  set(oneValueArgs PACK)\n  set(multiValueArgs INCLUDE_DIRS LIB_DIRS)\n  cmake_parse_arguments(ARG \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\"\n                        ${ARGN})\n  if(ARG_PACK)\n    set(PACK_NAME ${ARG_PACK})\n  else()\n    set(PACK_NAME ${name}.tar)\n  endif()\n  cpmaddpackage(\n    NAME ${name} URL\n    https://xxx/nfs/pkgs/${name}/${ARCH_PLAT_VENDOR}/${PACK_NAME}\n  )\n  if(${name}_ADDED)\n    if(ARG_INCLUDE_DIRS)\n      foreach(INCLUDE_DIR ${ARG_INCLUDE_DIRS})\n        if(EXISTS \"${${name}_SOURCE_DIR}/${INCLUDE_DIR}\")\n          list(APPEND INCLUDE_DIRS ${${name}_SOURCE_DIR}/${INCLUDE_DIR})\n        endif()\n      endforeach()\n    else()\n      list(APPEND INCLUDE_DIRS ${${name}_SOURCE_DIR}/include)\n    endif()\n\n    if(ARG_LIB_DIRS)\n      foreach(LIB_DIR ${ARG_LIB_DIRS})\n        list(APPEND LIB_DIRS ${${name}_SOURCE_DIR}/${LIB_DIR})\n      endforeach()\n    else()\n      list(APPEND LIB_DIRS ${${name}_SOURCE_DIR}/lib)\n    endif()\n\n    foreach(LIB_DIR ${LIB_DIRS})\n      # shared libs\n      execute_process(\n        COMMAND ls\n        # COMMAND sh -c \"awk '/.so\\$/{print \\$0}'\" COMMAND sh -c \"awk -F '.'\n        # '{print substr($1,4)}'\"\n        WORKING_DIRECTORY ${LIB_DIR}\n        OUTPUT_VARIABLE LIBS\n        OUTPUT_STRIP_TRAILING_WHITESPACE)\n      string(REPLACE \"\\n\" \";\" LIBS \"${LIBS}\")\n      foreach(LIB ${LIBS})\n        string(REGEX MATCH \"lib.*\\.so$\" LIB ${LIB})\n        if(LIB)\n          get_filename_component(LIB ${LIB} NAME_WE)\n          string(REPLACE \"lib\" \"\" LIB \"${LIB}\")\n          message(STATUS \".......lib=${LIB}\")\n          if(NOT TARGET ${LIB})\n            add_library(${LIB} SHARED IMPORTED GLOBAL)\n            add_library(${name}::${LIB} ALIAS ${LIB})\n            set_target_properties(${LIB} PROPERTIES IMPORTED_LOCATION\n                                                    \"${LIB_DIR}/lib${LIB}.so\")\n            target_include_directories(${LIB} INTERFACE ${INCLUDE_DIRS})\n          endif()\n        endif()\n      endforeach()\n\n      # static libs\n      execute_process(\n        COMMAND ls\n        # COMMAND sh -c \"awk '/.a\\$/{print \\$0}'\" COMMAND sh -c \"awk -F '.'\n        # '{print substr($1,4)}'\"\n        WORKING_DIRECTORY ${LIB_DIR}\n        OUTPUT_VARIABLE LIBS\n        OUTPUT_STRIP_TRAILING_WHITESPACE)\n      string(REPLACE \"\\n\" \";\" LIBS \"${LIBS}\")\n      foreach(LIB ${LIBS})\n        string(REGEX MATCH \"lib.*\\.so$\" LIB ${LIB})\n        if(LIB)\n          get_filename_component(LIB ${LIB} NAME_WE)\n          string(REPLACE \"lib\" \"\" LIB \"${LIB}\")\n          message(STATUS \".......lib=${LIB}\")\n          if(NOT TARGET ${LIB})\n            add_library(${LIB} STATIC IMPORTED GLOBAL)\n            add_library(${name}::${LIB} ALIAS ${LIB})\n            set_target_properties(${LIB} PROPERTIES IMPORTED_LOCATION\n                                                    \"${LIB_DIR}/lib${LIB}.a\")\n            target_include_directories(${LIB} INTERFACE ${INCLUDE_DIRS})\n          endif()\n        endif()\n      endforeach()\n    endforeach()\n  endif()\nendfunction(pg_add_third_party_lib)\n```\n\n1. final\n\n```cmake\ninclude(cmake/CPM.cmake)\nfunction(pg_add_third_party_lib name)\n  set(options \"\")\n  set(oneValueArgs PACK)\n  set(multiValueArgs INCLUDE_DIRS LIB_DIRS)\n  cmake_parse_arguments(ARG \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\"\n                        ${ARGN})\n  if(ARG_PACK)\n    set(PACK_NAME ${ARG_PACK})\n  else()\n    set(PACK_NAME ${name}.tar)\n  endif()\n  cpmaddpackage(\n    NAME ${name} URL\n    https://mirrors.phigent.io/nfs/pkgs/${name}/${ARCH_PLAT_VENDOR}/${PACK_NAME}\n  )\n  # if(${name}_ADDED)\n  if(ARG_INCLUDE_DIRS)\n    foreach(INCLUDE_DIR ${ARG_INCLUDE_DIRS})\n      if(EXISTS \"${${name}_SOURCE_DIR}/${INCLUDE_DIR}\")\n        list(APPEND INCLUDE_DIRS ${${name}_SOURCE_DIR}/${INCLUDE_DIR})\n      endif()\n    endforeach()\n  else()\n    list(APPEND INCLUDE_DIRS ${${name}_SOURCE_DIR}/include)\n  endif()\n\n  if(ARG_LIB_DIRS)\n    foreach(LIB_DIR ${ARG_LIB_DIRS})\n      list(APPEND LIB_DIRS ${${name}_SOURCE_DIR}/${LIB_DIR})\n    endforeach()\n  else()\n    list(APPEND LIB_DIRS ${${name}_SOURCE_DIR}/lib)\n  endif()\n\n  foreach(LIB_DIR ${LIB_DIRS})\n    # shared libs\n    execute_process(\n      COMMAND ls\n      WORKING_DIRECTORY ${LIB_DIR}\n      OUTPUT_VARIABLE LIBS\n      OUTPUT_STRIP_TRAILING_WHITESPACE)\n    string(REPLACE \"\\n\" \";\" LIBS \"${LIBS}\")\n    foreach(LIB ${LIBS})\n      string(REGEX MATCH \"lib.*\\.so$\" LIB ${LIB})\n      if(LIB)\n        string(LENGTH ${LIB} STR_LEN)\n        math(EXPR STR_LEN \"${STR_LEN} - 6\")\n        string(SUBSTRING ${LIB} 3 ${STR_LEN} LIB)\n        if(NOT TARGET ${LIB})\n          add_library(${LIB} SHARED IMPORTED GLOBAL)\n          add_library(${name}::${LIB} ALIAS ${LIB})\n          set_target_properties(${LIB} PROPERTIES IMPORTED_LOCATION\n                                                  \"${LIB_DIR}/lib${LIB}.so\")\n          target_include_directories(${LIB} INTERFACE ${INCLUDE_DIRS})\n        endif()\n      endif()\n    endforeach()\n\n    # static libs\n    execute_process(\n      COMMAND ls\n      WORKING_DIRECTORY ${LIB_DIR}\n      OUTPUT_VARIABLE LIBS\n      OUTPUT_STRIP_TRAILING_WHITESPACE)\n    string(REPLACE \"\\n\" \";\" LIBS \"${LIBS}\")\n    foreach(LIB ${LIBS})\n      string(REGEX MATCH \"lib.*\\.a$\" LIB ${LIB})\n      if(LIB)\n        string(LENGTH ${LIB} STR_LEN)\n        math(EXPR STR_LEN \"${STR_LEN} - 5\")\n        string(SUBSTRING ${LIB} 3 ${STR_LEN} LIB)\n        if(NOT TARGET ${LIB})\n          add_library(${LIB} STATIC IMPORTED GLOBAL)\n          add_library(${name}::${LIB} ALIAS ${LIB})\n          set_target_properties(${LIB} PROPERTIES IMPORTED_LOCATION\n                                                  \"${LIB_DIR}/lib${LIB}.a\")\n          target_include_directories(${LIB} INTERFACE ${INCLUDE_DIRS})\n        endif()\n      endif()\n    endforeach()\n  endforeach()\n  # endif()\nendfunction(pg_add_third_party_lib)\n```\n\n1. 版本管理\n\n```cmake\nset(CPM_FUNCTIONS_VERSION 0.1)\nset(CPM_DOWNLOAD_LOCATION \"${CMAKE_SOURCE_DIR}/cmake/pg_cpm_functions.cmake\")\n\nif(EXISTS ${CPM_DOWNLOAD_LOCATION})\n  include(${CPM_DOWNLOAD_LOCATION})\nendif()\n\nif(NOT (CPM_FUNCTIONS_VERSION VERSION_EQUAL CURRENT_CPM_FUNCTIONS_VERSION))\n  message(\n    STATUS\n      \"CURRENT_CPM_FUNCTIONS_VERSION ${CURRENT_CPM_FUNCTIONS_VERSION} not equal to CPM_FUNCTIONS_VERSION ${CPM_FUNCTIONS_VERSION}}\"\n  )\n  message(\n    STATUS\n      \"Downloading https://xxxxcpm/functions/v${CPM_FUNCTIONS_VERSION}/pg_cpm_functions.cmake to ${CPM_DOWNLOAD_LOCATION}\"\n  )\n  file(\n    DOWNLOAD\n    https://xxxxxx/functions/v${CPM_FUNCTIONS_VERSION}/pg_cpm_functions.cmake\n    ${CPM_DOWNLOAD_LOCATION})\nendif()\n\ninclude(${CPM_DOWNLOAD_LOCATION})\nmessage(\n  STATUS \"CURRENT_CPM_FUNCTIONS_VERSION=\\\"${CURRENT_CPM_FUNCTIONS_VERSION}\\\"\")\n```\n\n## Links\n\n1. [使用 CPM 管理 CMake C++工程中的外部依赖库](https://blog.csdn.net/davidhopper/article/details/121030549)\n"},{"title":"Clang","url":"/2022/07/07/Clang/","content":"\n## base\n1. `-###` 打印要执行的命令，不执行\n1. `-Wall`：启用大多数警告。这是开发中常用的选项，可以帮助发现潜在的代码问题。\n1. `-Wextra`：启用额外的警告，比 -Wall 更严格。\n1. `-Werror`：将所有警告当作错误处理。这意味着任何警告都会导致编译失败，迫使开发者修复这些潜在的问题\n1. clang参数：\n   - 传给驱动器:\n   - 传给前端： 如果你要传递参数给 前端 (cc1)，驱动器是不认识的。 需要加-Xclang\n1. -Xclang：告诉 clang 驱动，把后面的参数直接传给内部的 clang 前端（frontend）。\n\n## clang编译基本流程\n1. 源码 ——> 预处理(Preprocess) -> 词法分析(Lex) -> 语法分析(Parse 生成AST) -> 语义分析(Sema) -> IR生成(IR CodeGen, 从AST生成LLIR) -> 优化(LLVM opt) ->  汇编生成(LLVM CodeGen; 使用llc将LLIR生成.s) -> 汇编 -> 链接\n1. 源码到LLIR生成是前端，优化是中端，汇编生成是后端\n1. 后端：指令选择、寄存器分配、指令调度\n1. clang 实际上是一个 驱动程序（driver），它会根据参数调用不同的子阶段。\n\n### 前端流程\n1. lexer阶段将所有的输入当作一个特别长的string， 然后进行解析， 得到token流\n1. Parser和Sema的输入是一个token流， 结果是树状结构的AST， 节点是各种AST的node， 所以首先需要定义AstNode。当需要增加前端支持的语法时， 比如在基础的语法上想要支持 if else语法时， 就需要增加对应的AST node， 并且要有对应的Parser来解析\n1. 调用llvm接口可以将AST生成llvm IR\n\n### sema\n1. Clang 前端最核心、也最复杂的模块之一, 代码占比最多\n1. 作用\n   - 名称解析\n   - 类型检查\n   - 作用域与可见性\n   - 展开模板、推到模板参数，实例化模板\n   - 属性与修饰符检查 constexpr, 例如 constexpr函数内不允许运行时表达式\n   - 添加诊断信息: 生成准确的错误位置和信息\n1. 代码位于llvm工程下\n\n``` c++\nclang/lib/Sema/\nclang/include/clang/Sema/\n```\n\n## clang VS gcc\n\n1. clang 优势\n   - 编译速度更快\n   - 编译产出更小\n   - 出错提示更友好\n   - 内置有静态分析工具\n   - 专注\n\n## cmake 使用\n\n1. env\n\n```sh\nexport CC=clang\nexport CXX=clang++\n```\n\n1. cmake\n\n```sh\ncmake -DCMAKE_C_COMPILER=clang  -DCMAKE_CXX_COMPILER=clang++ ..\n```\n\n## [include-what-you-use](https://github.com/include-what-you-use/include-what-you-use)\n\n1. 检查在头文件中用了哪些 symbol\n1. `sudo ln -s /usr/bin/python3 /usr/bin/python`\n1. `iwyu_tool.py -p build/` build 下有 compile_commands.json, 分析工程\n1. fix_includes.py 修复 include\n1. build\n\n```sh\n# git clone git@github.com:include-what-you-use/include-what-you-use.git\n# cd include-what-you-use\n# git checkout 0.14\n# mkdir build && cd build\n# cmake -G \"Unix Makefiles\" -DCMAKE_PREFIX_PATH=/usr/lib/llvm-10 ../\n# make && make install\n# bin/include-what-you-use ../../cpp_sandbox/io/read_shape/test.cpp\n```\n\n## links\n\n1. [linux 下 Clang 和 gcc 的区别](https://cloud.tencent.com/developer/article/1449757)\n\n\n## command\n\n``` shell\n#!/bin/bash\necho -e \"\\n=== 0. -### 打印编译命令, 不执行 ===\"\nclang++ -### test.cpp > test.cmd 2>&1   \n\necho -e \"\\n=== 1. -save-temps 保存编译中间文件 ===\"\n# clang++ -save-temps test.cpp\n\necho -e \"\\n=== 2. 词法分析 ===\"\nclang++ -fsyntax-only -Xclang -dump-tokens test.cpp > test.tok 2>&1\n\necho -e \"\\n=== 3. 语法分析（AST） ===\"\nclang++ -fsyntax-only -Xclang -ast-dump test.cpp > test.ast 2>&1\n\necho -e \"\\n=== 4. LLVM IR生成 ===\"\nclang++ -S -emit-llvm  test.cpp -o test.ll\n\necho -e \"\\n=== 5. 打印编译阶段 ===\"\nclang++ -ccc-print-phases test.cpp > test.phase 2>&1\n\necho -e \"\\n=== 6. ast 可视化 ===\"\nclang++ -cc1 -ast-view test.cpp\n# 有了ast文件后可视化 https://www.cnblogs.com/qs-smile/p/17445908.html\n```\n\n\n## clang适配一种cuda like语法，nvidia gpu like硬件\n1. llvm clang中如何添加对类似cuda编程语言的支持，硬件架构类似nvidia gpu?\n1. 分析/data/users/jiaxiyang/llvm-project/clang文件中对sipu的支持，梳理代码逻辑?\n1. 前端后端都需要适配\n1. 开发生态要点:\n   - 语言/语义: 通过属性/关键字标注哪些函数/数据在设备侧执行（如 global/device，在 SIPU 中为 sipu_global/sipu_device）。\n   - 编译流程: 主机与设备分别编译，设备代码用目标后端生成对象/ELF；再由驱动把设备二进制打包/嵌入到主机对象或最终可执行里。\n   - 运行时: 主机侧通过 runtime 初始化设备、管理内存与内核启动，设备侧执行并返回。\n1. Clang 驱动识别 SIPU 源（.su），创建 SIPU 设备工具链，独立完成设备编译与链接（RISC‑V rv64）。设备产物直接嵌入到主机对象（不走多架构 fatbin），主机通过句柄触发设备执行。   \n1. Offloading（异构卸载）: 指把部分程序的计算从“主机”（CPU 端）转移到“设备”（如 GPU、加速器、专用 IP）上执行，由主机发起、设备加速、结果回传。\n"},{"title":"Bazel","url":"/2022/07/05/Bazel/","content":"\n## docker\n\n1. `docker pull cockroachdb/bazel:latest-do-not-use`\n1. [安装脚本](https://bazel.build/install/ubuntu?hl=zh-cn#run-installer)\n\n## tutorial\n\n1. [c++ base samples](https://bazel.build/start/cpp?hl=zh-cn)\n1. [c++ tutorials](https://bazel.build/tutorials/cpp-use-cases?hl=zh-cn)\n\n## links\n\n1. [参考文档: 构建百科全书， 测试百科全书，命令行参考文档，查询参考文档](https://bazel.build/reference?hl=zh-cn)\n1. [简介](https://bazel.build/about/intro?hl=zh-cn)\n1. [build system](https://bazel.build/basics?hl=zh-cn)\n1. [概念](https://bazel.build/concepts/build-ref?hl=zh-cn)\n1. [基础知识](https://bazel.build/build/style-guide?hl=zh-cn)\n1. [最佳做法](https://bazel.build/configure/best-practices?hl=zh-cn)\n1. [配置 BUILD](https://bazel.build/configure/attributes?hl=zh-cn)\n","tags":["Tools","Bazel"],"categories":["Tools","Bazel"]},{"title":"Ninja","url":"/2022/07/05/Ninja/","content":"\n## basic\n\n1. `ninja -v`\n1. build目录下`ninja {TARGET} -v`查看某个target编译的命令\n1. `sudo apt-install ninja-build`\n1. `cmake .. -G Ninja`\n1. 除了加上`-G Ninja`使用方式与`make`一致\n\n```sh\nBUILD_TYPE=Release\nBUILD_DIR=build_gpu_tensorrt\nINSTALL_DIR=${BUILD_DIR}/$(basename \"$PWD\")_$(echo ${BUILD_TYPE} | tr '[:upper:]' '[:lower:]')\nBUILD_SYSTEM=Ninja\nif ! [ -x \"$(command -v ninja)\" ]; then\n    BUILD_SYSTEM='Unix Makefiles'\nfi\n\ncmake -G \"${BUILD_SYSTEM}\" -S . -B ${BUILD_DIR} \\\n    -DCMAKE_BUILD_TYPE=${BUILD_TYPE} \\\n    -DUSE_API_V2=OFF \\\n    -DUSE_ASCEND=OFF \\\n    -DUSE_NEUWARE=OFF \\\n    -DUSE_GPU=ON \\\n    -DUSE_TENSORRT=ON \\\n    -DUSE_GPU_PRE_PROCESS=ON \\\n    -DDEBUG_INPUT_OUTPUT=OFF \\\n    -DCMAKE_VERBOSE_MAKEFILE=ON \\\n    # -DCMAKE_C_COMPILER=clang-10 \\\n    # -DCMAKE_CXX_COMPILER=clang++-10 \\\n    -DARCH_PLAT_VENDOR=x86_64-ubuntu-linux-gcc9.3.0 \\\n    $@\ncmake --build ${BUILD_DIR} -j4\ncmake --install ${BUILD_DIR} --prefix ${INSTALL_DIR}\n```\n\n1. ninja color output\n\n```cmake\nif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n    add_compile_options (-fdiagnostics-color=always)\nelseif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\")\n    add_compile_options (-fcolor-diagnostics)\nendif ()\n```\n\n1. 错误信息绝对路径\n\n## Link\n\n1. [wikipedia](<https://zh.m.wikipedia.org/zh-hans/Ninja_(%E6%9E%84%E5%BB%BA%E7%B3%BB%E7%BB%9F)>)\n"},{"title":"数据落盘","url":"/2022/07/04/数据落盘/","content":"\n## basic\n\n1. ![经典图](https://img-blog.csdnimg.cn/2020091210500712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpZXlpaHVhMTk5NA==,size_16,color_FFFFFF,t_70#pic_center)\n\n## Links\n\n1. [数据落盘机制](https://blog.csdn.net/xieyihua1994/article/details/108545746)\n"},{"title":"Model-deploy","url":"/2022/06/30/Model-deploy/","content":"\n## pgpredict\n\n1. 负责模型推理\n1. 接口:\n   - load model\n   - set input data\n   - forward\n   - get output data\n\n## dcv-cpp\n\n1. 负责将全流程（前后处理+模型推理）运行起来（单路）\n\n## sdk\n\n1. 负责模型相关业务逻辑（多路)\n"},{"title":"gitlab","url":"/2022/06/30/gitlab/","content":"\n## base usage\n\n1. `grep`\n1. `find file`\n1. `edit in Web IDE`\n1. `history` direct to branch commit\n1. `Permalink`\n1. `commits tags branch Storage` behind project name\n1. `snippets`\n1. `Use wiki`\n\n## [DevOps lifecycle](https://about.gitlab.com/stages-devops-lifecycle/value-stream-analytics/)\n\n1. [中文文档](https://gitlab.cn/stages-devops-lifecycle/)\n\n## [value-stream-analytics](https://about.gitlab.com/stages-devops-lifecycle/value-stream-analytics/)\n\n1. On the left sidebar, select `Analytics > Value stream`.\n\n## merge request\n\n## code review\n\n## issue\n\n### issue board\n\n## milestone\n\n## Package Registry\n\n## links\n\n1. [基于 GitLab 的工作流程设计](https://zhuanlan.zhihu.com/p/38774185)\n1. [GitLab Flow 介绍](https://docs.gitlab.cn/jh/topics/gitlab_flow.html#gitlab-flow-%E4%BB%8B%E7%BB%8D)\n1. [GitLab Flow 的 11 条规则](http://dockone.io/article/2350)\n","tags":["gitlab"],"categories":["Tools","gitlab"]},{"title":"magit","url":"/2022/06/24/magit/","content":"\n## basic\n\n1. `C-h m` 查看 magit 快捷键\n1. `$` 查看具体命令及历史\n1. `S-tab`\n1. `M-2` magit-section-show-level-2-all\n1. `y` show branchs and tags\n1. `du` and `ds`: diff unstaged and staged\n1. `X` reset: 撤销，commit 不保存记录\n1. `V` revert: 撤销， 保存 commit 撤销记录\n1. `AA` + `commit id`: cherry pick 将某次提交应用到当前分支\n\n## commit\n\n1. 修改： `c` => `w` => `C-c C-c`\n1. 合并： magit: rebase `r` =>`i` => `C-c C-c` => `f .. f` => `C-c C-c`\n1. `ce` Extend 当前 Staged 的文件合并到上一次提交中 git commit –amend –no-edit\n1. `ca` Amend 只修改上次提交的日志 git commit –amend\n\n## tag\n\n1. `tt`: create tag\n1. `pt`: push all tags\n\n## stash\n\n1. 单一文件：\n   - stage\n   - `zi`\n\n## command (such as: add a new file in a directory)\n\n1. `!` run git command\n1. `!!` run git command in current repo root path\n"},{"title":"Cpp awesome src","url":"/2022/05/19/Cpp-awesome-src/","content":"\n## links\n\n1. [chromium](https://github.com/chromium/chromium)\n1. [taskflow](https://github.com/taskflow/taskflow)\n"},{"title":"taskflow","url":"/2022/05/19/taskflow/","content":"\n## 调度框架实现\n\n1. [queue 示意图](https://excalidraw.com/#json=TDm5o05q9INkFNkP9Xcu-,s4EkPpKyeCgTF4nqCC6fAQ)\n\n## 心得\n\n1. 顶点是函数，边是数据\n1. 所谓 flow 是指计算顺序，数据并不会移动。\n1. 顶点和边组成 DAG\n1. DAG 需要调度器来运行\n1. 各框架：\n   - vitis ai runtime: op 是顶点，tensor 是边, runner 是调度器\n   - dpx flow: module(forward?) 是顶点，message 是边, engine 是调度器\n   - taskflow: Node 是顶点，Executor 是调度器\n   - CGraph:\n   - sogou workflow:\n   - tensorflow:\n   - pytorch:\n1. CGraph 中的[claster](http://www.chunel.cn/archives/cgraph-loop-introduce)和 flow 中的 module 很像, 继承自 group，由多个 functionNode 线性组合而成。执行 cluster 的时候，内部的 node 依次顺序执行。简而言之就是可以依次完成多个功能。\n1. flow 是异步事件驱动的运行模式，使用异步回调方式，feed msg 后执行完 workflow 后调用回调函数。\n1. taskflow 也是异步模式，都有 observer, 执行完后调用回调函数，观察者模式。 tf::ObserverInterface 和 RunObserver\n   q1. 顶点需要注册到图中, 并设定依赖关系。\n\n## 先学习图，再来学 taskflow\n\n## 有向无环图（DAG, Directed Acyclic Graph）\n\n1. [纯序员给你介绍图化框架的简单实现——循环逻辑](http://www.chunel.cn/archives/cgraph-loop-introduce)p\n1. [纯序员给你介绍图化框架的简单实现——执行逻辑](http://www.chunel.cn/archives/cgraph-run-introduce)\n1. [大数据工作流任务调度--有向无环图(DAG)之拓扑排序](https://dolphinscheduler.apache.org/zh-cn/blog/DAG.html)\n\n## links\n\n1. [oneTBB](https://github.com/oneapi-src/oneTBB)\n1. [taskflow](https://github.com/taskflow/taskflow)\n1. [tutorial](https://taskflow.github.io/taskflow/pages.html)\n1. [Building and Installing](https://taskflow.github.io/taskflow/install.html)\n1. [Learning from Examples](https://taskflow.github.io/taskflow/Examples.html)\n1. [TaskFlow 代码阅读笔记](https://zhuanlan.zhihu.com/p/438678706)\n1. [CGraph](https://github.com/ChunelFeng/CGraph)\n1. [workflow](https://github.com/sogou/workflow)\n1. [gparallel](https://github.com/galois-advertising/gparallel)\n"},{"title":"windows-docker","url":"/2022/04/26/windows-docker/","content":"\n## install\n\n1. [install git](https://git-scm.com/download/win)\n1. [Visual Studio Code Remote Containers Extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)\n\n## NOTES\n\n1. 安装完 docker 后可以在 windows WLS2 ubuntu 下使用\n"},{"title":"windows WSL2","url":"/2022/04/26/docker-windows/","content":"\n## [install WSL2](https://docs.microsoft.com/zh-cn/windows/wsl/install)\n\n1. [update kernel](https://docs.microsoft.com/zh-cn/windows/wsl/install-manual#step-4---download-the-linux-kernel-update-package)\n1. [install manual](https://docs.microsoft.com/zh-cn/windows/wsl/install-manual)\n1. [install windows terminal](https://docs.microsoft.com/zh-CN/windows/terminal/install)\n"},{"title":"windows WSL2","url":"/2022/04/26/windows-WSL2/","content":"\n## NOTE\n1. vpn 网络问题解决：\n   - [wsl-vpnkit](https://github.com/sakai135/wsl-vpnkit?tab=readme-ov-file) 不需要admin权限\n   - powershell执行一次：`wsl --import wsl-vpnkit --version 2 $env:USERPROFILE\\wsl-vpnkit wsl-vpnkit.tar.gz`\n   - wsl里使用vpn每次执行 `wsl.exe -d wsl-vpnkit --cd /app wsl-vpnkit`\n   - [注意不用vpn时要自动解析/etc/resolf.conf](https://learn.microsoft.com/en-us/windows/wsl/troubleshooting#wsl-has-no-network-connectivity-once-connected-to-a-vpn)\n1. [run-emacs-in-graphical-display](https://github.com/hubisan/emacs-wsl#run-emacs-in-graphical-display)\n1. [gui-apps](https://learn.microsoft.com/en-us/windows/wsl/tutorials/gui-apps)\n1. wsl 里可以直接显示，如 opencv show， gpuplot\n1. [WSL 2 配置代理 clash](https://solidspoon.xyz/2021/02/17/%E9%85%8D%E7%BD%AEWSL2%E4%BD%BF%E7%94%A8Windows%E4%BB%A3%E7%90%86%E4%B8%8A%E7%BD%91/) 配置 WSL2 使用 Windows 代理上网 有用\n\n## [install WSL2](https://docs.microsoft.com/zh-cn/windows/wsl/install)\n\n1. [update kernel](https://docs.microsoft.com/zh-cn/windows/wsl/install-manual#step-4---download-the-linux-kernel-update-package)\n1. [install manual](https://docs.microsoft.com/zh-cn/windows/wsl/install-manual)\n1. [install windows terminal](https://docs.microsoft.com/zh-CN/windows/terminal/install)\n1. [内存限制](https://www.cnblogs.com/yyfh/p/16526955.html)\n\n## WSL2 ubuntu usage\n\n1. `/mnt/c` windows C 盘 mount 路径\n1. `explorer.exe .` 在 wsl 中打开 windows 资源管理器\n1. `cd ~/ && explorer.exe .` 查看 wsl 路径\n1. `explorer.exe *.jpg` 查看图片\n1. explorer.exe 可以用默认方式打开各类文件 `alias open='explorer.exe'`\n1. samba mount(fast)\n\n```sh\nsudo mkdir -p /data0/users/xiyang.jia/d\nsudo mount.cifs //10.31.1.225/share /data0/users/xiyang.jia/d -o user=xiyang.jia\n```\n\n1. [mount(slow)](https://linuxnightly.com/mount-and-access-hard-drives-in-windows-subsystem-for-linux-wsl/)\n\n```sh\nsudo mkdir -p /data0/users/xiyang.jia/\nsudo mount -t drvfs Z: /data0/users/xiyang.jia/\n```\n\n## [windows terminal](https://github.com/microsoft/terminal) 美化\n1. 设置 -> 默认值 -> 外观：\n   - 配色 high-contrast zenburn: \"foreground\": \"#C6C6C6\",  \"brightGreen\": \"#cbc48d\"\n   - ZSH_HIGHLIGHT_STYLES[arg0]='fg=#26ff00,bold'\n   - ZSH_HIGHLIGHT_STYLES[command]='fg=#26ff00,bold' 放入.zshrc\n   - export FZF_DEFAULT_OPTS=\"--color=hl:green,hl+:#00ff00\" # 设置fzf color\n   - 背景见 wallpaper.md\n   - emacs .zshrc设置 export TERM=\"screen-256color\"\n1. 快捷键设置\n   - [autohotkey](https://www.autohotkey.com/)\n``` shell\n   RWin::Ctrl\nCapsLock::Ctrl\n\n#f:: { \n    Send(\"{F11}\") ; 发送 F11 键切换全屏\n}\n```\n\n   - 按 Win + R，输入 shell:startup，然后回车，将autohotkey脚本copy到自启动文件夹\n1. 超高效： win + \\` 打开一半 windows terminal, 然后使用 win + f 全屏，后边切换都是全屏， 使用 tmux，可以连接服务器\n   - windows terminl\n   - wsl + tmux\n   - server\n   - sevver + docker\n   - 需要解决 ssh 经常断开问题 [link](https://blog.csdn.net/qq_15505637/article/details/80881770)\n     - `sudo /etc/init.d/ssh restart`\n   - tmux-resurrect 回复环境\n1. 如何设置 win 快捷键？\n1. C-- and C-+ 放大或缩小\n1. [官方设置教程](https://docs.microsoft.com/zh-CN/windows/terminal/install)\n1. [zenburn 主题](https://github.com/agkozak/windows-terminal-zenburn)\n1. [口袋妖怪适合 zenburn 背景的壁纸](https://wallpaperaccess.com/full/1644661.jpg)\n1. [口袋妖怪壁纸](https://wallpaperaccess.com/awesome-gengar)\n1. [口袋妖怪壁纸黑](https://wallpaperaccess.com/black-pokemon)\n1. [ubuntu 每一版壁纸汇总](http://ubuntu.ecchi.ca/wallpapers/)\n1. [ubuntu 壁纸](https://wallpaperaccess.com/ubuntu)\n1. [windows 壁纸](https://wallpaperhub.app/creators/genrolecaspe)\n1. [windows 壁纸 2](https://wallpaperhub.app/wallpapers/?tags=windows)\n1. [close scroll bar](https://github.com/microsoft/terminal/issues/3878#issuecomment-735253417)\n1. [themes](https://windowsterminalthemes.dev/)\n1. [programmer backgroud](https://wallpapers.com/wallpapers/4k-programming-codes-in-black-screen-cl72lv0j7uvh4bmy.html)\n"},{"title":"sphinx-doc","url":"/2022/04/18/sphinx-doc/","content":"\n## base\n\n1. install sphinx and theme\n\n```sh\n% pip install Sphinx\n% pip install sphinx-rtd-theme\n```\n\n1. `sphinx-quickstart` start a project\n1. `make html` build doc\n1. `make man` build man doc\n\n## Links\n\n1. [sphinx-doc/sphinx](https://github.com/sphinx-doc/sphinx)\n1. [简明教程](https://iridescent.ink/HowToMakeDocs/Basic/Sphinx.html#sphinx)\n1. [restructuredtext 语法](https://iridescent.ink/HowToMakeDocs/Basic/reST.html#restructuredtext)\n1. [文档托管网站 readthedocs.org](https://readthedocs.org/)\n1. [中文文档](https://sphinx-docs.readthedocs.io/zh_CN/latest/)\n1. [mkdocs](https://github.com/mkdocs/mkdocs/)\n1. [几款文档框架：Mkdocs、Sphinx、Teadocs、docsify](https://blog.csdn.net/m0_46521785/article/details/119812280)\n"},{"title":"onnx","url":"/2022/04/14/onnx-md/","content":"\n## base\n\n1. 打印信息\n\n```python\nimport onnx\nmodel = onnx.load('linear_func.onnx')\nprint(model)\n```\n\n1. `wget https://github.com/onnx/models/raw/b9a54e89508f101a1611cd64f4ef56b9cb62c7cf/vision/classification/resnet/model/resnet50-v2-7.onnx`\n1. [resize 算子示例](https://github.com/onnx/onnx/blob/main/docs/Operators.md#resize)\n   - Attributes(注意和输入区别)\n   - Inputs\n   - Outputs\n   - Examples\n1. `model = onnx.load(\"mnist_cc.onnx\")` 模型无结构信息，print(model)无结果，model 是 onnx.onnx_ml_pb2.ModelProto 类型\n1. vscode 使用 vscode-netron 可以和源码一起对比看\n1. onnx: 奥尼克死\n1. vscode 打开 onnx 文件，可以看出 op 生成所依赖的文件路径\n1. onnx 支持设置明确 batch size. By default, TensorFlow does not set an explicit batch size. [link](https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-861/quick-start-guide/index.html#onnx-export)\n1. why onnx?\n   ![onnx 流程](https://pic2.zhimg.com/80/v2-3214168ce95096026d2079623342317d_720w.webp)\n1. ONNX （Open Neural Network Exchange）\n1. ONNX 文件不仅仅存储了神经网络模型的权重，同时也存储了模型的结构信息以及网络中每一层的输入输出和一些其它的辅助信息。\n1. ONNX 使用的是 Protobuf 这个序列化数据结构去存储神经网络的权重信息。\n1. onnx 实现了一个 python 运行时，可用于评估 ONNX 模型和评估 ONNX 操作。目的是阐明 ONNX 的语义并帮助理解和调试 ONNX 工具和转换器。它无意用于生产，性能也不是目标\n1. ONNX 是强类型的，其定义不支持隐式强制转换。\n1. Constant 值预先已知，用于存 weights 和 bias,\n1. Variable 值在推理时之前未知，但可能包含有关数据类型和形状的信息。用于存中间结果\n1. Constant 和 Variable 都是 tensor\n1. 在 ONNX (Open Neural Network Exchange) 模型中，\"attributes\" 是与模型的各个节点或算子相关联的关键信息。这些属性为算子提供了额外的规范和配置信息，使得算子能够正确执行其预定的操作。理解属性在 ONNX 模型中的作用是重要的，尤其是在模型转换、优化或理解模型行为时。\n1. 属性用于指定算子的具体行为。例如，在一个卷积算子中，属性可以定义卷积核的大小、步长（stride）、填充（padding）等。\n1. 不变参数(weights bias div mul 需要的参数)使用 constant tensor 类型，输入是 variable tensor, attributes 是 op `初始化`用到的数值， 计算不用\n1. onnx 模型 variable tensor 不需要 type 信息，转化后才有信息; weights 有 type 信息\n\n## tools\n\n### netron\n\n1. 神经网络本质上是一个计算图。计算图的节点是算子，边是参与运算的张量\n1. 线是 tensor, 方块是 op，op 中的 weight 也是 tensor, op 中的 type 是算子类型，输入 tensor 和 op weight 根据算子类型和参数计算得到输出 tensor\n1. op 可以想象成函数，tensor 和属性是函数输入输出参数\n1. [samples 直接看](https://github.com/lutzroeder/netron?tab=readme-ov-file#models)\n1. [可以显示 type shape 等信息](https://hiranh.medium.com/visualize-keras-neural-networks-with-netron-9d3f9b3e4b5a)\n1. 显示 shape 信息（需要实例）\n\n```python\nfrom onnx import shape_inference\nmodel = onnx.load(\"mnist_cc.onnx\")\ninferred_model = shape_inference.infer_shapes(model)\nonnx.save(inferred_model, \"shape.onnx\")\n```\n\n1. netron 点击 input 或 output 可以显示模型整体信息\n1. netron 查看 weights 需要点击+\n1. netron 左侧菜单栏可以点击 toggle names, weights, attributes\n1. Netron supports ONNX, TensorFlow Lite, Core ML, Keras, Caffe, Darknet, MXNet, PaddlePaddle, ncnn, MNN and TensorFlow.js. Netron has experimental support for PyTorch, TorchScript, TensorFlow, OpenVINO, RKNN, MediaPipe, ML.NET and scikit-learn.\n\n### [onnx-simplifier](https://github.com/daquexian/onnx-simplifier)\n\n1. [tensorrt polygraphy surgeon 也能 folding constant](https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy/examples/cli/surgeon/02_folding_constants)\n1. 为了支持动态图，onnx 不得不使用复杂的方式来实现，例如 reshape，如果输入是变量，不是常量，会多出来很多分支。[link](https://zhuanlan.zhihu.com/p/346511883)\n   - 直接转化为常量 int(x.shape[0])\n   - 很多节点比如 cast 节点，Identity 这些节点可能都不需要，我们需要进行简化\n1. [网页版本](https://convertmodel.com/#input=onnx&output=onnx)\n1. sample\n\n```python\nimport onnx\nfrom onnxsim import simplify\n\n# load your predefined ONNX model\nmodel = onnx.load(filename)\n\n# convert model\nmodel_simp, check = simplify(model)\n\nassert check, \"Simplified ONNX model could not be validated\"\n\nonnx.save(onnx_model, \"path/to/the/model.onnx\")\n```\n\n### [PaddlePaddle/VisualDL](https://github.com/PaddlePaddle/VisualDL/tree/develop)\n\n### [zetane](https://github.com/zetane/viewer)\n\n1. can load onnx model and show intermediate results when the model is executed. 查看模型中间结果\n\n### [onnx2py.py](https://github.com/microsoft/onnxconverter-common/blob/master/onnxconverter_common/onnx2py.py)\n\n1. creates a python file from an ONNX graph. This script can create the same graph. It may be modified by a user to change the graph.\n\n### [onnx-tool 可以统计计算量，每层输出 shape](https://github.com/ThanatosShinji/onnx-tool/blob/main/README_CN.md)\n\n1. `python3 -m onnx_tool -m profile -i tensorrt/resnet50/model.onnx`\n\n### [pytorch-OpCounter](https://github.com/Lyken17/pytorch-OpCounter)\n\n### [polygraph](https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy/)\n\n1. `polygraphy inspect model tensorrt/resnet50/model.onnx`\n\n## links\n\n1. [官方介绍](https://onnx.ai/onnx/intro/concepts.html)\n1. [netron.app](https://netron.app/)\n1. [onnx 介绍](https://zhuanlan.zhihu.com/p/346511883)\n1. [onnx/models](https://github.com/onnx/models)\n1. [Operators](https://github.com/onnx/onnx/blob/main/docs/Operators.md)\n1. [ops 变化](https://onnx.ai/onnx/operators/index.html)\n1. [支持 onnx 的训练和推理框架](https://onnx.ai/supported-tools.html#deployModel)\n\n## 查看 onnx 算子列表 python_sandbox/onnx/op_set.py\n\n```python\nmodel = onnx.load(model_path)\nnodes = model.graph.node\nopset = set()\nfor node in nodes:\n    opset.add((node.op_type))\n\nsorted_opset = list(opset)\nsorted_opset.sort()\nprint(f\"{model_path} 使用的算子集：\")\nfor op in sorted_opset:\n    print(f\"{op}\")\n\n```\n\n## model conver\n\n### pytorch\n\n1. [pytorch docker](https://hub.docker.com/r/pytorch/pytorch)\n1. [(OPTIONAL) EXPORTING A MODEL FROM PYTORCH TO ONNX AND RUNNING IT USING ONNX RUNTIME](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)\n1. 需要!pip install onnx onnxruntime Pillow\n\n```sh\nimport torchvision.models as models\nmodel = models.resnet50(pretrained=True)\n    if torch.cuda.is_available():\n        model.cuda()\ntorch.onnx.export(......)\n```\n"},{"title":"AI-models","url":"/2022/04/13/AI-models/","content":"\n## base\n\n1. 发布的模型包括模型的结构，模型的参数(onnx)\n1. LLM 获取参数很关键\n\n## 模型参数计算量数据量趋势\n\n1. [Model Size of Notable Machine Learning Systems Over Time](https://epochai.org/mlinputs/visualization?yAxis=Parameters) 可交互, 右上角 option 可搜索，可直接到论文\n1. [Parameter, Compute and Data Trends in Machine Learning](https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/edit#gid=0) good：包含参数，计算量, 训练数据量，论文引用\n1. [Machine Learning Model Sizes and the Parameter Gap](https://arxiv.org/abs/2207.02852)\n1. [parameter-counts-in-machine-learning](https://towardsdatascience.com/parameter-counts-in-machine-learning-a312dc4753d0)\n1. [COMPUTE TRENDS ACROSS THREE ERAS OF MACHINE LEARNING](https://arxiv.org/pdf/2202.05924.pdf)\n1. [常用模型参数量](https://github.com/Lyken17/pytorch-OpCounter)\n   ![常用模型参数量](https://i.ibb.co/hHLnMp7/LAmu-Jv4-DId.png)\n1. [results-of-onnx-model-zoo-and-sota-models](https://github.com/ThanatosShinji/onnx-tool/blob/main/README_CN.md#results-of-onnx-model-zoo-and-sota-models)\n1. 可以通过模型文件大小和数据类型估计模型参数量\n1. [Pre-training costs of representative LLMs](https://i.ibb.co/RCVGL1j/2s-Zv-Zl-E5-Ns.png)\n1. [llm-perf-leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)\n   - lantency vs memory vs score\n\n## model zoo\n\n### caffe\n\n1. [SnailTyan/caffe-model-zoo](https://github.com/SnailTyan/caffe-model-zoo)\n\n### onnx\n\n1. [onnx/models](https://github.com/onnx/models)\n\n### pytorch\n\n1. [Cadene/pretrained-models.pytorch](https://github.com/Cadene/pretrained-models.pytorch)\n1. [pytorch official](https://pytorch.org/vision/stable/models.html)\n"},{"title":"AI-chip-performance","url":"/2022/04/11/AI-performance/","content":"\n## throughput and latency\n\n1. throughput：吞吐量一般指相当一段时间内测量出来的系统单位时间处理的任务数或事务数。如：1s 处理 10 帧图片 (10FPS)\n1. latency: 执行一次任务需要的时间。如：处理一帧图片耗时 10ms\n1. Throughput – successful transactions per second\n1. Latency – response time per transaction\n\n## base\n\n1. `计算芯片的峰值算力` 理论计算得出的、理想状况下的峰值算力。具体通过计算核的主频、核数量、单核运算能力等来输出理论峰值，与实际场景中的真实有效处理能力有很大差距。\n1. `计算芯片的有效算力` 设备在实际运作过程中能真实输出的算力。比如 ResNet50 在浮点 16 位精度下，推理一次的计算量是 7.8Gflops 左右，芯片每秒钟处理的 ResNet50 的帧率乘以 7.8G 就是真实有效的输出算力。如果推理帧率是 400fps，真实算力是 400\\*7.G=3Tflops。\n1. `计算芯片的有效利用率` 代表了芯片真实输出的运算能力。其计算方式为：真实有效的输出算力/理论算力。计算结果值越高，表明该芯片工作越高效。实际应用场景中，多数传统架构计算芯片，有效利用率一般在 30%左右，而新型架构的计算芯片，芯片有效利用率可以到 50%以上。领启 KA200 异构众核、存算一体芯片有效利用率可达 60%以上。\n   - 利用率： 模型计算量 \\* FPS / 芯片性能\n   - VCK190 133T, tf_resnetv1_50_imagenet_224_224_6.97G, 1421FPS, 使用率：6.97 \\* 1421 / 1000 / 133 = 7.4%\n1. `计算芯片对主流神经网络的支持程度` 下一代人工智能有两个不同的发展线路图，一个是以深度学习算法为代表的传统计算科学，一个是以生物神经网络为代表脑科学。多数计算芯片只能支持二者之一。是否能将二者兼并支持，也是衡量计算芯片的核心指标之一，异构融合是达到这一指标的重要途径之一。\n1. `芯片能效比` 计算方式为：典型网络（如 ResNet50）的推理能力/芯片的功耗，单位：推理帧率（fps/w）。\n1. `芯片的性价比` 每平方毫米推理的帧率数，计算方式为：典型网络（如 ResNet50）的推理能力/芯片的面积。\n1. 多`batch`会增加模型计算量\n1. 浮点转定点会降低计算量，可能剪枝优化\n1. NCHW 和 NHWC 会影响性能\n\n## 模型计算量\n\n### caffe\n\n1. [netscope](https://dgschwend.github.io/netscope/#/editor)\n1. [quickstart](https://dgschwend.github.io/netscope/quickstart.html)\n1. 复制 prototxt 内容, `shift + enter`: 查看计算量\n\n### onnx\n\n1. [onnx-opcounter](https://github.com/gmalivenko/onnx-opcounter) 需要模型结构，只有 pt 文件不行\n\n### pytorch\n\n1. [pytorch-OpCounter](https://github.com/Lyken17/pytorch-OpCounter)\n1. [mmdetection](https://github.com/open-mmlab/mmdetection/blob/master/README_zh-CN.md)\n\n### tensorflow\n\n## 芯片性能\n\n1. [地平线 J2](https://www.horizon.ai/journey2.html)\n   - 4TOPS\n   - 2W\n1. [J3](https://www.horizon.ai/journey3.html)\n   - 5TOPS\n   - 2.5W\n   - 16nm\n1. [J5](https://www.horizon.ai/journey5.html)\n   - 128TOPS\n   - 30W\n1. [Nvidia Xavier](https://www.nvidia.cn/autonomous-machines/embedded-systems/jetson-orin/) 比较 Jetson Orin 和 Jetson Xavier 的规格\n   - 21-32TOPS\n   - 10-40W\n   - 12nm\n1. [Nvidia Orin](https://www.nvidia.cn/autonomous-machines/embedded-systems/jetson-orin/) 比较 Jetson Orin 和 Jetson Xavier 的规格\n   - 70-274TOPS\n   - 10-60W\n   - 8nm\n1. [Xilinx Versal ACAP VC1902(VCK190)]()\n   - 133TOPS\n   - 7ns\n1. [昇腾（HUAWEI Ascend) 910](https://www.hisilicon.com/cn/products/Ascend/Ascend-910)\n   - FP16: 320TFLOPS\n   - INT8: 640TOPS\n   - 310W\n   - 7nm\n1. [特斯拉 FSD 计算平台]()\n   - 144TOPS\n   - 单芯片 72TOPS\n   - 72W\n1. [Mobileye EQ4 High](https://www.mobileye.com/eyeq-chip/)\n   - 2TOPS\n   - 28nm\n1. [Mobileye EQ5 High]()\n   - 16TOPS\n   - 7nm\n1. [Mobileye EQ6 High]()\n   - 34TOPS\n   - 7nm\n\n## links\n\n1. [佐思汽研《2024 年自动驾驶 SoC 研究报告》](https://www.eefocus.com/article/1726287.html)\n   - 各种芯片算力\n1. [jetson xavier params](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-xavier-series/)\n1. [jetson parms](https://developer.nvidia.com/embedded/jetson-modules)\n1. [TOPS GOPS FLOPS](https://zhuanlan.zhihu.com/p/337618803)\n1. [指标](https://wuchenxu.com/2021/10/02/computing-power-metrices/)\n1. [J5 官方介绍](https://www.horizon.ai/journey5.html)\n1. [link1](https://auto.gasgoo.com/a/70266230.html)\n1. [jetson-benchmark](https://developer.nvidia.com/embedded/jetson-benchmarks)\n1. [deep-learning-performance-training-inference](https://developer.nvidia.com/deep-learning-performance-training-inference)\n1. [orin performance](https://developer.nvidia.com/blog/delivering-server-class-performance-at-the-edge-with-nvidia-jetson-orin/)\n1. [orin 产品系列](https://www.nvidia.cn/autonomous-machines/embedded-systems/jetson-orin/)\n1. [VCK190 性能](https://github.com/Xilinx/Vitis-AI/tree/master/models/AI-Model-Zoo#performance-on-vck190)\n1. [地平线 MAPS](https://new.qq.com/omn/20220127/20220127A06JD900.html)\n1. [中信车载芯片调研](https://img3.gelonghui.com/pdf/bc70f-43194a12-e915-4416-9ffa-2e241931ae61.pdf)\n1. [亿欧网调研](https://pdf.dfcfw.com/pdf/H3_AP202112301537477959_1.pdf?1640871632000.pdf)\n1. [艾瑞咨询-中国智能驾驶行业研究报告](https://pdf.dfcfw.com/pdf/H3_AP202112061533188037_1.pdf?1639041416000.pdf)\n"},{"title":"ChangeLog","url":"/2022/04/08/ChangeLog/","content":"\n## links\n\n1. [如何维护更新日志](https://keepachangelog.com/zh-CN/1.0.0/)\n1. [mmsegmentation](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/changelog.md)\n1. [glog](https://github.com/google/glog/blob/master/ChangeLog)\n"},{"title":"clang-tidy","url":"/2022/04/07/clang-tidy/","content":"\n## [clang-tidy AST 静态检查工具](https://clang.llvm.org/extra/clang-tidy/)\n\n## basic\n\n1. 见 cpp-debug.md lint\n1. 需要 compile_commands.json\n\n## [C++静态检查工具总结](https://blog.csdn.net/u013377887/article/details/108651945)\n","tags":["Tools","Clang Tidy"],"categories":["Tools","Clang"]},{"title":"gtest","url":"/2022/04/02/gtest/","content":"\n## basic\n\n1. [doc](https://google.github.io/googletest/primer.html)\n1. [断言列表](https://google.github.io/googletest/reference/assertions.html)\n1. opencv 源码 PERF_TEST\n1. `test_exe --gtest_filter=TestSuite.TestName` 单独运行某个测试，当测试失败时，GoogleTest 允许您单独运行它以进行快速调试。\n1. 断言是成对的，测试同一件事，但对当前函数有不同的影响。\n   - ASSERT\\_ \\*版本在失败时会生成致命错误，并中止当前函数。\n   - EXPECT\\_ \\*版本生成非致命性故障，不会中止当前函数。\n   - 通常优先使用 EXPECT* \\*，因为它们允许在测试中报告多个故障。但是，如果失败时函数继续运行没有意义，则应使用 ASSERT* \\*。\n1. gtest_main.a 有什么用？可以不用写自己的 main 函数，链接 libgtest_main.a 就可以了。\n1. `add_test(NAME gtests COMMAND gtests --gtest_output=xml:report.xml)`\n1. CMakeLists.txt\n\n```sh\nfind_package(GTest REQUIRED)\ninclude_directories(${GTEST_INCLUDE_DIRS})\ntarget_link_libraries(xxx ${GTEST_LIBRARIES})\n```\n\n## NOTE\n\n1. `enable_testing()` 需要在第一级 CMakeLists.txt 里，否则`make test`不生效\n1. `ctest -N && ctest -VV --test-dir build`\n1. `make test`: gtest 想要被 make test 发现需要[link](https://google.github.io/googletest/quickstart-cmake.html)\n1. `Testing/Temporary/LastTest.log` 为`make test`结果\n\n```cmake\ninclude(GoogleTest)\ngtest_discover_tests(hello_test)\n```\n\n## report (xml json html)\n\n1. gtest 结果 xml 可以显示到 ci pipeline 结果中去 [link](https://docs.gitlab.com/ee/ci/testing/unit_test_reports.html)\n\n```yaml\nstages:\n  - test\n\ncpp:\n  stage: test\n  script:\n    - ./gtxxx --gtest_output=xml:report.xml\nunit_test_reports\n  artifacts:\n    when: always\n    reports:\n      junit:\n        - report.xml\n        - fail.xml\n```\n\n1. ./gtxxx --gtest_output=xml:filename\n\n1. `make test`\n\n```\n  ::testing::GTEST_FLAG(output) = \"xml:test2.xml\";\n  ::testing::GTEST_FLAG(output) = \"json:test2.json\";\n  ::testing::InitGoogleTest(&argc, argv);\n```\n\n1. [to html](https://blog.csdn.net/Neil4/article/details/104484792)\n\n## links\n\n1. [cmake 集成](https://google.github.io/googletest/quickstart-cmake.html)\n1. base\n\n```cmake\ntarget_link_libraries(\n  hello_test\n  GTest::gtest_main\n)\n```\n\n## samples\n\n1. 通用框架: 添加 gtest 不用修改 CMakeLists.txt\n\n```cmake\nenable_testing() # 需要在第一级 CMakeLists.txt 里，否则`make test`不生效\n\nset(gtest-source-pattern \"gtest/*.cpp\")\nfile(GLOB gtest-sources ${gtest-source-pattern})\n\nadd_executable(gtests)\ntarget_sources(gtests PRIVATE ${gtest-sources})\ntarget_include_directories(gtests\n                           PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/gtest/)\ntarget_link_libraries(gtests PRIVATE gtest ${COMMON_LIB})\n\nadd_test(fcw::gtests gtests)\n\n```\n\n1. main, 测试可以分散在不同文件，只有一个 main\n\n```c++\n#include \"gtest/gtest.h\"\n\nint main(int argc, char** argv) {\n  ::testing::GTEST_FLAG(output) = \"xml:report.xml\";\n  // ::testing::GTEST_FLAG(output) = \"json:test2.json\";\n  ::testing::InitGoogleTest(&argc, argv);\n  return RUN_ALL_TESTS();\n}\n```\n\n### yaml gtest CMakeLists.txt\n\n1. NOTE: use pattern\n\n```cmake\nfind_package(Threads REQUIRED)\n\nset(gtest_force_shared_crt ON CACHE BOOL \"\" FORCE)\nset(BUILD_MOCK ON CACHE BOOL \"\" FORCE)\nset(CMAKE_POLICY_DEFAULT_CMP0048 NEW)\n\nadd_subdirectory(\n  \"${CMAKE_CURRENT_SOURCE_DIR}/gtest-1.11.0\"\n  \"${CMAKE_CURRENT_BINARY_DIR}/prefix\")\n\ninclude_directories(SYSTEM \"${CMAKE_CURRENT_SOURCE_DIR}/gtest-1.11.0/googletest/include\")\n\nset(test-new-api-pattern \"new-api/*.cpp\")\nset(test-source-pattern \"*.cpp\" \"integration/*.cpp\" \"node/*.cpp\")\nif (CMAKE_VERSION VERSION_GREATER 3.11)\n  list(INSERT test-new-api-pattern 0 CONFIGURE_DEPENDS)\n  list(INSERT test-source-pattern 0 CONFIGURE_DEPENDS)\nendif()\n\nfile(GLOB test-new-api-sources ${test-new-api-pattern})\nfile(GLOB test-sources ${test-source-pattern})\n\nadd_executable(yaml-cpp-tests \"\")\ntarget_sources(yaml-cpp-tests\n  PRIVATE\n    ${test-new-api-sources}\n    ${test-sources})\ntarget_include_directories(yaml-cpp-tests\n  PRIVATE\n    ${CMAKE_CURRENT_SOURCE_DIR}/integration\n    ${CMAKE_CURRENT_SOURCE_DIR}\n    ${PROJECT_SOURCE_DIR}/src)\ntarget_compile_options(yaml-cpp-tests\n  PRIVATE\n    $<$<CXX_COMPILER_ID:Clang>:-Wno-c99-extensions -Wno-variadic-macros -Wno-sign-compare>\n    $<$<CXX_COMPILER_ID:GNU>:-Wno-variadic-macros -Wno-sign-compare>)\ntarget_link_libraries(yaml-cpp-tests\n  PRIVATE\n    Threads::Threads\n    yaml-cpp\n    gmock)\n\nset_property(TARGET yaml-cpp-tests PROPERTY CXX_STANDARD_REQUIRED ON)\nif (NOT DEFINED CMAKE_CXX_STANDARD)\n  set_target_properties(yaml-cpp-tests PROPERTIES CXX_STANDARD 11)\nendif()\n\n\nadd_test(yaml-cpp::test yaml-cpp-tests)\n\nif (build-windows-dll)\n  add_custom_command(\n    TARGET yaml-cpp-tests\n    POST_BUILD COMMAND ${CMAKE_COMMAND} -E\n    copy_if_different \"$<TARGET_FILE:yaml-cpp>\" \"$<TARGET_FILE_DIR:yaml-cpp-tests>\")\nendif()\n\n```\n\n## links\n\n1. [doctest](https://github.com/doctest/doctest)\n","tags":["Cpp"],"categories":["Program","Cpp"]},{"title":"自动驾驶专业术语","url":"/2022/03/31/自动驾驶专业术语/","content":"\n## [缩写](https://blog.csdn.net/LEON1741/article/details/89510034)\n\n1. `OTA`: over the air\n1. `FCW`: forward collision warning 前车碰撞预警\n1. `TTC`: time to collision 碰撞时间\n1. `NVS`: Night Vision System 夜视系统\n1. `NOA` “自动辅助导航驾驶（Navigate on Autopilot）”的中文翻译也有人叫“按导航辅助驾驶”或“领航辅助功能”，本质意思是，把“导航”和“辅助驾驶”结合。在原来 L2 辅助驾驶的基础上（如车道线保持、自动跟车），加上车机的导航信息（如百度地图），自动变道，实现从 A 点到 B 点的自动驾驶。\n1. `OEM` 又称主机厂，OEM 是英文 Original equipment manufacturer 的缩写。\n1. `DCU` (Domain Control Unit）域控制器\n1. `ECU`（Electronic Control Unit）电子控制单元\n1. `V2X`（Vehicle to everything）\n1. `TSN`（Time-Sensitive Network）\n1. `AUTOSAR`（AUTomotive Open System Architecture）\n1. `EEA` (Electrical/Electronic Architecture) 电子电气架构\n1. `SOME/IP` Scalable service-Oriented Middleware over IP。即“运行于 IP 之上的可伸缩的面向服务的中间件”\n1. `AUTOSAR` (AUTomotive Open System ARchitecture)\n1. `DDS` (Data Distribution Service,数据分发服务)\n1. `RTPS` (Real-Time Publish Subscribe)\n1. `FOV` (field of view) 视场; 在光学仪器或传感器的情况下，视场是检测器对电磁辐射敏感的立体角 [link](https://blog.csdn.net/weixin_33358099/article/details/112102463?spm=1001.2101.3001.6650.6&utm_medium=distribut)\n\n## 其他\n\n1. 标定：\n\n## Links\n\n1. [汽车软件研报](https://pdf.dfcfw.com/pdf/H3_AP202202221548462770_1.pdf?1645518929000.pdf)\n1. [华为 MDC 智能驾驶计算平台白皮书](https://www-file.huawei.com/-/media/corporate/pdf/news/intelligent-driving-computing-platform-whitepaper-new.pdf?la=zh)\n1. [软件定义汽车（2）-软件中间件（Autosar 为例）](https://zhuanlan.zhihu.com/p/261291971)\n1. [国内主机整车 EEA 架构汇总](https://zhuanlan.zhihu.com/p/561699068)\n1. [深度分析汽车芯片的现状与发展（一）汽车电子电气架构的变革](https://zhuanlan.zhihu.com/p/460683077)\n1. [深度分析汽车芯片的现状与发展（二）域控制器时代](https://zhuanlan.zhihu.com/p/461714692)\n1. [深度分析汽车芯片的现状与发展（三）ADAS/AD 域控制器及芯片平台分析](https://zhuanlan.zhihu.com/p/462675111)\n1. [深度分析汽车芯片的现状与发展（四）整车控制域](https://zhuanlan.zhihu.com/p/464930132)\n"},{"title":"eigen","url":"/2022/03/26/eigen/","content":"\n## basic\n\n1. gdb 调试 [printer](https://gitlab.com/libeigen/eigen/-/blob/master/debug/gdb/printers.py)\n1. vector 转 tensor [link](https://stackoverflow.com/a/44664806) [link1](https://blog.csdn.net/weareu/article/details/86486682)\n1. Tensor 转 vector\n\n```c++\ntemplate <typename Tensor> static std::vector<float> run(const Tensor &tensor) {\n  typedef typename Eigen::internal::remove_const<typename Tensor::Scalar>::type\n      Scalar;\n  typedef typename Tensor::Index Index;\n  const Index total_size = Eigen::internal::array_prod(tensor.dimensions());\n  std::vector<float> result;\n\n  if (total_size > 0) {\n    Eigen::Map<const Eigen::Array<Scalar, Eigen::Dynamic, 1>> array(\n        const_cast<Scalar *>(tensor.data()), total_size);\n    std::cout << array[0] << std::endl;\n    for (auto i = 0u; i < total_size; ++i) {\n      result.push_back(array[i]);\n    }\n\n    // os << array;                                                                                                                                         }\n  return result;\n};\n```\n\n1. 头文件和编译选项\n\n```\n#include <eigen3/Eigen/Eigen>\nfind_package(Eigen3 REQUIRED)\n```\n\n1. 矩阵默认[按列存储](https://eigen.tuxfamily.org/dox/group__TopicStorageOrders.html)， 按行需要指定\n\n```c++\n  Eigen::Matrix<double, 4, 4, Eigen::RowMajor> cam_intra;\n```\n\n1. 常用数据结构：\n\n   - Vector3d: `auto col = Eigen::Vector3d(0, 0, 0);`\n   - RowVector4d: `auto row = Eigen::RowVector4d(0, 0, 0, 1);`\n   - MatrixXd: `Eigen::MatrixXd tmp(3, 4);`\n   - Matrix: `Eigen::Matrix<double, 4, 4, Eigen::RowMajor> cam_intra;`\n   - Map: `auto T_cab2cam = Eigen::Map<Eigen::Vector3d>(std_vector.data());`\n\n1. 矩阵相乘： `A * B`\n1. 矩阵求逆： `A.inverse()`\n1. [矩阵取列：](https://eigen.tuxfamily.org/dox/group__TutorialBlockOperations.html) `A.col()`\n1. 矩阵取行： `A.row()`\n\n## basci module\n\n1. Matrix init\n\n```c++\nEigen::Matrix3d R_e2cab;\n  R_e2cab << 0, 0, -1,  //\n      0, 1, 0,          //\n      1, 0, 0;          //\n\n```\n\n1. std::vecotr to Eigen::Vecotr or Eigen::Matrix\n\n```\nauto R_cab2cam_tmp = Eigen::Map<Eigen::RowVector3d>(v_R_cab2cam.data());\nauto cam_k = Eigen::Map<Eigen::Matrix<double, 3, 3, Eigen::RowMajor>>(v_cam_k.data());\n\n```\n\n1. Eigen::Vecotr or Eigen::Matrix to std::vector\n\n```c++\n\n std::vector<double> vec_cam_intra(\n      cam_intra.data(), cam_intra.data() + cam_intra.rows() * cam_intra.cols());\n```\n\n1. Matrix concat\n\n```c++\nEigen::MatrixXd cab2cam_tmp(3, 4);\nauto col_tmp = Eigen::RowVector4d(0, 0, 0, 1);\nEigen::MatrixXd cab2cam(4, 4);\ncab2cam << cab2cam_tmp, col_tmp;\n\nEigen::MatrixXd cab2cam_tmp(4, 3);\nauto col_tmp = Eigen::Vector3d(0, 0, 1);\nEigen::MatrixXd cab2cam(4, 4);\ncab2cam << cab2cam_tmp, col_tmp;\n\n```\n\n1. Matrix slice ([block](https://eigen.tuxfamily.org/dox/group__TutorialBlockOperations.html))\n\n```c++\nEigen::Matrix<double, 4, 4, Eigen::RowMajor> cam2ego;\nEigen::Matrix<double, 3, 3, Eigen::RowMajor> R_cam2ego = cam2ego.block(0, 0, 3, 3);\nEigen::Vector3d T_cam2ego = cam2ego.block(0, 3, 3, 1);\n\n```\n\n## links\n\n1. [gitlab repo](https://gitlab.com/libeigen/eigen.git)\n1. [getting started](https://eigen.tuxfamily.org/dox/GettingStarted.html)\n\n## sample\n\n```c++\n#include <glog/logging.h>\n#include <yaml-cpp/yaml.h>\n\n#include <eigen3/Eigen/Eigen>\n#include <fstream>\n#include <iostream>\n#include <vector>\n\nvoid GetPgCamParm(const std::string camera_param, const std::string cali_param,\n                  const std::string result_param) {\n  YAML::Node cam_info = YAML::LoadFile(camera_param);\n  YAML::Node cb_xyz = YAML::LoadFile(cali_param);\n\n  // cam_k\n  std::vector<double> v_cam_k;\n  if (cam_info[\"mtx\"][\"data\"]) {\n    v_cam_k = cam_info[\"mtx\"][\"data\"].as<std::vector<double>>();\n  } else {\n    LOG(FATAL) << \"Can't get cam_info[\\\"mtx\\\"][\\\"data\\\"]\";\n  }\n  auto cam_k =\n      Eigen::Map<Eigen::Matrix<double, 3, 3, Eigen::RowMajor>>(v_cam_k.data());\n  VLOG(2) << \"\\ncam_k:\\n\" << cam_k;\n\n  // cam_intra\n  auto col = Eigen::Vector3d(0, 0, 0);\n  Eigen::MatrixXd tmp(3, 4);\n  tmp << cam_k, col;\n  auto row = Eigen::RowVector4d(0, 0, 0, 1);\n  Eigen::Matrix<double, 4, 4, Eigen::RowMajor> cam_intra;\n  cam_intra << tmp, row;\n  VLOG(2) << \"\\ncam_intra:\\n\" << cam_intra;\n\n  // R_e2cab\n  Eigen::Matrix3d R_e2cab;\n  R_e2cab << 0, 0, -1,  //\n      0, 1, 0,          //\n      1, 0, 0;          //\n  VLOG(2) << \"\\nR_e2cab:\\n\" << R_e2cab;\n\n  // T_e2cab\n  Eigen::Vector3d T_e2cab;\n  if (cb_xyz[\"boarad2ego\"]) {\n    T_e2cab << cb_xyz[\"boarad2ego\"]['x'].as<double>(),\n        cb_xyz[\"boarad2ego\"]['y'].as<double>(),\n        cb_xyz[\"boarad2ego\"]['z'].as<double>();\n  } else {\n    LOG(FATAL) << \"Can't get cb_xyz[\\\"boarad2ego\\\"]\";\n  }\n  VLOG(2) << \"\\nT_e2cab:\\n\" << T_e2cab;\n\n  // ego2cab\n  Eigen::MatrixXd ego2cab_tmp(3, 4);\n  ego2cab_tmp << R_e2cab, T_e2cab;\n  VLOG(2) << \"\\nego2cab_tmp:\\n\" << ego2cab_tmp;\n\n  // R_cab2cam\n  std::vector<double> v_R_cab2cam;\n  if (cam_info[\"rvec\"][\"data\"]) {\n    v_R_cab2cam = cam_info[\"rvec\"][\"data\"].as<std::vector<double>>();\n  } else {\n    LOG(FATAL) << \"Can't get cam_info[\\\"rvec\\\"][\\\"data\\\"]\";\n  }\n  auto R_cab2cam_tmp = Eigen::Map<Eigen::RowVector3d>(v_R_cab2cam.data());\n  double n_norm = R_cab2cam_tmp.norm();\n  Eigen::AngleAxisd rotation_vector(n_norm, R_cab2cam_tmp / n_norm);\n  Eigen::Matrix3d R_cab2cam = rotation_vector.toRotationMatrix();\n  VLOG(2) << \"\\nR_cab2cam:\\n\" << R_cab2cam;\n\n  // T_cab2cam\n  std::vector<double> v_T_cab2cam;\n  if (cam_info[\"tvec\"][\"data\"]) {\n    v_T_cab2cam = cam_info[\"tvec\"][\"data\"].as<std::vector<double>>();\n  } else {\n    LOG(FATAL) << \"Can't get cam_info[\\\"tvec\\\"][\\\"data\\\"]\";\n  }\n  auto T_cab2cam = Eigen::Map<Eigen::Vector3d>(v_T_cab2cam.data());\n  VLOG(2) << \"\\nT_cab2cam:\\n\" << T_cab2cam;\n\n  // cab2cam\n  Eigen::MatrixXd cab2cam_tmp(3, 4);\n  cab2cam_tmp << R_cab2cam, T_cab2cam;\n  auto col_tmp = Eigen::RowVector4d(0, 0, 0, 1);\n  Eigen::MatrixXd cab2cam(4, 4);\n  cab2cam << cab2cam_tmp, col_tmp;\n  VLOG(2) << \"\\ncab2cam:\\n\" << cab2cam;\n\n  // ego2cab\n  Eigen::MatrixXd ego2cab(4, 4);\n  ego2cab << ego2cab_tmp, col_tmp;\n  VLOG(2) << \"\\nego2cab:\\n\" << ego2cab;\n\n  // cam2ego\n  Eigen::MatrixXd cam2ego = (cab2cam * ego2cab).inverse();\n  VLOG(2) << \"\\ncam2ego:\\n\" << cam2ego;\n  LOG(INFO) << \"\\ncam2ego row:\\n\" << cam2ego.row(0);\n  LOG(INFO) << \"\\ncam2ego row:\\n\" << cam2ego.col(0);\n\n  // get results\n  LOG(INFO) << \"\\nintrinsic: \\n\" << cam_intra;\n  std::vector<double> vec_cam_intra(\n      cam_intra.data(), cam_intra.data() + cam_intra.rows() * cam_intra.cols());\n\n  Eigen::Matrix<double, 3, 3, Eigen::RowMajor> R_cam2ego =\n      cam2ego.block(0, 0, 3, 3);\n  LOG(INFO) << \"\\nR_cam2ego: \\n\" << R_cam2ego;\n  std::vector<double> vec_R_cam2ego(\n      R_cam2ego.data(), R_cam2ego.data() + R_cam2ego.rows() * R_cam2ego.cols());\n\n  Eigen::Vector3d T_cam2ego = cam2ego.block(0, 3, 3, 1);\n  LOG(INFO) << \"\\nT_cam2ego: \\n\" << T_cam2ego;\n  std::vector<double> vec_T_cam2ego(\n      T_cam2ego.data(), T_cam2ego.data() + T_cam2ego.rows() * T_cam2ego.cols());\n\n  // save result to yaml\n  YAML::Node config;\n  // test[\"camera\"][\"v_cam_k\"].push_back(v);\n  // note: push_back and = are not same\n  config[\"camera\"][\"intrinsic\"] = vec_cam_intra;\n  config[\"camera\"][\"cam2ego_R\"] = vec_R_cam2ego;\n  config[\"camera\"][\"cam2ego_t\"] = vec_T_cam2ego;\n  std::ofstream fout(result_param);\n  fout << config;\n}\n\nint main(int argc, char *argv[]) {\n  GetPgCamParm(\"camera_params.yaml\", \"cali_board2vehicle_xyz.yaml\",\n               \"pg_cam2_param.yaml\");\n  return 0;\n}\n\n```\n","tags":["Eigen"],"categories":["Program"]},{"title":"yaml","url":"/2022/03/26/yaml/","content":"\n## opencv 对数\n\n1. python\n\n```python\n`cv2.FileStorage(\"output.yml\", cv2.FILE_STORAGE_WRITE).write(\"mat\", mat)`\n\n\nfs = cv2.FileStorage(\"predict_benz_trajectory_v2_crop.yml\", cv2.FILE_STORAGE_WRITE)\nfs.write(\"Q\", Q.astype(np.float32))\nfs.write(\"traj_out_wid\", traj_out_wid)\nfs.release()\n```\n\n2. C++\n\n```c++\ncv::FileStorage ymlfile(\"predict_benz_trajectory_v2_crop.yml\",                                                                                                                                                                                                                          cv::FileStorage::READ);\nif(!fs.isOpened()){\n        std::cout << \"Cannot open config file!\" << std::endl;\n        return -1;\n}\ncv::Mat disparity;\nymlfile[\"disparity\"] >> disparity;\nfloat traj_wid = (float)ymlfile[\"ground_traj_wid\"];\n\n\n  cv::Mat Q_mat;\n  cv::FileNode node = ymlfile[\"Q\"];\n  if (node.empty()) {\n    std::cerr << \"Key not found in yaml file\" << std::endl;\n  }\n  if (node.type() == cv::FileNode::MAP && node.size() > 0) {\n    node >> Q_mat;\n    if (Q_mat.type() == CV_64F) {\n      Q_mat.convertTo(Q_mat, CV_32F);\n    }\n  } else {\n    std::cerr << \"Node is not an opencv mat  or is empty\" << std::endl;\n  }\n\n```\n\n3. numpy and vector\n\n```\nimport cv2\nimport numpy as np\nnp_array = np.array([[1, 2, 3], [4, 5, 6]], dtype='float32')\nfs = cv2.FileStorage('output.xml', cv2.FileStorage_WRITE)\nfs.write('my_array', np_array)\nfs.release()\n```\n\n```\nstd::vector<float> vec;\nvec.assign((float*)mat.datastart, (float*)mat.dataend);\n```\n\n### 读写\n\n1. yaml-cpp\n1. [opencv cv::FileStorage](https://blog.csdn.net/learning_tortosie/article/details/97815514)\n\n### Links\n\n1. [github](https://github.com/jbeder/yaml-cpp)\n1. [tutorial](https://github.com/jbeder/yaml-cpp/wiki/Tutorial)\n\n### sample\n\n```c++\n#include <yaml-cpp/yaml.h>\n\n#include <fstream>\n#include <iostream>\n#include <vector>\n\nint main(int argc, char *argv[]) {\n\n\n  YAML::Node config = YAML::LoadFile(\"config.yaml\");\n\n  std::vector<double> intrinsic;\n  if (config[\"camera\"][\"intrinsic\"]) {\n    intrinsic = config[\"camera\"][\"intrinsic\"].as<std::vector<double>>();\n    for (auto i = 0u; i < intrinsic.size(); ++i) {\n      std::cout << intrinsic[i] << std::endl;\n    }\n  }\n\n  std::vector<double> cam2ego_R;\n  if (config[\"camera\"][\"cam2ego_R\"]) {\n    cam2ego_R = config[\"camera\"][\"cam2ego_R\"].as<std::vector<double>>();\n    for (auto i = 0u; i < cam2ego_R.size(); ++i) {\n      std::cout << cam2ego_R[i] << std::endl;\n    }\n  }\n\n  std::vector<double> cam2ego_t;\n  if (config[\"camera\"][\"cam2ego_t\"]) {\n    cam2ego_t = config[\"camera\"][\"cam2ego_t\"].as<std::vector<double>>();\n    for (auto i = 0u; i < cam2ego_t.size(); ++i) {\n      std::cout << cam2ego_t[i] << std::endl;\n    }\n  }\n\n  std::ofstream fout(\"./out.yaml\");\n  fout << config;\n\n  YAML::Node test;\n  // test[\"camera\"][\"intrinsic\"].push_back(v);\n  // note: push_back and = are not same\n  test[\"camera\"][\"intrinsic\"] = intrinsic;\n  test[\"camera\"][\"cam2ego_R\"] = cam2ego_R;\n  test[\"camera\"][\"cam2ego_t\"] = cam2ego_t;\n  std::ofstream fout1(\"./test.yaml\");\n  fout1 << test;\n  return 0;\n}\n\n```\n\n```yaml\ncamera:\n  intrinsic:\n    - 1888.7618313608834\n    - 0\n    - 968.83548164132105\n    - 0\n    - 0\n    - 1896.5639945248038\n    - 547.4612962459845\n    - 0\n    - 0\n    - 0\n    - 1\n    - 0\n    - 0\n    - 0\n    - 0\n    - 1\n  cam2ego_R:\n    - 0.034896920649101944\n    - 0.015135626613007833\n    - 0.99927629698862219\n    - -0.99939075949347744\n    - 0.0010902662284327418\n    - 0.034884404167777945\n    - -0.00056147988681672133\n    - -0.99988485586850695\n    - 0.015164452300128389\n  cam2ego_t:\n    - 1.84679730821996\n    - -0.089090448031248784\n    - 1.3980633087528846\n```\n","tags":["YAML"],"categories":["Program"]},{"title":"samba","url":"/2022/03/18/samba/","content":"\n## install\n\n```sh\nsudo apt-get install samba\n\n```\n\n## config\n\n```\nsudo vim /etc/samba/smb.conf\n\n[share]\n     path = /home/jiaxiyang/share\n     available = yes\n     valid users = jiaxiyang\n     read only = no\n     browsable = yes\n     public = yes\n     writable = yes\n```\n\n## add user\n\n```\nsudo smbpasswd -a jiaxiyang\n```\n\n## restart\n\n### ubuntu\n\n```\nsudo /etc/init.d/samba restart\nsudo /etc/init.d/smbd restart\n```\n\n### centos\n\n```\nsudo systemctl start smb.service\nsudo systemctl restart smb\n```\n\n## link\n\n### ubuntu\n\n```\nsudo mount.cifs //10.10.0.61/jiaxiyang/ samba_16 -o user=jiaxiyang\n```\n\n### windows\n\n```\n映射网络驱动器\n\\\\ipaddr\\share\nNOTE: share is [share] in smb.conf\n```\n"},{"title":"Cuda","url":"/2022/03/10/Cuda/","content":"\n## [install](https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=deb_network)\n1. command\n\n``` shell\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\nsudo apt-get update\nsudo apt-get -y install cuda-toolkit-12-x\nsudo apt-get install -y nvidia-open\n```\n\n## Note\n1. Scale Up 侧重加速单节点/单集群内部的极限性能，Scale Out 侧重扩展系统整体的处理能力。\n1. Scale-Up 专注于在有限范围内挖掘单节点的极致性能，适用于需要低延迟、高带宽的高强度计算；Scale-Out 则致力于通过增减节点来适应更大规模的任务，适用于任务可分布、对弹性和扩展性要求高的应用场景。\n1. 1维用int, 多维度用dim3\n\n``` cuda\nint numBlocks = 1;\ndim3 threadsPerBlock(N, N);\nMatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);\n```\n\n1. [Execution Configuration](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#execution-configuration)\n   - <<< >>> 叫做 kernel launch configuration（核函数启动配置）, launch参数：<<<gridDim, blockDim, sharedMemSize, stream>>>  CUDA核函数运行参数\n   - <<< Dg, Db, Ns, S >>>\n1. [built-in-variables](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#built-in-variables)\n   - gridDim\n   - blockIdx\n   - blockDim\n   - threadIdx\n   - warpSize\n1. 在 CUDA 编程中，CTA（Cooperative Thread Array） 是 线程块（Thread Block） 的另一种称呼。\n1. Block and Tile\n   - Block (线程块) 是CUDA线程层次结构中的一部分，位于网格（grid） 之下。它是CUDA中的一个基本执行单元，由多个线程（thread） 组成，每个线程在一个SM（Streaming Multiprocessor）上执行。\n   - Tile（数据分片） 通常是手动优化时的计算划分方式，用于块状处理数据（类似分块矩阵计算）。它不属于CUDA的核心执行模型，而是开发者用来优化CUDA程序时的概念。\n   - Block是线程的概念，Tile是数据划分的概念; Tile数据并不等于Block处理的数据，如矩阵运算是，A矩阵划分为Tile: bm x bk, Block对应的C数据为bm x bn; Block也可能处理多个tile的数据\n1. block swizzle:\n   - runtime按idx线性调度：0, 1, 2....\n   - 用户自己将idx映射到要处理的block\n1. `CUDA_LAUNCH_BLOCKING=1` 有错误立即停下来，效率会差一点，主要用于调试\n1. [kernel utils](https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/detail/KernelUtils.h) 简化写 kernel 流程\n1. [手动释放显存](https://zhuanlan.zhihu.com/p/480719273)\n\n```shell\napt-get install psmisc\nfuser -v /dev/nvidia*\nkill -9 ***(PID)\nkillall python\n```\n\n1. fp32, tf32, fp16, bf16, fp8\n   ![data type](https://i.ibb.co/vdTGHkJ/txy-EFVOs5-P.png)\n1. NVIDIA GPU 的缓存行（cache line）大小一般是 128 字节。\n1. cache line 又分为 4 个 sectors, 1 个 sector 32B\n1. sector 是 global memory 访问的最小单位，32 个线程一起运行，最少访问 32B\n1. gridDim 划分的是数据，blockDim 划分的是线程：例如: sgemm 分块矩阵乘\n\n```c++\nN=M=1024;\nBN=BM=128, TN=TB=8;\n    dim3 blockDim(BN / TN, BM / TM);  // block 按线程分为 16x16 thread,threadDim=(16, 16) 每个线程处理8x8数据; x维度是列，y维度是行;\ndim3 gridDim((N + BN - 1) / BN, (M + BM - 1) / BM); // grid 按数据分为 8x8 block, blockDim=(8, 8)\n```\n\n1. `GPU通过计算而不是深度缓存层次结构来隐藏访存延迟`\n   - 当数据访存的时候，就让 warp stall，而后再选一个 warp 进行计算，通过这种方式交错开计算和访存，让访存单元一直忙碌，带宽打满。计算延迟的时候也会让 warp stall\n1. 并发的 warp 切换没有开销，因为每个 warp 都分配了硬件资源\n   - 不需要上下文切换\n   - 以空间换时间\n1. 假如 GPU 有 108 个 SM。每个 SM 可以并行处理多个线程块，具体取决于所使用的内核；为了获得最佳并行化，隐式 GEMM 应包含 108 个图块的整数倍。\n1. gpu sram 带宽为什么比主存大很多：\n   - 并行，多个 SM 访问 shared memory， 如有 80 个 sm，每个 sm4 个 partition， 每个 shared memory 有 32 个 bank， 等效位宽为：`80*4*32*8 = 81920`, hbm 位宽为 8192， 带宽为 1.5TB/s, sram 为 19TB/s\n   - shared memory 带宽：`2(?) * freq * 32(banks) * 8/8 *4(sm partion) * 80(sm num)`\n1. `kernel融合主要是使用sram来减少对显存的访问，注意不是device和host数据搬移` 也会减少调度开销\n1. kernel 在编译的时候需要明确 block grid size 吗？不需要， 这些参数通常在运行时通过 CUDA 内核启动语法指定，这提供了更高的灵活性和动态调整的可能性。\n1. nvcc 在翻译单元的顶部隐式包含了 cuda_runtime.h 。\n1. 核函数 K（kernel function）就是指 K(x, y) = ，其中 x 和 y 是 n 维的输入值，f(·) 是从 n 维到 m 维的映射（通常而言，m>>n）。是 x 和 y 的内积（inner product），严格来说 应该叫欧式空间的标准内积，也就是很多人常说的点积（dot product）。\n1. `sudo update-alternatives --display cuda`显示系统 cuda 版本\n1. `export PATH=/usr/local/cuda/bin/:${PATH}`找不到 nvcc 可能需要 export PATH\n1. 在 CUDA 中，你会以类似于 C/C++函数的形式来表达想要在 GPU 上运行的计算，这个函数被称为 kernel。\n1. GPU 函数耗时统计不能只记录一次的，GPU 可能做一些准备工作，教训： nppiResize_8u_C3R 不管大小第一次运行耗时都很大 The cuda context is lazily initialized\n1. `autotuning` 搜索 kernel grid 划分参数(结果不变)，找性能最优\n1. L1 和 shared memory 共享一块存储 可动态分配比例; 可用比例见[link](https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf)\n   - 多用 shared memory 就多分点给 shared memory 多用寄存器就多分点给 L1 cache\n1. warp 调度:延时隐藏 只执行 warp 一部分 当 warp 需要等待时 先执行其他 warp\n1. shared memory 和 register 是 SM 中的稀缺资源.\n1. warp 执行类似 simd\n1. [Hardware Model](https://docs.nvidia.com/nsight-compute/2023.3/ProfilingGuide/index.html#metrics-hw-model)\n   - 对 sm 介绍比较好\n   - Each SM is partitioned into four processing blocks, called SM sub partitions.\n   - 一个 SM 又可以由若干个 SMP（SM Partition）组成\n   - A warp is allocated to a sub partition and resides on the sub partition from launch to completion. warp 被分配给子分区，并且从启动到完成都驻留在子分区上。\n1. `FMA (Fused Multiply Add)`: z=a\\*x+y …z,x,y are vectors or scalars\n1. `4FMA (Quad FMA)`: z=A\\*x+z …A is a FP32 matrix; x,z are vectors\n1. `WMMA: Warp-level Matrix Mulitply and Accumulate (Tensor Core)`: Z=AB+C …A,B are FP16 matrices; Z,C are FP32\n1. global memory -> shared memory 时，计算每个线程需要搬移的数据量。假如每个线程要搬移 A，B 矩阵为 4 个 single float point, 4x4 = 16B\n   - A_tile（128,8): 256 个线程， 每个线程搬移 4 个数，每行由 2 个线程处理， 假如线程一维索引为 tid； 行 m = tid / 2 = tid >> 1; 列 k = (tid % 2) x 4 = (tid & 1) << 2\n   - B_tile（8,128): 256 个线程， 每个线程搬移 4 个数，每行由 32 个线程处理， 假如线程一维索引为 tid； 行 k = tid / 32 = tid >> 5; 列 n = (tid % 32) x 4 = (tid & 31) << 2\n1. load/store 关键点在行列索引(分别计算各 tile 维度的 m, n, k)， 通过行列可以找到起始地址。`#define OFFSET(row, col, ld) ((row) * (ld) + (col))`\n1. load/store memory 的时候先通过行列坐标找到 global 起始点，然后再算偏移，用宏定义来访问矩阵，先找最外层的 tile，再找里层的 tile。\n1. sgemm 分块 A 矩阵 global 到 shared load 时，shared memory 需要列优先排列，一次 load 四个数都寄存器，然后再分别赋值给列。\n1. shred memory double buffering: 在 for 循环里只用 sync 一次\n   - load tile[0] 到 buffer[0]; sync\n   - for: load tile[i]; compute tile[i-1]; sync\n   - comute tile[-1]; sync\n1. wave 的概念：wave 表示 GPU 上同时执行的 thread block。例如一个 kernel 中 thread block 为 256 线程，每个线程使用了 128 个寄存器，那么在 GV100 上每个 SM 可同时执行 2 个 thread block，GV100 共 80 个 SM，一个 wave 就是 160 个 thread block。\n1. cuda 遵循 IEEE 754 standard for binary floating-point representation； 由于融合和乘加， 其结果与分别乘加结果略有不同 [ieee-754-compliance](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#ieee-754-compliance)\n   - One of the key differences is the fused multiply-add (FMA) instruction, which combines multiply-add operations into a single instruction execution. `Its result will often differ slightly from results obtained by doing the two operations separately.`\n1. 每个浮点算术运算都涉及一定量的舍入。因此，算术运算的执行顺序很重要。如果 A、B 和 C 是浮点值，则 (A+B)+C 不能保证等于 A+(B+C)\n1. 我们可以将 CUDA 内核编写为许多短 `__device__` 函数的集合，而不是一个大型的整体 `__global__` 函数；每个设备功能可以在将它们连接在一起之前进行独立测试。\n1. 如果大多数函数都定义为 `__host__ __device__` 而不仅仅是 `__device__` 函数，那么这些函数就可以在 CPU 和 GPU 上进行测试，\n1. `local memory`之所以如此命名，是因为它的作用域是线程本地的，而不是因为它的物理位置。事实上，本地存储器位于片外。因此，访问本地内存与访问全局内存一样昂贵。换句话说，名称中的“本地”一词并不意味着访问速度更快。本地内存仅用于保存自动变量。当 nvcc 编译器确定没有足够的寄存器空间来保存变量时，就会执行此操作。可能放置在本地内存中的自动变量是大型结构或数组，它们会消耗太多寄存器空间，并且编译器确定可以动态索引的数组。\n1. `constent memory`\n1. `The NVIDIA Management Library (NVML)` is a C-based interface that provides direct access to the queries and commands exposed via nvidia-smi intended as a platform for building 3rd-party system management applications.\n1. 可以通过 CUDA_VISIBLE_DEVICES 环境变量重新排列已安装的 CUDA 设备的集合; `CUDA_VISIBLE_DEVICES=0,2,1,3`\n1. Starting with CUDA 11.0, devices of compute capability 8.0 and above have the capability to influence persistence of data in the L2 cache, potentially providing higher bandwidth and lower latency accesses to global memory.\n1. 从 Hopper 开始，CUTLASS 3.0 将 Warp Specialization 的概念纳入了内核设计的一部分。线程块被划分为两组 warp，生产者 warp 组和消费者 warp 组。生产者 warp 组使用新的张量内存加速器（TMA）将数据从全局内存加载到共享内存缓冲区中。[link](https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#warp-specialization)\n1. TMA 可以异步一次 load 大块数据到 shared memory， ampere 一次最多只能 load 128 bit 数据(指令限制)\n1. 想象 shared memory 和 register 是二维的, bank 不冲突\n   - shared memory: `smem[n][32]`\n   - register: `re[n][4]`\n1. 虽然 block 可以划分为二维 thread, 但调度时按一维调度，每 32 个一个 warp\n1. laneID 是 warp 内第几个线程 threadIdx.x %32\n1. WarpID = threadIdx.x /32\n1. 常量内存位于显存中 片上有缓存 类似于 shared memeory warp 内多个线程读同一个地址最快，不同地址要串行\n1. 常量内存对于 kernel 是只读的 对于主机可读写\n1. constant 由主机代码准备 kernel 中直接用\n\n```c++\n__constant__ float coef[100];\nvoid setup_coef_constant(void){\n    cudaMemorycpyToSymbol(coef, h_coef, size);\n}\n```\n\n1. kernel 第三个参数是动态共享内存的大小\n\n## [TMA](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-data-copies-using-the-tensor-memory-accelerator-tma)\n1. The primary goal of TMA is to provide an efficient data transfer mechanism from global memory to shared memory for multi-dimensional arrays. TMA 的主要目标是为多维数组提供一种从全局内存到共享内存的高效数据传输机制。\n1. TMA 根据tensor_map和coord等信息产生ptr，用于搬数\n1. 批量异步复制操作的源和目标地址可以位于共享或全局内存中。这些操作可以将数据从全局内存读取到共享内存，将数据从共享内存写入全局内存，还可以从共享内存复制到同一集群中另一个块的分布式共享内存 。此外，在cluster中时，可以将批量异步操作指定为多播(multicast)。在这种情况下，数据可以从全局内存传输到cluster内多个块的共享内存。\n1. mbarrier（内存屏障）是 CUDA 12 引入的新机制，用于 跨线程块（CTA） 进行同步，主要用于 生产者-消费者（producer-consumer）模式的通信。\n   - 可以多个SM\n   - 支持多个线程块（CTA）之间的同步，比 __syncthreads() 更强大。\n   - 允许 生产者线程（Producer） 和 消费者线程（Consumer） 在不同的 CTA 中进行数据交换和同步。\n1. Bulk Async Group 是 CUDA 12.2 引入的 批量任务管理机制，允许在 SM（流式多处理器） 内进行任务分配，同时异步调度多个 warp 或 CTA 执行任务.\n- 单个SM内部\n1. tensor_map创建完在host memory上 [link](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-data-copies-using-the-tensor-memory-accelerator-tma:~:text=Host%2Dto%2Ddevice%20transfer.)\n   - tensor_map中的tensor_ptr需要指向device global memory上\n\n## [driver](https://docs.nvidia.com/cuda/cuda-driver-api/index.html)\n\n## runtime\n\n## cuda graph\n\n1. 减少 Launch 开销\n1. [cuda-graphs blog](https://developer.nvidia.com/blog/cuda-graphs/)\n1. trtexec 有参数指定\n\n## 性能\n\n1. 想办法喂饱硬件, 好好调教它，让它努力打工, 工作饱和，不能偷懒\n1. 分析计算部件停顿的原因。\n1. 主要由下列因素决定：\n   - 算力(peak)\n   - 带宽(peak)\n   - 指令和访存延迟\n1. 性能 roofline bound 是满流水线时分析，一直忙; 指令和访存延迟会使得处理器空闲\n1. 一直忙条件：`warp 数量 = 延迟 x 吞吐`, 如：延迟 20 cycle; SM 吞吐为 64 fma 每 cycle；SM warp 数量为： 20 x 64 / 32 = 40\n1. Use peak performance metrics to guide optimization\n1. Optimize your algorithm, then unroll loops\n1. Use template parameters to generate optimal code\n1. bandwidth(带宽)和 throughput(吞吐)区别：\n   - bandwitdh: 理论最大吞吐\n   - throughput: 实际吞吐\n1. [Fundamental Optimizations in CUDA](https://developer.download.nvidia.com/GTC/PDF/1083_Wang.pdf)\n1. kernels are too small -> kernel launch bound; gpu is idle in many time, reason:kernels are too small [link](https://github.com/jiaxiyang/CUDA-PPT/blob/main/GTC2020/s21417-faster-transformer.pdf)\n   - a simple solution: using tensorflow XLA to fuse kernel automaticlly\n1. 在 CUDA 编程中，`谓词替换`通常指的是一种编程技巧，它通过使用谓词（即条件表达式）来代替显式的分支语句（如 if-else 或 switch 语句）。这种技巧可以帮助`减少线程发散`，从而提高在 GPU 上的并行执行效率。使用条件运算符（如? :）或逻辑运算符（如&&和||）来替换 if-else 语句。这样可以保证所有线程执行相同数量的指令，尽管这些指令的实际作用可能因条件而异。\n\n```cuda\n__global__ void traditionalBranch(int *data, int value, int threshold) {\n    int index = threadIdx.x + blockIdx.x * blockDim.x;\n    if (data[index] > threshold) {\n        data[index] = value;\n    }\n}\n\n__global__ void predicateReplacement(int *data, int value, int threshold) {\n    int index = threadIdx.x + blockIdx.x * blockDim.x;\n    data[index] = (data[index] > threshold) * value + !(data[index] > threshold) * data[index];\n}\n```\n\n1. cache locality 表示程序对缓存中数据的访问 locality 和重用情况。如果同一个 warp 中的线程迭代访问同一缓存线(cache line)上的数据(比如遍历一个数组),那么可以最大化利用 cache,称之为良好的 cache locality。\n\n### papers\n\n1. [Optimization Principles and Application Performance Evaluation](http://impact.crhc.illinois.edu/shared/papers/optimization2008.pdf)\n\n### [cuda performance-guidelines](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#performance-guidelines)\n\n1. `整体优化策略` Overall Performance Optimization Strategies；Performance optimization revolves around four basic strategies:\n   - Maximize parallel execution to achieve maximum utilization; 最大化并行执行以达到最大利用率；\n   - Optimize memory usage to achieve maximum memory throughput; 优化存储使用，实现最大存储吞吐量；\n   - Optimize instruction usage to achieve maximum instruction throughput; 优化指令使用，实现最大指令吞吐量；\n   - Minimize memory thrashing. 最大限度地减少内存抖动。(减少内存申请释放频率)\n1. Which strategies will yield the best performance gain for a particular portion of an application depends on the performance limiters for that portion;根据情况使用哪种策略，比如 reduction 时受 memory bound，因此我们应该争取峰值带宽\n1. compute-bound 需要争取达到最大算力，memory bound 需要争取达到最大带宽\n\n### 优化技术\n\n1. memory coalescing\n   - 让连续的线程访问连续的内存地址。这样就有合并机会，多个线程间会合并访存，可能并不是 32 个线程都合并, 比如一个线程访问 4 个 32bit float 数据，8 个线程可合并，8 x 4 x 32 / 8 = 128Bx\n   - 不连续的时候每个数据都需要load一次，合并了之后能减少load次数，完全合并后需要n/32次，32个线程的数据一次load进来(不考虑cache的情况)\n1. swizzle\n1. bank conflict\n1. 分支优化\n   - 谓词替换\n   - 分支预测\n   - 循环展开\n\n### 访存优化\n\n1. [漫谈高性能计算与性能优化：访存](https://zhuanlan.zhihu.com/p/600489819)\n1. 访存优化是第一性原理\n1. 当我们在说访存优化的时候，我们具体需要做些什么。总的来说，就是三板斧。\n   - 减少数据搬运\n     - 如何减少数据搬运，最主要的手段就是分块，或者说 tiling。\n     - 要尽可能地保证数据连续访问，其中最主要的一个原因就是提高 cache 命中率，从而避免不必要的数据搬运\n   - 减少数据访存延时(数据搬运确定)\n     - 减少 bank 冲突\n     - pipeline (double buffer, 预取)\n     - 就是切分更多的块，启动更多的 warp 来掩盖访存延时。\n   - 保证负载均衡。\n     - 关于负载均衡的话题，主要是在 sparse 里面谈的比较多\n1. 如果发现实际带宽比较差，数据搬运效率比较低，这个时候就要去思考，是不是可以有办法，通过分块的一些技巧来减少数据搬运。如果数据搬运不能够再减少了的话，是否可以通过一些方式来提高数据的搬运效率，比如向量化访存、合并访问来提高对 DRAM 的访存性能、避免 bank 冲突来提高对 shared memory 的访存性能、调整分块大小来让更多的 warp 跑起来从而减少访存的延时，如果不是 SIMT 架构，就需要精细地设计各级访存的 pipeline，让访存操作尽可能地 pingpong 起来，从而让访存流水尽可能地连续起来不要被打断。理论大概是这样，但是每一个问题都有着不同的处理方式，每一个问题可能都是不同的瓶颈。总之就是万变不离其宗，准确地评估每一级存储的访存效率然后尽可能地提高每一级的访存效率，尽可能地把访存流水打满，不要有空跑。\n1. 其实所谓“加速”或者“性能优化”的本质就是让软件充分利用计算硬件，提升利用率，从而逼近理论性能上限。从这个角度，“通用方法”就是：`分析计算部件停顿的原因-选择合理的计算模型减少数据依赖和对流水线的破坏(能兼顾缓解访存墙更好)-通过专用硬件或者结构优化消除剩下的瓶颈，然后不断迭代上述过程，直至各方面因素达到平衡`。\n1. cuda gemm 为什么是三级分块，不是四级或者两级。因为 NV 的 GPU 内存结构是三级的，global mem->shared mem，shared mem->register。\n\n### reduction 优化\n\n1. (great)[Optimizing Parallel Reduction in CUDA](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf)\n   - Memory coalescing\n   - Divergent branching\n   - Bank conflicts\n   - Latency hiding\n1. `What is Our Optimization Goal?` 先确定最大目标 We should strive to reach GPU peak performance Choose the right metric:\n   - GFLOP/s: for compute-bound kernels\n   - Bandwidth: for memory-bound kernels\n1. 通过不断迭代优化，从而达到硬件最优性能。\n1. Reductions have very low arithmetic intensity; Therefore we should strive for `peak bandwidth`\n1. [中文解析](https://zhuanlan.zhihu.com/p/426978026)\n\n### elementwise 优化\n\n1. [elementwise 优化](https://zhuanlan.zhihu.com/p/488601925)\n\n## NVCC\n\n1. 交叉编译时用本地 nvcc 就行，不存在 x86 和 aarch64 区别\n1. [NVIDIA CUDA Compiler Driver NVCC](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html)\n1. [编译过程中各类型文件作用说明](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#supported-input-file-suffixes)\n1. [CUDA 编译过程](https://www.linmao.dev/joy/1165/)\n\n## PTX(Parallel Thread Execution)\n\n1. a low-level parallel thread execution virtual machine and instruction set architecture (ISA)\n1. PTX 是上承 GPU 编程语言 CUDA C++，下启 GPU 硬件 SASS 指令，可以借助 NVRTC 实现运行时优化，某些层面上来说可以称之为 GPU 设备无关代码，因此 PTX 可以理解为`CUDA IR`\n1. PTX 独立于特定 GPU 架构,可以重用相同的代码适用于不同的 GPU 架构,相当于前端\n1. 使用虚拟架构生成 PTX 中间文件，虚拟框架由`compute_`开头。虚拟架构通常是从大的 GPU 代上控制的，真实框架必须大于等于虚拟框架，真实框架对应真正运行的 GPU，即编译阶段就确定要运行的 GPU 是什么。真实框架由`sm_`开头。\n1. `nvcc -ptx program.cu -o _program.ptx -arch=sm_86`\n1. `cat program.ptx | cu++filt > program_demangle.ptx` demangle ptx\n\n## SASS(Shader-Assembly)\n\n1. 真正的机器汇编，由 cubin 文件经过 cuobjdump 工具转换而来。目前没有官方的 sass to cubin 的工具。\n1. cuobjdump 可以用来分析 cubin 文件和 host 文件。而 nvdisasm 只能用来分析 cubin 文件，但是可以得到更多的输出信息。我用的比较多的是 nvdisasm。用来看代码的控制流图。\n1. 只有官方反汇编器，没有官方汇编器\n1. generate\n\n```shell\nptxas -arch=sm_86 mma_gemm_demangle.ptx -o mma_gemm.cubin\ncuobjdump -sass mma_gemm.cubin > mma_gemm.sass\n```\n\n## sync\n\n1. `block level`: `__syncthreads()`\n1. `grid level`: `__threadfence()`\n1. `stream level`: `cudaStreamSynchronize(cudaStream_t stream)`\n   - 这个函数会阻塞 CPU 线程，直到特定的 CUDA stream 中的所有操作完成。\n1. `device level`: `cudaDeviceSynchronize()`\n\n### event sync\n\n1. CUDA 事件（CUDA Event）是 CUDA 编程中用于时间测量和流同步的一种机制。\n1. Event 是 stream 相关的一个重要概念，其用来标记 strean 执行过程的某个特定的点。\n1. Cuda api 提供了相关函数来插入 event 到 stream 中和查询该 event 是否完成（或者叫满足条件？）。只有当该 event 标记的 stream 位置的所有操作都被执行完毕，该 event 才算完成。关联到默认 stream 上的 event 则对所有的 stream 有效。\n1. Events 标记了 stream 执行过程中的一个点，我们就可以检查正在执行的 stream 中的操作是否到达该点，\n1. [使用 event 测量性能](https://blog.csdn.net/qq_24990189/article/details/89602618)\n   - cudaEventSynchronize\n1. CUDA Events 是 NVIDIA 的 CUDA（Compute Unified Device Architecture）编程模型中的一个特性，用于在 GPU 上进行高精度的计时操作。`它们允许开发者在异步任务和内核（kernel）执行中精确地测量时间，帮助分析和优化 CUDA 应用程序的性能`\n1. 主要功能\n   - `时间测量`：CUDA Events 可以用于测量 CUDA kernel 执行时间，或数据传输操作的持续时间。这对于性能分析和调优非常有用。\n   - `同步操作`：CUDA Events 可以作为同步点，确保在某些操作完成后再执行下一步。例如，可以等待一个 event 完成后再启动另一个 kernel。\n\n### stream sync\n\n## bank conflict\n\n1. [概念](https://stackoverflow.com/a/3842483/23011500)\n\n```shell\nBank    |      1      |      2      |      3      |     ...     |      16     |\nAddress |  0  1  2  3 |  4  5  6  7 |  8  9 10 11 |     ...     | 60 61 62 63 |\nAddress | 64 65 66 67 | 68 69 70 71 | 72 73 74 75 |     ...     |     ...     |\n...\n```\n\n1. Each bank has a bandwidth of 32 bits per clock cycle\n1. shared memory 一次不冲突只能读 32x4B， 看一次读取的数据量，多于 32x4B 需要读取多次 n/(32x4)，只要在 n/(32x4) 次内读完就是最高效的。\n1. 使用 float4 类型访存，用向量化的 LDG.128 和 STG.128 指令一次读 4 个元素，以减少访存指令数， 提高计算访存比\n1. thread tile 时如果 TM 是 8，一个 warp 需要读 32\\*8 个 TM 的数(shared memory 上)，至少需要 8 次(如果有 32 个 bank)\n   - 如果一个线程一次处理 8 个连续的数，一个 warp 一次只有 4 个线程不 bank 冲突，\n     - 如果一个线程 1 次读 1 个数，一个 warp 一次只读 4 个数， 需要 64 次读完，\n     - 如果一个线程一次读 4 个数，一个 warp 一次读 16 个数， 需要读 16 次，\n     - 如果一个线程一次读 8 个数，一个 warp 一次读 32 个数，需要读 8 次（最优），但不存在一次读 8 个数的指令。\n   - 如果一个线程一次操作 4 个连续的数(处理两个在空间上属于同一 bank 的数)， 那么一个 warp 一次有 8 个线程 bank 不冲突， 一次操作 4\\*8 个数， 需要 8 次能读完。与 bank 不冲突等效。\n1. 这种情况也可以看作是：shared memory 基本单元为 16byte，总 bank 数为 8，冲突与否的分析不在是 32 线程，而变成 4 个 phase 中的不同线程。如果采用 64bit 的访问形式，则相应的基本单元可以看作是 8byte，总 bank 数目为 16，冲突与否的条件变成两个 phase 内的线程是否冲突。\n1. 4x32 or 8x16 or 16x8 (16B, 8bank)\n\n## 关键字\n\n1. `__restrict__` 用于限定指针,表示该指针是唯一访问目标内存的途径。可以避免出现不同指针引用同一内存区域的情况,编译器可以更自由地进行优化。这意味着编译器可以假设这个指针没有别名（alias），即没有其他指针指向相同的内存位置。\n\n```c++\n__global__ void myKernel(float* __restrict__ ptrA, float* __restrict__ ptrB, int size) {\n    // 内核代码，假设 ptrA 和 ptrB 指向不重叠的内存区域\n}\n```\n\n## occupancy 设计\n\n1. (重要) `最理想的情况下为 sm max threads 个 thread 都分配资源，占用率 100%， 但受限于 Max warps or max blocks per SM, registers per SM, shared memory per SM, register, 占用率可能不到 100%`\n1. 想得到令人满意的 GPU 性能`关键是找到合适的group size与资源的平衡点`\n1. `--ptxas-options=-v` or `-Xptxas -v` or `--resource-usage` 加上编译选项， 显示 register， shared memory 使用\n   - `nvcc  -Xptxas=\"-v\" --ptxas-options=-v -O3 -o my_sgemm my_sgemm.cu -lcublas  2>&1 | c++filt` demangle\n1. nishgt compute 里 occupancy 有详细显示\n1. [cuda-calculator](https://xmartlabs.github.io/cuda-calculator/) 填入数值，计算 occupancy\n1. 注意 shared memory 是动态配置的，可以尝试改变 shared memory 大小来提升性能\n\n## stream\n\n1. cudaStream_t 和 cudaEvent_t 都是数字类型， stream 默认为 0，如果使用默认 stream，直接用 0，如果使用其他 stream，需要 `cudaStream_t stream; cudaStreamCreate(&stream);doing_something(); cudaStreamDestroy(stream);`\n\n```c++ cuda_runtime.h\ntypedef int cudaEvent_t;\ntypedef int cudaStream_t;\n```\n\n1. stream 主要为了隐藏 host device 之间数据搬移的延迟，不是内存和计算\n1. CUDA 流表示一个 GPU 操作队列，该队列中的操作将以添加到流中的先后顺序而依次执行。\n1. stream 作用：在 Stream 的帮助下，CUDA 程序可以有效地将内存读取和数值运算并行，从而提升数据的吞吐量。 [link](https://zhuanlan.zhihu.com/p/51402722)\n   由于 GPU 和 CPU 不能直接读取对方的内存，CUDA 程序一般会有一下三个步骤：1）将数据从 CPU 内存转移到 GPU 内存，2）GPU 进行运算并将结果保存在 GPU 内存，3）将结果从 GPU 内存拷贝到 CPU 内存。\n1. cuda7 可以开启每个线程有一个默认 stream, 之前每个设备有一个 stream [gpu-pro-tip-cuda-7-streams-simplify-concurrency/](https://developer.nvidia.com/zh-cn/blog/gpu-pro-tip-cuda-7-streams-simplify-concurrency/)\n   - `nvcc --default-stream per-thread ./pthread_test.cu -o pthreads_per_thread`需要加编译选项\n\n## cuda grammer\n1. 在 CUDA 编程中，高效的并行算法往往需要线程协作(threads cooperate)以及共享数据(share data)来完成集体计算(collective computations)。要共享数据，线程间必然会涉及同步，而共享的粒度因算法而异，因此线程间的同步应尽量足够灵活，比如开发者可以显示地指定线程间同步，这样就可以确保程序的安全性、可维护性和模块化设计。\n1. [Cooperative Groups](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#cooperative-groups:~:text=do%20not%20match.-,8.%20Cooperative%20Groups,%EF%83%81,-8.1.%20Introduction)\n   - CUDA Cooperative Groups 提供了一种更灵活、更高效的线程协作方式，适用于高性能并行计算。它增强了传统 __syncthreads() 的能力，并支持 warp 级、block 级、grid 级的同步，在某些场景下可以显著提升计算效率。\n1. [Function Execution Space Specifiers](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#function-execution-space-specifiers)\n   - `__global__` 在设备上执行，在主机上调用\n   - `__device__` 在设备上执行，在设备上调用\n   - `__host__` 在主机上执行，在主机上调用\n   - 不填默认在`__host__`\n   - __global__和__host__执行空间说明符不能一起使用。\n   - __device__和__host__执行空间说明符可以一起使用，在这种情况下，函数将针对主机和设备分别进行编译。\n1. [shared](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#shared:~:text=7.2.3.-,__shared__,-%EF%83%81)\n   - 要共享数据，线程间必然会涉及同步   \n   - __shared__： `__shared__ unsigned int u1 = 1;`\n   - extern:\n``` c++\nextern __shared__ float array[];\n__device__ void func()      // __device__ or __global__ function\n{\n    short* array0 = (short*)array;\n    float* array1 = (float*)&array0[128];\n    int*   array2 =   (int*)&array1[64];\n}\n```\n\n1. 算index的时候需要用到built-in variables, 由Index确定要处理的数据\n   - blockIdx\n   - blockDim\n   - threadIdx\n   - 总体的原则是 global_index = block首地址的index + local_index `int tid = blockIdx.x * blockDim.x + threadIdx.x;`\n1. `int tid = blockIdx.x * blockDim.x + threadIdx.x; => int index = h * W + w;`\n1. 所有的 kernel 函数返回类型都是 void\n1. `<<< M , T >>>` Which indicate that a kernel launches with a grid of M thread blocks. Each thread block has T parallel threads.\n1. dim3 threadsPerBlock(16, 16); dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y); MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);\n1. NOTE: dim3是按(x, y, z)设置的，矩阵输入数据是按(y1, x1)索引的\n   - dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE); name: blockDim,\n   - dim3 dimGrid(B.width / dimBlock.x, A.height / dimBlock.y); name: gridDim \n1. 首先确定thread和block处理的数据shape, 根据input shape， 推出gridDim(blocksPerGrid)\n   - 注意cuda的block处理的数据shape和threadsPerBlock的区别，threadsPerBlock和一个thread处理的数据shape共同确定一个block处理的数据shape，等效triton的BLOCK_SIZE\n   - triton tuning 一般是tune 一个block要处理的数据大小, block处理的数据大小确定也能算出一个thread处理的数据大小\n1. `vectorAdd<<<blocksPerGrid, threadsPerBlock>>>`\n1. 可以认为 M, T 对应图片的 H, W; 一个 thread 对应一个像素点；一个 block 对应一行，一个 grid 对应一张图片\n1. [checkCudaErrors](https://github.com/NVIDIA/cuda-samples/blob/master/Common/helper_cuda.h#L595) helper_cuda.h\n\n```c++\nvoid check(T result, char const *const func, const char *const file,\n           int const line) {\n  if (result) {\n    fprintf(stderr, \"CUDA error at %s:%d code=%d(%s) \\\"%s\\\" \\n\", file, line,\n            static_cast<unsigned int>(result), _cudaGetErrorEnum(result), func);\n    exit(EXIT_FAILURE);\n  }\n}\n\n// This will output the proper CUDA error strings in the event\n// that a CUDA host call returns an error\n#define checkCudaErrors(val) check((val), #val, __FILE__, __LINE__)\n\n```\n\n1. cutil.h NVIDIA 公司在 CUDA5 之后便不再使用 cutil.h [CUDA_SAFE_CALL ](https://github.com/NVIDIA/cuda-samples/blob/master/Samples/2_Concepts_and_Techniques/EGLSync_CUDAEvent_Interop/EGLSync_CUDAEvent_Interop.cu#L83)\n1. [replacing-cutil-in-cuda-5-0](https://bohipat.wordpress.com/2014/07/11/replacing-cutil-in-cuda-5-0/)\n\n## tensor core\n\n1. [4 种 tensor core 调用方法](https://zhuanlan.zhihu.com/p/620185229)\n   - WMMA (Warp-level Matrix Multiply Accumulate) API\n   - WMMA PTX (Parallel Thread Execution)\n   - MMA (Matrix Multiply Accumulate) PTX\n   - SASS\n1. [tensor-cores](https://www.nvidia.com/en-us/data-center/tensor-cores/)\n1. `--set roofline`能看到 tensor core roofline; 例如运行`cuda-samples/Samples/3_CUDA_Features/cudaTensorCoreGemm`\n1. cuda-samples/Samples/3_CUDA_Features 包含多个 tensor core 实例\n1. `Samples/3_CUDA_Features/cudaTensorCoreGemm`比`Samples/0_Introduction/matrixMul` 计算性能高很多\n   - cudaTensorCoreGemm 测试的是 fp16 性能\n1. cublas 满足特定条件才会使用 tensor core [link](https://docs.nvidia.com/cuda/cublas/index.html#tensor-core-usage)\n1. [Tensor Core Requirements](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc)\n1. [FP32 vs TF32](https://cloud.tencent.com/developer/article/1893665)\n   - tf32 整数部分精度与 fp32 相同，小数部分与 fp16 相同， 只用了 19 位\n   - 包含 Amphere 架构性能\n   - A100、H100 tf32 算力比 fp32 算力高 8 倍左右\n   - 3090 上 tf32 算力跟 fp32 算力相同\n   - 最新 trtexec 默认是 tf32，注意和 python fp32 计算比较时结果误差可能较大\n\n## wmma vs mma\n\n1. [warp-level-matrix-multiply-accumulate-instructions](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-multiply-accumulate-instructions)\n\n### tenosr core 和 cuda core\n\n1. 计算层级：CUDA Core 是线程级别，Tensor Core 是 warp 级别\n1. 计算维度：CUDA Core 是一维逐点计算，Tensor Core 是二维逐 tile 计算\n1. CUDA Core 是为通用计算设计，而 Tensor Core 是为特定类型的计算（主要是深度学习中的矩阵运算）优化。\n1. 在 NVIDIA 的某些 GPU 架构中，例如 Volta、Turing 和 Ampere，CUDA Core 和 Tensor Core 共同存在。它们可以根据计算任务的性质协同工作，提高整体的计算效率。\n1. 在执行深度学习任务时，Tensor Core 可以显著加速计算过程，相较于仅使用 CUDA Core，能实现更快的训练和推理速度。\n1. RT core 用于光线追踪\n1. 芯片手册中有 cuda core 算力和 tensor core 算力\n1. 利用 tensor core 才能达到最大算力\n\n## build\n\n1. [application-compatibility](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#application-compatibility)\n1. `-arch=sm_70` is a shorthand for `-arch=compute_70 -code=compute_70,sm_70` (which is the same as `-gencode arch=compute_70,code=\\\"compute_70,sm_70\\\"`)\n1. [Your GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) 包含各种 gpu arch\n\n## [cuda samples](https://github.com/NVIDIA/cuda-samples)\n\n1. 可以全部 sample 一起编译\n1. `cuobjdump -all  ./matrixMulDrv`可以看可执行程序 arch 等信息\n1. `make SMS=\"86\"`选择 arch\n1. `make dbg=1` debug 编译\n1. `Samples/1_Utilities/bandwidthTest` 可以查看 host <-> memory 之间传输速度\n1. (good)`Samples/1_Utilities/deviceQuery` 可以查看设备信息, 包含 arch 信息, 多少 sm，每个 sm 多少 cuda core\n1. 11.6 之后代码放在 github 上\n1. [hello world](https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial01/)\n1. `/usr/local/cuda-11.4/samples` tree -L 2\n1. `/usr/local/cuda-10.2/samples/0_Simple/vectorAdd`\n1. `nvprof ./vectorAdd` 查看 kenerl 耗时\n\n## [CUDALibrarySamples](https://github.com/NVIDIA/CUDALibrarySamples)\n\n1. cublas\n1. cutlass\n1. npp\n\n### cuda info\n\n1. [查看 cuda core](https://linuxconfig.org/how-to-get-cuda-cores-count-on-linux)\n   `cd /usr/local/cuda-11.4/samples/1_Utilities/deviceQuery && make && ./deviceQuery`\n1. bandwith test\n   `cd /usr/local/cuda-11.4/samples/1_Utilities/bandwidthTest && make && ./bandwidthTest`\n\n## [NVIDIA Developer Tools](https://developer.nvidia.com/tools-overview)\n\n1. 各工具关系\n   ![tools](https://i.ibb.co/2qN87rv/QVJcq2r-QQ3.png)\n\n### nsight system\n\n1. [cuda toolkit 不自带 需要下载安装](https://developer.nvidia.com/nsight-systems/get-started)\n   - wget 下载 linux CLI Only deb, dpkg 安装\n   - [package-manager-installation](https://docs.nvidia.com/nsight-systems/InstallationGuide/index.html#package-manager-installation)\n     - apt install\n1. viztracer + nsys + ncu + nvtx 分析性能瓶颈\n1. [nsight system python-profiling](https://docs.nvidia.com/nsight-systems/UserGuide/index.html#python-profiling)\n1. 分析系统性能， help you to pinpoint performance issues and identify optimization opportunities; 帮助分析性能问题和识别优化机会\n   - gpu kernel 运行时间太长\n   - 某些 process cpu 处理时间太长\n   - cpu gpu pcie copy 耗时过大\n1. GPU timeline 方便分析\n1. profiling 可以简单地分为粗粒度和细粒度。粗粒度主要是判断瓶颈是不是在 GPU 上，具体又是哪个 kernel，典型代表就是 nsight system 工具，会显示出整个程序的 timeline。可以从 timeline 上直接清晰明了地看到瓶颈是在 CPU 还是 GPU，如果是 GPU，那又是在 GPU 的哪个 kernel 上。\n1. 如果是 timeline 中 GPU kernel 的占比很小，CPU 占比很大，那说明瓶颈在 CPU 侧，需要注意是不是数据读取花了太多时间。如果 GPU kernel 的占比很大，说明瓶颈在 GPU 侧，需要重点花精力去优化 GPU kernel 实现。还有一种情况是，如果数据一直放在 GPU 上，但是 kernel 的时间占比不是特别多，那可能是因为 kernel 本身不太耗时，可能只运行了 4us。但 kernel lauch 就花了 6us。这个时间就要想着采用 kernel fusion 的方式，尽可能地在一个 kernel 里面多干点活。\n1. 点击时注意竖线上的小三角号, 表示关联\n1. 从结果分析看多个 stream 下 kernel 可以同时运行\n   - all stream 或者 kernels 显示不出所有的 kernel 运行，起始位置被其他 kernel 覆盖的检测不出来\n   - all stream 显示不出的 kernel 可能在被隐藏的 stream 里，鼠标放到 kernel 上能显示出在第几个 stream\n1. `enqueue`异步接口没有很快返回的原因：\n   - nsight system 上看 tensorrt node 调用， 对应 cuda api 里有 cudaStreamSync()函数， 会阻塞 cpu 导致 enqueue 不返回\n   - 有多个 stream sync, 每个 stream sync 执行之后之前通过 cuda api 调用的 kernel 都已执行完\n   - 为什么要多个 stream？node 不相关可以并行加速, 可以看到 kernel 执行时间有并行， 为什么要 sync? 后面的节点需要前面的节点都执行完，有关联\n   - 点击 tensorrt 下的 node 可以看到 node 执行信息\n1. 打开文件注意生成 log 时的错误\n1. gui 可以远程 profiling， 将 nsys 安装到 target 机器, 类似 compute\n   - `~/.local/share/nsight_systems/nsys`安装路径\n1. `nsys profile` 类似 perf record 来记录信息\n   - `nsys profile --trace=cuda --gpu-metrics-device all`\n1. `--trace` 可以看看参数：cuda,nvtx, cudnn, python-gil ...\n1. `nsys status --all`打印 nsys 支持的状态，比如是否支持采集 cpu 信息\n1. nsys stats 类似 perf stats 来查看统计信息\n   - `nsys stats report1.nsys-rep`\n   - `nsys stats --report cuda_gpu_trace report1.nsys-rep`\n1. gui summary 里可以看 log 具体执行命令\n1. [python-profiling](https://docs.nvidia.com/nsight-systems/UserGuide/index.html#python-profiling)\n   - Python Sampling requires Python version 3.9 or later. Python Sampling is therefore disabled.\n1. [跟踪 cuda](https://docs.nvidia.com/nsight-systems/UserGuide/index.html#cuda-trace)\n   - `--trace=cuda`\n1. [可以收集 gpu-metrics](https://docs.nvidia.com/nsight-systems/UserGuide/index.html#gpu-metrics)\n   - Is my GPU idle?\n   - Is my GPU full? Enough kernel grids size and streams? Are my SMs and warp slots full?\n   - Am I using TensorCores?\n   - Is my instruction rate high?\n   - Am I possibly blocked on IO, or number of warps, etc\n1. 分析系统性能\n1. `nvprof` 旧版本\n1. [user guide](https://docs.nvidia.com/nsight-systems/UserGuide/index.html)\n1. NsightSystems-2023.2.1.122-3259852.msi 安装包\n1. Nsight_Systems_User_Guide_2023.2.1.122-3259852.pdf 文档\n1. 鼠标放到 kernel 上有 kernel 详细信息，\n   - 24.5.1 版本之后点击 kernel，可以在左侧(蓝边)找到关联的事项，如 cuda 调用对应的 kernel\n   - 包括执行时间\n   - latency：launch latency, 与执行时间不一样，latency 是 api 调用到 kernel 开始执行时间\n     - [NSys Timeline: End - Start is not the same as latency](https://forums.developer.nvidia.com/t/nsys-timeline-end-start-is-not-the-same-as-latency/238043)\n     - CUDA kernel launch latency could be defined as the time range from the beginning of the launch API call to the beginning of the kernel execution.\n     - 执行的是 `cudaStreamSynchronize`, 没执行这个函数之前，stream 中的 kernel 不会 launch\n   - gird, block 设置;(注意，同一个 stream 下相同函数调用可能设置不一样导致时间不同)\n   - 一个线程用多少 register\n   - 理论 occupancy, 打开 gpu metrics 可以看对应的实际 occupancy\n   - 注意各种颜色\n1. 可以在 thread 上看到 `cpu call stack`\n   - 每个线程对应行分层的最后一层 `sampling point`\n   - gpu 空闲时看 CPU 采样点 可以知道 CPU 在干啥\n   - 是否可以生成火焰图？\n1. timeline 上点击 stream 下的 tensorrt node， 可以显示对应的 kernel 执行和 cuda api 调用时间点, 注意竖线上的小三角号\n1. tensorrt 多个维度来看\n   - thread\n   - stream\n   - cuda\n1. 左上角 timeline view 可以选择 analysis summary，有各种总结\n1. 左侧右键选`show in events view`， 可以看具体时间, 可以在 all 上操作，看所有 event 运行时间\n1. 右键 reset room 显示全部\n1. `shift + mouseleftdoubleclick` timeline 可以找到对应 event 在 event view 位置, 刚打开时可以按 name 排序，会看到相关算子集中到一起，再按其他指标排序会混乱\n1. `ctrl + mouseleftdoubleclick` timeline 可以 fit to screen\n1. `backspace` timeline 可以 undo room\n1. `ctrl + mouseleftdoubleclick` 可以找到 event view 对应的 timeline 位置\n1. `sudo nsys profile <app>`\n1. `nsys stats report1.nsys-rep` 输出各种 report\n1. 可以看 cpu 执行情况， tensort 可以看详细算子耗时，也有对应 cuda 执行情况\n1. analysis summary 中有各个线程的 cpu 利用率总结\n1. 可以关注 cpu 空闲的地方，为什么会空闲(同步数据？)\n1. 可以缩小看颜色占比，关注占比大的模块\n1. cudaMemcpy 会阻塞 cpu 执行， 可以多注意 cudaMemcpy 影响\n1. cudaStreamSynchronize 是 CUDA API 中的一个函数，用于等待指定的 CUDA 流上的所有 CUDA 核函数执行完毕。当 CUDA 核函数被执行时，它们会被添加到一个 CUDA 流中，这些核函数的执行可能是异步的，也可能是同步的，具体取决于如何在代码中调用它们。当我们调用 cudaStreamSynchronize 时，它将会阻塞当前 CPU 线程，直到指定的流上的所有核函数都执行完毕。\n\n### nsight compute\n\n1. [enable-docker-collection](https://docs.nvidia.com/nsight-systems/UserGuide/index.html#enable-docker-collection)\n1. [cudatoolkit 自带](https://developer.nvidia.com/nsight-compute)\n1. `balance throughput` compute and memory throughput are near; 平衡比较重要，不平衡时说明使用率较高的是瓶颈\n1. Both SM (Compute) and Memory SOL report the categories’ throughput as the achieved percentage of utilization with respect to the theoretical maximum, i.e. the “Speed Of Light”. Both metrics are composed of sub-metrics, with the respective highest contributor defining the resulting value. The Breakdown tables below the chart can be used to identify all such contributors and their values. SM（计算）和内存 SOL 都将类别的吞吐量报告为相对于理论最大值（即“speed of light”）所实现的利用率百分比\n1. SOL(speed of light): 相对于理论最大值的比例\n1. 有开销 [overhead](https://docs.nvidia.com/nsight-compute/2023.3/ProfilingGuide/index.html#overhead)\n1. `metrics` 是指性能指标，这些指标用于衡量 CUDA 应用程序的性能和行为。性能指标可以包括各种硬件级别的统计数据，如内存访问效率、计算操作的执行时间、流处理器（SM）的利用率、寄存器使用情况、分支效率等等。\n1. [permission 问题](https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-performance-counters)\n   - 可以增加临时权限\n1. 分析 kernel 性能, 可以选 kernel\n1. NVIDIA Nsight Compute is an interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool. In addition, its baseline feature allows users to compare results within the tool. NVIDIA Nsight Compute provides a customizable and data-driven user interface and metric collection and can be extended with analysis scripts for post-processing results.\n1. 需要 windows 客户端通过 connect 将 ncu 送到开发环境\n   - 可以通过 activity 复制命令到板端执行\n   - metrics 可以选择 Sets 和 rules\n1. profile -> metrics details\n   - 点击统计界面某个 metrics 可以看 metrics 详细信息\n1. `--import-source yes --source-folders ./` 导入 source\n1. `ncu --replay-mode application --set full  ./preprocess` 详细信息显示到命令行\n   - Duration 为 kernel 执行时间\n1. `ncu -k matrixMul --print-summary per-gpu ./test` 查看某个 kernel 信息\n1. `/tmp/var/target/linux-desktop-glibc_2_11_3-x64/ncu --config-file off --export \"/tmp/var/test\" --force-overwrite --section-folder /tmp/var/sections --set full ./test`\n1. `ncu --set full -f --export nsight_compute ./test`\n1. `docker run -itd -v /mnt:/mnt -p 30022:22 --user root --gpus all --name=Ubuntu20.04-CUDA-admin --shm-size 2g --cap-add=SYS_ADMIN nvidia/cuda:11.4.3-cudnn8-devel-ubuntu20.04` 尽量使用官方 docker\n   - 需要 `--gpus all --cap-add=SYS_ADMIN`\n1. [nsight-compute docs](https://docs.nvidia.com/nsight-compute/index.html)\n   - [ProfilingGuide](https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html)\n   - [user manual](https://docs.nvidia.com/nsight-compute/NsightCompute/index.html)\n1. 左上角可以显示：\n   - session: gpu cpu 等信息\n   - detailed：overview, compute, memory 等\n   - source: 汇编代码及耗时\n   - summary: 耗时，compute and memory throughput; 底下有优化建议\n1. 可以添加 baseline，对比多次结果, 分析结果变化\n1. `ncu --set roofline`可以测量详细 roofline\n1. `ncu --metrics smsp__inst_executed.sum ./matrixMul` 打印 metrics\n1. `--set roofline`能看到 tensor core roofline; 例如运行`cuda-samples/Samples/3_CUDA_Features/cudaTensorCoreGemm`\n\n#### 遇到的问题\n\n1. The application returned an error code (9).\n   - `ncu --replay-mode <application/kernel>` 加参数； 见[link](https://forums.developer.nvidia.com/t/nsight-profiling-crashes-with-error-code-9/230094/5)\n   - 可以使用`–replay-mode application`切换到应用程序重播。 这避免了内存存储需要重放。\n1. [replay](https://docs.nvidia.com/nsight-compute/2023.3/ProfilingGuide/index.html#replay)\n   - kernels might need to be replayed one or more times, since not all metrics can be collected in a single pass\n   - the number of metrics originating from hardware (HW) performance counters that the GPU can collect at the same time is limited.\n   - kernal replay 只重跑 kernel, application replay 重跑应用\n\n#### [metrics](https://docs.nvidia.com/nsight-compute/2023.3/ProfilingGuide/index.html#metrics-guide)\n\n1. [metrics-reference](https://docs.nvidia.com/nsight-compute/2023.3/ProfilingGuide/index.html#metrics-reference)\n1. [nvprof-metric-comparison](https://docs.nvidia.com/nsight-compute/2019.5/NsightComputeCli/index.html#nvprof-metric-comparison)\n1. `ncu --devices 0 --query-metrics >my_metrics.txt` 查看 metrics\n\n#### 结果分析\n\n1. 每项下面会有建议\n1. compute 分析延迟很关键 目标是计算要掩盖延迟; 分析 stall 原因，stall 就是 warp 闲着\n1. 避免 bank 冲突能解决指令的延迟\n1. overview(GPU speed of light throughput)\n   - `用于查看计算和内存吞吐对理论值的占比以及roofline`\n   - High-level overview of the `throughput for compute and memory` resources of the GPU 主要关注计算和内存\n   - Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues.\n   - 低于 60% 表明有延迟问题, 例如：指令有延迟，可以看 scheduler statistics 和 warp state statistics 进一步分析;延迟问题可能是由于 occupancy 较低， 也可能是指令执行延迟太大，无法隐藏;\n   - roofline\n1. comupte workload\n   - `用于分析 SM 计算资源使用情况`\n   - IPC\n   - 指令执行占比\n   - LSU: load store unit\n1. memory workload\n   - `用于分析 GPU memory 使用情况`\n   - 可以选择看 transfer size 和 throughput\n   - 要提升 cache 命中率\n   - 多用 shared memory(可作为中间结果)\n   - 多种 memory\n     - global\n     - local: 线程私有的。local memory 不是物理空间，而是 global memory 的一部分，所以延时较大。\n     - texture: 只读?\n     - surface\n     - load global stroe shared\n     - shared\n1. scheduler statistics\n   - `用于分析每个 scheduler active, eligible, issue warp 情况`\n   - Issued Warp Per Scheduler: 平均每个 cycle 发射的 warp 数\n   - On cycles with no eligible warps, the issue slot is skipped and no instruction is issued. Having many skipped issue slots indicates poor latency hiding.\n   - Out of the maximum of 12 warps per scheduler(Ampere 1 个 SM 有 4 个 scheduler), this kernel allocates an average of 2.00 active warps per scheduler, but only an average of 0.07 warps were eligible(合格的) per cycle.\n   - 可以看出平局每个 sm 有几个 warps; 参考可以看 Theoretical Occupancy 是多少，有可能受 regitster 和 shared memory 等限制\n   - Eligible warps are the subset of active warps that are ready to issue their next instruction.\n1. warp state statistics\n   - `用于分析 warp stall 原因`\n   - Check the Warp Stall Sampling (source counters 中) table for the top stall locations in your source based on sampling data. 查看 stalll source 位置\n   - statll 解决办法： Try to increase the number of active warps to hide the existent latency or try changing the instruction mix to utilize all available pipelines in a more balanced way.\n   - The warp cycles per instruction define the latency between two consecutive instructions. 每条指令的 warp 周期定义两个连续指令之间的延迟\n   - 可以查看两个 warp instruction 之间 cyles 组成： stall math pipe throttle, stall mio throttle;\n1. instructions statistics\n   - `用于查看指令的类型和执行次数`\n   - Statistics of the executed low-level assembly instructions (SASS). SASS 指令统计\n   - 统计执行的指令数，可以看出哪些指令执行的较多\n1. launch statistics\n   - `用于查看 grid, block 设置和 register, shared memory 使用情况`\n1. occupancy\n   - `用于查看occupancy情况及限制原因`\n   - Occupancy is the ratio of the number of active warps per multiprocessor to the maximum number of possible active warps.\n   - 可以看出理论占用率及不能到 100%的原因\n   - 一个 warp 中用太多 register 和 shared memory 会影响 Occupancy， Occupancy 会影响 scheduler， 进而会影响延迟隐藏\n   - Occupancy 是 CUDA 编程中一个重要的性能指标,它表示 GPU 中 Streaming Multiprocessor (SM)上的处理单元被运用的比例。\n   - 右上角点开 table，可以可视化\n1. source counters\n   - `用于分析分支指令是否有影响，是否合并访存`\n   - Source metrics, including branch efficiency and sampled warp stall reasons.\n\n### NVTX\n\n1. [(good)llm.c nvtx 使用](https://github.com/karpathy/llm.c/blob/2ddf128365773670ceb4e76ab7b04ab671edda64/llmc/cuda_common.h#L108-L117)\n\n```c++\nclass NvtxRange {\n public:\n    NvtxRange(const char* s) { nvtxRangePush(s); }\n    NvtxRange(const std::string& base_str, int number) {\n        std::string range_string = base_str + \" \" + std::to_string(number);\n        nvtxRangePush(range_string.c_str());\n    }\n    ~NvtxRange() { nvtxRangePop(); }\n};\n#define NVTX_RANGE_FN() NvtxRange nvtx_range(__FUNCTION__)\n```\n\n1. 和 nsight system, viztracer 一起非常好用\n1. [doc](https://nvtx.readthedocs.io/en/latest/)\n   - Annotate code ranges and events in Python 在 Python 中注释代码范围和事件\n1. sample\n\n```python\n@nvtx.annotate()\ndef my_func():\n    time.sleep(1)\n\nwith nvtx.annotate(\"for_loop\", color=\"green\"):\n    for i in range(5):\n        sleep_for(i)\n        my_func()\n\nrng = nvtx.start_range(message=\"my_message\", color=\"blue\", domain=\"rng\")\n# ... do something ... #\nnvtx.end_range(rng)\n```\n\n1. [NVIDIA Tools Extension API: An Annotation Tool for Profiling Code in Python and C/C++](https://developer.nvidia.com/blog/nvidia-tools-extension-api-nvtx-annotation-tool-for-profiling-code-in-python-and-c-c/)\n\n### [cupti](https://developer.nvidia.com/cupti)\n\n### nsgiht graphics\n\n## [cuda-debug](https://docs.nvidia.com/nsight-visual-studio-code-edition/cuda-debugger/index.html)\n\n1. [02_dump_reg_shmem](https://github.com/NVIDIA/cutlass/tree/main/examples/02_dump_reg_shmem)\n   - dump_shmem\n   - dump_fragment\n1. [compiling 要求](https://docs.nvidia.com/cuda/cuda-gdb/index.html#compiling-the-application)\n   - `nvcc -g -G foo.cu -o foo`\n1. [vs code plugin: nsight-vscode-edition](https://marketplace.visualstudio.com/items?itemName=NVIDIA.nsight-vscode-edition)\n1. shared memory(L1 cache?) dump\n   - 申请 shared memory 变量，存中间结果， shared memory 对 block 全局可见。\n   - 可以申请多于 L1 cache 大小的存储，系统会自动调度\n1. 传递主存指针到 kernel，用于 dump\n1. print\n   - `if(threadIdx.x == 0) printf...` 条件打印\n   - cudaDeviceReset(); 不打印可以加\n1. [nsight-visual-studio-code-edition](https://developer.nvidia.com/nsight-visual-studio-code-edition)\n1. cuda-gdb\n   - [Can’t use Cuda-gdb](https://forums.developer.nvidia.com/t/cant-use-cuda-gdb/235380/4)\n   - [Error disabling address space randomization](https://stackoverflow.com/a/46676907/23011500)\n1. cuda-memcheck\n\n## link\n\n1. [cuda 相关开发](https://developer.nvidia.com/cuda-zone)\n1. [gpu-accelerated-libraries](https://developer.nvidia.com/gpu-accelerated-libraries)\n\n## build\n\n### cmake\n\n1. [build-systems-exploring-modern-cmake-cuda-v2.pdf](https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9444-build-systems-exploring-modern-cmake-cuda-v2.pdf)\n1. [building-cuda-applications-cmake](https://developer.nvidia.com/blog/building-cuda-applications-cmake/)\n1. [CUDALibrarySamples](https://github.com/NVIDIA/CUDALibrarySamples/blob/master/cuBLAS/Level-3/gemm/CMakeLists.txt)\n1. basic sample\n\n```cmake\ncmake_minimum_required(VERSION 3.8)\n# project(my_cuda_project LANGUAGES CXX CUDA)\nproject(my_cuda_project)\nenable_language(CUDA) # 需要enable language\nadd_executable(preprocess kernel.cu preprocess.cpp )\nfind_package(CUDA REQUIRED)\ntarget_link_libraries(preprocess ${CUDA_cudart_LIBRARY})\n```\n\n### compile\n\n1. 混合编译, 注意-lcudart 顺序\n\n```sh\nnvcc -O3 -c kernel.cu -o kernel.o\ng++ -L /usr/local/cuda/targets/x86_64-linux/lib/ preprocess.cpp kernel.o -o preprocess -lcuda -lcudart && ./preprocess\n```\n\n## unified memory\n\n1. 不明白原理时不推荐使用\n1. 优缺点\n   - 优点\n     - 简化代码编写和内存管理：cudaMallocManaged 可以简化 CPU 和 GPU 之间数据传递的代码，无需手动管理内存迁移。\n   - 缺点\n     - 可能降低性能：在某些情况下，统一内存可能会降低性能，例如在数据访问模式为稀疏的情况下。\n     - 可能增加内存占用：统一内存可能会增加内存占用，因为它需要在 CPU 和 GPU 内存中都保留一份数据副本。\n1. `cudaMalloc -> cudaMallocManaged(&x, N*sizeof(float));`\n1. [使用 cudaMallocManaged 情况](https://stackoverflow.com/a/21990899/23011500)\n   - You are working on a Jetson device.\n1. runtime 负责 copy\n1. unified memory cpu 访问时需要同一个线程。不同线程会 bus error。\n1. [Unified Memory for CUDA Beginners](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)\n1. [Maximizing Unified Memory Performance in CUDA](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/)\n\n## design\n\n1. lidar prprocess; 三重 for, 最外层作为 x, 一个线程执行一个最里面 for 的内容，\n\n```c++\nvoid TransposeDim(int kmax_num_point_pillar,\n                   int kmax_num_point,\n                   int kdim,\n                   int voxel_num, float *voxel_data, int8_t *features_s8)\n{\n  int kWC = kmax_num_point_pillar * kdim;\n  int kHW = kmax_num_point * kmax_num_point_pillar;\n  for (int c = 0; c < kdim; ++c)\n  {\n    for (int w = 0; w < kmax_num_point_pillar; ++w)\n    {\n      for (int h = 0; h < voxel_num; ++h)\n      {\n        int old_index = h * kWC + w * kdim + c;\n        int new_index = c * kHW + w * kmax_num_point + h;\n        float features_tmp = round(static_cast<float>(voxel_data[old_index]));\n        features_tmp = std::min(std::max(features_tmp, -128.f), 127.f);\n        features_s8[new_index] = static_cast<int8_t>(features_tmp);\n      }\n    }\n  }\n}\n\n__global__ void TransposeDim5Kernel(int kmax_num_point_pillar,\n                                    int kmax_num_point,\n                                    int kdim,\n                                    int voxel_num,\n                                    float *voxel_data,\n                                    int8_t *features_s8)\n{\n  int c = blockIdx.x * blockDim.x + threadIdx.x;\n  int w = blockIdx.y * blockDim.y + threadIdx.y;\n  int h = blockIdx.z * blockDim.z + threadIdx.z;\n\n  if (c < kdim && w < kmax_num_point_pillar && h < voxel_num)\n  {\n    int kWC = kmax_num_point_pillar * kdim;\n    int kHW = kmax_num_point * kmax_num_point_pillar;\n\n    int old_index = h * kWC + w * kdim + c;\n    int new_index = c * kHW + w * kmax_num_point + h;\n    float features_tmp = round(voxel_data[old_index]);\n\n    features_tmp = fmaxf(fminf(features_tmp, 127.f), -128.f);\n\n    features_s8[new_index] = static_cast<int8_t>(features_tmp);\n  }\n}\n\ndim3 blockSize(4, 4, 32);\ndim3 gridSize((kdim + blockSize.x - 1) / blockSize.x,\n                (kmax_num_point_pillar + blockSize.y - 1) / blockSize.y,\n                (voxel_num + blockSize.z - 1) / blockSize.z);\n\nTransposeDim5Kernel<<<gridSize, blockSize>>>(kmax_num_point_pillar,\n                                               kmax_num_point,\n                                               kdim,\n                                               voxel_num,\n                                               d_voxel_data,\n                                               d_features_s8);\n```\n","tags":["Cuda"],"categories":["Program","Cuda"]},{"title":"Doxygen","url":"/2022/03/03/Doxygen/","content":"\n## base\n\n1. markdown 添加 [toc] 为 html 生成目录\n1. 参考 opencv\n1. [multi projects](https://stackoverflow.com/a/57979732)\n1. 可以用 cmake 生成 doxygen 配置 [link](https://github.com/opencv/opencv/blob/17234f82d025e3bbfbf611089637e5aa2038e7b8/doc/Doxyfile.in)\n1. [templates](https://github.com/doxygen/doxygen/tree/master/templates)\n\n## 流程图\n\n### moritz\n\n1. [moritz 插件 生成流程图](https://sourceforge.net/projects/moritz/)\n   - 可以单独使用，也可以和 doxygen 一起使用 doxywizard\n   - [pdf 文档](https://sourceforge.net/p/moritz/wiki/Home/#31b2)\n   - Moritz_Linux_2020_06_10.zip 执行 ansi_c_create.sh 生成 dot 和 html\n   - 默认 arm 二进制， x86 需要源码编译 src_abc2xml_xml2abc_2020_06_10.zip, 需要 codeblocks 图形界面来编译，String_help.h 找不到直接替换字符\n   - cfg/Doxyfile_ansi_c_xml 中 INPUT 改源码位置， RECURSIVE 要改为 YES\n\n### XML pareser\n\n1. `pip install  doxmlparser`\n1. [metrics](https://github.com/doxygen/doxygen/blob/master/addon/doxmlparser/examples/metrics/metrics.py)\n   - `python metrics.py xml/`能够输出统计信息\n   - [location 来获取定义文件，start end line](https://github.com/doxygen/doxygen/blob/master/addon/doxmlparser/doxmlparser/compound.py#L7025)\n1. 跟据 xml 信息提取函数再加上 cxx2flow 生成流程图\n1. 如果使用宏定义来区分平台，doxygen 生成 xml 时需要删除其他平台的代码或者提供 compile_commands.json 来生成；否则类的关系会有问题，生成不了具体实现的流程图\n1. 也可以配置 doxygen 中的 INPUT, 只包含要使用的平台\n1. 给别人生成时需要提供 compile_commands.json 和源码，根据路径在自己电脑创建相应目录，放入源码\n1. `sed -i 's/\\/builds\\/platform/\\/home\\/xiyang\\/d\\/working/g' compile_commands.json`修改路径\n1. compile_commands.json 不太好使;\n1. (good)可以 grep compile_commands.json 要用的 cpp 文件，写到 INPUT(注意 INPUT 不支持正则), FILE_PATTERN 只设置搜索头文件(INPUT 要填最外层文件夹)， EXCLUDE 排除一些文件, 头文件路径设置到 INCLUDE_PATH，\n\n```shell\n# sudo apt install jq\nrm file_list.txt; cat compile_commands.json | jq '.[] | .file' | while read -r line; do\n  echo \"$line\\\\\"\ndone | sort >> file_list.txt\n```\n\n1. 写个脚本处理 compile_commands.json 文件， 得到文件和 include 路径，设置到 INPUT 里面\n1. doxyxml\n1. [graphviz 要用最新的，否则生成的图片有问题](https://gitlab.com/graphviz/graphviz)\n\n### word 文档\n\n1. XML 格式： Word 文档本质上是一个 XML 文件。可以使用文本编辑器或编程语言（如 Python、C#）打开 Word 文档的 XML 格式，搜索<w:altText>标签，即可找到图片的 Alt 文本。\n   - unzip docx: 解压 docx 文件，在 word 里查看 document.xml\n   - windows 下另存为 xml\n   - 通过 xml 可以看出通过图片的 name 和 descr 属性可以区分图片\n1. [python-docx](https://github.com/python-openxml/python-docx?tab=readme-ov-file)\n\n   - 加入可选文字，后面使用 template 替换\n   - 把图片写在 bytesIO 里面然后用 from docxtpl import InlineImage 插入的，用了快两年了，很稳 就是楼上这个\n   - 通过 descr 属性来处理图片替换问题 [link](https://chatgpt.com/c/66e15cd9-20f4-8004-a2fd-945070c91601)\n\n   ```python\n    inline_shape = run.add_picture(image_path, width=Inches(2))\n\n    # 添加 descr（图片描述）\n    inline_shape._inline.graphic.graphicData.pic.nvPicPr.cNvPr.set('descr', descr)\n   ```\n\n1. [python-docx-template](https://github.com/elapouya/python-docx-template)\n   - [merge_doc](https://github.com/elapouya/python-docx-template/blob/master/tests/merge_docx.py)\n     - 合并之后在下面添加{{xxx}}用于下次合并\n   - [通过 json 来生成 docx](https://github.com/elapouya/python-docx-template/blob/master/tests/module_execute.py)\n   - [replace_picture](https://github.com/elapouya/python-docx-template/blob/master/tests/replace_picture.py)\n     - 右击图片，可选文字显示名称\n   - [sample](https://blog.csdn.net/h1773655323/article/details/119876821)\n1. 流程图加到文档里\n1. pandoc\n1. [详解 Python-docx 自动生成 word 图文报告](https://blog.csdn.net/yuetaope/article/details/119444970)\n\n## theme\n\n1. [doxygen-awesome-css](https://github.com/jothepro/doxygen-awesome-css)\n1. [效果](https://jothepro.github.io/doxygen-awesome-css/index.html)\n\n## doxygen config\n\n1. `sudo apt install doxygen`\n1. `doxygen -g <config-name>` 产生配置文件，默认为 Doxyfile `-s` 生成不带注释的配置文件\n1. `doxygen <config-name>` 生成文档\n1. UML 生成 svg 图片\n   - `DOT_IMAGE_FORMAT       = svg(png)`\n   - `INTERACTIVE_SVG        = YES`\n1. 生成 UML, 需要修改配置文件, 以下改为 YES\n\n```\nRECURSIVE\nHAVE_DOT\nUML_LOOK\nEXTRACT_ALL\n```\n\n1. `CLANG_DATABASE_PATH=compile_commands.json`提供编译信息更准确的拿到信息 [link](https://stackoverflow.com/a/64476238/23011500)\n1. `INLINE_SOURCES`标签。如果设置为 YES，可以将源代码直接包含到文档中。\n1. `INPUT` 标签是输入的源码文件的目录, 空格分开 例如： `INPUT = README.md include/pg`\n1. `OUTPUT_LANGUAGE` = 改为 Chinese\n1. `HTML_TIMESTAMP`标签设置为 YES，则每个生成的 HTML 页面的页脚将包含生成页面的日期和时间。将此设置为“是”有助于显示上次运行 doxygen 的时间，从而显示文档是否最新。\n1. `GENERATE_TREEVIEW` 标签用于指定是否应生成树状索引结构以显示分层信息。\n1. `GENERATE_XML = YES` 产生 xml\n1. `GENERATE_HTML = YES` 产生 html\n1. `EXTRACT_ALL`\n1. `EXTRACT_PRIVATE` If the EXTRACT_PRIVATE tag is set to YES, all private members of a class will be included in the documentation.\n1. `EXTRACT_STATIC`\n1. `EXTRACT_LOCAL_METHODS`\n1. `EXTRACT_ANON_NSPACES`提取匿名函数\n1. `PROJECT_NAME`\n1. `PROJECT_NUMBER`\n1. `PROJECT_LOGO` 首页左侧图标\n1. `EXTRACT_STATIC` 如果设置为 YES，一个文件的所有静态成员都将包含在文档中。\n1. `EXTRACT_PRIVATE` 如果设置为 YES，一个类的所有私有成员都将包含在文档中。\n1. `IMAGE_PATH` 修复 markdown 生成网页时 doc 出错问题\n1. `USE_MDFILE_AS_MAINPAGE = ./README.md` 主页\n1. `HTML_OUTPUT = doc_html`\n1. `LATEX_OUTPUT = doc_latex`\n1. 输出 color [link](https://github.com/jothepro/doxygen-awesome-css/blob/main/docs/customization.md#doxygen-generator)\n\n```sh\nHTML_COLORSTYLE_HUE    = 209\nHTML_COLORSTYLE_SAT    = 255\nHTML_COLORSTYLE_GAMMA  = 113\n```\n\n## pdf\n\n1. `sudo apt-get install texlive-full texlive-latex-base` [link](https://tex.stackexchange.com/a/481681)\n1. `cd latex && make`\n1. `doxygen -w latex header.tex footer.tex doxygen.sty` 先生成默认配置，修改 doxygen.config 指定`LATEX_HEADER = header.tex` [link](https://stackoverflow.com/a/16222657)\n1. 修改 refman.tex 来改变 pdf 生成\n1. `sed -i \"s/Doxygen/xx xx/\" refman.tex`修改制作者名字\n1. 中文问题 `\\begin{CJK}{UTF8}` 换成 `\\begin{CJK}{gbsn}` [link](https://www.cnblogs.com/zyl910/archive/2013/06/02/doxygen_pdf_chinese.html)\n1. 空页问题： `COMPACT_LATEX` to YES\n1. 换页问题： `\\doxysection` 之前加 `\\newpage`\n1. refman.tex 处理\n\n```sh\nsed -i \"/fancyfoot/d\" latex/refman.tex\nsed -i \"/fancyhead/d\" latex/refman.tex\nsed -i '/{fancyplain}/a\\\\\\cfoot{\\\\thepage}' latex/refman.tex\nsed -i '/{fancyplain}/a\\\\\\rhead{}' latex/refman.tex\nsed -i '/{fancyplain}/a\\\\\\lhead{}' latex/refman.tex\nsed -i '/doxysection/i\\\\\\newpage' latex/refman.tex\nsed -i \"s/Doxygen 1.8.17/PhiGent Robotics/\" latex/refman.tex\nsed -i \"s/{min}/{gbsn}/\" latex/refman.tex\n```\n\n1. `\\pagestyle{fancyplain}` 删除之后页眉页脚不再有横杠 `\\pagestyle{plain}`\n\n## notes\n\n1. 不要在有 build 的目录下\n1. 如果想生成详细的 UML 图，需要设置`EXTRACT_STATIC` `EXTRACT_PRIVATE`为 YES\n\n## 文档解析\n\n1. Class Hierarchy 可以查看类层级关系\n\n## links\n\n1. [fossies doxygen lists](https://fossies.org/dox/all.html)\n1. [getting start](https://www.doxygen.nl/manual/starting.html)\n1. [参数详解](https://www.guyuehome.com/35640)\n1. [使用 doxygen 的项目](https://doxygen.nl/projects.html)\n1. [deepstream doxygen](https://docs.nvidia.com/metropolis/deepstream/sdk-api/index.html)\n1. [ion](https://google.github.io/ion/base_2logging_8h.html)\n1. [opencv tutorial](https://docs.opencv.org/4.x/d9/df8/tutorial_root.html)\n1. [opencv config](https://github.com/opencv/opencv/blob/17234f82d025e3bbfbf611089637e5aa2038e7b8/doc/Doxyfile.in)\n1. [templates](https://github.com/doxygen/doxygen/tree/master/templates)\n","tags":["Doxygen"],"categories":["Program"]},{"title":"UML","url":"/2022/03/03/UML/","content":"\n## UML 介绍\n\n1. [历史](https://www.visual-paradigm.com/cn/guide/uml-unified-modeling-language/what-is-uml/)\n1. 图表可大致分为结构性图表和行为性图表两种。结构性图表显示了系统在不同抽象层次和实现层次上的静态结构以及它们之间的相互关系。结构性图表中的元素表示系统中具意义的概念，可能包括抽象的、现实的和實作的概念。\n1. 结构性图表有七种类型：\n   - 类图 (Class Diagram)\n   - 组件图 (Component Diagram)\n   - 部署图 (Deployment Diagram)\n   - 对象图 (Object Diagram)-\n   - 包图 (Package Diagram)\n   - 复合结构图 (Composite Structure Diagram)\n   - 轮廓图 (Profile Diagram)\n1. 行为性图表显示了系统中对象的动态行为 ，可用以表达系统随时间的变化。行为性图表有七种类型：\n   - 用例图 (Use Case Diagram)\n   - 活动图 (Activity Diagram)\n   - 状态机图 (State Machine Diagram)\n   - 序列图 (Sequence Diagram)\n   - 通訊圖 (Communication Diagram)\n   - 交互概述图 (Interaction Overview Diagram)\n   - 时间图 (Timing Diagram)\n\n## 图种类\n\n1. class diagram 类图\n2. sequence diagram 时序图\n3. flow chart 流程图\n\n### class diagram\n\n### [sequence diagram](https://www.woshipm.com/ucd/607593.html)\n\n#### 关系\n\n![关系](https://pica.zhimg.com/v2-941a070601f399d992125ef31261637e_1440w.jpg?source=172ae18b)\n\n<!-- prettier-ignore -->\n| 名字     | 图形                     | 说明                                 | 其他                                                                               |\n|:---------|:-------------------------|:-------------------------------------|:-----------------------------------------------------------------------------------|\n| 实现关系 | 空心三角和虚线组成的箭头 | 实现类指向接口                       | 类似 rust trait                                                                    |\n| 泛化关系 | 空心三角和实线组成的箭头 | 从子类指向父类                       | \"is a\"                                                                             |\n| 关联关系 | 带箭头的实线             | 指向被关联的对象                     | 一个对象含有另一个对象的引用                                                       |\n| 依赖关系 | 带虚线的箭头             | 使用方指向被使用方                   | \"use a\", 弱关联关系,<br>构造器或方法中的局部变量、方法或构造器的参数、方法的返回值 |\n| 聚合关系 | 空心菱形加实线箭头       | 空心菱形在整体一方，箭头指向部分一方 | \"has a\", 具有各自的生命周期                                                        |\n| 组合关系 | 实心菱形加实线箭头       | 实心菱形在整体一方，箭头指向部分一方 | \"contains a\" 强聚合, 作为整体的对象负责部分的对象的生命周期                        |\n\n<!-- prettier-ignore -->\n## mermaid\n\n## [mermaid](https://github.com/mermaid-js/mermaid)\n\n1. [在线 server](https://mermaid.live/)\n1. [cheat sheet](https://jojozhuang.github.io/tutorial/mermaid-cheat-sheet/)\n1. markdown 支持\n1. theme `%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#ffcccc', 'edgeLabelBackground':'#ffffee', 'tertiaryColor': '#fff0f0', 'fontFamily': 'verdana'}}}%%`\n1. [theming](https://mermaid-js.github.io/mermaid/#/theming)\n1. 可以用 graph 画关系图[bitcoin sample](https://github.com/bitcoin/bitcoin/blob/master/doc/design/libraries.md#L36)\n1. gatte 图也好用\n1. github 用 chrome 插件[markdown-diagrams](https://chrome.google.com/webstore/detail/markdown-diagrams/pmoglnmodacnbbofbgcagndelmgaclel)可以生成图片, 密集图片截屏效果不好\n1. `%%{init: {'securityLevel': 'loose', 'theme':'base'}}%%` 设置主题\n1. `%%{init:{\"themeVariables\": { \"fontSize\": \"30px\" }}}%%` 设置字体大小\n1. `%%{init:{'theme':'neutral', \"themeVariables\": { \"fontSize\": \"20px\" }}}%%`\n1. [字体设置只支持几种图，其他不支持](https://github.com/mermaid-js/mermaid/issues/3033)\n1. sequence\n\n```mermaid\n{% mermaid sequenceDiagram %}\nAlice->>John: Hello John, how are you?\nloop Healthcheck\n    John->>John: Fight against hypochondria\nend\nNote right of John: Rational thoughts!\nJohn-->>Alice: Great!\nJohn->>Bob: How about you?\nBob-->>John: Jolly good!\n{% endmermaid %}\n```\n\n1. flow chart 支持 [fontawesome](https://fontawesome.com/icons)\n\n```mermaid\nclassDiagram\n    user ..> image\n    image ..> MediaProc\n    MediaProc <|-- ImgProcImpl\n    ImgProcImpl ..> ImageFrame : 代理\n    ImageFrame <|-- ImageFrameImpl\n    ImageFrameImpl <|-- BpuImageFrame\n\n    class MediaProc{\n      <<interface>>\n      +Crop()\n      +Resize()\n      +CvtColor()\n      +Decode()\n      +Encode()\n    }\n\n    class ImgProcImpl{\n      +Crop()\n      +Resize()\n      +CvtColor()\n      +Decode()\n      +Encode()\n    }\n\n    class ImageFrame{\n      <<interface>>\n      +Crop()\n      +Resize()\n      +CvtColor()\n    }\n\n    class ImageFrameImpl{\n      +Crop()\n      +Resize()\n      +CvtColor()\n    }\n\n    class BpuImageFrame{\n      +Crop()\n      +Resize()\n      +CvtColor()\n    }\n\n```\n\n1. samples\n   - [fastapi](https://github.com/tiangolo/fastapi/blob/master/docs/en/docs/tutorial/dependencies/dependencies-with-yield.md#dependencies-with-yield-and-httpexception)\n   - [三次握手 config 中有中文支持](https://mermaid.live/edit#pako:eNqdVFFr01AU_iuX-6QYW9vabuvDQLoNYVKGKYIaH26Tk-ZCkhtvbqe1FMQndZsOhogwNycMBWHTFwU39M_YrG_-BW9605rNZk7zkNDvnPud85379XSxySzAVRzC3Tb4JsxR0uLEM3wkn4BwQU0aEF-gmkvBF3_iOvBl4ArnYArEW81zxXJZQ8VScfg6r4J1JgBx2nIEYnZCV0X9w4f9dyuoUVtC0fOd6MmKSlbhi7Ozir6q36yrADEFXSaSKd2PSpLZCavMvnCltnjiRLpTCyYSnSw7Jknlp2nAt04ql6Jj9X_TLQUrvUcHG9Hr7Z-Hq_311ejlZzWP_sHXow8rjWv64PtW9Gx3cnMJgK6C67J_mE4CqHMaqoG8TJua8piWhBahM3_fdIjfGkPD5DnmQwbn_8AZlzBx0hnij3VaG35rNHCA6wGYGlqgPg0dsDI4ToUzrvxsIz5eOEPoZPck_5tyOe0fF-yhfUYtSmsg5Q1lldg_Q-dELz5Ga3s_Dt8Mvm1kyJur6yja3h3svx3sf4k-PTpF18Ta0eZa_-lO_9V7RZLL5TKG8LtQbPKtzbMNY1iTxQZVYW1Ut7-3Gj1eT7EavjoxerCGPeAeoZZcaN04ZmDhgAcGlpeKbcYhFAbWUpEbhFPSdCGMU7qKLs70xQLxqNuJ4dujKgZuUA9CVId76DrziJ9wqZjO2twceoXaaIkzA6vYHS1Nq9MHSTuFQiBUTs_we7J30hZM7_gmrgreBg23A0sOKVnIuGoTNxyj8xYVjI9BlxEL5M8uFp0gXuctGgpJaTLfpq0Yb3NXwo4QQVjN5-NwrkWF027mTOblQ2o5cqc7yzOVfKVYmSbFElSmSqRcKllmszAzbRcvF2xr6lKhSHCvp2G5-m8xNuqq9ws-JjhZ)\n\n## [plantuml](https://github.com/plantuml/plantuml)\n\n1. [在线 server](http://www.plantuml.com/plantuml/uml/)\n1. [本地 plantuml-server](https://github.com/plantuml/plantuml-server)\n   - `docker run -d -p 10093:8080 plantuml/plantuml-server:jetty`\n   - `docker run -d -p 10093:8080 -e THE_ENV_VARIABLE=THE_ENV_VALUE plantuml/plantuml-server:jetty`\n1. [pdf guide](https://plantuml.com/zh/guide)\n1. [class](https://plantuml.com/zh/class-diagram)\n1. [时序图](https://plantuml.com/zh/sequence-diagram)\n1. [java-design-patterns](https://github.com/iluwatar/java-design-patterns)\n1. [puml config file demo](https://github.com/iluwatar/java-design-patterns/blob/master/factory-method/etc/factory-method.urm.puml)\n1. markdown gitlab github 不支持， 用[markdown-diagrams](https://chrome.google.com/webstore/detail/markdown-diagrams/pmoglnmodacnbbofbgcagndelmgaclel)来支持浏览器显示\n\n1. test plantuml\n\n```puml\n@startuml\nobject Object01\nobject Object02\nobject Object03\nobject Object04\nobject Object05\nobject Object06\nobject Object07\nobject Object08\n\nObject01 <|-- Object02\nObject03 *-- Object04\nObject05 o-- \"4\" Object06\nObject07 .. Object08 : some labels\n@enduml\n```\n\n## [dot language](https://graphviz.org/doc/info/lang.html)\n\n1. [dot 命令源码](https://gitlab.com/graphviz/graphviz/-/tree/main/cmd/dot)\n1. `sudo apt-get install graphviz`\n1. [维基介绍](https://zh.wikipedia.org/zh-cn/DOT%E8%AF%AD%E8%A8%80)\n1. [属性](https://itopic.org/graphviz.html)\n1. [samples](http://www.graphviz.org/gallery/)\n1. [Graphviz 使用教程](https://www.zywvvd.com/notes/tools/graphviz/graphviz/)\n1. 流程图条件分支连接线不居中的原因是 portpos 不对齐\n1. sample\n\n```dot\ndigraph test123 {\n        pack=8\n        subgraph cluster0 {\n          packmode=array\n          x y x0 y0 x1\n          subgraph cluster1 {\n            m n\n          }\n        }\n        b [shape=box];\n        c [label=\"helloenworld\",color=blue,fontsize=24,\n             fontname=\"Palatino-Italic\",fontcolor=red,style=filled];\n        a -> z\n        x -> z\n        a -> b -> c;\n        a -> {x y};\n        edge [style=dashed,color=red];\n        b -> x;\n}\n```\n\n1. 颜色, 给 node 加上属性\n\n```graphviz-(dot)\ndigraph G {\n  node [style=filled, color=\"#999999\", fillcolor=\"#eeeeee\"]; edge [color=\"#666666\"];\n  node [\n    style=\"filled\"\n    color=\"#9370db\"\n    fillcolor=\"#ececff\"\n  ]\n  start -> a0;\n  start -> b0;\n  a1 -> b3;\n  b2 -> a3;\n  a3 -> a0;\n  a3 -> end;\n  b3 -> end;\n\n  start [shape=Mdiamond];\n  end [shape=Msquare];\n}\n```\n\n## doxygen 生成 UML\n\n## links\n\n1. [关系](https://www.cnblogs.com/shindo/p/5579191.html)\n1. [基于 SOME/IP 的 AP AUTOSAR 实战](https://www.51fusa.com/client/information/informationdetail/id/1561.html)\n1. [适合程序员的 UML 绘图工具](https://juejin.cn/post/6844904039902085134)\n","tags":["UML"],"categories":["Program"]},{"title":"Nvidia-NPP","url":"/2022/03/02/Nvidia-NPP/","content":"\n## notes\n\n1. need `sudo` to run exe\n1. `export CUDACXX=/usr/local/cuda/bin/nvcc` when build cuda\n1. CMakeLists.txt: `CUDA_ADD_EXECUTABLE(resize resize.cpp)`\n\n## yuyv 2 rgb\n\n1. [link](https://stackoverflow.com/a/71473363) nvcc 替换成 g++也可用\n\n## basic\n\n1. install path: `/usr/local/cuda-xxx/`\n1. [函数名称中简写的意义](https://docs.nvidia.com/cuda/npp/nppi_conventions_lb.html)\n1.\n\n```\n\"A\" if the image is a 4 channel image this indicates the result alpha channel is not affected by the primitive.\n\"Cn\" the image consists of n channel packed pixels, where n can be 1, 2, 3 or 4.\n\"Pn\" the image consists of n separate image planes, where n can be 1, 2, 3 or 4.\n\"C\" (following the channel information) indicates that the primitive only operates on one of the color channels, the \"channel-of-interest\". All other output channels are not affected by the primitive.\n\"I\" indicates that the primitive works \"in-place\". In this case the image-data pointer is usually named pSrcDst to indicate that the image data serves as source and destination at the same time.\n\"M\" indicates \"masked operation\". These types of primitives have an additional \"mask image\" as as input. Each pixel in the destination image corresponds to a pixel in the mask image. Only pixels with a corresponding non-zero mask pixel are being processed.\n\"R\" indicates the primitive operates only on a rectangular \"region-of-interest\" or \"ROI\". All ROI primitives take an additional input parameter of type NppiSize, which specifies the width and height of the rectangular region that the primitive should process. For details on how primitives operate on ROIs see: Region-of-Interest (ROI).\n\"Sfs\" indicates the result values are processed by fixed scaling and saturation before they're written out.\n```\n\n1. [image malloc functions](https://docs.nvidia.com/cuda/npp/nppi__support__functions_8h_source.html)\n1. [resize function](https://docs.nvidia.com/cuda/npp/nppi__geometry__transforms_8h_source.html)\n1. [npp utils](https://github.com/NVIDIA/cuda-samples/tree/master/Common/UtilNPP)\n1. [cvtcolor](https://docs.nvidia.com/cuda/npp/group__image__color__model__conversion.html)\n\n## samples\n\n1. [NVIDIA-CUDALibrarySamples](https://github.com/NVIDIA/CUDALibrarySamples.git)\n1. [simple sample](https://blog.51cto.com/u_15357586/3790330)\n1. [malloc sample](https://sourcegraph.com/github.com/PacktPublishing/Learn-CUDA-Programming/-/blob/Chapter08/08_cuda_libs_and_other_languages/05_npp/imageFilter.cpp)\n\n## Links\n\n1. [main page](https://docs.nvidia.com/cuda/npp/index.html)\n"},{"title":"Gitlab-CI","url":"/2022/02/15/Gitlab-CI/","content":"\n## base\n\n1. [使用 clang-format 在 CI 中自动格式化 C++代码](https://lrita.github.io/2022/07/30/auto-clang-format-cpp-code/)\n\n## badges\n\n1. 显示 build 状态\n1. [what-are-gitlab-badges](https://www.benjaminrancourt.ca/what-are-gitlab-badges/)\n\n## format check\n\n```shell\ncheck-format:\n  image:\n    name: witekio/clang-format-checker\n    entrypoint: [\"\"]\n  script:\n    - run-clang-format.py -r src\n```\n\n## [include](https://docs.gitlab.com/ee/ci/yaml/#includelocal)\n\n1. 可以 include local file, private project and remote file。\n\n## [release artifacts](https://gitlab.phigent.io/help/user/project/releases/index)\n\n1. [release-artifacts-using-gitlab-cicd](https://crypt.codemancers.com/posts/2021-08-31-release-artifacts-using-gitlab-cicd/#generating-the-artifacts)\n1. [gitlab release 和 artifactory 结合](https://gitlab.phigent.io/help/user/project/releases/index#use-a-generic-package-for-attaching-binaries)\n   - notes: 注意 tag-name， ci 跑完之后会打 tag，然后又会运行 ci, 总共会跑两次\n   - 不能缺少 tag-name， 使用 rules: - if: $CI_COMMIT_TAG, 只有打 tag 的时候才运行\n   - [401 error](https://kinsta.com/knowledgebase/401-error/)\n1. 删除 tag 就能删除 release 对应内容\n1. gitlab release comment 支持 markdown 和 mermaid\n\n## pipeline\n\n1. [trigger other pipeline](https://docs.gitlab.com/ee/ci/triggers/)\n1. 前面加 dot，可以隐藏 job [link](https://docs.gitlab.com/ee/ci/jobs/#hide-jobs)\n\n## [ssh](https://docs.gitlab.com/ee/ci/ssh_keys/)\n\n1. 区分多种执行方式：\n   - shell: 需要 runenr\n   - docker: 在 docker 下执行，不需要指定 runner\n   - ssh\n1. [example](https://gitlab.com/gitlab-examples/ssh-private-key/-/blob/master/.gitlab-ci.yml)\n1. [ssh-key 要求](https://gitlab.com/gitlab-examples/ssh-private-key/-/issues/1#note_48526556)\n1. 需要在 deploy 机器上生成 ssh， private key 作为变量传入配置中\n1. shell 需要在 runner 机器上秘钥对，把公钥复制到部署板子上 `ssh-copy-id -i ~/.ssh/id_rsa.pub xxx@xxx`\n\n## [runner](https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-runners-section)\n\n1. 可以 reset token [link](https://docs.gitlab.com/runner/commands/#gitlab-runner-reset-token)\n1. gitlab repo ci 配置可以显示安装 runner 命令\n1. [commands](https://docs.gitlab.com/runner/commands/)\n1. `gitlab-ci-multi-runner list` list runners\n1. `gitlab-ci-multi-runner status`\n1. `/etc/gitlab-runner/` 配置路径\n1. 注册 project for runner:\n   - `gitlab-runner register`\n   - `gitlab-runner verify` 如果是感叹号执行这条命令\n   - `gitlab-runner unregister --name test-runner` 删除 runner\n1. [docker in docker](https://docs.gitlab.com/ee/ci/docker/using_docker_build.html#docker-in-docker-with-tls-enabled-in-the-docker-executor)\n1. [docker image param setting](https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-runnersdocker-section)\n   - 需要再创建 runner 的时候设置\n   - /etc/gitlab-runner/config.toml 可以直接改传递参数\n1. 使用 [shared runner](https://docs.gitlab.com/ee/ci/runners/runners_scope.html):\n   - 需要 admin 用户\n   - group 下默认有\n1. Group runners:This project does not belong to a group and cannot make use of group runners. 只需要 group tocken 就好\n\n## script\n\n1. 所有 job 都执行\n\n```\ndefault:\n  before_script:\n    - echo \"Execute this script in all jobs that don't already have a before_script section.\"\n\njob1:\n  script:\n    - echo \"This script executes after the global before_script.\"\n\njob:\n  before_script:\n    - echo \"Execute this script instead of the global before_script.\"\n  script:\n    - echo \"This script executes after the job's `before_script`\"\n```\n\n## deploy\n\n1. `sshpass -p $PASSWORD scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -r ./build $CUSTOM_USERNAME@$CUSTOM_IP:/var/www/html` sshpass 设置 ci 变量\n1. `sshpass -p $PASSWORD sshpass -p root rsync -avz -e \"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\" -r ./build $CUSTOM_USERNAME@$CUSTOM_IP:/var/www/html`\n\n## prefill variables\n\n1. [how to use prefill variables](https://gitlab.com/gitlab-org/gitlab/-/issues/300106)\n1. 两种方式：\n   - yml 文件中\n   ```yaml\n   variables:\n   PASSWORD:\n     value: \"123456\"\n     pdescription: \"password\"\n   CUSTOM_USERNAME:\n     value: \"petalinux\"\n     description: \"username\"\n   ```\n   - project CI/CD variable settings\n\n## [workflow](https://docs.gitlab.com/14.8/ee/ci/yaml/workflow.html)\n\n## keys\n\n### needs\n\n1. 可以用来串联 stage\n1. 传递 stage 文件需要 [link](https://docs.gitlab.com/ee/ci/yaml/index.html#needsartifacts)\n1. 可以看 pipeline efficiency [link](https://docs.gitlab.com/ee/ci/pipelines/pipeline_efficiency.html)\n1. [DAG](https://docs.gitlab.com/ee/ci/directed_acyclic_graph/)\n\n## rules\n\n1. [rules 模板](https://github.com/gitlabhq/gitlabhq/blob/2ea638391497c495798e0bab7c704af112789299/.gitlab/ci/rules.gitlab-ci.yml)\n1. 只手动编译， 有个按键可以执行所有 job\n\n```yaml\n.rules:if-tag-or-manual:\n  rules:\n    - <<: *if-tag\n    - when: manual\n\n.rules:always:\n  rules:\n    - when: always\n      allow_failure: true\n\n## fix mr detach issue； merge_request_event or push 禁止一个\nworkflow:\n  rules:\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n      when: never\n    - when: always\n\nstages:\n- build\n- test\n\nBuild::GPU_ubuntu20.04:\n  extends:\n    .rules:if-tag-or-manual\n\n```\n\n1. 失败自动 format\n\n```yaml\nBuild::Format:\n  rules:\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n      when: on_failure\n    - when: never\n  stage: build\n  tags:\n    - xxx\n  image: xxx\n  script:\n    - bash ci/format.sh\n```\n\n1. 在 mr 时自动运行全流程，or 手动选择要编译的平台\n\n```yaml\n\n.if-tag: &if-tag\n  if: '$CI_COMMIT_TAG'\n\n.if-merge-request: &if-merge-request\n  if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n\n.if-tag-or-mr: &if-tag-or-mr\n  if: '$CI_COMMIT_TAG'\n  if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n\n.rules:if-tag-or-mr-or-manual:\n  rules:\n    - <<: *if-tag-or-mr\n    - when: manual\n\n.rules:always:\n  rules:\n    - when: always\n\nstages:\n- build\n- test\n\nBuild::GPU_ubuntu20.04:\n  extends:\n    .rules:if-tag-or-mr-or-manual\n\nBuild::Vitis:\n  extends:\n    .rules:if-tag-or-mr-or-manual\n\n```\n\n1. [workflowrules 控制整个 pipeline](https://docs.gitlab.com/ee/ci/yaml/#workflowrules)\n\n```yaml\nworkflow:\n  rules:\n    - if: $CI_COMMIT_TITLE =~ /-draft$/\n      when: never\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n```\n\n1. when, can only be always or never when used with workflow. 不能 when: manual, 不支持\n1. 可以设置一个伪 job， manual 触发，后续 job 依赖伪 job\n1. 完善解决方案： 使用 [temlate](https://github.com/GNOME/gnome-shell/blob/4bbf6d497d34793693e5ac24fbe23854b481002b/.gitlab-ci.yml#L41)\n1. [avoid-duplicate-pipelines](https://docs.gitlab.com/ee/ci/jobs/job_control.html#avoid-duplicate-pipelines)\n\n```\n rules:\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n```\n\n1. [default-rules(>=14.3)](https://docs.gitlab.com/ee/ci/jobs/job_control.html#reuse-rules-in-different-jobs)\n\n## Notes\n\n1. [split-long-commands](https://docs.gitlab.com/ee/ci/yaml/script.html#split-long-commands)\n1. gtest 结果 xml 可以显示到 ci pipeline 结果中去 [link](https://docs.gitlab.com/ee/ci/unit_test_reports.html)\n\n```yaml\nstages:\n  - test\n\ncpp:\n  stage: test\n  script:\n    - ./gtxxx --gtest_output=xml:report.xml\n\n  artifacts:\n    when: always\n    reports:\n      junit:\n        - report.xml\n        - fail.xml\n```\n\n1. cache 时相同的 Key path 文件内容要完全一样，否则不能 cache\n1. deploy 两种方式：\n   - runner shell Executor: 交叉编译后传到板子上执行\n   - runner ssh Executor: 登录到板子，Mount 服务器执行(板子存储受限，不可重复，慢)\n1. `sudo su - gitlab-runner` ssh board 时要进入 runner 服务器 gitlab-runner 用户\n1. 如果想在板子上运行脚本，需要 ssh 执行器[link](https://blog.csdn.net/lihao21/article/details/109820904)\n1. ssh 后跟在远端执行的命令\n\n```\nscript:\n    - echo \"ssh board......................\"\n    - ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@10.31.1.170 \"whoami\"\n```\n\n1. [修改 runner build 位置 builds_dir](https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-runners-section) 配置路径`/etc/gitlab-runner/`或`~/.gitlab-runner/`\n\n## Links\n\n1. [Gitlab-ci:从零开始的前端自动化部署](https://zhuanlan.zhihu.com/p/184936276)\n1. [GitLab CI/CD](https://docs.gitlab.com/ee/ci/)\n1. [Enable Debug logging](https://docs.gitlab.com/ee/ci/variables/index.html#enable-debug-logging)\n1. [List all environment variables](https://docs.gitlab.com/ee/ci/variables/index.html#list-all-environment-variables)\n1. [CI/CD templates](https://docs.gitlab.com/ee/ci/examples/#cicd-templates)\n1. [Using SSH keys with GitLab CI/CD](https://docs.gitlab.com/ee/ci/ssh_keys/)\n1. [概念](https://segmentfault.com/a/1190000006120164)\n1. [gitlab 中文文档](https://docs.gitlab.cn/jh/ci/variables/)\n1. [test reports](https://gitlab.phigent.io/help/ci/unit_test_reports)\n1. [搭建一个使用 GitLab CI 的项目](https://juejin.cn/post/6847902219837292558)\n1. [Gitlab-Runner 原理与实现](https://blog.csdn.net/yejingtao703/article/details/83065591)\n\n## samples\n\n1. gitlab release + jfrog artifactory（如果 docker 有问题，通过 artifacts 传递文件， 在 release 传文件和配置链接）\n\n```yaml\nstages:\n  - build\n  - release\n\nvariables:\n  TEST_FILE0: \"test0-${PACKAGE_VERSION}-${CI_COMMIT_TAG}\"\n  TEST_FILE1: \"test1-${PACKAGE_VERSION}-${CI_COMMIT_TAG}\"\n  PACKAGE_VERSION: \"1.2.3\"\n  PACKAGE_REGISTRY_URL: \"http://artifacts.xxx.io/artifactory/dcv-cpp\"\n\ndefault:\n  before_script:\n    - JFROG_USER=release\n    - JFROG_PASSWD=xxx\n    - |\n      push_to_jfrog() {\n          FILE=$1;\n          JFROG_PATH=http://artifacts.xxx.io/artifactory/$2;\n          MD5=$(md5sum $FILE | awk '{print $1}');\n          curl -u\"${JFROG_USER}:${JFROG_PASSWD}\" -T $FILE -H \"X-Checksum-MD5:${MD5}\" -L \"${JFROG_PATH}\";\n      }\n\nbuild:\n  stage: build\n  image: hub.xxx.io/avp/cuda11.3.1-trt8.0.3.4-ros2-desktop-ubuntu20.04:v2.0\n  tags:\n    - phigent_runner_gpu\n  script:\n    - echo $CI_COMMIT_TAG\n    - mkdir bin\n    - echo \"${TEST_FILE0}\" > bin/${TEST_FILE0}\n    - echo \"${TEST_FILE1}\" > bin/${TEST_FILE1}\n    - |\n      if [ \"$CI_COMMIT_TAG\" != '' ]; then\n        push_to_jfrog bin/${TEST_FILE0} dcv-cpp/${CI_COMMIT_TAG}/${TEST_FILE0}\n        push_to_jfrog bin/${TEST_FILE1} dcv-cpp/${CI_COMMIT_TAG}/${TEST_FILE1}; fi\n\nrelease:\n  tags:\n    - phigent_runner_gpu\n  stage: release\n  image: registry.gitlab.com/gitlab-org/release-cli:latest\n  rules:\n    - if: $CI_COMMIT_TAG\n  script:\n    - |\n      release-cli create --name \"Release $CI_COMMIT_TAG\" --tag-name $CI_COMMIT_TAG \\\n        --assets-link \"{\\\"name\\\":\\\"${TEST_FILE0}\\\",\\\"url\\\":\\\"${PACKAGE_REGISTRY_URL}/${CI_COMMIT_TAG}/${TEST_FILE0}\\\"}\" \\\n        --assets-link \"{\\\"name\\\":\\\"${TEST_FILE1}\\\",\\\"url\\\":\\\"${PACKAGE_REGISTRY_URL}/${CI_COMMIT_TAG}/${TEST_FILE1}\\\"}\"\n```\n","tags":["Tools","CI"],"categories":["Tools","CI"]},{"title":"Image Process","url":"/2022/02/05/image_process/","content":"\n## 工具\n\n1. 7yuv 可以直接显示二进制格式图片 图片命名为 xxxx_960_576.yuv, 可以直接打开不用再输入尺寸\n\n## 视差图与深度图\n\n### 视差图\n\n1. [原理](https://blog.csdn.net/ppdyhappy/article/details/113618409)\n\n### 深度图\n\n### 视差图转深度图\n\n1. 公式: `depth[i] = f * b / disp[i]; ` f 为焦距，b 为基线\n1. [视差转深度](https://blog.nowcoder.net/n/947d9e8e6e3d41aa95d25e2daa9d14f6?from=nowcoder_improve)\n\n```\n#include <iostream>\n\n#include \"opencv2/highgui.hpp\"\n#include \"opencv2/imgcodecs.hpp\"\n#include \"opencv2/imgproc.hpp\"\n\nusing namespace std;\nusing namespace cv;\n\nconst double fx = 6.9972691554787775e+02;\nconst double baseline = 1 / 4.9899299400374328;\n\nint main(int argc, char** argv) {\n  Mat img = imread(\"disparity.png\", 0);\n  cout << img.type() << endl;\n  cout << \"fx: \" << fx << endl;\n  cout << \"baseline: \" << baseline << endl;\n  Mat depth(img.rows, img.cols, CV_32F);  //深度图\n  cout << depth.type() << endl;\n  //视差图转深度图\n  for (int row = 0; row < depth.rows; row++) {\n    for (int col = 0; col < depth.cols; col++) {\n      float d = img.ptr<uchar>(row)[col];\n\n      if (d == 0) continue;\n\n      depth.ptr<float>(row)[col] = fx * baseline / d;\n    }\n  }\n  imwrite(\"depth.png\", depth);\n  // Mat depth_8u1 = depth.clone();\n  Mat depth_8u1;\n  depth.convertTo(depth_8u1, CV_8U);\n  Mat color_mat;\n  applyColorMap(depth_8u1, color_mat, COLORMAP_TURBO);\n  imwrite(\"color_depth.png\", color_mat);\n}\n\n```\n\n## opencv 函数\n\n1. `.convert`类型转换 `depth.convertTo(depth_8u1, CV_8U);`\n1. `applyColorMap` 灰度图转彩色(深度彩色输出) `applyColorMap(depth_8u1, color_mat, COLORMAP_TURBO);`\n1. 获取 cv::Mat 矩阵元素 ` mat.ptr<float>(row)[col] = camera_f * camera_b / d;`\n\n## vector to rgb jpg\n\n```\n    std::vector<char> source_out(w * h * 3)\n    cv::Mat img(h, w, CV_8UC3);\n    for (unsigned int row_ind = 0; row_ind < h; ++row_ind) {\n      for (unsigned int col_ind = 0; col_ind < w; ++col_ind) {\n        img.at<cv::Vec3b>(row_ind, col_ind) =\n            cv::Vec3b(source_out[(row_ind * w + col_ind) * 3 + 2],\n                      source_out[(row_ind * w + col_ind) * 3 + 1],\n                      source_out[(row_ind * w + col_ind) * 3 + 0]);\n      }\n    }\n    cv::imwrite(\"output.jpg\", img);\n```\n\n## opencv 多张图片同事显示\n\n1. [OpenCV 一个窗口同时显示多张图片](https://python.iitter.com/other/302393.html)\n\n```\nstatic cv::Mat concat_images(std::vector<cv::Mat> imgs) {\n  using namespace cv;\n  const int MAX_PIXEL = 540;\n  int imgs_num = imgs.size();\n  auto sqrt_value = std::sqrt(imgs_num);\n  Size first_img_size = imgs[0].size();\n  int max_pixel = std::max(first_img_size.height, first_img_size.width);\n  double prop = max_pixel < MAX_PIXEL ? (double)max_pixel / (double)MAX_PIXEL\n                                      : (double)MAX_PIXEL / (double)max_pixel;\n  Size std_size(first_img_size.width * prop, first_img_size.height * prop);\n  VLOG(1) << \"std size: \" << std_size;\n\n  int img_cols = (sqrt_value - (int)sqrt_value == 0.0)\n                     ? std::floor(sqrt_value)\n                     : std::floor(sqrt_value) + 1;\n  VLOG(1) << \"img_cols:  \" << img_cols;\n  Mat tmp_img;\n  Point2i location(0, 0);\n  Mat result(std_size.height * ((imgs_num - 1) / img_cols + 1),\n             std_size.width * img_cols, imgs[0].type());\n  VLOG(1) << \"concat image size: \" << result.size();\n  for (int i = 0; i < imgs_num; i++) {\n    location.x = (i % img_cols) * std_size.width;\n    location.y = (i / img_cols) * std_size.height;\n    resize(imgs[i], tmp_img, std_size, prop, prop, INTER_LINEAR);\n    tmp_img.copyTo(result(Rect(location, std_size)));\n  }\n  return result;\n}\n\n```\n\n## 双线性 resize\n\n1. [示意图](https://excalidraw.com/#json=eOxeValvEDgQU1IrMOjU_,arZv-AnyFoupAOXfEKY1-w)\n2. [双线性插值算法（C++实现）](https://www.freesion.com/article/6921829775/)\n","tags":["Algorithm"],"categories":["Algorithm"]},{"title":"Debug","url":"/2022/02/03/Cpp-debug/","content":"\n## NOTE\n\n1. 这是因为链接时（link time）写死的 RPATH 或 RUNPATH 在 .so 文件中被保存了下来，即使运行时没有设置 LD_LIBRARY_PATH，动态链接器（ld.so）也能根据 .so 里的这些路径去找到对应的库。\n   - `readelf -d /usr/local/lib/python3.10/dist-packages/torch_sipu/lib/libtorch_sipu.so | grep -i 'rpath\\|runpath'` 查看动态库rpath和runpath\n1. `readelf --debug-dump=decodedline  build/cmake.linux-x86_64-cpython-3.10/bin/triton-tensor-layout | grep -oE '/[^ ]+\\.cpp:' | sort -u` 获取文件中debug信息包含的C++文件   \n1. \n1. 使用 -H（GCC/Clang）查看头文件包含的顺序。用于定位头文件顺序引起的问题\n1. (good)log 太多可以只把 error 重定向到文件 `command 2> error_log.txt`\n1. `std::numeric_limits<float>::max()` float 最大值\n1. `-Wno-xxx` 不启动 xxx warning\n1. [gcc 编译阶段打印宏定义的内容](https://www.cnblogs.com/suiyek/p/14164160.html)\n1. [100-gcc-tips](https://github.com/hellogcc/100-gcc-tips/blob/master/src/index.md)\n1. 没 link 报`undefined reference to`, link 但没 found `xxx was not found, cannot find xxx`\n1. `target_link_options(xxx PUBLIC -Wl,-unresolved-symbols=ignore-in-shared-libs)` 只要能找到 xxx 中的符号就行， link 的 shared lib 中未找到符号不报错， 忽略 link 的库解析，用于交叉编译，运行时能找到库\n1. [dbg-macro](https://github.com/sharkdp/dbg-macro)\n   - 灵感来源[rust !dbg](https://doc.rust-lang.org/std/macro.dbg.html)\n1. c++23 支持 stack trace [basic_stacktrace](https://en.cppreference.com/w/cpp/utility/basic_stacktrace)\n   - [backward-cpp](https://github.com/bombela/backward-cpp)\n   - [cpptrace](https://github.com/jeremy-rifkin/cpptrace)\n   - [boost stack trace](https://www.boost.org/doc/libs/1_80_0/doc/html/stacktrace/getting_started.html)\n1. 二进制依赖时， 需要先 find_package(glog REQUIRED), 否则\n   - op PRIVATE 依赖 glog: undefined reference to 错误, 未传递出 glog 依赖 （静态链接的可以 PRIVATE 依赖)\n   - op PUBLIC 依赖 glog: target was not found 错误, 传递出 glog 依赖\n1. `gcc/g++  --verbose`显示编译过程\n   - `set(CMAKE_CXX_FLAGS \"--verbose\") or -DCMAKE_CXX_FLAGS=\"--verbose\"` cmake 也可以显示编译信息； 比 `-DCMAKE_VERBOSE_MAKEFILE=ON` 显示更详细\n1. strip 是一个在 Unix-like 操作系统中用于删除或剥离目标文件或对象文件的符号信息的工具。\n   - 删除符号表信息：符号表包含了源代码中的函数和变量的名称。\n   - 删除调试信息：debug section 被删除\n1. function tracing 只需要 symbols，不需要 debug symbols（gcc -g）。strip 之后不能跟踪\n1. 动态符号（.dynsym）vs. 局部符号（.symtab）， .symtab 包含.dynsym; 可以用命令 strip 来手动去掉局部符号表， strip 前后用`readelf -s ./test`看, strip 只去除了.symtab\n1. strip 之后 gdb 不能使用 function 名字加断点\n1. strip 保留 symbol 文件，gdb 可使用 symbol-file 命令加载 symbol\n\n```sh\nmkdir -p symbol\n\n# 遍历lib文件夹下的所有.so文件\nfor file in lib/*.so*; do\n    # 提取文件名（不包括路径和扩展名）\n    filename=$(basename -- \"$file\")\n    filename_no_ext=\"${filename%.*}\"\n\n    # 去除符号表并将其保存到symbol文件夹下\n    strip -s -o symbol/\"$filename_no_ext.sym\" \"$file\"\n    strip -s \"$file\"\ndone\n```\n\n1. debug info 作用：\n   - 将内存地址映射到具体某行源代码\n   - 调用栈展开（stack unwinding）\n1. 在 C++中，如果函数有指定返回类型，但在实际代码中没有返回值或者在所有路径上都没有返回值，那么它的行为是未定义的（Undefined Behavior）。根据 C++标准，未定义的行为可能导致任何结果，包括程序崩溃、生成错误的结果或者在某些情况下看起来像是“正常工作”。\n1. `addr2line`其功能是将函数地址解析成文件名或行号的工具。 [addr2line debug](https://cloud.tencent.com/developer/article/1974629)\n1. 查看 debug 模式：gdb `l` and `info source`\n1. `-DCMAKE_BUILD_TYPE=RelWithDebInfo` release 模式编译，带 debug info\n   - 优化：开启（通常 -O2 或 -O3），以提高运行效率。 调试信息：仍然保留调试符号（-g），但由于优化的影响，变量和代码结构可能会与源代码对不上。\n   - 和Debug的区别：[link](https://chatgpt.com/share/67d7f153-5e80-8004-aa7f-4ab0353ad467)\n1. `-DCMAKE_CXX_FLAGS=\"-g\"` cmake 编译选项添加 debug info； release 模式也适用\n1. gcc 选项顺序很重要，如果两个选项冲突，则以后一个为准。可以用`-O3 -fno-inline-functions`既使用-O3 的功能又关闭函数内嵌功能。\n\n<!-- more -->\n\n1. 踩内存就指访问了不应该访问的内存地址\n1. 火焰图可以看调用关系\n1. multidefination 可能是在头文件中定义了函数，多个文件 include 函数, 导致多重定义\n1. segmantation fault 和 bus error 均是内存地址访问导致的问题，\n   - segmantation fault 是写了只读区域或访问了不能访问区域\n   - bus error 是访问地址未对齐\n1. `调试要用 debug 模式，性能测试用 release`\n1. crash 排查:\n   - 引用空指针\n   - 使用未初始化的内存\n   - 释放后使用，也就是使用悬垂指针\n   - 缓冲区溢出，比如数组越界\n   - 非法释放已经释放过的或者未分配的指针\n1. static 变量编译顺序，sample: dmlc 注册后处理函数时，如果注册分布在不同文件，需要先声明全局 static 变量后才能 register。 声明文件要在注册文件前执行。解决方法：专门找一个文件，先声明，后注册。\n1. debug 是编译 -DCMAKE_BUILD_TYPE=Debug， 否则 gdb 显示信息不全\n1. export LD_LIBRARY_PATH=xxxx:xxxx:xxx:$LD_LIBRARY_PATH, $LD_LIBRARY_PATH 放后面，否则容易链接到其他版本库\n1. 因为 assert 是一种 debug 工具，而 release 不是 debug，所以不启用。\n1. `NDEBUG` 这个编译宏可以控制是否启用 assert，如果定义了就不会执行\n1. debug 模式 assert 会出错， release 无问题，因为 assert 不起作用 [link](https://murphypei.github.io/blog/2020/01/assert-debug-release)\n\n```c++\nfor (i = 0; i < 100; ++i) {\n  int status = do_something ();\n  assert (status == 0);\n}\n\n// 不要写成这样\nfor (i = 0; i < 100; ++i)\n  assert (do_something () == 0);\n```\n\n## 未定义的行为\n\n1. 行为\n   - 在函数中遗漏返回语句（如果函数声明了返回类型）\n   - 读取或写入非法的内存地址\n   - 整数除以零\n   - 整数溢出\n   - 对 null 指针进行解引用\n   - 对已经释放的内存进行操作\n1. 结果\n   - 程序崩溃\n   - 程序正常运行，但生成错误的结果\n   - 程序看似正常运行，但在后台有潜在的错误或安全问题\n   - 在不同的编译器、不同的平台或不同的编译器优化级别下，有不同的行为\n\n## debug 和 releae 模式行为不一致\n\nC++在 Debug 和 Release 模式下运行不一致，可能由多种原因造成。以下是一些常见的原因：\n\n1. **未初始化的变量**：在 Debug 模式下，一些编译器会自动初始化变量（如将指针设置为 null 或整数设置为零），但在 Release 模式下，编译器通常不会这么做。如果你的代码依赖于这些自动初始化的行为，那么在 Release 模式下可能会出现问题。\n2. **内存越界访问**：Debug 模式通常包含额外的内存检查，例如检测数组越界访问。然而，Release 模式为了提高性能，通常不包含这些检查。如果你的代码中存在内存越界访问，可能在 Debug 模式下不会出现问题，但在 Release 模式下会导致各种未定义行为。\n3. **断言（assert）语句**：断言在 Debug 模式下是启用的，可以帮助找出不满足预期条件的情况。但在 Release 模式下，为了提高性能，通常会禁用断言。因此，一些在 Debug 模式下由于断言而被发现的问题，在 Release 模式下可能会被忽视。\n4. **优化**：Release 模式通常会开启更多的编译优化。这些优化有时可能会暴露出在 Debug 模式下不会出现的问题，特别是当代码包含未定义行为或者对于优化的假设不正确时。\n5. **多线程竞态条件**：在 Debug 模式下，线程的调度和执行速度可能与 Release 模式有所不同。如果代码中存在依赖于特定线程执行顺序的竞态条件，这种条件在 Debug 模式下可能永远不会出现，但在 Release 模式下就可能出现。\n   要解决这种问题，首先需要确定问题出现的具体位置。然后，检查是否存在上述问题，如未初始化的变量、内存越界、断言、错误的编译优化假设或多线程竞态条件等。使用静态代码分析工具、动态内存检查工具或者并发问题检查工具等也可以帮助找出问题的原因。\n\n## 对数\n\n1. opencv yml 文件\n1. python\n1. dump to file\n1. gdbserver + gdb + vscode\n1. gdb script to dump binary + tools(转为 float， 对比数值) 工具一键获取\n1. 自动比对功能， [ref](https://github.com/Xilinx/Vitis-AI/blob/29c76cb5c29eaebb31c553572bbad56228b27c17/src/vai_runtime/vart/dpu-runner/src/dpu_runner_base_imp.cpp#L457)\n\n## 常见错误\n\n### 链接错误\n\n1. [C++ 链接库顺序导致的符号未定义问题](https://murphypei.github.io/blog/2019/06/link-sort)\n1. `nm -C _.a/_.so` 查看库里符号 `-C` Decode low-level symbol names into user-level names\n1. 链接顺序： 从右到左\n\n### `bus error`\n\n1. Bus errors are rare nowadays on x86 and occur when your processor cannot even attempt the memory access requested, typically:\n\n   - using a processor instruction with an address that does not satisfy its alignment requirements. such as: 0x000000003, 4Bytes 对齐\n\n2. A bus error is trying to access memory that `can't possibly be there`. You've used an address that's meaningless to the system, or the wrong kind of address for that operation.\n\n3. x86 机器不设置就不会 bus_error [link](https://stackoverflow.com/a/8412760)\n\n### `segmentation fault`\n\n1. Segmentation faults occur when accessing memory which does not belong to your process. They are very common and are typically the result of:\n   - using a pointer to something that was deallocated.\n   - using an uninitialized hence bogus pointer.\n   - using a null pointer.\n   - overflowing a buffer.\n2. A segfault is accessing memory that you're `not allowed to access`. It's read-only, you don't have permission, etc...\n3. 主要与指针相关值相关, 未初始化，初始化为 nullptr\n\n### file leak: `Too many open files`\n\n1. file leak, not close\n1. `watch cat /proc/sys/fs/file-nr` get the current number of open files\n1. find open files limit per process: `ulimit -n`\n1. count all opened files by all processes: `lsof | wc -l`\n1. get maximum allowed number of open files: `cat /proc/sys/fs/file-max`\n1. `lsof -u xiyang.jia | wc -l` or `lsof | wc -l` to see how many files are open.\n1. Please note that `lsof | wc -l` sums up a lot of duplicated entries (forked processes can share file handles etc).\n   That number could be much higher than the limit set in `/proc/sys/fs/file-max`.\n   To get the current number of open files from the Linux kernel's point of view, do this:\n   `cat /proc/sys/fs/file-nr`\n   Example: This server has 40096 out of max 65536 open files, although lsof reports a much larger number:\n\n```\n# cat /proc/sys/fs/file-max\n65536\n# cat /proc/sys/fs/file-nr\n40096   0       65536\n# lsof | wc -l\n521504\n```\n\n## libs\n\n### errno\n\n```c++\n#include <errno.h>\n\nauto fp = fopen(\"/sys/class/gpio/gpio3/value\", \"r\");\nif (!fp) {\n  LOG(INFO) << \"strerror(errno)=\" << strerror(errno);\n}\n\n```\n\n1. NOTE: errno is key word, should not use be a variable\n1. errno is a preprocessor macro used for error indication\n\n### errno command\n\n1. `sudo apt install errno`\n1. errno -l\n\n## info\n\n1. `cat /proc/xxxx/status` 查看进程信息\n\n## crash 命令\n\n## [lldb](https://lldb.llvm.org/)\n\n1. [GDB to LLDB command map](https://lldb.llvm.org/use/map.html)\n1. `command script import /data/xiyang/llvm-project-86b69c3/llvm/utils/lldbDataFormatters.py`\n1. `call op.dump()`\n1. `type lookup xxx`\n1. `lldb -- python test.py` => `b emitError` 出错立即报错\n1. `bt`显示完整文件\n1. `f 8` 跳转到frame 8\n1. `frame variable` or `fr v` 打印局部变量\n1. `p results.end() - results.begin()` llvm smallvector打印length\n\n## [GDB 使用](https://sourceware.org/gdb/current/onlinedocs/gdb.html/)\n\n1. [100 个小技巧](https://wizardforcel.gitbooks.io/100-gdb-tips/content/break-on-entry.html)\n1. [100-gdb-tips](https://github.com/hellogcc/100-gdb-tips)\n1. [gdb-a-quick-guide-to-make-your-debugging-easier](https://johnysswlab.com/gdb-a-quick-guide-to-make-your-debugging-easier/)\n1. [原理](https://zhuanlan.zhihu.com/p/336922639)\n1. [入门指南](https://www.yanbinghu.com/2019/04/20/41283.html)\n1. [cheatsheet](http://users.ece.utexas.edu/~adnan/gdb-refcard.pdf)\n\n### 常用\n\n1. `(good) generate-core-file or gcore` 让被调试的进程产生 core dump 文件，记录现在进程的状态，以供以后分析; `gdb <program> <core_file>` or `gdb <porgam> => core <core_file>`\n1. `gcore $(pidof processname)` 命令行直接执行产生 coredump `-a`产生更详细信息\n1. `directory dir` 设置源文件查找路径(类似 include，设置到最后一级, 适合找单一文件, 或者把所有文件放到一个文件夹里)\n1. `(good) set substitute-path /from /to` 映射源码位置， `show substitute-path`查看映射表,`info sources`查看源码路径 `info source`查看 Compilation directory\n1. `(good) info proc all` 查看当前进程各种信息，包括 pid， 调试的命令, 是否 coredump, threads nums 等\n1. `set step-mode on`默认情况下，gdb 不会进入不带调试信息的函数。可以执行`set step-mode on`命令，这样 gdb 就不会跳过没有调试信息的函数, 可以使用调试汇编程序的办法去调试函数\n1. `info functions add` 列出 add 函数\n1. dump binary memory max_scores_dump.bin max_scores_vec.data() max_scores_vec.data()+max_scores_vec.size() 保存内存内容到文件, 可以配合脚本 dump 文件\n1. show env and show path 查看环境变量和 path\n1. gdb 调试时打不上断点，要使用 LD_PRELOAD 先加载动态库，找不到文件所在动态库，使用 speedscope\n1. sharedlibrary libxxx.so 加载库 libxxx.so\n1. help or help xxx 查看帮助\n1. `set logging file xxx; set trace-commands on;set logging on` 保存 log\n1. `start` 自动停止到 main\n1. until or u 函数里运行到第几行， 比打断点再运行效率高很多\n1. i b, i s, i w\n1. i locals or i lo 打印局部变量(i 不能打印单独变量)\n1. bt full or i s full 显示各个函数的局部变量值\n1. p xx 打印单个变量\n1. `x/40i 0x400000` `disassemble 0x4004e9,0x40050c` 查看.text 反汇编内容， i 表示 instruction。`info files`查看.text 地址\n1. x [/Nuf ] expr: examine memory at address expr; 打印内存\n1. x/12wf &test[0]: 打印 test 开始 12 个 float 内存\n1. x/16gf Q.data 打印 double\n1. x/12bx 打印二进制, 方便对数\n1. x/12bu 打印二进制, 8 位无符号\n1. display expr: show value of expr each time program stops\n1. set print pretty on 打印 class， 一个变量一行\n1. i functions\n1. `frame` 查看当前运行堆栈, 包含运行哪一行, bt 之后可以回溯栈上变量\n1. `up n or down n` 命令向上或向下选择函数堆栈帧，其中 n 是层数\n1. c-x a 可以快速看哪一行，再按一遍\n1. bt 可以显示运行到哪一行, l -1 或 l +1 显示当前行前后代码, l function\n1. b Foo::Foo 构造函数打断点\n1. b \\*0x1111 地址打断点\n1. save breakpoints test.bk && source test.bk 保存和加载断点\n1. 在程序入口处打断点, 先 readelf -h xxx 读入口地址，然后 b \\* 设置断点 [link](https://wizardforcel.gitbooks.io/100-gdb-tips/content/break-on-entry.html), 火焰图也可以看调用关系\n1. `info files` 获取各 section 地址\n1. info files 获取入口地址，b \\*地址或 b \\_start\n1. `info proc mappings` 查看进程的内存映射信息, 配合 info files 使用\n1. info sources 获取源码路径, 可用于 debug 具体使用到的头文件， 如 json 使用系统默认，未使用 cpm 依赖的\n   - gcc -H 编译选项可以列出来用到的头文件 [link](https://stackoverflow.com/a/60570938)\n1. (good)`i source` 列出`当前文件`各种信息，包括编译选项，路径等\n1. b \\_start 然后 ni 看指令执行\n1. shell ls 或 !ls 在 gdb 中执行 shell 命令\n1. whatis xxx 打印变量类型，\n1. ptype xxx or pt xxx 打印详细信息，xxx 可以是变量也可以是类型名\n   - 可以查看成员函数和变量，然后调用或者打印\n1. ptype + p sizeof() 查看类型占内存大小\n1. p object 直接打印对象，可以看出具体内存分配\n1. gdb -x gdb-script.txt --args ./my_program\n   or (gdb) source gdb-script.txt 执行命令集合\n1. 脚本里#号是注释\n1. set var x = value 程序运行可设置变量值\n\n### 图形化\n\n1. `(good)gdb -tui program`，或者运行 gdb 过程中使用`C-x a`组合键, 使用图形界面输出历史会不好查看\n1. `layout split or c-x 2` 如果既想显示源代码，又想显示汇编代码\n1. `layout asm`显示汇编\n1. `layout regs`显示寄存器\n1. winheight src -5 调整窗口\n\n### [shoart key](https://sourceware.org/gdb/onlinedocs/gdb/TUI-Keys.html)\n\n1. C-x s [single mode](https://sourceware.org/gdb/onlinedocs/gdb/TUI-Single-Key-Mode.html#TUI-Single-Key-Mode)\n1. C-x o Change the active window.\n1. c-n c-p next/previous history\n\n### base\n\n1. show endian 查看大小端\n1. 特别是当你在构建环境(build 目录)下调试程序的时候，可以直接运行 make：\n1. set var a = 1 改变变量值\n1. p func2::b 打印 func2 函数中变量 b 值\n1. p 'static-1.c'::var 打印文件中的变量\n1. i files 查看程序 elf 信息\n1. print sizeof(Type) 打印变量或类型占用内存 size\n1. .gdbinit gdb 配置文件\n1. 编译要加 -g [link](https://zhuanlan.zhihu.com/p/74897601)\n1. readelf -S xxx|grep debug 查看有没有 debug 段\n1. file xxx 查看有没有 stripped (编译不能加 -s), 也可以看到有没有 debug\n1. -fkeep-inline-functions 调试内联，编译的程序会变大很多\n1. set logging file log.txt, set logging on, set logging overwrite on 记录执行 log\n1. gdb -q 启动时不显示版本等信息\n1. gdb --args xxx ... && r 或 gdb xxx && set args ... && r 或 gdb xxx && r ...\n1. show env xxx 显示环境变量\n1. bt 或 i s(info stack) 显示程序栈 backtrace\n1. l 或 l + 向下显示源码， l - 向上显示源码，l 1,100 打印 1 到 100 行， l xxx:10 显示文件 xxx 第 10 行, l funciton 显示函数，\n1. bt 可以显示运行到哪一行, l -1 或 l +1 显示当前行前后代码\n1. step/finish 进入/返回函数 [link](https://wizardforcel.gitbooks.io/100-gdb-tips/content/finish-and-return.html)\n1. tb 如果想让断点只生效一次，可以使用“tbreak”命令\n1. source script 执行脚本中的命令\n1. p/x c 十六进制打印\n1. d 删除所有断点\n1. watch expr\n1. set print array-indexes on 打印数组下标\n1. i args 显示函数参数\n1. i locals 打印局部变量\n1. i sharedlibrary 显示共享链接库\n1. whatis xxx 打印变量类型， ptype xxx 打印详细信息，\n1. set history filename ~/.gdb_history， set history save on 保存历史\n1. shell ls 或 !ls 在 gdb 中执行 shell 命令\n1. 可以直接执行 cd 和 pwd\n1. 远程调试：\n1. emacs gdb-many-window, gdb 多窗口调试\n\n### [汇编调试](https://wizardforcel.gitbooks.io/100-gdb-tips/content/patch-program.html)\n\n1. trick: `layout split` => `layout regs` => `info files` => `b *start_adress` => `ni` 一步一步查看寄存器\n1. `ni` 下一条汇编指令\n1. `si` step by machine instructions rather than source lines\n1. `disassemble` 显示汇编\n1. `disassemble /m main` 将函数代码和汇编指令映射起来\n1. `disassemble /r main` 添加 16 进制信息\n1. `disassemble /mr main`\n1. `set disassemble-next-line on` 显示下一条汇编\n1. `display /3i $pc` 显示 pc 对应的后 3 条汇编指令, 一直显示`d display`删除\n1. `i line 13` 显示汇编地址\n1. `disassemble 0x4004e9,0x40050c`根据地址显示汇编\n1. `disassemble [Start],[End]`\n1. `disassemble [Function],+[Length]` 配合`info functions`使用\n1. `disassemble [Address],+[Length]`\n1. `b *main+12`函数偏移来打断点\n1. `p $pc`显示 PC 值\n1. `set var $pc=0x11111`设置 PC 值 or `jump line or jump *address`\n1. `set write on` and `disassemble /mr drawing` and `set variable *(short*)0x400651=0x0ceb` 修改二进制\n1. `info registers ` or `info all-registers` all 也显示向量寄存器\n\n### [~/.gdbinit](https://github.com/gdbinit/Gdbinit/blob/master/gdbinit)\n\n```\n# 保存历史命令\nset history filename ~/.gdb_history\nset history save on\n\n# 退出时不显示提示信息\nset confirm off\n\n# 按照派生类型打印对象\nset print object on\n\n# 打印数组的索引下标\nset print array-indexes on\n\n# 每行打印一个结构体成员\nset print pretty on\n```\n\n### funcutions\n\n1. gcc 自带 python stdlib 支持: 非常有用\n\n```python\npython\nimport glob\nsys.path.insert(0, glob.glob('/usr/share/gcc/python')[0])\nfrom libstdcxx.v6.printers import register_libstdcxx_printers\nregister_libstdcxx_printers (None)\nend\n```\n\n1. [打印 c++容器](https://wizardforcel.gitbooks.io/100-gdb-tips/content/print-STL-container.html)\n1. [print vector](https://stackoverflow.com/a/25499805)\n\n### 多线程\n\n1. gdb attach [pid], 如果卡死另外一个窗口打开\n1. info threads\n1. thread <thread id> 查看出问题 thread\n1. bt\n1. thread apply all bt 打印所有线程信息\n1. thread apply 1-2 bt 打印线程 1-2 堆栈信息\n\n### gdbserver\n\n1. `gdbserver --multi  :2000 ./test1`可以多次运行程序\n1. `gdbserver --once  :2000 ./test1` 运行一次就退出\n1. `gdbserver --attach  :2000 <pid>` 运行一次就退出\n1. why? 编译信息和路径都在 host 端，板端源码路径会有问题, 比如只读系统\n1. on client: `gdbserver :8000 ./test <args>`\n1. on host: `gdb: target remote <ip>:8000`\n1. c 开始，不能用 s\n1. 跨平台使用 gdb-multiarch， 或找相同 aarch 开开发机， cgdb 无 gdb-mutltiarch\n1. host 有源码就行, target 端不需要源码\n1. detach 断开链接\n1. jump \\_start 重新执行\n1. First, you need to have a copy of the program you want to debugput onto the target system. The program can be stripped to savespace if needed, as gdbserver doesn't care about symbols. All symbol handling is taken care of by the GDB running on the host system.\n\n## [gdbgui](https://github.com/cs01/gdbgui)\n\n1. 可以绘图： expression， 记录历史值\n1. memory view\n1. `sudo pip install gdbgui`\n1. `gdbgui -g gdb-multiarch` 使用 gdbserver\n1. `--port`可能不好使\n1. vnc 下调试用, 不是很流程\n1. `gdbgui --port 8080 --gdb-cmd=\"gdb -x gdbcmds.txt\"`\n\n## [seer](https://github.com/epasveer/seer)\n\n## [gdb-dashboard](https://github.com/cyrus-and/gdb-dashboard)\n\n1. `wget -P ~ https://git.io/.gdbinit` 只需要配置文件\n1. `help dashboard`\n1. `dashboard xxx` 可以选择打开和关闭 item 显示\n1. `dashboard` 到 dashboard\n1. `dashboard -layout !assembly breakpoints !expressions !history !memory !registers source stack threads variables`\n1. 竖屏显示会非常有用\n\n## [rr](https://github.com/rr-debugger/rr)\n\n1. gdb 回放\n\n## vscode C++调试\n\n1. variable右击可以加到watch中去\n1. 打不上断点，看下 info sharedlibrary 是否加载， Syms Read 是否为 yes, 注意配置文件中的 cwd 路径, 要与 board 板子上库的相对位置路径一致\n1. [vscode gdb gdbserver 关系](https://excalidraw.com/#json=5TP0xwb0oD-C3RgVJIvjW,YzWZI8S8UEP_-nFMxL-rxA)\n1. [source file map](https://code.visualstudio.com/docs/cpp/launch-json-reference#_sourcefilemap) 用于 gdbserver, 指定源码位置\n1. 安装 c/c++插件\n1. [教程](https://code.visualstudio.com/docs/cpp/config-linux) 生成 task.json（编译) 和 lauch.json (执行)\n1. [debug 教程](https://code.visualstudio.com/docs/editor/debugging)\n1. [settings 配置](https://code.visualstudio.com/docs/getstarted/settings)\n1. .vscode 配置文件下可以改 build 和执行 exe\n1. cmake 工程使用：命令来编译， lauch.json 配置执行环境\n1. [VsCode + gdb + gdbserver 远程调试 C++程序](https://blog.csdn.net/u014552102/article/details/122793256)\n   - 其中\"program\"是要被调试的程序在 vmware 中的路径 程序要和板子上一致\n   - \"cmd\"是要被调试的程序在 vmware 中的目录\n   - \"miDebuggerPath\" 是我们刚刚编译出来的 aarch64-buildroot-linux-gnu-gdb 在 vmware 中的路径\n   - \"miDebuggerServerAddress\"对应我们刚刚在板子上执行的 gdbserver 指令\n1. 注意不同 arch 要用 gdb-multiarch\n1. C-S-d 快速 debug\n1. debug console 中可以输入命令：`-exec info registers`\n1. gcc c++ stl 支持： `/usr/share/gcc/python/libstdcxx`, 需配置.gdbinit\n1. debug 点到变量上就会有值输出\n1. 可以选择 stack，看改 stack 各变量值\n1. terminal 下使用 tmux，否则容易断开\n1. 可以配置 compile_commands.json\n1. 配置快捷键，左手单独操作\n1. `open disassembly view`打开汇编窗口\n1. 选中字符 然后 add to watch\n1. debug variables 如果嵌套比较深，可以用`-exec print xxx`来看结果\n1. `Debug: start without debugging` debug 模式不进入断点，直接运行\n1. `Debug: run to cursor` 运行到当前 cursor\n1. `Debug: add logpoint`研究一下\n1. `compile_commands.json` 可以精准跳转，能帮助定位编译中的问题，如：json 头文件找错地方， 多重 include\n   - 也可以先编译一个 debug 程序 gdb info sources 看具体用到了哪个路径下的文件。\n1. 通过命令行 `-exec p xxx` 来完全显示变量\n1. 先打开 log， 然后执行命令，关闭 log， 搜索 [link](https://stackoverflow.com/a/41965636)\n\n### debug console\n\n1. 可以输入 gdb 命令，加`-exec`\n1. [intro](https://code.visualstudio.com/docs/editor/debugging)\n1. 可以进行算术运算\n1. 可以直接输入变量名\n\n## emacs gdb 使用\n\n1. emacs gdb-many-window, gdb 多窗口调试\n1. M-x gdb-many-windows and then gdb\n1. If the variable gdb-many-windows is nil (the default), M-x gdb normally displays only the GUD interaction buffer.\n1. NOTE: -g and not -O3(优化会跳过某些代码)\n1. gdb reload file: `file build/test` 不用重新加载， r 时自动重新加载。\n\n## [cgdb](https://github.com/cgdb/cgdb.git)\n\n1. [快捷键](https://cgdb.github.io/docs/CGDB-Mode.html#CGDB-Mode)\n1. esc 源码 i gdb commond\n1. `esc s` scroll mode in the GDB mode.\n1. o open the file dialog. debug 编译用到的所有源码，可用于定位头文件路径问题, 与`info sources`列出的文件相同，不过每行一个文件，更方便看\n\n1. [web](http://cgdb.github.io/)\n1. 两个窗口，一个显示源码，一个显示 gdb 信息.\n1. - - 调整窗口\n\n## strace\n\n1. strace - trace system calls and signals\n1. `strace -f -e file perf report 2>&1 | grep tips.txt` 可用于定位问题\n1. `strace -p $pid` 跟踪正在运行的进程\n\n## [Sanitizers](https://github.com/google/sanitizers)\n\n1. [ThreadSanitizer](https://clang.llvm.org/docs/ThreadSanitizer.html)\n   - 出错了会有两个线程信息，看信息对应的源码可以看到 data race 位置\n   - [bug 类型](https://github.com/google/sanitizers/wiki/ThreadSanitizerDetectableBugs)\n   - [输出信息说明](https://github.com/google/sanitizers/wiki/ThreadSanitizerReportFormat)\n1. To enable continue-after-error, compile with `-fsanitize-recover=address` and then run your code with `ASAN_OPTIONS=halt_on_error=0`. [link](https://github.com/google/sanitizers/wiki/AddressSanitizer#faq)\n1. [Memory error checking in C and C++: Comparing Sanitizers and Valgrind](https://developers.redhat.com/blog/2021/05/05/memory-error-checking-in-c-and-c-comparing-sanitizers-and-valgrind)\n1. sanitizer 消毒剂\n1. tools\n   - AddressSanitizer (detects addressability issues) and LeakSanitizer (detects memory leaks)\n   - ThreadSanitizer (detects data races and deadlocks) for C++ and Go\n   - MemorySanitizer (detects use of uninitialized memory)\n   - HWASAN, or Hardware-assisted AddressSanitizer, a newer variant of AddressSanitizer that consumes much less memory\n   - UBSan, or UndefinedBehaviorSanitizer\n1. `-fsanitize=thread` 多线程数据竞争编译之后运行会报警告 [sample](https://github.com/jiaxiyang/cpp_sandbox/blob/d2e077877f1a3dae47215540429b679bcf032c54/concurrency/atomic/atomic1.cpp)\n1. `-DCMAKE_CXX_FLAGS=\"-fsanitize=address(thread) -g -lasan(-static-libasan)\"` cmake 编译选项, 加了-lasan 就不需要 LD_PRELOAD\n\n```\nLD_PRELOAD=libasan.so\n```\n\n1. cmake address\n\n```cmake\nif(WITH_COMPILER_ASAN)\n    add_compile_options(-fsanitize=address)\n    link_libraries(asan)\nendif(WITH_COMPILER_ASAN)\n```\n\n## [core dump](https://www.jianshu.com/p/e38a3f1cf7f7)\n\n1. `(good) generate-core-file or gcore` 让被调试的进程产生 core dump 文件，记录现在进程的状态，以供以后分析; `gdb <program> <core_file>` or `gdb <porgam> => core <core_file>`\n1. 当程序意外终止时，系统将进程的地址空间内容及终止时的一些信息转存到 core 文件里。\n1. `file core.xxx`: ELF 64-bit LSB core file\n\n```\n% ulimit -c unlimited\n% ulimit -a\n% echo 1 > /proc/sys/kernel/core_uses_pid\n% echo '/tmp/core_%t_%p' > /proc/sys/kernel/core_patter\n% gdb ./build/heimdallr-app /tmp/core_1643444420_1426\n% bt\n% info frame\n% p symbol\n\n# 多线程\n% info threads\n% thread apply all bt\n\n```\n\n## [drmemory](https://github.com/DynamoRIO/drmemory)\n\n## [Valgrind](http://senlinzhan.github.io/2017/12/31/valgrind/)\n\n1. Release 模式快很多\n1. [常见问题](https://valgrind.org/docs/manual/faq.html)\n1. [quick start](https://valgrind.org/docs/manual/quick-start.html)\n1. [manual](https://valgrind.org/docs/manual/manual.html)\n1. [内存泄漏解释](https://blog.csdn.net/weixin_42144707/article/details/117642613?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-1-117642613-blog-126759520.pc_relevant_3mothn_strategy_and_data_recovery&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-1-117642613-blog-126759520.pc_relevant_3mothn_strategy_and_data_recovery&utm_relevant_index=1)\n1. [错误解释](https://valgrind.org/docs/manual/mc-manual.html#mc-manual.errormsgs)\n1. [工具解释](https://learnku.com/articles/46663)\n1. [DEBUG 神器 valgrind 之 memcheck 报告分析](https://blog.csdn.net/jinzeyu_cn/article/details/45969877)\n1. 编译程序时，需要加上-g 选项\n1. `sudo apt-get install libc6-dbg` # system libs should not stripped\n1. 编译加`-g`没有调试信息需要安装`valgrind-dbg`\n1. `--num-callers=10`调整 stack size\n1. `LD_PRELOAD=xxxx.so or --keep-debuginfo=yes` 定位 so 中的内存泄漏 [link](https://stackoverflow.com/a/13136609)\n\n### install\n\n```\n% wget ftp://sourceware.org/pub/valgrind/valgrind-3.13.0.tar.bz2\n% bzip2 -d valgrind-3.13.0.tar.bz2\n% tar -xf valgrind-3.13.0.tar\n% cd valgrind-3.13.0\n% ./configure && make\n% sudo make install\n\n# 交叉编译: NOTE: 开发板上路径要与--prefix安装路径一致\n% export CC=/opt/gcc-linaro-6.5.0-2018.12-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc\n% export CXX=/opt/gcc-linaro-6.5.0-2018.12-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-g++\n% export LD=/opt/gcc-linaro-6.5.0-2018.12-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-ld\n% export AR=/opt/gcc-linaro-6.5.0-2018.12-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-ar\n% export CPP=/opt/gcc-linaro-6.5.0-2018.12-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-cpp\n% cd valgrind-3.13.0\n% ./configure --host=aarch64-linux-gnu --target=aarch64-linux-gnu --prefix=/userdata/valgrind CFLAGS=-static\n% make -j8\n% sudo make install\n```\n\n### command\n\n1. `valgrind --log-file=test_memcheck.log --tool=memcheck --leak-check=full ls` : no summary\n1. `valgrind --log-file=test_memcheck.log --leak-check=full ls` 只显示 submary\n1. `valgrind --log-file=test_memcheck.log --leak-check=full --show-leak-kinds=all ls` 显示详细内容\n\n## procrank\n\n1. VSS - Virtual Set Size 虚拟耗用内存（包含共享库占用的内存）\n1. RSS - Resident Set Size 实际使用物理内存（包含共享库占用的内存）\n1. PSS - Proportional Set Size 实际使用的物理内存（比例分配共享库占用的内存）\n1. USS - Unique Set Size 进程独自占用的物理内存（不包含共享库占用的内存）\n\n## [jeprof](https://phantom9999.github.io/posts/361305b0.html)\n\n1. [jemalloc 的 heap profiling](https://www.yuanguohuo.com/2019/01/02/jemalloc-heap-profiling/)\n1. [jemalloc 检查 c++服务内存泄漏](https://juejin.cn/post/7041453881255018504)\n1. 内存增长不一定是代码里 new 了没有 delete 造成内存泄漏，还可能是消费能力不足导致内存堆积\n1. 如果发现某数据结构内存增加，多在日志中打印其 size 看看\n\n## Static application security testing (SAST) 静态扫描工具\n\n1. [lists](https://en.wikipedia.org/wiki/List_of_tools_for_static_code_analysis#C,_C++)\n1. [C++静态检查工具总结](https://blog.csdn.net/u013377887/article/details/108651945)\n\n### [lint](https://blog.csdn.net/you_shou/article/details/51199293)\n\n1. 在计算机科学中，lint 是一种工具程序的名称，它用来标记源代码中，某些可疑的、不具结构性（可能造成 bug）的段落。它是一种静态程序分析工具，最早适用于 C 语言，在 UNIX 平台上开发出来。后来它成为通用术语，可用于描述在任何一种计算机程序语言中，用来标记源代码中有疑义段落的工具。\n1. apt search lint 可以搜索各种 lint， 如 shellcheck， yamllint\n\n### [clang-tidy AST 静态检查工具](https://clang.llvm.org/extra/clang-tidy/)\n\n1. 提供 run-clang-tidy 脚本用于实现对整个项目的文件做检查，用起来非常的方便。Runs clang-tidy over all files in a compilation database.\n1. `run-clang-tidy -quiet -use-color` 需要 compile_commands.json\n1. `run-clang-tidy -p build_path -header-filter=.*` 指定编译路径； build_path 下包含 compile_commands.json\n1. `clang-tidy test.cpp -checks=-*,clang-analyzer-*,-clang-analyzer-cplusplus*` will disable all default checks (-_) and enable all clang-analyzer-_ checks except for clang-analyzer-cplusplus\\* ones.\n1. `clang-tidy -list-checks` 列出所有的 check\n1. `clang-tidy -checks=\"-*,misc-unused-using-decls\" path/to/simple.cc --` 找出 simple.cc 中所有没有用到的 using declarations. 后面的`--`表示这个文件不在 compilation database 里面，可以直接单独编 l 译；\n1. `clang-tidy -checks=\"-*,misc-unused-using-decls\" -fix path/to/simple.cc --`找出 simple.cc 中所有没有用到的 using declarations 并自动 fix(删除掉)\n1. `clang-tidy -checks=\"-*,misc-unused-using-decls\" path/to/project/a.cc` 找出 a.c 中没有用到的 using declarations. 这里需要 path/to/project/compile_commands.json 存在\n1. [check list](https://clang.llvm.org/extra/clang-tidy/checks/list.html)\n1. [.clang-tidy](https://github.com/llvm/llvm-project/blob/main/.clang-tidy)\n1. `clang-tidy --dump-config`查看当前配置\n\n### [cppcheck](https://github.com/danmar/cppcheck)\n\n### [cpplint](https://github.com/cpplint/cpplint)\n\n1. `#include <chrono>  // NOLINT [build/c++11]`\n1. 配置文件 CPPLINT.cfg\n1. An open-source tool that checks for compliance with Google's style guide for C++ coding.\n1. 它可以检测代码是否符合 Google 的编码规范，会把不符合规范的地方都指出来。\n1. `cpplint --recursive my_project_directory`\n1. [github clang-format-check](https://github.com/marketplace/actions/clang-format-check)\n\n## shared_ptr 调试\n\n1. 注意析构顺序，变量没被析构，memory pool 就被析构了，导致内存泄漏\n\n```c++\nclass ...{\n  std::shared_ptr<hbSysMem> output_mem_; // 还未加到memory pool\n  pg::utils::MemoryPool<hbSysMem> mem_pool_; //先析构了\n}\n```\n\n1. 注意 deleter， 可能会被改变，如： memory pool 改变传进去的 deleter\n1. use_count()打印引用次数\n\n## links\n\n1. [Writing a Linux Debugger](https://github.com/TartanLlama/minidbg)\n","tags":["Debug"],"categories":["Program","Cpp"]},{"title":"Links","url":"/2021/11/29/Links/","content":"\n## Usefull Tools\n\n1.  [`搜代码`：sourcegrah](https://sourcegraph.com/)\n1.  [`搜文档`：readthedocs](https://readthedocs.org/)\n1.  [`搜名字`：codelf](https://unbug.github.io/codelf/)\n1.  [`搜图书`：创世纪图书馆](http://libgen.rs/)\n1.  [`搜文章`：SCI Hub](https://sci-hub.se/)\n1.  [`搜图片`：unsplash](https://unsplash.com/)\n1.  [`搜主题`：programmingfonts](https://www.programmingfonts.org/#ubuntu)\n\n<!-- more -->\n\n1. [free-programming-books](https://github.com/EbookFoundation/free-programming-books/blob/main/books/free-programming-books-zh.md)\n1. [ai trends](https://epochai.org/trends)\n1. [csv view server](https://jiaxiyang-streamlit-example-streamlit-app-t8698c.streamlit.app/)\n1. [it-tools](https://it-tools.tech/)\n   - [text-diff](https://it-tools.tech/text-diff)\n   - [json-diff](https://it-tools.tech/json-diff)\n   - [json-prettify](https://it-tools.tech/json-prettify)\n1. [discord](https://discord.com/app)\n1. [developer-roadmap](https://github.com/kamranahmedse/developer-roadmap)\n1. [build-your-own-x](https://github.com/codecrafters-io/build-your-own-x)\n1. [system-design-primer(200K star)](https://github.com/donnemartin/system-design-primer)\n1. [gradio 机器学习应用](https://github.com/gradio-app/gradio)\n1. [图片共享](https://imgbb.com/)\n1. [imgur](https://imgur.com/)\n1. [Sage](https://poe.com/Sage)\n1. [ChatGPT app](https://github.com/lencx/ChatGPT)\n1. [chat.openai](https://chat.openai.com/)\n1. [aicodehelper](https://aicodehelper.com/chat/index.html)\n1. [在线 coding](https://code.meideng.dev/)\n1. [性能分析：speedscope](https://www.speedscope.app/)\n1. [代码仓库 fossies(可以设置 theme)](https://fossies.org/)\n1. [fossies doxygen lists](https://fossies.org/dox/all.html)\n1. [mermaid 在线 uml](https://mermaid.live/)\n1. [plantUML 在线 uml](http://www.plantuml.com/plantuml/uml/)\n1. [GraphvizOnline](https://dreampuf.github.io/GraphvizOnline)\n1. [cheatsheat 汇总](https://devhints.io/)\n1. [Linux man pages online](https://manned.org/)\n1. [在线画图工具 excalidraw](https://excalidraw.com/)\n1. [在线设计工具 canva](https://www.canva.com/)\n1. [在线设计工具 express.adobe](https://express.adobe.com/zh-Hans-CN/sp)\n1. [在线流程图 processon](https://www.processon.com/)\n1. [在线代码运行](https://wandbox.org/)\n1. [正则表达测试](https://regex101.com/)\n1. [正则表达式解释](https://tool.oschina.net/uploads/apidocs/jquery/regexp.html)\n1. [数学公式可视化](https://www.desmos.com/calculator?lang=zh-CN)\n1. [mathjax](https://www.mathjax.org/#demo)\n1. [color palettes](https://coolors.co/palettes/palettes)\n1. [paperwithcode](https://paperswithcode.com/)\n1. [在线 markdown](https://stackedit.io/)\n1. [在线 LaTeX](https://www.overleaf.com/)\n1. [在线抠图](https://picwish.cn/upload)\n1. [在线 web2pdf](https://www.web2pdfconvert.com/)\n1. [在线 image to ASCII art](https://manytools.org/hacker-tools/convert-images-to-ascii-art/)\n1. [在线 string to ASCII art](http://patorjk.com/software/taag/#p=testall&c=c%2B%2B&f=3D%20Diagonal&t=PhiGent%20Heimdallr) Larry 3D\n1. [fontawesome](https://fontawesome.com/icons)\n1. [threejs 网页动画](https://threejs.org/examples/#webgl_animation_skinning_morph)\n1. [北京时间，用于拍屏测延迟](http://www.daojishiqi.com/bjtime.asp)\n1. [移除图片视频背景 backgroundremover](https://github.com/nadermx/backgroundremover)\n\n## C++\n\n1. [c++ roadmap](https://roadmap.sh/cpp)\n1. [cling online](https://cppcli.net/)\n1. [cling the-interactive-c++-interpreter](https://github.com/root-project/cling/)\n1. [cpp.sh online cpp compiler](https://cpp.sh/)\n1. [modern-cpp-tutorial](https://github.com/changkun/modern-cpp-tutorial/blob/master/book/zh-cn/toc.md)\n1. [Awesome cpp](https://github.com/fffaraz/awesome-cpp#readme)\n1. [更快的 log spdlog](https://github.com/gabime/spdlog)\n1. [新 test 框架 Catch2](https://github.com/catchorg/Catch2)a\n1. [代码在线运行](https://www.techiedelight.com/generate-random-float-value-in-cpp/)\n1. [Compiler on line wandbox](https://wandbox.org)\n1. [CppInsights](https://cppinsights.io/)\n1. [Godbolt](https://godbolt.org/)\n1. [Cppreference](https://zh.cppreference.com/w/%E9%A6%96%E9%A1%B5)\n1. [modern-cpp-tutorial](https://github.com/changkun/modern-cpp-tutorial/tree/master/book/zh-cn)\n1. [modern-cpp-features](https://github.com/AnthonyCalandra/modern-cpp-features)\n1. [Bjarne Stroustrup](https://stroustrup.com/videos.html)\n1. [CppCoreGuidelines](http://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#main)\n1. [CppCon](https://cppcon.org/)\n1. [Back to Basics](https://cppcon.org/b2b/)\n1. [Youtube CppCon video](https://www.youtube.com/user/CppCon/)\n1. [CppCon resources](https://github.com/CppCon)\n1. [Compiler explore](https://github.com/compiler-explorer/compiler-explorer)\n1. [Compiler explore video](https://www.youtube.com/watch?v=kIoZDUd5DKw)\n1. [Quick C++ benchmark](https://quick-bench.com/)\n1. [Compile support](https://en.cppreference.com/w/cpp/compiler_support)\n1. [Debug tools](https://github.com/CppCon/CppCon2019/blob/master/Presentations/modern_linux_cpp_debugging_tools__under_the_covers/modern_linux_cpp_debugging_tools__under_the_covers__greg_law_and_dewang_li__cppcon_2019.pdf)\n1. [Lifetime analysis for everyone](https://github.com/CppCon/CppCon2019/blob/master/Presentations/lifetime_analysis_for_everyone/lifetime_analysis_for_everyone__matthias_gehre_gabor_horvath__cppcon_2019.pptx)\n1. [Interview](https://github.com/huihut/interview)\n1. [CS-Notes](https://github.com/CyC2018/CS-Notes)\n1. [Factory pattern](https://blog.csdn.net/qq_38238296/article/details/79841395?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param)\n\n## C++ Concurrent Programming\n\n1. [awesome-parallel-computing](https://github.com/taskflow/awesome-parallel-computing)\n1. [taskflow](https://github.com/taskflow/taskflow)\n1. [C++ Concurrency in action 2](https://b-ok.lat/book/3688262/d57395?dsource=recommend&regionChanged=&redirect=4729105)\n1. [C++ Concurrency in action Chinese](https://github.com/xiaoweiChen/CPP-Concurrency-In-Action-2ed-2019)\n1. [Back to basics locks and tasks](https://github.com/CppCon/CppCon2019/blob/master/Presentations/back_to_basics_atomics_locks_and_tasks/back_to_basics_atomics_locks_and_tasks__rainer_grimm__cppcon_2019.pdf)\n1. [thrust](https://github.com/thrust/thrust)\n1. [nebula](https://github.com/vesoft-inc/nebula)\n\n## Emacs\n\n1. [GNU Emacs](https://www.gnu.org/software/emacs/refcards/pdf/refcard.pdf)\n1. [Melpa](http://melpa.org/)\n1. [Emacs 黑客列表](https://manateelazycat.github.io/emacs/2019/05/12/emacs-hackers.html)\n1. [Emacs China](https://emacs-china.org/)\n1. [Tour Of Emacs](https://www.gnu.org/software/emacs/tour/index.html)\n1. [Awesome emacs](https://github.com/emacs-tw/awesome-emacs)\n1. [Purcell config](https://github.com/purcell/emacs.d)\n1. [Xah Lee](http://ergoemacs.org/emacs/emacs.html)\n1. [Zamansky Config](https://github.com/zamansky/dot-emacs)\n1. [Zamansky Tutorial](https://cestlaz.github.io/stories/emacs/)\n1. [My Config](https://github.com/jiaxiyang/100ms_dot_emacs)\n1. [My Leader Key](https://github.com/jiaxiyang/leader-key-mode/blob/master/leader-key-mode.el)\n1. [Wcy Config](https://github.com/wcy123/100ms_dot_emacs)\n1. [Emacs Pinky](http://ergoemacs.org/emacs/emacs_pinky_2020.html)\n1. [Org Mode](https://orgmode.org/)\n1. [Org Guide](https://orgmode.org/orgguide.pdf)\n1. [org card](https://orgmode.org/worg/orgcard.html)\n1. [Org Mode Chinese Tutorial](https://www.cnblogs.com/Open_Source/archive/2011/07/17/2108747.html)\n\n## Haskell\n\n1. [learn you haskell book](http://learnyouahaskell.com/chapters)\n2. [haskell Tutorial](https://www.youtube.com/watch?v=02_H3LjqMr8)\n3. [Functors, Applicatives, And Monads In Pictures](https://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html)\n\n## Rust\n\n1. [Rust 中文杂志](https://rustmagazine.github.io/rust_magazine_2021/)\n2. [陈天](https://www.zhihu.com/people/tchen/posts)\n3. [陈天 rust 培训](https://www.youtube.com/watch?v=ZVIlcsYaDZY)\n4. [Cheats](https://cheats.rs/)\n5. [Officail website](https://www.rust-lang.org/)\n6. [Official resource](https://www.rust-lang.org/learn)\n7. [Awesome Rust](https://github.com/rust-unofficial/awesome-rust#readme)\n8. [Rust learning](https://github.com/ctjhoa/rust-learning)\n9. [Forum](https://users.rust-lang.org/)\n10. [crates.io](https://crates.io/)\n11. [Rust book](https://doc.rust-lang.org/book/)\n12. [Rust book Chinese](http://120.78.128.153/rustbook/)\n13. [Editions](https://doc.rust-lang.org/edition-guide/editions/index.html)\n14. [The Rust Reference](https://doc.rust-lang.org/reference/index.html)\n15. [The Rustonomicon](https://doc.rust-lang.org/nomicon/index.html)\n16. [Cargo book](https://doc.rust-lang.org/cargo/)\n17. [Rust examples](https://doc.rust-lang.org/stable/rust-by-example/)\n18. [Rust examples Chinese](https://rust-by-example.budshome.com/index.html)\n19. [STD library](https://doc.rust-lang.org/std/index.html)\n20. [Rust with C](https://doc.rust-lang.org/stable/embedded-book/interoperability/index.html)\n21. [Rust online programming](https://play.rust-lang.org/)\n22. [Ferris pictures](https://rustacean.net/)\n23. [Rust gameboy](https://github.com/mohanson/gameboy)\n24. [Rust zhihu](https://www.zhihu.com/topic/19674381/intro)\n25. [Rust 张汉东](https://www.infoq.cn/article/Uugi_eIJusEka1aSPmQM)\n26. [Concept video](https://www.youtube.com/watch?v=SZvs15hC81U)\n27. [Youtube Rust channel](https://www.youtube.com/channel/UCaYhcUwRBNscFNUKTjgPFiA)\n\n## Cmake\n\n1. [Cmake Tutorial](https://cmake.org/cmake/help/v3.19/guide/tutorial/)\n1. [Cmake Buildsystem](https://cmake.org/cmake/help/latest/manual/cmake-buildsystem.7.html)\n1. [Effective Modern Cmake](https://gist.github.com/mbinna/c61dbb39bca0e4fb7d1f73b0d66a4fd1)\n1. [Deniz Bahadir 2019](https://www.youtube.com/watch?v=y9kSr5enrSk)\n1. [Deniz Bahadir 2018 traditional and modern camke](https://www.youtube.com/watch?v=y7ndUhdQuU8)\n1. [Beniz Bahadir PPT](https://github.com/Bagira80/More-Modern-CMake)\n1. [OO Cmake](https://zhuanlan.zhihu.com/p/76975231)\n1. [Cmake Concept](https://ukabuer.me/blog/more-modern-cmake)\n\n## Cheatsheet\n\n1. [Gdb](http://users.ece.utexas.edu/~adnan/gdb-refcard.pdf)\n1. [Pdb](https://github.com/nblock/pdb-cheatsheet/releases/download/v1.2/pdb-cheatsheet.pdf)\n1. [Emacs](https://www.gnu.org/software/emacs/refcards/pdf/refcard.pdf)\n1. [Org mode](https://www.gnu.org/software/emacs/refcards/pdf/orgcard.pdf)\n1. [Dired](https://www.gnu.org/software/emacs/refcards/pdf/dired-ref.pdf)\n1. [Vim](https://linuxhandbook.com/vim-cheat-sheet/)\n1. [Bash](https://github.com/zhouyiqi91/awesome-cheatsheets/blob/master/languages/bash.sh)\n1. [Tmux](http://comtronic.com.au/blog/wp-content/uploads/comtronic_cheatsheet_tmux_A4.pdf)\n1. [Github Git](https://github.github.com/training-kit/downloads/github-git-cheat-sheet.pdf)\n1. [Matlab](http://sites.nd.edu/gfu/files/2019/07/cheatsheet.pdf)\n1. [Markdown](https://www.markdownguide.org/cheat-sheet/)\n1. [Oh my zsh](https://github.com/ohmyzsh/ohmyzsh/wiki/Cheatsheet)\n1. [Rust](https://cheats.rs/rust_cheat_sheet.pdf)\n\n## Git\n\n1. [git linus](https://www.youtube.com/watch?v=4XpnKHJAok8)\n1. [Git Book](https://git-scm.com/book/en/v2/Git-Internals-Plumbing-and-Porcelain)\n1. [Git Book Chinese](https://git-scm.com/book/zh/v2)\n1. [Git Internals: the most important concept](https://git-scm.com/book/en/v2/Git-Internals-Plumbing-and-Porcelain)\n1. [Useful Git Commands](https://zhuanlan.zhihu.com/p/132573100)\n\n## AI\n\n1. [Netron](https://github.com/lutzroeder/Netron)\n\n## Data Structures and Algorithms\n\n1. [hello-algo](https://github.com/krahets/hello-algo?tab=readme-ov-file)\n1. [算法动画](https://visualgo.net/zh)\n1. [Boost Graph Library(BGL)](https://www.boost.org/doc/libs/1_74_0/libs/graph/doc/)\n\n## Design Patterns\n\n1. [Microsoft Application Architecture Guide, 2nd Edition](<https://docs.microsoft.com/en-us/previous-versions/msp-n-p/ff650706(v=pandp.10)>)\n1. [Refactoring Guru](https://refactoring.guru/)\n1. [Picture of Design Patterns](https://design-patterns.readthedocs.io/zh_CN/latest/index.html)\n1. [Awesome Design Patterns](https://github.com/DovAmir/awesome-design-patterns)\n\n## Awesome\n\n1. [Awesome](https://github.com/sindresorhus/awesome)\n\n## Code Style\n\n1. [C++](https://zh-google-styleguide.readthedocs.io/en/latest/google-cpp-styleguide/contents/#)\n1. [Python](https://zh-google-styleguide.readthedocs.io/en/latest/google-python-styleguide/contents/)\n1. [Shell](https://zh-google-styleguide.readthedocs.io/en/latest/google-shell-styleguide/contents/)\n\n## Code Format\n\n1. All: [format-all](https://github.com/lassik/emacs-format-all-the-code)\n1. C/C++: [ClangFormat](https://clang.llvm.org/docs/ClangFormat.html)\n1. Rust: [rustfmt](https://github.com/rust-lang/rustfmt)\n1. Python: [black](https://github.com/ambv/black)\n1. Shell: [shfmt](https://github.com/mvdan/sh) (install from github release)\n1. Cmake: [cmake-format](https://github.com/cheshirekow/cmake_format)\n1. Markdown: [prettier](https://prettier.io/)\n\n## Code Analysis\n\n1. [Clang-tidy](http://clang.llvm.org/extra/clang-tidy/index.html)\n","tags":["Links"],"categories":["Summary"]},{"title":"resume","url":"/2021/10/05/resume/","content":"\n## links\n\n1. [rxresu.me](https://rxresu.me/)\n1. [Reactive-Resume](https://github.com/AmruthPillai/Reactive-Resume) 填写内容，自动生成简历\n1. [简历用语](https://github.com/resumejob/awesome-resume)\n1. [Orbit-Theme](https://github.com/xriley/Orbit-Theme)\n1. [Awesome-CV](https://github.com/posquit0/Awesome-CV) CV: Curriculum Vitae 个人简历\n1. [overleaf awesome-cv template](https://www.overleaf.com/latex/templates/awesome-cv/dfnvtnhzhhbm)\n1. [latexcv](https://github.com/jankapunkt/latexcv/tree/master/sidebar)\n1. [awesome-resume-for-chinese](https://github.com/dyweb/awesome-resume-for-chinese)\n"},{"title":"Programming experience","url":"/2021/10/03/Programming-experience/","content":"\n## NOTE\n\n1. gpt 作为老师\n1. 如果一个问题能轻易用 chatgpt 回答的，那不是学习重点\n1. chatgpt 将提升人类全体的水平, 最差也是 chatgpt 水平，chatgpt 是 baseline\n1. 思路 first\n1. `Think twice, code once`\n1. 开发方式： windows wsl + docker\n1. 可以通过 discord 宣传和需求建议\n1. [OpenSSF Best Practices](https://www.bestpractices.dev/en/projects) [doctest](https://github.com/doctest/doctest)\n\n## 工程组织\n\n1. 两种方式, 主要区别是 include 和 test 是不是和模块 src 在一个文件夹\n   - 一起：[参考 opencv](https://github.com/opencv/opencv)\n   - 分离：[参考 grpc](https://github.com/grpc/grpc)\n\n## Software Development Modules\n\n1. 基础功能\n   - 日志系统和调试系统\n   - 错误处理\n   - [测试](https://cs.lmu.edu/~ray/notes/unittesting/)：单元测试,集成测试,smoke test 系统测试，性能测试，回归测试，压力测试。通过无法证伪来证明正确性。\n   - samples/demo\n   - docs and docs tests\n   - changelog or release note [tensorrt release notes](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html) Ensure you are familiar with the NVIDIA TensorRT Release Notes for the latest new features and known issues.\n1. 重构\n   - 架构\n   - 易读\n   - 代码风格\n1. 性能\n   - profiling\n   - benchmarks\n   - 加速\n1. 支撑\n   - 版本控制\n   - CI/CD\n\n<!-- more -->\n\n## 重构\n\n1. 最重要的是重构的`节奏感`，小的步子可以更快的前进，请保持代码永远处于可工作状态。（chunye 就是这样)\n1. 重构前现有`测试`\n1. 小步修改\n1. 每次修改后就运行测试\n1. 每次重构就提交代码，push 前把修改合并成更有意义的提交\n1. 永远将函数的返回值命名为\"result\"\n1. 尽量`移除局部变量`，使用小函数替代（临时变量会带来麻烦）\n1. 重构的关键在于运用大量微小且保持软件行为的步骤，一步步达成大规模修改。重构过程中几乎`可以随时停下来，不影响软件功能`。\n1. 区分`重构`和`性能优化`；重构是为了让代码更容易理解，更容易改变。程序可能更快，也可能更慢。在性能优化是，只关心让程序运行的更快，代码可能更难理解和维护。\n1. 两顶帽子：`添加新功能`和`重构`。添加新功能时`不应该修改既有代码，只管添加新功能`，重构时`不能添加新功能，只管调整代码结构`\n1. 重构的主要功能：\n   - `改进软件设计`\n   - `使软件更容易理解`\n   - `帮助找到bug`\n   - `提高编程速度`\n1. 重构的`唯一目的就是让我们开发更快，用更少的工作量创造更大的价值`。\n1. 代码所有权边界：接口的使用者（用户）与声明者（作者）彼此独立，声明者无权修改使用者的代码。添加新接口，旧的接口标记为不推荐使用(deprecated)。接口会变得复杂。\n1. 重构和性能：有性能测试工具，发现最耗时代码进行优化\n\n## code of conduct\n\n1. [llvm code of conduct](https://llvm.org/docs/CodeOfConduct.html)\n\n## Principles of Software Engineering\n\n1. 迭代和重构。\n1. 自动化\n1. 可重复\n1. 学习性测试：使用第三方代码，第一步不要在生产代码中实验新东西，而是单独编写测试来浏览和理解第三方代码。区分学习和整合。\n1. 小步快跑，节奏\n1. KISS (Keep It Simple Stupid)\n1. DRY (Don’t Repeat Yourself) (generic programing, 提出重复代码公用....)\n1. `层次`(抽象接口组成):写代码思路不应该是树状，而应该是有层次的，抽象出来层次，屏蔽细节，只依赖上个层次，不关注更低级别的层次, 每一层都相当于一种语言，解决一类问题，这样的系统更健壮。见[sicp](https://www.youtube.com/watch?v=YCR03O5EUdI&list=PLkEwH_Z2WOlppy8oUfrGwFVlOuKyo3RO_&index=5)\n1. 抽象,控制复杂度， 黑盒抽象\n1. `边界，隔离`\n1. 具体规则：\n   - `命名规则`：类名是名词，方法名是动词。\n   - `函数规则`：尽可能短小，只做一件事，每个函数一个抽象层次，名字不怕长，尽量少参数，无 side effect.\n   - `类规则`：短小，单一权责，高内聚(方法和变量互相依赖）\n   - `系统设计规则`： 多写测试，不可重复，表达力(好名字，好结构...), 尽可能少的类和函数(和前三条冲突，优先级最低）\n1. 一键： 单个命令构建系统，单个命令运行所有测试\n1. 出错仔细看 log，调试步步为营\n1. 多看文档\n1. 记录工作日志\n1. 区分对内和对外，区别对待\n1. 不要重复造轮子\n1. YAGNI (You Aren’t Gonna Need It) [link](https://medium.com/swlh/6-principles-of-software-engineering-that-every-developer-should-know-7868f362b633)\n1. 高内聚，低耦合\n1. 正交\n1. 分离`控制和逻辑`。`控制`：与业务逻辑无关的代码或系统的控制，如：多线程，异步，部署等，`逻辑`：业务逻辑，即解决用户问题的逻辑。\n1. 权衡\n1. 实用易用\n1. SOLID 原则\n   - `单一职责原则 SRP` 一个类只做一件事\n   - `开闭原则 OCP` 软件中的对象（类，模块，函数等等）应该对于扩展是开放的，但是对于修改是封闭的\n   - `里氏替换原则 LSP` 子类应该可以完全替代父类，也就是说在使用继承时，只扩展新功能，不要破坏父类原有的功能。\n   - `接口隔离原则 ISP` 客户端不应该依赖它不需要的接口。如果一个接口在实现时，部分方法由于冗余被客户端空实现，则应该将该接口拆分，让实现类只依赖自己需要的接口。\n   - `依赖倒置原则 DIP` 细节应该依赖与抽象，抽象不应该依赖于细节。把抽象层放在程序设计的高层，并保持稳定，程序的细节变化由低层的实现层来完成。\n\n## Theories of Programming Languages\n\n1. SICP\n1. [book](https://people.cs.uchicago.edu/~blume/classes/aut2008/proglang/text/offline.pdf)\n1. [程序设计语言原理](https://www.jb51.net/books/163654.html) [英文版本](http://www.sci.brooklyn.cuny.edu/~chuang/books/sebesta.pdf)\n1. 通用的编程语言模型: 基本元素(过程和数据)，组合的方法(函数，结构体，类)，抽象的方法\n\n## [The Study of Programming Languages](https://cs.lmu.edu/~ray/notes/plstudy/)\n\n## [Programming paradigm](https://en.wikipedia.org/wiki/Comparison_of_programming_languages)\n\n1. [Functional Programming](https://www.info.ucl.ac.be/~pvr/paradigms.html)\n2. Object Oriented Programming\n3. Imperative programming\n4. Procedural programming\n5. Generic\n6. Reflective\n7. Event-drive\n8. [paper Programming Paradigms for Dummies](https://www.info.ucl.ac.be/~pvr/VanRoyChapter.pdf)\n\n## [Programming Language Concepts](https://cs.lmu.edu/~ray/notes/plconcepts/)\n\n1. Structure (Syntax 句法)\n2. Meaning (Semantics 语义)\n3. Names, Binding, and Scope (Declarations)\n4. Evaluation (Expressions)\n5. Execution (Control Flow)\n6. Types\n7. Functional Abstraction (Subroutines and Coroutines)\n8. Data Abstraction (Objects and Modules)\n9. Concurrency\n10. Metaprogramming\n11. [link](https://cs.lmu.edu/~ray/classes/pl/)\n\n## Language Features [王垠 如何掌握所有的程序语言](http://www.yinwang.org/blog-cn/2017/07/06/master-pl)\n\n1. 编程语言取舍：[link](https://rustmagazine.github.io/rust_magazine_2021/chapter_4/rust-to-system-essence-lang.html)\n   - `性能`：编译或解释语言，并发\n   - `安全`： 内存安全(GC 或 RAII,所有权), 并发安全,类型安全\n   - `表达力`：泛型，宏\n2. Compiled language or Interpreted language\n3. Purely functional, Side effect\n4. Statically typed or Dynamically typed\n5. Statement and Expression\n6. Type inference\n7. pattern matching\n8. Lazy\n9. RAII or GC (Performance)\n10. Safety(memory-safety and thread-safety)\n11. Abstraction Level\n12. Zero-cost Abstraction\n13. Multi-paradigm\n14. Packages Management\n15. Concurrency Mechanisms\n\n## Rust 例子\n\n![概念层次图](https://static001.infoq.cn/resource/image/2d/91/2d36b45f6905a13a310e6447778ca391.jpg)\n\n## 思维模式\n\n1. 官方资料 first\n1. `3W`: what, why, how [dds introduction about](https://www.dds-foundation.org/what-is-dds-3/#)\n   - 查看各种技术先看 about\n1. `5W2H`: 原因（为什么 Why）、对象（是什么 What）、地点（在哪里 Where）、时间（什么时候 When）、人员（是谁 Who）、方法（怎么做 How）、程度（How much）\n1. `可重复`和`自动化`是非常重要的思维工作方式。版本管理 git, Jenkins, docker, CI, Rust Cargo.lock 文件, emacs tmux 插件, 软件一条命令安装(自动化，可重复), 代码风格 fmt 是自动化也是风格可重复，work log(可重复）。\n1. `effect`和`side effect副作用`。`纯函数`的行为只依赖它的接口。非纯函数副作用：引起环境，上下文的改变。[side effect](https://wangji.pro/%E6%9C%89%E5%85%B3%E6%B3%9B%E5%87%BD%E7%BC%96%E7%A8%8B%E4%BD%A0%E9%9C%80%E8%A6%81%E7%9F%A5%E9%81%93%E7%9A%84%E4%B8%80%E4%BA%9B%E7%9F%A5%E8%AF%86-effect-%E5%92%8C-side-effect/) 指的是那些任何主动或被动地对“非当前局部环境变量”的访问的操作。注意，“非当前局部环境变量”不仅仅包括访问全局变量和静态变量。这个操作可能非常广泛，甚至不局限于变量，还包括访问外部 IO，执行某些特殊指令，以非正常返回的方式引起的主动或被动出栈（比如异常）等行为。\n1. 多用纯函数，非纯函数会增加系统状态，使系统复杂化，容易出问题。组合爆炸\n1. [monad](http://www.ruanyifeng.com/blog/2015/07/monad.html)\n\n## Programming experience\n\n1. 一个函数不要超过 50 行，车企验证\n1. 多看 FAQ, 例如 tensorrt FAQ\n1. 软件架构多关注接口，接口决定了系统运行方式，项目讲解多关注接口; 用到的架构能够手写\n1. 关注问题的本质，剥离出不重要的东西，多关注核心。比如学 cutlass，关注 cute，分层结构，什么测试代码，无关紧要的东西就不要关注了\n1. 计算机科学里面，万变不离其宗，最核心的只有：`算法、数据结构、体系结构` [link](https://www.zhihu.com/question/52965880/answer/133178593)\n1. 多关注`硬件架构、操作系统原理、编译原理、分布式系统` 别搞花里胡哨的东西\n1. 可以看一下 repo 发版历史，了解一下过程, 着重看 readme\n1. 了解一个软件永远是从 API 开始\n1. 使用 copilot\n1. 让 gpt 看文章，学工具， 看使用说明\n   - 总结\n   - 提问\n1. 让 gpt 解释图片， 如英伟达不同 sm 对比图\n1. 多看 arxiv 和 google scolar 论文；\n   - 新事情多看综述\n1. 将自己记录输入到 gpt 中，看看自己理解对不对\n1. (very good)`梳理`： 输入关键理解，让 gpt 帮助梳理, 写 blog 很有用\n1. python 多看 help\n1. 开发机先看性能\n   - cpu 个数与频率 `cat /cpu/info`\n   - 内存 `free -h`\n   - gpu 个数、显存、频率 `nvidia-smi` `nvidia-smi -q` `jtop`\n   - 硬盘信息 `df -h`\n1. 找一台强力的服务器进行开发，加快迭代速度; 本地适合记录和测试简单代码，不适合开发\n1. 第一性原理：从头开始\n1. 多问自己问题\n1. 给自己信念\n1. 找一个好老师： 李沐， 李宏毅, youtube 查找(观看量)\n1. what how why\n   - what 知道有什么名字， 入门，了解\n   - how 如何使用技术， 工程师\n   - why 为什么， 学生，研究员\n1. 几种学习新东西效果，依次增大\n   - 看文章\n   - 听\n   - 视频\n   - 视频+动手\n   - 给自己讲\n   - 给他人讲\n1. 注意力是有价值的，容易受到干扰, 注意力集中到有价值的事情上,\n1. 要有自己观点和依据，不管对错\n1. c++中接口类可以以 I(interface 开头)，例如 tensorrt 的接口类，这种命名方式帮助开发者清晰地区分接口和实现，以及其他非接口类型的类。\n1. 项目进度管理很重要，计划制定相当于思路，无思路特别乱\n1. 排期也相当于思路, 做事要排期，职业规划也需要排期, 思路清晰事半功倍\n1. 无思路不写代码，思路不明确就开始写是自欺欺人，如刷题的思路\n1. `接口`注释相当于 leetcode 刷题时的`思路`，.cpp 相当于思路的具体实现。具体实现根本记不得，但接口或思路看一下就知道功能或实现方法\n1. 接口多用结构体，灵活性更好\n1. [命名法：驼峰、下划线、匈牙利](https://www.cnblogs.com/linuxAndMcu/p/11280748.html)\n1. `sample + 注释` 学习方法\n1. 学习语法的时候，写一些 sample code 并注释上关键点，积累起来。 类似 cppreference\n1. 多看 man\n1. 知识点要对应例子，如 sandbox 举例\n1. 写脚本时要先屏蔽外界变量，将外界变量转化为内部变量。\n1. 区分好静态概念和动态概念：如 graph 是静态概念，runner 是动态概念， 静态是一种表示(程序)， 动态是一种运行(进程)，有 map 机制，将静态映射到动态(调度)\n1. 代码分为静态代码和动态代码，静态是写死的，动态是根据配置自动生成的\n1. 标准化然后自动化\n1. 简化程序， 去除杂项。用 sandbox\n1. `版本控制， 无情的测试，完全自动化`\n1. `抽象`和`分治`是降低程序复杂度的两种方法\n1. 了解开源库结构先看[fossies doxygen lists](https://fossies.org/dox/all.html)\n1. 新功能用 sourcegraph 探索大家都怎么使用的， 如探索 mermaid 使用\n1. mermaid readme 中画 uml 图和流程图, 可以作为写代码的 roadmap, 用 Snipaste 贴到空白地方写代码\n1. 注意 debug log\n   - cmake: `-DCMAKE_VERBOSE_MAKEFILE=ON`\n   - opencv:`OPENCV_LOG_LEVEL=DEBUG`\n1. `have fun` neofetch and tty pts send\n1. `能够轻易得到答案的简单问题不要问出口`，那并不会显得你很好学，反而会显得你无知又懒惰。\n1. `Think twice, code once`. Don't start “doing something” before you fully understand the problem and have acquired sufficient background knowledge\n1. 文档写好可以出书\n1. 讲解要有画面感\n1. `doxygen 文档` 为每个库生成 doxygen 文档，远程统一拉取配置， 生成文档发送到 mirros 上推送到 mirrors 上面。大家都可以访问， 作为 user guide。Jenkins 自动完成\n1. `Roadmap`: 路线图很关键， 例如[software-design-and-architecture-roadmap](https://github.com/stemmlerjs/software-design-and-architecture-roadmap)\n1. glog 只运行一次的 log 可以用 INFO\n1. `开发环境和测试环境分离`： 开发环境公用，每个场景有自己测试环境， 不用来回切换测试环境, 测试环境用 Jenkins 来搭建。\n1. `梳理好流程`：(效果拔群：6yolox + deepsort + 2lanes workflow 一天搞定)\n\n   - 数据通路图\n   - 数据结构 + 函数\n   - 待做的事\n\n1. 弄清需求，画图，然后再开始做, 迭代。\n1. 重复的事情尽量用 ci 搞定。ci 流程熟练。\n1. 编译不要漏过`warning`\n1. **加速编译**： `configure the project and generate a native build system`后直接执行`make -C $BUILD_DIR -j4` or `cmake --build ${BUILD_DIR}`，不用每次都重新构建编译系统\n1. 任务分解：先搭框架，慢慢填充，上节奏。\n1. 搭个框架，编译通过： 类接口 + 测试程序 => 一步步调试(构造， 输入，输出)\n1. 专注：\n   - 高亮\n   - 折叠\n   - narrow\n   - beacon\n   - fullscreen\n   - 统一界面\n1. 可重复, 自动化： 保留自己的操作历史\n   - .zsh_history\n   - z.lua\n   - docker_file\n   - work_log\n   - snippets\n   - blog\n   - bug 跟踪历史\n   - org mode\n1. 多使用 emacs c++ man 功能。可以作为增强版 snippets\n1. markdown 折叠很有用： worklog, blog\n1. 高亮相当于荧光笔，关注关键点\n1. 问清楚事情的来龙去脉，不要埋头苦干。\n1. 记录调试状态，步骤。不用重复跟别人解释，也好回忆内容。\n1. html 文档比 pdf 好用\n1. release 给别人时加一下 [changlog](https://keepachangelog.com/zh-CN/1.0.0/) 或 release note\n1. 看见好的地方就吸收\n1. 从开发者的角度开发功能，从使用者的角度来使用。开发功能要配套 test， test 越简单越好。\n1. 不要陷入细节，从高层次来看问题\n1. 开发方式：作为用户，先写测试程序架子，抽象出用到的接口。然后作为开发者实现接口， 用户和开发者要。\n1. `[自顶向下和自底向上结合](https://www.jianshu.com/p/608ac1e6d05d)`\n   - 自顶向下: 先搭框架，抽象出各个层次，再往里填。\n   - 自底向上: 先完成基本功能，再搭框架。\n1. `抽象层次`很重要，\n1. `sandbox` 快速验证，非常有用\n1. 热爱编程，从中找到乐趣，代码写的简洁优雅, 易用。\n1. 统一：\n   - emacs 和命令行统一操作\n   - emacs 统一编辑等功能\n   - format all 统一格式化\n   - docker 统一开发环境\n   - tmux 统一 terminal\n   - Nvidia Unified Memory\n   - opencv 统一接口\n1. 参考常用库接口,如 opencv, 使用 doxygen 生成 UML, yaml(gtest)\n1. 解决编程痛点：\n   - 不同版本 clang-format 不一样\n   - 命令行 error warning 高亮\n   - mount uid 与主机一致\n   - 新板子分配用户，uid 一致\n1. 多使用 profilling(chrono)测试性能(看哪种写法性能更好)\n1. 讲故事编程, 串起来\n1. `通用`: 写的代码尽量通用，尽量少的特例，少 if。linus.\n1. `迭代`： 先想怎么写就怎么写，可能非常丑陋，然后再打磨代码。是一个迭代过程，不是一蹴而就,例如：rust xmodel product 模块，刚开始特别烂，不断迭代，抽象了接口，使用了 trait object, 工厂模式，模板，前后提交了几十个 commit。\n1. `讲故事`：大师级程序员把系统当作故事来讲，而不是当作程序来写。学会讲故事会提升编程能力和应表达能力。\n1. 编程艺术：编程的目的是搭建系统，系统是由语言来描述的，函数是语言的动词，类是名词。编程的艺术其实也是语言的艺术，合理的使用动词，名词。\n1. `语言feature`: 编程语言有一堆 feature，每种语言在其中选择权衡，理解了各种 feature，也就学会了各种语言，各 feature 之间的关系，互斥，相交。[haskell feature](https://wiki.haskell.org/Functional_programming) [haskell book](http://learnyouahaskell.com/chapters)\n1. 函数式编程没有变量也没有 side effect。隔离了可变性，变量或者默认状态默认就是不可变的。避免了死锁，状态冲突等众多麻烦。\n1. 函数式编程，命令式编程，面向对象编程区别\n1. 以前的面向对象系统和如今的分布式系统。\n1. 表达式 expression 和语句 statement 区别，表达式有返回值。\n1. `测试`分为`单元测试，集成测试和文档测试`。单元测试独立地验证库的不同部分，也能够测试私有函数实现细节。集成测试则检查多个部分是否能结合起来正确地工作，并像其他外部代码那样测试库的公有 API。\n1. 最重要的是`官方资源`，看英文原文，如 rust 官网教程，c++ cppcon 会议。\n1. 找`最专业`的，从原理上学习。如 rust 设计者讲 rust，git 原理。\n1. 找`创始人相关`视频学习，如 Bjarne Stroustrup c++视频，Linus Torvalds git 原理。Richard Stallman emacs.\n1. `先见识，再模仿，后摸索`。`Good artist copy, great artist steal` 学习新语言，可以先看看视频教程，再自己摸索。\n1. `将知识点放入体系`中，不能孤立的学习。比如学习 C++知识时要有`big picture`，要把编译器，操作系统考虑进去，这样才能理解深刻，不要陷入细节中无法自拔。\n1. 搭建自己`编程体系，哲学`。写代码是`实践`，用来验证体系的好坏。重点应该关注体系哲学而不是具体的代码。\n1. `将知识点带入现实，自己融入进去`。便于理解和讲解。比如，想象自己是操作系统，管理一堆资源，向应用提供服务，交互过程。\n1. 如何看待编程？`编程是一个建造世界的过程(会涉及不同层级的抽象)`，建造完成后的世界就相当于程序，完成了编程的目标。建造流程为生产对象，组织对象，对象行为约定。（对应设计模式里的创建型模式，结构型模式，行为型模式），设计模式就相当与搭建过程中用到的各种套路，是一种设计思路。参考《我的世界》游戏。\n1. `区分抽象层次`很重要，有助于理解系统的横向(同层次）纵向（上下层）关系。理解编写代码要从最高层次（顶层设计）入手，一层一层的往里剥，越往里越细节（the onion principle）。\n1. 有`目标`或者`带着问题`看文档或代码。\n1. 无论是整体还是模块，都要先弄清楚模块`目的`，再学习`基本概念`, 然后学习`框架和设计模式`，最后再学习具体代码。\n1. 看文档或代码的时候`抓关键点记笔记`，有利于`专注`，记录的时候会强化，加深理解，也有利于`新思路涌现`。可以记录到`blog`或者`git`上。如有需要进行`可视化`\n1. 看文档代码先问`why`。要从模块`存在的目的？解决了什么问题？特点特性？优缺点？适用范围？概念？架构？设计思路？具体实现方式？`等方面考虑问题。\n1. 概念刚开始可能很难理解，可以从`具体实例`来理解，`多动手`，写一遍理解深刻很多。\n1. 找到`语言优势`，重点关注。C++的优势在于`性能`，死磕性能。\n1. 学习`建模`，对系统建模。\n1. 读代码时要先理解总体设计架构，设计思路，再具体到细节，架构理解了代码就不是问题。\n1. 多学点架构，设计方法，少学点语法。\n1. 最终目标创造而不是复制。\n1. 架构是撑起项目的基础，理解架构了就能看清楚系统轮廓。架构就相当于系统的草图，有了草图才能起高楼。\n1. 注意软件架构和设计模式的区别。\n1. `设计`是解决问题的方案，不是一种算法。\n1. 分主题分模块学习语言，分拆效率高，更容易 focus，不能一股脑的啥都往脑子里塞。\n1. 学习新知识时`先明确概念`，如 cy 学习 git，明确 git 的原理，四个 object；学习 cmake 时明确 component 和 target；学习 gstreamer 明确 component,pad 等，给别人讲解是也是从基本概念、基本组成讲起。\n1. 文档越详细越好，肯定会忘\n1. 读代码的时候可以记录自己的理解，可加深印象。用 git 仓库保存起来。\n1. github 排名，github trending\n1. 查看开发者的 github 找资料\n1. 先`模仿`github 开源代码。youtube 视频教程。\n1. 随时写一些简单的例子验证想法。很重要。\n1. Github awesome 系列，awesome emacs, shell，rust\n1. emacs org 管理项目，todo，日志\n1. 遇见问题记得看 github issues\n1. 设计的思路，设计原则，理解之后对使用帮助很大(例如： emacs, git)\n1. 编译时出错`仔细看错误`信息\n1. 调试时要一小步一小步的调，步步为营\n1. `乐于学习新东西`，学习 emacs 之后才知道 vim 该怎么用，学习 Rust 之后才知道语言一些基本特性，更好的理解 C++\n1. `重头文件(接口)，轻源文件`。看一个新库时例如 xir, vart，先看接口（接口注释很重要），知道接口功能就行，不需要明白具体实现。通过接口明白整个库的结构。\n1. `突出重点`:读音加重，停顿。看 C++代码要具备忽略次要信息的能力，看重点的信息要用不同的眼光，例如函数名，函数功能重点看和理解，具体实现可以先不用管\n1. 程序有 Debug 和 Release 模式，Release 开优化，编译慢，运行快\n1. 程序有编译期和运行期。\n1. 程序包含功能和性能，注重分析程序的性能，比如帧率，时间\n1. 流程: 开发(开发新库），测试(jenkins)，发布(上 github)，运营(gihub issues)\n1. 重设计，轻实现\n1. 按部就班，不能急于求成\n1. 知乎话题：rust, emacs\n1. 学习编程语言要关注几点：起因，解决什么问题，设计哲学，特性 [参考文章](https://www.infoq.cn/article/Uugi_eIJusEka1aSPmQM)\n1. 大项目中，模块之间要检查`输入输出`是否正确，不用在乎模块里内部实现\n1. 日志写在实现的外面，可以从使用者的角度检查输出，日志写在里面，可以从实现者的角度检查问题。倾向写在实现外面，先定位哪个模块出问题了，再深入到模块内部，看看为什么出问题。\n1. 一个函数完成一个小功能，方便理解定位问题\n1. main.rs 函数中处理程序运行，lib.rs 中处理任务逻辑\n1. 操作符重载打印结构体很方便\n1. API 文档目的是让他人理解如何使用库，而不是让他人明白库如何实现的。\n1. 创建一个有用的公有 API 结构更像是一门艺术而非科学，你可以反复检视他们来找出最适合用户的 API\n1. 留意 FAQ\n1. 函数(一层抽象)，类，泛型都是为了重复代码。\n1. 写代码时明确（输入，输出，算法）（开，关，做）（构造，析构，虚函数)\n1. RFC 流程。\n1. C++ API -> C API 可能拆分， C API -> Rust API 可能合并\n1. 区分公开与非公开部分。\n1. 学习语言，应该了解编译器，编译器是整个编程中重要一环。\n1. 使用 cppinsights 从编译器的角度看程序。有助于对语法的了解。\n1. 区分计算密集和 I/O 密集，计算密集多线程，I/O 密集异步？\n\n## Productivity [陈天](https://zhuanlan.zhihu.com/p/19968368) [link](https://zhuanlan.zhihu.com/p/366187306) [知乎生产力工具](https://www.zhihu.com/topic/19772918/hot)\n\n1. snipaste\n1. 画中画\n1. 光标样式设置大一号, 个性化 -> 主题 -> 鼠标光标\n1. 统一开发环境 docker\n1. [sourcegraph 插件](https://chrome.google.com/webstore/detail/sourcegraph/dgjhfomjieaadpoljlnidmbgkdffpack)\n1. [gitzip 插件](https://chrome.google.com/webstore/detail/gitzip-for-github/ffabmkklhbepgcgfonabamgnfafbdlkn)\n1. chrome 快捷搜索设置，github g https://github.com/search?q=%s; translate t https://translate.google.cn/?source=osdd&who=test&sl=en&tl=zh-CN&text=%s&op=translate;\n1. 多看高手 screencast [sample](https://www.youtube.com/results?search_query=emacs+screencast&sp=CAM%253D)\n1. 好电脑，大屏幕\n1. 自动化，重构\n1. snippet\n1. 统一编辑器，emacs\n1. 工程加 make 命令： make init，make build，make run，make create-pr [link](https://mp.weixin.qq.com/s?__biz=MzA3NDM0ODQwMw==&mid=2649828020&idx=1&sn=59668db47a49e023735152e51b450b88&chksm=8704a8a8b07321be4bb8f540909f2a0b1be4c3a250c71d04e115f78ab19dc804310b55170ed4&scene=21#wechat_redirect)\n1. sandbox 或模拟器\n1. 搭建 ftp 服务器，使用`put`上传，`get`下载，`gls`查看列表。多个服务器之间共享传递文件，windows 上浏览器也可访问。见 Bash-usage.md\n1. 用好 tmux, fd, rg, zh, fzf\n1. 使用格式化工具刷代码，不要手动刷\n1. 开始时多使用 cheatsheet\n1. 常用命令 alias\n1. 保持工作空间整洁，经常清理，立即删除无用文件\n1. git 管理代码，配置，日志\n1. 多用 google，少用百度\n1. 使用好日志系统\n1. 学会 bg, jobs, fg, C-z, &来切换前后台运行。注意：多个 jobs -l 时需要 fg %n 才能切换到前台, kill -9 <tab> fzf to kill ps\n\n## Keyboard usage\n\n1. EMACS alt, shift 一只手同时按，另一只手按%,>\n1. sharpkeys 修改键位\n1. emac 快捷键设置技巧\n   - 右手 leader key,左手按键\n   - 常用命令才设置，不常用直接输命令\n   - 快捷键有意义\n   - 尽量简单简洁\n   - 尽量少用左手小拇指，多用命令\n\n## 编程名言 [link](https://www.jianshu.com/p/234452c2d88d)\n\n1. 在物理学中，第一性原理(或起源)，或称从头算，指从基本的物理学定律出发，不外加假设与经验拟合的推导与计算。例如利用薛定谔方程在一些近似方法下解电子结构，但不从实验数据得到拟合参数的从头计算法。\n1. LLVM 之父 Chris Lattner：`这正是我们从第一性原理出发开展工作的原因。你必须从头完成所有的工作，如果做得对，就不应跳过任何重要的步骤` [link](https://mp.weixin.qq.com/s?__biz=MzU5ODY2MTk3Nw==&mid=2247492618&idx=1&sn=a20f4828b9ab3e3cee3fedfd906e0eb2&chksm=fe426a3cc935e32a8312ce9efbb4f2640787508d3e811579bbffe918685cdb07a8bd8e3ffc4b&scene=132&exptype=timeline_recommend_article_extendread_samebiz#wechat_redirect)\n1. `Any problem in computer science can be solved by another layer of indirection` 计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决;例如虚拟地址\n1. `Think twice, code once`\n1. `Hackers write better code when it's for their own satisfaction instead of for pay.`\n1. 控制复杂性是计算机编程的本质。—— Brian Kernighan\n1. Any fool can write code that a computer can understand. Good programmers write code that humans can understand. –Martin Fowler\n   任何一个傻瓜都会写能够让机器理解的代码，只有好的程序员才能写出人类可以理解的代码。——Martin Fowler\n1. “The first 90% of the code accounts for the first 90% of the development time. The remaining 10% of the code accounts for the other 90% of the development time.” – Tom Cargill\n   “最开始的 90%的代码使用了程序员 90%的时间，剩下的 10%的代码也需要 90%的开发时间”——Tom Cargill（这不就是中国谚语——“行百步半九十”）\n1. 过早的优化是万恶之源。Premature optimization is the root of all evil! - Donald Knuth\n1. 作为一个程序员，郁闷的事情是，面对一个代码块，却不敢去修改。更糟糕的是，这个代码块还是自己写的。—— Peyton Jones\n1. 用几个小时来制定计划，可以节省几周的编程时间。—— 匿名\n1. 当你试图解决一个你不理解的问题时，复杂化就产成了。——Andy Boothe\n1. 靠代码行数来衡量开发进度，就像是凭重量来衡量飞机制造的进度。——比尔·盖茨\n1. `Talk is cheap. Show me the code.`\n\n## have fun\n\n1. `/etc/profile` 添加 ssh 自启动\n1. [cfonts](https://github.com/dominikwilkowski/cfonts) 可作为 ssh 登录输出，区别不同机器\n   - `npm i cfonts -g`\n   - `cfonts \"heimdallr\" --gradient red,blue`\n   - `cfonts \"heimdallr\" --gradient red,blue -f shade`\n   - `cfonts \"heimdallr\" --gradient \"#e60012\",blue > heimdallr.txt`or `script -q -c \"cfonts \"heimdallr\" --gradient red,blue\" heimdallr.txt > /dev/null` 将彩色输出保存到到文件中\n   - cat file on board\n1. `neofetch` ssh 登录显示 no bug car\n   - [link](https://github.com/dylanaraps/neofetch)\n   - `neofetch --ascii /home/nvidia/phigent_color.txt`\n1. ASCII art: 抠图 -> ASCII 转换 -> 颜色(logtool)\n1. `linuxlogo` 命令行显示 linux logo\n1. w, who 查看登录的用户; 向登录用户发消息 `echo jia > /dev/pts/20 ` `for i in $(who | awk '{print $2}'); do echo \"${i}\" > /dev/${i}; done`\n   `port=0; echo -e \"From xiyang: hi 你好 xx\\c\" > /dev/pts/${port} ; for i in $(seq 1 100); do echo -e \".\\c\" > /dev/pts/${port}; sleep 1; done`\n1. `cmatrix` [link](https://www.tecmint.com/20-funny-commands-of-linux-or-linux-is-fun-in-terminal/)\n1. [figlet and lolcat](https://opensource.com/article/21/11/fun-linux-commands) `figlet centos.com | lolcat` `cat no_bug_car.txt | lolcat`\n1. [ascii car](https://www.asciiart.eu/vehicles/cars) 需要删除空行，更好看\n1. [在线 image to ASCII art](https://manytools.org/hacker-tools/convert-images-to-ascii-art/)\n1. [在线 string to ASCII art](http://patorjk.com/software/taag/#p=testall&c=c%2B%2B&f=3D%20Diagonal&t=PhiGent%20Heimdallr) Larry 3D\n1. no_bug_car.txt 在 `source/images/software-diagram/`目录下\n1. [scratch language for children](https://scratch.mit.edu/)\n\n## Links\n\n1. [Hacker laws](https://github.com/dwmkerr/hacker-laws)\n2. [陈天](https://www.zhihu.com/people/tchen/columns)\n","tags":["Program"],"categories":["Summary"]},{"title":"Amazing emacs","url":"/2021/09/30/Amazing-emacs/","content":"\n## Emacs Architecture\n\n![Emacs Architecture 《架构之美》](https://pic2.zhimg.com/964f7a10ac8a4158896500858efb4a55_b.png)\n\n## NOTE\n\n1. `ln -s build-debug/compile_commands.json` 设置软连接\n1. `lsp`提示时按 i, 否则不能补全\n1. `list-face-display and list-colors-display` 列出颜色\n1. `(add-hook 'window-setup-hook 'on-after-init)` 启动时防止 buffer 模糊不通透\n\n<!-- more -->\n\n1. gcc emacs 性能提升\n1. vnc x11 meta[问题](https://www.emacswiki.org/emacs/MetaKeyProblems#:~:text=There%20is%20no%20mention%20of%20Meta%20for%20any,may%20want%20Meta%20to%20be%20distinct%20from%20Alt.)\n\n```sh\n% xmodmap\n% xmodmap -e \"clear mod4\"\n```\n\n1. windows terminal 透明, 启动配置里加\n\n```sh\n(defun on-after-init ()\n  (unless (display-graphic-p (selected-frame))\n    (set-face-background 'default \"unspecified-bg\" (selected-frame))))\n\n(add-hook 'window-setup-hook 'on-after-init)\n\n```\n\n1. [固定创建一个窗口](https://emacs-china.org/t/topic/17035)\n\n## [Calc](https://www.gnu.org/software/emacs/manual/html_mono/calc.html)\n\n1. `C-x * *`\n1. `calc-reset` 重启\n1. `D` redo\n1. `U` or `ctrl+/` undo\n1. `Backspace` calc-pop: can't redo\n1. stack 计算方式, 可以直接输入+ - \\* /， 最近 stack 进行计算\n1. [计算 buf 均值](https://superuser.com/questions/1077154/emacs-commands-to-calculate-sum-average-etc-of-region)\n1. `C-x * q (quick-calc)`\n1. step\n   - put num in a buffer `ctlr-x space` select rectangle, remove other\n   - select all buffer `ctrl-x h`\n   - M-x: statistics-in-rectangle\n   - M-x: calc-grab-region\n   - M-x: calc-vector-mean (u M)\n1. step\n   - `ctlr-x space` select rectangle:(can slect string, such as: 10ms, 101ms NOTE: 后缀字符串必须一样)\n   - M-x: calc-grab-rectangle\n   - M-x: calc-vector-mean\n\n## [Rectange](http://xahlee.info/emacs/emacs/emacs_string-rectangle_ascii-art.html)\n\n1. `ctrl-x space` + `M-w` + `C-y` 选择，复制，粘贴\n1. `ctrl-x r`\n1. `ctrl-x r t` insert\n1. `ctrl-x r k` kill\n1. `ctrl-x r y` paste\n1. `ctrl-x r N` insert a column of numbers\n\n## ibuffer\n\n1. `, b l`\n1. `h` for help\n1. `% n` - Mark buffers by their name, using a regexp.\n1. `U` - Unmark all marked buffers.\n1. `/ n` - Add a filter by buffer name.\n1. `/ /` - Remove all filtering currently in effect.\n\n## dired\n\n1. filter: search file and ivy-occur\n1. `wdired-change-to-wdired-mode` (可批量改文件名)Put a Dired buffer in Writable Dired (WDired) mode. `C-c C-c` wdired-finish-edit\n1. `dired-hide-details-mode` toggle dired details shortcut:`(`\n1. `s` sort by date\n1. `ctrl+u s`\n1. xah-dired-sort\n1. `>` next dir\n1. `<` previous dir\n1. `f and b, j and k` dired-find-file and dired-up-directory, next line and previous line\n1. `+` add directory\n1. `C-x C-d` recent dired\n\n## speed up\n\n1. set windows repeat rate [link](https://www.dummies.com/computers/pcs/set-your-keyboards-repeat-delay-and-repeat-rate/)\n1. `profiler-start, profiler-report, profiler-stop` to see cpu report\n1. next-line is slow: `(setq auto-window-vscroll nil)` [link](https://emacs.stackexchange.com/questions/28736/emacs-pointcursor-movement-lag)\n1. yascroll is slow; close\n1. recentf (set len 10)\n1. flycheck 应该很慢\n1. 如果卡了就完全从头编译一下\n\n## macro\n\n1. `kmacro-start-macro-or-insert-counter` F3\n1. `kmacro-end-or-call-macro` F4\n1. `call-last-kbd-macro` C-x e e e e e\n1. F3 -> (F3) -> F4 counter\n1. `C-u 222 F3 ... F3 ... F4` start counter from 222\n1. `C-u 100 F4` repeat macro 100 times\n\n## useful keys\n\n1. `M-S-n` 快速选中多行， 比`M-; -> M-n`更快\n1. `repeat` (global-set-key (kbd \"C-x .\") 'repeat)\n1. `xterm-paste`\n1. `ibuffer` -> `% n` -> \"Occur\" -> `D` remove all Occur buffer\n1. isserch: Typing `M-s o` in incremental search invokes isearch-occur\n1. occur 正则： yolox.*Raw.*running `next-error`\n1. `clm/toggle-command-log-buffer` C-c o\n1. in minibuffer: ivy-occur 默认绑定在`C-c C-o`上，它可以将当前的候选集合保存至 buffer 内并退出 minibuffer\n1. ivy-call 默认绑定在`C-M-m`上，它可以看做不退出 minibuffer 的 ivy-done 操作\n1. `M-x M-o` see ivy-mini-buffer-map; i: insert comand; d: go to defination;\n1. `C-M-j` ivy-immediate-done, when you call find-file to create a new file, but the desired name matches an existing file.\n1. 利用好 lsp\n\n```sh\n(setq lsp-ui-doc-enable nil) ;; lsp关闭弹窗\n(setq lsp-ui-sideline-mode nil) ;; lsp 关闭右侧错误提示\n(setq lsp-signature-mode nil) ;; 防止M-n被占用\n(setq lsp-enable-symbol-highlighting nil)) ;; lsp 不高亮\n```\n\n## 编程操作\n\n1. symbol-overlay 高亮选中后可以用 t 切换 scope，然后 r 重命名。废弃： `narrow` => `symbol-overlay-put` => `n or r`跳转或替换 => `fancy-widen`\n1. `symbol-overlay`: 双击左键高亮或取消高亮， 中间键跳转到上一个， 右键跳转到下一个\n1. `origami-toggle-all-nodes` and `origami-recursively-toggle-node`, `origami-show-only-node`, `origami-undo`, `origami-redo`\n1. `outline-show-all`, `outline-hide-other`, `outline-show-subtree`\n1. `bm-toggle` and `bm-next`\n1. `git-gutter`: next, previous, revert\n1. `view-mode`: (help man 都适用)(单手，大拇指 space 无名指 Delete)\n   - `e` quit and stay current buffer\n   - `space` scroll page down\n   - `Delete` scroll page up\n   - `(setq view-read-only t)` read-only 打开 view-mode\n1. `projectile-switch-project` 切换工程 `projectile-toggle-project-read-only` 工程只读\n1. `deadgrep`\n1. `occur` + `next-error`\n1. `goto-last-change`\n1. `indent-guide`\n1. `markdown mode` shift+tab 折叠, head 上 tab 折叠\n1. `company-toggle-delay` to toggle company automatically\n1. `F11` 全屏操作\n1. `follow-mode` + `balance-window(C-x +) or balance-windows-area(, w b)`\n1. zap\n\n## [snippets](http://joaotavora.github.io/yasnippet/snippet-development.html#orgcde188c)\n\n1. [inspired from textmate](https://macromates.com/manual/en/snippets)\n1. M-x yas-tryout-snippet, key binding: C-c C-t C-t When editing a snippet, this opens a new empty buffer,\n1. 可以执行 emacs lisp 命令 `#include \"${1:`(file-name-nondirectory (file-name-sans-extension (buffer-file-name)))`.hpp}\"`\n1. 自动大写`constexpr ${1:type} ${2:$$(upcase yas-text)} = $3;`\n1. `${1:$$(yas-choose-value '(\"right\" \"center\" \"left\"))}` 选择值\n1. c++ man\n\n## Common\n\n1. [stdman](https://github.com/jeaye/stdman.git) 安装 stdman 可以在 emacs 中看 cppreference 内容\n1. 通过整数确定字符在缓冲区的位置，但对缓冲区操作时会引起字符位置变化，可以使用(marker)来跟踪文本位置，会随文本变化而变化。如 set mark [架构之美 235 页](https://github.com/0voice/expert_readed_books/blob/master/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/%E6%9E%B6%E6%9E%84%E4%B9%8B%E7%BE%8E.pdf)\n1. [keymaps 示意图](https://excalidraw.com/#json=5564509368352768,a62g1lvpbVbiHVddtzRpOQ)\n\n## The Amazing Packages\n\n1. treeemacs\n1. indent-guide\n1. [calctex](https://github.com/johnbcoughlin/calctex)\n1. tmux-cc(注意 emacs 中 tmux 版本大于 3.0a) (set windows.panel)[vim-slime](https://github.com/jpalardy/vim-slime)\n   - tmux-cc-set-target-window : .2 向当前windows第二个panel发送\n   - tmux ctrl-q w可以看panel编号\n   - markdown文档中用%号区分行\n1. woman\n1. emamux\n1. deadgrep\n1. hightlight\n1. bm\n1. company-ispell\n1. beginend\n1. fancy-narrow\n1. ctrlf\n1. mwim\n1. dogears\n1. keycast-mode\n1. command-log-mode: clm/toggle-command-log-buf\n1. symbol-overlay\n1. awesome-tab\n1. eyebrowse\n1. beacon\n1. dimmer\n1. mood-line\n1. smex\n1. keypression\n1. rainbow-mode\n1. rainbow-delimiters\n1. ace-window\n1. helpful\n1. imenu-list\n1. hydra\n1. anzu\n1. move-text\n1. goto-chg\n1. whole-line-or-region\n1. ace jump\n1. clang format\n1. compile and gdb\n1. next error\n1. jump to defination\n1. company\n1. undo tree\n1. which key\n1. expand region\n1. snippets\n1. help mans\n1. M-x: you can use commands to do everything\n1. magit\n1. counsel buffers and files\n1. ibuffer\n1. dired\n1. bookmarks\n1. leader key\n1. org mode and markdown mode\n1. google translate\n1. macro\n1. smartparens\n1. swiper\n1. shift select\n1. flycheck\n1. hungry delete\n1. ox-reveal\n1. crux\n1. leetcode\n1. pdf-tools\n","tags":["Emacs"],"categories":["Tools","Emacs"]},{"title":"Cpp grammar","url":"/2021/09/20/Cpp-grammar/","content":"\n## NOTE:\n\n### basic concept\n\n1. 程序(内存中完全链接): 算法(函数， 代码) + 数据(变量)\n1. 编译(未完全链接)：引用 + 定义 (变量和函数)\n1. 注意内存分布(代码 + 数据)\n1. 编译期，链接期，加载期， 运行期\n1. 语句， 表达式\n1. 作用域， 生命周期， 所有权\n1. 对象何时构造(编译期，运行期)及构造顺序\n\n## 参考代码\n\n1. [tabulate](https://github.com/p-ranav/tabulate)\n1. [perf-ninja 性能比较](https://github.com/dendibakh/perf-ninja/blob/main/GetStarted.md)\n1. [google abseil](https://github.com/abseil/abseil-cpp)\n1. [abseil doc](https://abseil.io/)\n1. [facebook folly](https://github.com/facebook/folly)\n1. [Software Engineering at Google](https://abseil.io/resources/swe-book)\n1. [Software-Engineering-at-Google 中文版本](https://github.com/qiangmzsx/Software-Engineering-at-Google)\n1. [Vitis-AI](https://github.com/Xilinx/Vitis-AI/blob/2.0/tools/Vitis-AI-Library/benchmark/include/vitis/ai/benchmark.hpp)\n1. [Vitis AI demo](https://github.com/Xilinx/Vitis-AI/blob/c26eae36f034d5a2f9b2a7bfe816b8c43311a4f8/src/Vitis-AI-Library/benchmark/include/vitis/ai/demo4.hpp)\n1. [hash-library](https://github.com/stbrumme/hash-library)\n\n<!-- more -->\n\n## [attribute](https://zhuanlan.zhihu.com/p/64493524)\n\n1. `[[maybe_unused]]`\n1. `__attribute__((visibility(\"default\")))` 函数符号不隐藏\n1. `[[deprecated]]/[[deprecated(\"reason\")]]`\n\n## Note\n\n1. std::array 编译期需要确定大小；在栈上分配内存，std::vector 在堆上;在栈上申请内存比堆快 [link](https://zhuanlan.zhihu.com/p/481687008);\n1. `类型擦除`指将原有类型消除或者隐藏，因为很多时候我们并不关心具体类型是什么，我们只需要去使用就可以了，提高提高代码的简洁性。是不是很耳熟，对，抽象继承的多态其实就是比较传统、常见及简单的类型擦除。我们使用的只是抽象接口，而不关心具体实现类的类型。类型擦除常用做法是`多态、模板、通用类型（C++17 std::any|std::variant）、闭包`\n1. 注意区分#ifdef 在头文件和.cpp 中的表现，在头文件中，不同的库也要定义才能生效，如果在.cpp 中，只需要相应 lib 定义就好\n1. [头文件中定义问题](https://blog.csdn.net/sksukai/article/details/105612235)/\n\n## 性能测试 chrono [link](https://github.com/Xilinx/Vitis-AI/blob/master/tools/Vitis-AI-Runtime/VART/vart/util/include/vitis/ai/profiling.hpp)\n\n1. 计时用 steady_clock, timestamp 用 system_clock\n1. steady_clock 是单调的时钟，相当于教练手中的秒表；只会增长，适合用于记录程序耗时；\n1. system_clock 是系统的时钟；因为系统的时钟可以修改；甚至可以网络对时； 所以用系统时间计算时间差可能不准。\n1. high_resolution_clock, 不建议使用\n1. 可以和 glog 与 get_env 结合(性能与 Debug 结合) [link](https://github.com/Xilinx/Vitis-AI/blob/master/tools/Vitis-AI-Runtime/VART/vart/util/include/vitis/ai/env_config.hpp)\n1. [CLOCK_MONOTONIC 与 CLOCK_REALTIME 区别](https://www.jianshu.com/p/1861a844a2fb)\n1. C++11 中的 system_clock::now() 使用 gettimeofday()或者 std::time()，而 steady_clock::now()则使用 clock_gettime(CLOCK_MONOTONIC,\\*);\n\n```\n#include <chrono>\nusing Clock = std::chrono::steady_clock;\n#define __TIC__(tag) auto __##tag##_start_time = Clock::now();\n\n#define __TOC__(tag)                                                  \\\n  auto __##tag##_end_time = Clock::now();                             \\\n  std::cout << #tag << \" : \"                                          \\\n            << std::chrono::duration_cast<std::chrono::microseconds>( \\\n                   __##tag##_end_time - __##tag##_start_time)         \\\n                   .count()                                           \\\n            << \"us\" << std::endl;\n\n```\n\n2. ns\n\n```\n#include <chrono>\nusing Clock = std::chrono::steady_clock;\n#define __TIC__(tag) auto __##tag##_start_time = Clock::now();\n\n#define __TOC__(tag)                                                  \\\n  auto __##tag##_end_time = Clock::now();                             \\\n  std::cout << #tag << \" : \"                                          \\\n            << std::chrono::duration_cast<std::chrono::nanoseconds>( \\\n                   __##tag##_end_time - __##tag##_start_time)         \\\n                   .count()                                           \\\n            << \"ns\" << std::endl;\n```\n\n## [数式编程](https://zhuanlan.zhihu.com/p/45750387)\n\n### map\n\n1. transform\n\n```\n  std::vector<float> vec(10000, 1.111);\n  std::vector<float> vec1(10000, 2.111);\n  std::vector<float> result;\n  // 操作一个vector\n  std::transform(vec.begin(), vec.end(), std::back_inserter(result),\n                 [](const auto item) -> float { return item + 100; });\n  // for_each(vec.begin(), vec.end(), [](auto &item) { item += 100; })\n\n  // 操作两个vector NOTE: 使用前需要检查vec, vec1 size是否匹配。\n  // 性能好 797us 13us(-O3)\n  std::vector<float> result1(10, 0.0);\n  std::transform(vec.begin(), vec.end(), vec1.begin(), result1.begin(),\n                 [](const auto &item1, const auto &item2) -> float {\n                   return item1 + item2;\n                 });\n\n  // 性能差 2605us 173us(-O3)\n  std::vector<float> result2;\n  std::transform(vec.begin(), vec.end(), vec1.begin(),\n                 std::back_inserter(result2),\n                 [](const auto &item1, const auto &item2) -> float {\n                   return item1 + item2;\n                 });\n  // for 463us 70us(-O3)\n  std::vector<float> result3(vec.size(), 0);\n  for (auto i = 0u; i < result3.size(); ++i) {\n    result[i] = vec[i] + vec1[i];\n  }\n\n\n```\n\n### filter\n\n1. copy_if\n2. remove_if\n\n### fold\n\n1. accumulate `float sum = std::accumulate(vec.begin(), vec.end(), 0.0, std::plus<float>());`\n\n### \\_\\_func\\_\\_函数名\n\n```\n#include <stdio.h>\nvoid myfunc(void)\n{\n  printf(\"%s\\n\", __func__);\n}\n```\n\n## RVO 返回值优化 effective modern c++ 166 页\n\n1. 返回值也是函数参数，RVO 直接在位函数返回值分配的内存上创建局部变量来避免复制\n2. 两个条件：\n   - 局部对象类型和返回值类型相同\n   - 返回的就是局部对象本身\n\n## [gcc -O0 -O1 -O2 -O3 四级优化选项及每级分别做什么优化](https://blog.csdn.net/qq_31108501/article/details/51842166)\n\n## [GDB 使用](https://wizardforcel.gitbooks.io/100-gdb-tips/content/index.html)\n\n1. .gdbinit gdp 配置文件\n2. 编译要加 -g [link](https://zhuanlan.zhihu.com/p/74897601)\n3. readelf -S xxx|grep debug 查看有没有 debug 段\n4. file xxx 查看有没有 stripped (编译不能加 -s)\n5. -fkeep-inline-functions 调试内联，编译的程序会变大很多\n6. set logging file log.txt, set logging on, set logging overwrite on 记录执行 log\n7. gdb -q 启动时不显示版本等信息\n8. gdb --args xxx ... && r 或 gdb xxx && set args ... && r 或 gdb xxx && r ...\n9. show env xxx 显示环境变量\n10. bt 或 i s(info stack) 显示程序栈 backtrace\n11. l 或 l + 向下显示源码， l - 向上显示源码，l 1,100 打印 1 到 100 行， l xxx:10 显示文件 xxx 第 10 行, l funciton 显示函数，\n12. bt 可以显示运行到哪一行, l -1 或 l +1 显示当前行前后代码\n13. step/finish 进入/返回函数 [link](https://wizardforcel.gitbooks.io/100-gdb-tips/content/finish-and-return.html)\n14. tb 如果想让断点只生效一次，可以使用“tbreak”命令\n15. source script 执行脚本中的命令\n16. p/x c 十六进制打印\n17. d 删除所有断点\n18. watch expr\n19. info functions add 列出 add 函数\n20. 在程序入口处打断点, 先 readelf -h xxx 读入口地址，然后 b \\* 设置断点 [link](https://wizardforcel.gitbooks.io/100-gdb-tips/content/break-on-entry.html)\n21. set print array-indexes on 打印数组下标\n22. i args 显示函数参数\n23. i locals 打印局部变量\n24. i sharedlibrary 显示共享链接库\n25. whatis xxx 打印变量类型， ptype xxx 打印详细信息，\n26. set history filename ~/.gdb_history， set history save on 保存历史\n27. shell ls 或 !ls 在 gdb 中执行 shell 命令\n28. 可以直接执行 cd 和 pwd\n29. 远程调试：\n30. emacs gdb-many-window, gdb 多窗口调试\n\n## input and output\n\n1. `std::flush` 立即输出， Without std::flush, the output would be the same, but may not appear in real time.\n\n## 标准函数 和 库\n\n1. std::filesystem 库 create_directoriesfile_size, exists, current_path,\n1. std::numeric_limits<float>::max()最大浮点数， std::numeric_limits<float>::lowest() 最小浮点数，注意不是 min。 #include<limits>\n1. std::round()四舍五入 #include<math.h>\n1. std::floor()向下取整 #include<math.h>\n1. std::ceil() 向上取整 #include<math.h>\n1. std::memcmp() 按位比较是否完全相等，解决 if(-0 == 0)问题\n1. std::memset() 只能用于设置连续内存，不能用于 vector [link](https://www.zhihu.com/question/408799127)\n1. int mi = std::min({x1, x2, x3, x4});\n1. std::max_element `auto max_value = *std::max_element(vec0.begin(), vec0.end()); `\n1. std::min_element `auto min_value = *std::min_element(vec0.begin(), vec0.end());`\n1. std::minmax_element `const auto [min, max] = std::minmax_element(vec0.begin(), vec0.end()); std::cout << \"min = \" << *min << \", max = \" << *max << '\\n';`\n\n## 第三方库 package\n\n1. [fmtlib/fmt 可以 format 各种字符串, 方便打印](https://github.com/fmtlib/fmt)\n1. [json 库集合](https://www.json.org/json-en.html)\n1. [json for modern c++](https://github.com/nlohmann/json)\n1. [jeson benchmark](https://github.com/miloyip/nativejson-benchmark#parsing-time)\n\n## ideas\n\n1. 记录语法时最好能举个例子\n1. `why?`。模块`存在的目的？解决了什么问题？特点特性？优缺点？适用范围？概念？架构？设计思路？具体实现方式？`等方面学习？\n1. 模块的存在的目的，功能作用，实现方式\n1. 按照 Bjarny Stroustrup 列的特性重新进行分类。\n\n## 原则\n\n1. `开闭原则` 软件中的对象（类，模块，函数等等）应该对于扩展是开放的，但是对于修改是封闭的\n1. `单一职责原则` 一个类只做一件事\n1. `里氏替换原则` 子类应该可以完全替代父类，也就是说在使用继承时，只扩展新功能，不要破坏父类原有的功能。\n1. `依赖倒置原则` 细节应该依赖与抽象，抽象不应该依赖于细节。把抽象层放在程序设计的高层，并保持稳定，程序的细节变化由低层的实现层来完成。\n1. `迪米特法则/最少知道原则` 一个类不应该知道自己操作类的细节，换言之，只和朋友谈话，不和朋友的朋友谈话。\n1. `接口隔离原则` 客户端不应该依赖它不需要的接口。如果一个接口在实现时，部分方法由于冗余被客户端空实现，则应该将该接口拆分，让实现类只依赖自己需要的接口。\n\n## [struct 对齐和补齐](http://www.lingjun.online/2020/07/21/%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90%E9%82%A3%E7%82%B9%E4%BA%8B/)\n\n结构体的对齐和补齐的规则：\n对齐：假定从零地址开始，每成员的起始地址编号，必须是它本身字节数的整数倍。\n补齐：结构的总字节数必须是它最大成员的整数倍。\n\n## C++ language features\n\n## Concurrency, Parallelism and Async\n\n1. Concurrency(并发): Less freedom for the scheduler(usually because of missing information)\n1. Parallelism(并行): More information provided to the scheduler(thus more freedom)\n1. Serial(串行：单线程) is between concurrency and parallelism.\n1.\n\n### C++ concurrency development\n\n1. C++11: Memory model, Threads, Mutexes and locks, Thread local data, Condition vaviables, Tasks\n1. C++14: Reader-writer locks\n1. C++17: Parallel STL\n1. C++20: std::jthread, Atomic smart pointers, Latches and barriers, Semaphores, Coroutines\n1. C++23: Executors, std::future, extensions, Transactional memory, Task blocks, Data-parallel vector, library\n1. 多线程要统计各个函数的运行时间，根据时间进行线程数比例划分，例如，预处理函数 1s，处理函数 2s，那么给预处理分配 1 个线程，处理分配两个线程，\n\n## OOP\n\n1. Object-Oriented Programming\n1. base class: defines the API\n1. derived classes: provide different implementaions\n\n## Generic Programming\n\n1. 泛型编程主要是为算法流程编写的，不是为数据结构，使算法通用化，可以适应不同的数据结构。\n1. 可以先写一个具体的例子，抽象出算法，屏蔽数据结构（类型）。\n1. `A type: specifies the set of operations that can be applied to an object and specifies how an object is laid out in memory` 类型不仅规定对象的操作集合，还规定对象在内存中的排布\n1. `A concept: Specifies the set of operations that can be applied to an object and says nothing about the layout of the object` Concept 只规定对象的操作集合，不规定对象在内存中的排布。\n1. 模板类是比参数类型 T 更高一级的抽象。模板类实现高一级别的抽象，而不用关心低一级抽象的不同。\n\n### Template\n\n1. typename 关键字用于引入一个模板参数\n1. 使用 typename 标识嵌套类型名称。\n1. 使用从属类型时要加 typename。比如：`typename T::const_iterator iter()`不加 typename 会报错，因为编译器并不知道 T::const_iterator 是一个类型的名字还是摸个变量的名字。\n1. 可变参数模板(c++11 之前参数个数固定不可变)：`template<typename... Args> class test`表示 Args 个数不固定，使用时`void f(Args... args)`\n1. `template <typename T> using xxx = T`\n\n### 模板嵌套\n\n## RAII Resource Acquisition Is Initalization\n\n1. 资源获取初始化：`使用局部对象来管理资源的技术`被称为资源获取初始化。`局部对象`(有生命周期)是指存储在`栈`上的对象，它的生命周期由操作系统管理，无需人工介入。\n1. 利用： C++保证了所有栈对象在生命周期结束时会被销毁(调用析构函数)。\n   - `A a;` 在栈上分配 a\n   - `A* a = new A()` 在堆上分配 a，并在栈上保存 a 的指针，生命周期结束后只释放栈上的指针变量，需要 delete 释放资源。。\n   - `unique_ptr<A> a = make_unique<A>(new A())` 在栈上分配 a，在堆上分配 A 的对象，a 中保存了 A 对象的指针，a 用来管理 A 对象，当 a 生命周期结束，会调用 a 的析构函数，释放 A 对象资源。\n1. RAII 用来自动管理对象，例如 smart pointers，用栈来管理资源。\n1. RAII 流程：设计一个类封装资源，构造函数初始化，析构函数释放资源。\n1. RAII 可以极大地简化资源管理，并有效的保证程序的正确和代码的简洁。\n1. The slogan is about initialization, but its meaning is ready about cleanup.\n1. `resource`: anything that requires specail(manual) management.\n   - Allocated memory(molloc/free, new/delete)\n   - POSIX file handles(open/close)\n   - C FILE handles(fopen/fclose)\n   - Mutex locks(pthread_mutex_lock/pthread_mutex_unlock)\n   - C++ threads(spawn/join)\n   - objective-c resource-counted objects(retain/release)\n1. resource 管理涉及到的操作： 资源释放（destructor)，资源复制(copy constructor)，释放原来资源并复制其他资源(copy assignement operator)，资源所有权转移(move operator)，释放原来资源并转移其他资源(move assignment operator)。\n1. RRID(Resource Release Is Destruction)\n\n### 构造函数，析构函数，copy 构造函数，copy 赋值构造函数，move 构造函数，move 赋值构造函数\n\n1. 全局对象的构造函数在程序进入 main() 函数之前执行\n1. Initialization is not assignment.\n1. `T w = v;` This is an initialization(construction) of a new object. It calls a copy constructor.\n1. `T w; w = v;` This is an assignment to the existing object w. It calls an assignment operator.\n1. 使得函数 default 可能提醒他人调用 default 是可以正常工作的。\n1. 资源释放应该放到析构函数中，避免资源泄露。\n1. 赋值构造函数最好使用 copy-and-swap.这样可以解决 self-copy 等问题。\n1. 析构函数的调用与构造函数反序。\n\n```c++\n// copy assignment constructor\nT& T::operator=(const T& rhs) {\n    T copy(rhs);  // 调用copy构造函数\n    copy.swap(*this);  // good\n    return *this;\n}  // copy will be destruct\n\n// move assignment constructor\nT& T::operatr=(T&& rhs) {\n    T copy(std::move(rhs));  // rhs现在是左值？\n    copy.swap(*this);\n    return *this;\n}\n\n```\n\n### [类对象内存分布](https://blog.csdn.net/dxpqxb/article/details/102794132)\n\n1. [带基类](https://blog.csdn.net/chuncanL/article/details/70306740)\n1. [sizeof(vector<>)大小固定](https://www.zhihu.com/question/34955591)\n1. [虚函数](https://tangocc.github.io/2018/03/20/cpp-class-memory-struct/)\n1. `p/x *(long *)&T` gdb 打印虚表地址 (64 位机器), 对象起始地址前 8 字节\n1. `p/x *(long *)(*(long *)&T + 8)` 打印第 2 个虚函数地址\n\n### 虚析构函数的作用\n\n1. 子类对象析构时一定会调用到子类的析构函数，这可以保证对象的正确析构。（多态时，例如工厂模式，基类需要定义虚析构函数）\n1. 多态发生在父类指针或引用指向子类对象时。此时如果没有虚析构，编译器默认调用父类的析构函数，无法通过父类的指针或引用析构子类对象的空间，此时就有可能发生内存泄漏。当父类析构和子类析构定义为 virtual 虚函数时，就可以顺利通过父类的指针或引用析构子类对象的内存空间。\n\n### 构造析构顺序\n\n1. 构造顺序\n   - 如果某个类具有基类，执行基类的构造函数。\n   - 类的非静态数据成员，按照声明的顺序创建。\n   - 执行该类的构造函数。\n2. 析构顺序：相反\n   - 调用类的析构函数。\n   - 销毁数据成员，与创建的顺序相反。\n   - 如果有父类，调用父类的析构函数。\n\n### The Rule of Three or Five(after c++11)\n\n1. 含义：如果类里面需要管理 resource，例如 rall pointer，那么你需要手写 3 个 functions.否则会调用默认函数，可能会出现多个指针副本，引起悬垂指针等问题。（可以 delete 这些函数，使 non-copyalble)\n   - A destructor to free the resource\n   - A copy constructor to copy the resource\n   - A copy assignment operator to free the left-hand resource and copy the right-hand one.\n   - A move constructor to transfer owenership of resource (after c++11)\n   - A move assignment operator to free the left-hand resource and transfer ownership of the right-hand one(after c++11)\n\n### The Rule of Zero\n\n1. 含义：如果你的类没有管理任何资源，但是使用了库中的 vector，string 等，那么你应该避免写特殊的函数，使用默认函数。\n   - Let the compiler implicitly generate a default destructor\n   - Let the compiler generate the copy constructor\n   - Let the compiler generate the copy assignment operator\n   - (But your own swap might improve performance)\n\n### 两种设计良好的 value-semantic C++类\n\n1. Bussiness-logic classes: 不管理资源，follow the Rule of Zero\n1. Resource-management classes(small, single purpose)： 管理资源(最好使用 RAII)，follow the Rule of Three or Five.\n\n### std::ref 与 reference_wrapper\n\n1. [浅析 std::ref 与 reference_wrapper](https://blog.csdn.net/u014645632/article/details/78966340)\n\n### move\n\n1. move does not move anything. （只是所有权移动，为物理移动任何东西）\n1. move unconditionally casts its input into an rvalue reference(无变量保存的数据)，会将输入变为右值。\n1. move constructor `ClassXX(ClassXX&& w) = default` w 是右值引用\n1. move assignment operator `ClassXX& operator=(ClassXX&& w) = default`\n1. 类成员最好用智能指针。原始指针不能使用默认 move 构造函数。需要自己写 move 构造函数， 分两步： member-wise move and reset。\n1. move asignment operator 分 3 步： cleanup, member-wise move and reset\n1. make move operations(constructor) noexcept\n1. Don't return a T&&.\n1. 使用 move 后，原来的变量不再进行资源释放，它已经将所有权转移给新的变量，由新变量进行资源管理。\n\n### forward\n\n1. `& & = &`, `& && = &`, `&& & = &`, `&& && = &&` 变量，是 lvalue\n1. `void f(T&& x); auto&& var = var1;` 其中 T&&和 auto&&是 forward reference(T 类型不确定，T&&类型要通过推导（模板），如果 T 是确定的，那么是右值引用，如类的 move 构造函数 T 就是类名称)。转发引用。被称为 universal reference.\n1. forward reference 作为参数能接受左值也能接受右值。\n1. 应用: `make_unique`可以传左值和右值。`auto i = make_unique<int>(1); auto s1_ptr = make_unique<string>(s1)`\n1. std::forward（不是 forward reference, 是标准库函数）作用： 如果输入是 lvalue，将其转化为 lvalue reference，如果是 rvalue，将其转化为 rvalue reference。使用原因：当一个函数输入参数是右值，并且需要用到该参数调用其他函数，调用时会将右值转化为左值，因为有了名字。在调用的函数中被当做左值来处理。如果希望是右值，则实现不了。在调用其他函数时，将参数通过 std::forward 转化一下\n1. std::forward does not forward anything。\n1. 问题：forward reference 作为模板的参数时能接收任何参数，容易与其他函数冲突。\n\n### 左值(lvalue) 右值(rvalue)\n\n1. 左值：占据内存中某个可识别位置（有变量保存）的对象\n1. 右值：临时存储，没有变量标识。\n1. 如果表达式的结果是一个暂时的对象，那么这个表达式就是右值。\n1. 如果函数能直接返回，不要起名字。return 右值，否则要使用 move。(RVO)\n\n### && rvalue reference 右值引用\n\n1. 只有左值才能给引用`int nine = 9; int& ref = nine;` 不能`int& ref = 9;`，也不能`int& ref = get_value()`\n1. 右值引用用法：`int&& ref = 9`或`int&& ref = get_value()`\n\n## Lifetime and Allocation Deallocation\n\n1. 指的是 object lifetime，不是类。\n1. 对象涉及到生命周期和内存分配销毁两方面问题。一般生命周期起始分配内存，生命周期结束释放内存。但生命周期可能小于内存分配释放时间，比如 option.\n1. reference 能够延长临时变量生命周期。 `string & s = get_string();`无问题 `char *s = get_string().c_str();`有问题，get_string 返回右值，s 只接受了指针，右值内容被释放。\n\n## Smart Pointers\n\n1. 用来管理 raw pointer，属于资源管理类。\n\n### unique_ptr\n\n1. 防止内存泄露，使所有权清晰。\n1. 唯一所有权， 不能复制，只能 move\n1. 有一个 Deleter 成员变量\n1. 有两个参数，Deleter 有默认\n1. 智能指针传参和返回值应该`按值传递`，这样更简单，而且只会消耗很小的资源(8 字节)，栈上传递，很快.\n1. 不要通过引用传递指针\n\n```c++\ntemplate<class T, class Deleter = std::default_delete<T>>\nclass unique_ptr {\n    T* p_= nullptr;\n    Deleter d_;\n\n    ~unique_ptr() {\n        if (p_) d_(p_);\n    }\n}\n\ntemplate<class T>\nstruct default_delete {\n    void operator()(T* p) const {\n        delete p;\n    }\n}\n\n```\n\n1. 需要调用 free, close 等地方，可以封装为 unique_ptr, sample:\n\n```c++\nstruct FileClose {\n    void operator()(File *fp) const {\n        assert(fp != nullptr);\n        fclose(fp);\n    }\n}\n\nFile *fp = fopen(\"input.txt\", \"r\");\nstd::unique_ptr<File, FileClose> uptr(fp);\n```\n\n### shared_ptr\n\n1. 避免悬垂指针。\n1. shared mean reference counting 引用计数\n1. \"Will the last person out of the room please turn out the lights.\" 最后一个离开房间的人请关灯，人数就相当于引用计数，灯相当于共享的资源。最后一个释放资源。如果房间里还有人就把灯关了，剩下的人就相当于悬垂指针。\n1. 栈上有两部分 ptr to T and ptr to control block。分别指向堆上数据。\n1. uniqe_ptr 可以转化为 shared_ptr，反之不成立。\n\n### make_shared and make_unique\n\n1. 现代的 c++应该避免使用 raw new and delete，智能指针可以避免使用 delete，我们也应该避免使用 new，工厂函数能够避免 new。\n1. make_shared and make_unique 都是`工厂函数`。make_shared 能够产生一个 shared_ptr，make_unique 能够产生一个 unique_ptr\n1. `最好不要使用rall pointer`，。如果不用 rall pointer，就不用担心内存泄露。\n\n### weak_ptr\n\n2. weak_ptr 可以告诉你 xuan\n1. weak_ptr has the same physical layout ad shared_ptr\n1. weak_ptr 不是智能指针。不能对 weak_ptr 解引用\n1. weak_ptr 可以看作是获取 shared_ptr 的 ticket，如果拥有 weak_ptr 就有权获得 shared_ptr。\n1.\n\n### std::enable_shared_from_this\n\n## Cast\n\n### 四种关键字: const_cast, 常量性转除;dynamic_cast, 向下安全转型;reinterpret_cast, 重新解释转型; static_cast, 静态转型;\n\n1.  const_cast, 常量性转除:\n\n主要对变量的常量性(const)进行操作, 移除变量的常量性, 即可以被非常量指向和引用, 详见代码;\n\n2. dynamic_cast, 向下安全转型:\n\n主要应用于继承体系, 可以由 \"指向派生类的基类部分的指针\", 转换\"指向派生类\"或\"指向兄弟类\";\n\nstatic_cast 只能转换为\"指向派生类\";\n\n3. reinterpret_cast, 重新解释转型:\n\n主要是对 2 进制数据进行重新解释(re-interpret),不改变格式, 而 static_cast 会改变格式进行解释;\n\n如由派生类转换基类, 则重新解释转换, 不改变地址, 静态转换改变地址;\n\n4. static_cast, 静态转型:\n\n主要是数据类型的转换, 还可以用于继承;\n\n## Zero Cost Abstract\n\n## Lambdas\n\n1. 用于定义和创建匿名函数。\n1. 语法： `[capture list] (params list) mutable exception -> return type { function body}`\n\n## Macros\n\n1. [C++宏（Macro）的各种玩法](https://blog.csdn.net/qq_22660775/article/details/89255286)\n1. [thread local 来解决 macro 不同线程 static 变量相同问题](https://stackoverflow.com/a/31609246)\n\n### #define\n\n1. `#if __cplusplus < 201703L` 可用宏来兼容 c++11 和 c++17\n\n## Memory\n\n1. 内存分为 host 内存和 device 内存。需要管理 device 上需要大空间的对象，比如 tensor，还需要在 host 和 device 内存中传输数据。输入的数据在 host 端处理后需要发送到 device 上使用，device 上输出结果或 dump 的数据需要在 host 端显示或者保存。不同的设备管理方式不同，当设备段是直接通过物理地址管理内存的，可以在 host 端创建一个对象来管理设备端的内存。\n\n### 内存分配方式\n\n1. `栈` 函数参数，局部变量\n1. `堆` malloc 和 free。堆上操作系统维护的一块内存\n1. `自由存储区` new 和 delete。自由存储区是 C++中通过 new 和 delete 动态分配和释放对象的抽象概念。有些编译器使用 malloc 和 free 实现 new 和 delete。\n1. `全局/静态存储区` 全局变量和 static 变量。\n1. `常量存储区` 存放的是常量，不允许修改。\n\n### 内存管理方式\n\n1. `自动存储`\n1. `静态存储`\n1. `动态存储`\n1. `线程存储`\n\n## Multi Thread\n\n## Small Module\n\n### 值语义与引用语义\n\n1. 值语义(value sematics)指的是对象的拷贝与原对象无关，就像拷贝 int 一样，拷贝之后与原对象脱离关系。\n1. 引用语义(reference sematics)或者对象语义(object sematics)是指面向对象意义下的对象，对象是禁止拷贝的。因为拷贝对象是无意义的，如拷贝一个雇员不会变成两个雇员。\n1. 值语义：复制（赋值操作）以后，两个数据对象拥有的存储空间是独立的，相互之间互不影响。\n1. 引用语义：复制（赋值操作）以后，两个数据对象互为别名。操作其中一个会影响另一个。\n1. 引用语义赋值操作是按位复制，有可能只复制了栈上的数据，为复制堆\n1. 值语义的好处： 生命周期管理很简单，不用担心生命周期。\n1. 引用语义的 object 由于不能拷贝，我们只能通过指针或引用来使用它。需要考虑生命周期来释放资源，避免悬垂指着等。\n1. 使用指针和引用之后所有的赋值代表将有多个变量指向同一个对象，一旦其中一个变量释放了对象的资源，其他的变量的使用将是一个问题。\n1. (zero abstract cost) C++的 class 的 layout 与 C struct 一样，没有额外开销。定义一个只包含一个 int 的 class 的对象和定义一个 int 一样。\n1. 默认拷贝构造函数是最简单的浅拷贝。\n1. 智能指针实际上是将对象语义转化为值语义。\n\n### static\n\n1. [cppreference static members](https://en.cppreference.com/w/cpp/language/static)\n1. [thread local 来解决 macro 不同线程 static 变量相同问题](https://stackoverflow.com/a/31609246)\n1. [类 static 成员变量头文件初始化问题](https://caloud.tencent.com/developer/article/1858485)\n1. C++17 中可以用 inline 来在头文件中定义类的 static 成员变量 [link](https://stackoverflow.com/a/54994044)\n1. 局部 static 变量只被初始化一次，生命周期是从创建到程序结束。相比全局 static 变量只是作用域不是全局。\n1. 如果全局变量仅在单个函数中使用，则可以将这个变量改为该函数的静态局部变量。\n1. 全局变量，静态局部变量，静态全局变量都存在全局静态存储区。\n1. 函数中必须要使用 statci 变量的情况：当某个函数返回值为指针类型时，则必须是 static 的局部变量的地址作为返回值，因为他的生命周期是整个程序运行期间。\n1. static 全局变量限定作用范围为定义该变量的文件。\n1. 子类访问父类定义的 static 成员变量或函数`Son::Parent::xxx()`\n1. static 存储在全局静态存储区，因此父类中定义的 static 变量由所有子类父类对象共享。\n1. 作用\n   - 修饰普通变量，修改变量的存储区域和生命周期，使变量存储在静态区，在 main 函数运行前就分配了空间，如果有初始值就用初始值初始化它，如果没有初始值系统用默认值初始化它。\n   - 修饰普通函数，表明函数的作用范围，仅在定义该函数的文件内才能使用。在多人开发项目时，为了防止与他人命名空间里的函数重名，可以将函数定位为 static。\n   - 修饰成员变量，修饰成员变量使所有的对象只保存一个该变量，而且不需要生成对象就可以访问该成员。\n   - 修饰成员函数，修饰成员函数使得不需要生成对象就可以访问该函数，但是在 static 函数内不能访问非静态成员。\n1. 伪单例： 共享一个变量，但多线程不安全\n\n```c++\ninline std::vector<bool>& is_file() {                                                                                                                                                                                static std::vector<bool> ftype;                                                                                                                                                                                    return ftype;\n}\nis_file() = {true}\n```\n\n1. fps control\n\n```c++\n#define __FPS_CONTROL__(fps)                                                   \\\n  {                                                                            \\\n    static thread_local auto start_time = std::chrono::steady_clock::now();    \\\n    auto end_time = std::chrono::steady_clock::now();                          \\\n    auto time_us = std::chrono::duration_cast<std::chrono::microseconds>(      \\\n                       end_time - start_time)                                  \\\n                       .count();                                               \\\n    int sleep_us = static_cast<int>(1000.0 * 1000.0 / fps) - time_us;          \\\n    if (sleep_us > 0) {                                                        \\\n      std::this_thread::sleep_for(std::chrono::microseconds(sleep_us));        \\\n    }                                                                          \\\n    start_time = std::chrono::steady_clock::now();                             \\\n  }\n```\n\n### 操作符重载\n\n### 作用域\n\n1. `全局作用域`\n1. `局部作用域`\n1. `语句作用域`\n1. `类作用域`\n1. `命名空间作用域`\n1. `文件作用域`\n\n### 函数指针\n\n1. 声明\n\n```c++\ndouble cal(int);   // prototype\ndouble (*pf)(int);   // 指针pf指向的函数， 输入参数为int,返回值为double\npf = cal;    // 指针赋值\n```\n\n1. 作为函数参数\n\n```c++\nvoid estimate(int lines, double (*pf)(int));  // 函数指针作为参数传递\ndouble y = (*pf)(5);   // 通过指针调用， 推荐的写法\ndouble y = pf(5);     // 这样也对， 但是不推荐这样写\n```\n\n### typedef(传统)\n\n1.  任何声明变量的语句前面加上 typedef 之后，原来是变量的都变成一种类型。不管这个声明中的标识符号出现在中间还是最后。\n1.  作用：\n    - 促进跨平台开发\n    - 定义易于记忆的类型名\n1.  使用：\n    - `typedef int* IntPtr; int x = 5; IntPtr = &x; *IntPtr = 1;`\n    - `typedef void (*call_back)(int)； void add_one(int i) {return i+1}; call_back = add_one; call_back(2);` call_back 声明为函数指针\n\n### using(新标准)\n\n1. `using IntPtr = int*;int x = 5; IntPtr = &x; *IntPtr = 1;`\n1. `using V1F = std::vector<float>; using V2F = std::vector<V1F>;`\n1. `using CallBack = void (*)(int)；// using CallBack = decltype((add_one)); void add_one(int i) {return i+1}; CallBack call_back = add_one; call_back(2);`\n\n### overload (重载)and override or overwrite（覆盖或重写） [link](https://blog.csdn.net/Zeno_wrj/article/details/110478916)\n\n1. 类内：Overloading occurs when two or more methods in one class have the same method name but different parameters.\n1. 类间：Overriding or overwrite means having two methods with the same method name and parameters (i.e., method signature). One of the methods is in the parent class and the other is in the child class.\n\n### class 和 struct 区别\n\n1. C++中的 struct 对 C 中的 struct 进行了扩充，可以有成员函数，可以被继承，可以有多态。\n1. struct 和 class 最大的区别是访问权限，struct 成员默认是 public 的，class 默认是 private，struct 继承默认是 public，class 默认是 private。\n1. class 可以定义模板参数，就像 typename，而 struct 不行。\n\n### template 定义时的 typename 和 class 区别\n\n1. 最早使用的 class 可能会造成概念上的混淆，后面加上了 typename 替代 class。\n\n### new/delete 和 malloc/free 区别\n\n1. malloc/free 是 c++/c 标准库函数，new/delete 是 C++运算符，都可以用于动态内存申请和内存释放。\n1. new 一个对象时会调用构造函数，delete 一个对象时会调用析构函数。\n1. 对于非内部累来说，malloc/free 无法满足在对象创建时执行构造函数，在对象销毁时调用析构函数。\n1. 由于内部数据类型的对象没有构造和析构的过程，对他们而言 malloc/free 和 new/delete 是等价的。\n1. 为什么需要 malloc 和 free，因为 C++中经常调用 c 函数，而 c 只能用 malloc 和 free 管理动态内存（堆上）。\n\n### 函数调用的过程\n\n### for each any_of\n\n### rang-based for loop\n\n### std::tuple\n\n1. `std::get<0>(std::make_tuple(3.8, 'A', \"Lisa Simpson\")) `\n1. `float a; char b; string c; std::tie(a, b, c) = (std::make_tuple(3.8, 'A', \"Lisa Simpson\");`\n1. `auto [a, b ,c] = (std::make_tuple(3.8, 'A', \"Lisa Simpson\");`\n\n### std::tie\n\n1. 创建 tuple 的左值引用\n1. 可以用来解 tuple\n1. c++17 之后可以被 structured bindings 替代\n\n### Structured Bindings c++17\n\n1. `const auto &[elem1, elme2] = some_thing;`\n1. `auto [ gpa2, grade2, name2 ] = return std::make_tuple(3.8, 'A', \"Lisa Simpson\");`\n1. 类似引用，结构化绑定是既存对象的别名。不同于引用的是，结构化绑定的类型不必为引用类型。\n1. [reference](https://zh.cppreference.com/w/cpp/language/structured_binding)\n\n### concepts c++20\n\n### modules c++20\n\n### string_view c++17\n\n### std::format c++20\n\n### ranges c++20\n\n### fold expressions\n\n### std::exchange()\n\n### [[deprecated]] attribute\n\n```\n[[deprecated]]\nvoid old_method();\n[[deprecated(\"Use new_method instead\")]]\nvoid legacy_method();\n```\n\n### c 中的 const 和 c++中的 const 区别\n\n1. c 语言中只有 enum 能实现真正的常量\n\n### macro 和 inline\n\n### const 关键字作用\n\n### default， delete，override，final，noexcept\n\n1. `final`在基类中指定无法在派生类中重写的虚函数。还可以指定无法继承的类。\n1. 当使用 default 或者 delete 定义构造，析构，复制构造，赋值，move...其中一个时，也需要定义其他的。\n\n### explicit\n\n1. explicit 只能用来修饰类构造函数。作用是声明类构造函数是显示调用的，不能隐式调用。\n1. 只能显示使用`ClassXX a(args)`来创建对象，不能使用`ClassXX a = args`来隐士调用构造函数。\n1. 作为函数参数也必须使用`ClassXX(args)`，不能使用`args`隐式调用构造函数。\n1. 能用就用。\n\n### virtual\n\n1. [虚函数表 0](https://www.cnblogs.com/Mered1th/p/10924545.html)\n1. [虚函数表](https://cloud.tencent.com/developer/article/1599283)\n1. 接口类(基类)的析构函数需要为 virtual,这样才能正常调用子类的析构函数，如果不为 virtual，那么只会调用基类的析构函数，不会调用子类的。\n\n### 友元\n\n1. 友元函数\n2. 友元类\n3. 友元成员函数\n\n### decltype\n\n1. 获取变量的类型。`int x; decltype(x) y; // y is int`\n2. 可以用于匿名结构体。\n\n### constexpr\n\n1. 常量表达式是指`值不会改变`并且在`编译过程中就能计算结果`的表达式\n1. const int sz = get_size(); sz 不是常量表达式，sz 只是常量，因为值在编译期才确定\n1. 声明为 constexpr 的变量一定是个常量，并且必须用常量表达式初始化。\n\n### add pointer and is pointer remove_pointer\n\n1. 都是类模板，定义在 std 中\n1. `add_pointer<T>`：T 可以是具体类型也可以是类型引用。获取类型的指针，保存在 type 成员变量里。一般和 typede 一起使用`typedef std::add_pointer<x>::type IntPtr; IntPtr i;`\n\n### std::function\n\n1. std::function 是通用多态函数封装器。\n1. 定义：`template < class R, class... Args> class function<R(Args...)` R 为返回类型，Args 为参数。\n1. 例子：`void p(int i) { cout << i;}; std::fuction<void<int>> f = p; f(i);`\n1. 可用来实现函数回调\n\n### std::atomic\n\n## Design Patterns\n\n### 工厂模式\n\n1. 目的：将对象的创建与对象的使用解耦。\n1. `简单工厂函数` 将对象的创建放入到统一工厂函数中，根据类型判断具体创建哪一种类型对象。相当于将耦合问题从使用中转移到工厂函数。扩展性差，每增加一个产品就要修改工厂函数。\n1. `工厂方法模式` 每个产品都有一个工厂函数，相当于将耦合从总的工厂函数中转移到各个产品的工厂函数中，问题：使用时需要包含各个工厂头文件。\n1. `抽象工厂模式` 同工厂方法模式，只不过每一个具体工厂可以可以调不同接口（不是同一个接口传参数）创建不同的产品。\n1. `反射，依赖注入` 由类名来创建对象。相当于工厂方法模式+单例模式。全局有一个总的工厂，工厂里有保存产品类型及其工厂函数的 map 表(使用到函数指针)，每个产品都要有一个工厂，并且需要注册（依赖注入）到总的工厂 map 表中。解决了工厂方法模式中使用问题。map 可以使用全局变量，注册函数写成类的静态函数，就不需要专门设计一个总的工厂类。总工厂生产的产品不由总工厂控制，而是交给子工厂控制，这叫`控制反转`，将子工厂通过注册接口传入到总工厂中，这叫`依赖注入`\n1. `模板工厂模式`\n\n#### Reference\n\n1. [factory method](https://www.cnblogs.com/xiaolincoding/p/11524401.html)\n1. [reflection](https://blog.csdn.net/K346K346/article/details/51698184)\n\n### 代理模式\n\n## Program with C\n\n## 接口实现分离\n\n1. `Pimplldiom`(防火墙技术，代理模式？(未提供一个抽象接口)) 将实现细节隐藏于指针背后，比如：分成两个类，一个负责提供接口，一个负责提供实现。负责提供实现的类的对象作为负责提供接口类的私有成员。这种方式只能不能像工厂函数一样由多种实现，因为类中写死了一种实现方式。\n2. `Object Inerface` 将接口定义为抽象类，派生类实现这些借口。类似工厂函数(创建型设计模式)。\n\n## 命令行参数解析\n\n1. glib [usage](https://github.com/kiwibrowser/src/blob/master/third_party/wds/src/mirac_network/gst-test.cpp#L67)[link1](https://gstreamer.freedesktop.org/documentation/application-development/basics/init.html?gi-language=c#the-goption-interface) [link2](https://people.gnome.org/~ryanl/glib-docs/glib-Commandline-option-parser.html)\n2. getopt: see how to use with emacs woman; [link1](https://github.com/Xilinx/Vitis-AI/blob/master/tools/Vitis-AI-Library/usefultools/src/xir2elf.cpp#L38)\n3. [cxxopts](https://github.com/jarro2783/cxxopts): only include header\n\n### Reference\n\n1. [reference](https://blog.csdn.net/TAOKONG1017/article/details/79561856)\n1. [modern-cpp-features](https://github.com/AnthonyCalandra/modern-cpp-features)\n","tags":["Cpp"],"categories":["Program","Cpp"]},{"title":"plugins","url":"/2021/09/13/Plugin-System/","content":"\n## Basic\n1. emacs man dlopen有参考例子\n2. 用作插件：插件技术最大特点是更新插件无需编译主程序，设计良好的系统可以做到在线升级，只更新插件（补丁）\n3. 使用dlopen时链接发生在runtime, 不需要知道函数声明， 而-l链接发生在compile期，需要知道函数声明\n4. libc.so.6 GNU标准c库，含有getchar,system等函数\n\n## VVAS and DeepStream\n1. use gstreamer\n\n## VART and TensorRT\n1. use dlopen\n\n\n## samples\n1. [x3](https://github.com/rhcad/x3py/wiki)\n\n## famous projects\n1. [TensorRT](https://github.com/NVIDIA/TensorRT/blob/eb8442dba3c9e85ffb77e0d870d2e29adcb0a4aa/plugin/common/cudaDriverWrapper.cpp#L28)\n2. [Tensorflow](https://github.com/tensorflow/tensorflow/blob/96493ef750a69c5ab71a1744b282c0c6770df234/tensorflow/core/platform/default/load_library.cc#L27)\n3. [pytorch](https://github.com/pytorch/pytorch/blob/f23f21dafebae49c9670e2e695344edd01867b0c/torch/csrc/deploy/loader.cpp#L373)\n5. [linux](https://github.com/torvalds/linux/blob/master/tools/lib/traceevent/event-plugin.c#L461)\n6. [FFmpeg](https://github.com/FFmpeg/FFmpeg/blob/989febfbd0c986e9e3e0f269a6b22778bf79147b/libavcodec/omx.c#L115)\n7. [opencv](https://github.com/opencv/opencv/blob/c3ac834526c66840835737f03ed1b4d8f711534e/modules/core/src/gl_core_3_1.cpp#L62)\n\n\n## links\n1. [BUILDING A PLUGIN SYSTEM USING DYNAMIC LOADING](https://hackaday.com/2018/07/12/its-all-in-the-libs-building-a-plugin-system-using-dynamic-loading/)\n2. TensorRT [link1](https://github.com/NVIDIA/TensorRT/blob/eb8442dba3c9e85ffb77e0d870d2e29adcb0a4aa/samples/common/common.h#L930) [trtexec](https://github.com/NVIDIA/TensorRT/blob/eb8442dba3c9e85ffb77e0d870d2e29adcb0a4aa/samples/trtexec/trtexec.cpp#L145)\n","tags":["Cpp"],"categories":["Program","Cpp"]},{"title":"Profiling","url":"/2021/08/24/Cpp-Profiling/","content":"\n## great\n\n1. 通过 roofline 作为指导, 各种优化方法对应到 roofline\n1. [各种性能分析工具](https://www.brendangregg.com/overview.html)\n1. [easyperf](https://easyperf.net/)\n1. [VAI profiler](https://docs.xilinx.com/r/zh-CN/ug1414-vitis-ai/Vitis-AI-Profiler-%E6%9E%B6%E6%9E%84)\n1. [perf-book](https://github.com/dendibakh/perf-book)\n1. [perf-ninja](https://github.com/dendibakh/perf-ninja)\n1. [基于 CPU 性能调优的必要性和方法](https://www.eet-china.com/mp/a196988.html)\n1. [topics/profiling](https://github.com/topics/profiling)\n1. [Linux_Performance_Analysis_and_Tools](https://hhb584520.github.io/kvm_blog/files/perf/Linux_Performance_Analysis_and_Tools.pdf)\n1. [linuxperf](https://www.brendangregg.com/linuxperf.html)\n1. [glances](https://github.com/nicolargo/glances)\n<!-- more -->\n\n1. [profiling 与性能优化总结](https://zhuanlan.zhihu.com/p/362575905)\n   - 通常在计算密集型（CPU intensive）的任务中 CPU time 会占据较大的比重，而在 I/O 密集型（I/O intensive）任务中 off-CPU time 会占据较大的比重。\n   - 我们把程序在 CPU 上执行的时间（即 user CPU time + system CPU time）称为 CPU time（或 on-CPU time），程序处于睡眠等状态的时间称为 off-CPU time（or blocked time），程序实际运行的时间称为 wall clock time（字面意思是墙上时钟的时间，也就是真实世界中流逝的时间），对于一个给定的线程：wall clock time = CPU time + off-CPU time。\n1. [Linux tracing/profiling 基础：符号表、调用栈、perf/bpftrace](http://arthurchiao.art/blog/linux-tracing-basis-zh/)\n1. [Practical Linux tracing ( Part 1/5) : symbols, debug symbols and stack unwinding](https://medium.com/coccoc-engineering-blog/things-you-should-know-to-begin-playing-with-linux-tracing-tools-part-i-x-225aae1aaf13)\n1. [Perf IPC 以及 CPU 性能](https://zhuanlan.zhihu.com/p/385519404)\n1. [plantegg's blog](https://plantegg.github.io/)\n1. [VART trace](https://github.com/Xilinx/Vitis-AI/tree/master/src/vai_runtime/vart/trace)\n1. [vaitraceTools/mem_perf](https://github.com/Xilinx/Vitis-AI/tree/master/src/vai_runtime/vart/trace/vaitrace/vaitraceTools/mem_perf)\n1. [Vitis-AI-Profiler](https://docs.xilinx.com/r/en-US/ug1414-vitis-ai/Vitis-AI-Profiler?tocId=LjrELULUsJtA_mgGxWE2lQ)\n1. [linux 性能分析工具 perf：十八般武器之 cache](https://zhuanlan.zhihu.com/p/445267642)\n1. [Using perf On Arm platforms](https://static.linaro.org/connect/yvr18/presentations/yvr18-416.pdf)\n1. [viztracer](https://github.com/gaogaotiantian/viztracer)\n1. [orbit](https://github.com/google/orbit)\n1. 谓词替换分支， 谓词函数是一个返回布尔值的函数。\n   - 这种方法涉及将复杂或重复的条件逻辑（通常出现在 if-else 或 switch 语句中）替换为更简洁、更清晰的代码结构。这种替换通常通过使用函数指针、Lambda 表达式或对象方法来实现。如 map 查找来替换 else if\n1. 问 chatgpt 如何优化\n1. [TVM 各种优化方法](https://tvm.hyper.ai/docs/tutorial/tensor_expr#%E7%A4%BA%E4%BE%8B-2%E4%BD%BF%E7%94%A8-te-%E6%89%8B%E5%8A%A8%E4%BC%98%E5%8C%96%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95)\n   - 总体： 提高缓存命中率和使用 SIMD\n   - 块操作(cache)\n   - 循环置换(cache)\n   - 向量化(simd)\n   - 数组打包, 重新排序，类似 cutlass cute(cache)\n   - 通过缓存优化块写入(cache)\n   - 并行化\n1. [highway profiler 使用 RAII 可以设置 zone](https://github.com/google/highway/blob/master/hwy/profiler.h)\n   - 生命周期结束做一些操作\n   - add_definitions(-DPROFILER_ENABLED=1)\n1. [QPS、TPS、RT、吞吐量](https://www.51cto.com/article/718780.html)\n   - 系统吞吐量几个重要参数：QPS（TPS）、并发数、响应时间。\n   - QPS（TPS）= 并发数/平均响应时间\n   - QPS：Queries Per Second 意思是“每秒查询率”\n\n## roofline\n\n1. [berkeley roofline](https://crd.lbl.gov/divisions/amcr/computer-science-amcr/par/research/roofline/)\n   - [视频教程](https://crd.lbl.gov/divisions/amcr/computer-science-amcr/par/research/roofline/ecptutorial/)\n   - [SC23 Tutorial](https://crd.lbl.gov/divisions/amcr/computer-science-amcr/par/research/roofline/tutorial/)\n   - [roofline 工具](https://crd.lbl.gov/divisions/amcr/computer-science-amcr/par/research/roofline/software/)\n   - [(very good)相关文章](https://crd.lbl.gov/divisions/amcr/computer-science-amcr/par/research/roofline/publications/)\n     - [berkeley lab 2021](https://crd.lbl.gov/assets/Uploads/ECP21-Roofline-1-intro.pdf) [Uploads/ECP20-Roofline-1-intro.pdf](https://crd.lbl.gov/assets/Uploads/ECP20-Roofline-1-intro.pdf)\n       - Kernels near the roofline are making good use of computational resources; 在远离 roofline 的地方没利用好资源\n         - kernels can have low performance (GFLOP/s), but make good use (%STREAM) of a machine\n         - kernels can have high performance (GFLOP/s), but still make poor use of a machine (%peak)\n1. [Hierarchical Roofline Analysis on CPUs](https://crd.lbl.gov/assets/Uploads/ECP20-Roofline-4-cpu.pdf)\n1. [Hierarchical Roofline Analysis on GPUs](https://www.nersc.gov/assets/Uploads/Tutorial-ECP2020-GPUs.pdf)\n1. [Hierarchical Roofline Analysis on GPUs](https://crd.lbl.gov/assets/Uploads/ECP21-Roofline-2-NVIDIA.pdf)\n1. [Hierarchical Roofline Analysis on GPUs](https://crd.lbl.gov/assets/Uploads/ECP21-Roofline-6-advanced.pdf)\n1. papers\n   - [Roofline: AnInsightfulVisual Performance Modelfor Floating-PointProgramsand Multicore Architectures](https://escholarship.org/uc/item/78h8v7mr)\n   - [Hierarchical Roofline Analysis for GPUs: Accelerating Performance Optimization for the NERSC-9 Perlmutter System](https://onlinelibrary.wiley.com/doi/am-pdf/10.1002/cpe.5547)\n   - [Instruction Roofline: An Insightful Visual Performance Model for GPUs](https://escholarship.org/uc/item/0cg446n2)\n   - [An Instruction Roofline Model for GPUs](https://crd.lbl.gov/assets/Uploads/InstructionRooflineModel-PMBS19-.pdf)\n1. 优化目标是向屋顶靠近(上)，利用好计算资源，终极目标是用满计算资源; 降低延迟，增大吞吐\n1. 脊点（拐点）是系统平衡点\n1. 不同内存应该都是从 0 点出发，为什么有些图内存是平行的，坐标轴原点不是 0；从上面截断，看着像是平行的\n1. Roofline 模型讲的是程序在计算平台的算力和带宽这两个指标限制下，所能达到的理论性能上界(可实测最大 flops 和内存带宽作为理论)，而不是实际达到的性能，因为实际计算过程中还有除算力和带宽之外的其他重要因素，它们也会影响模型的实际性能，这是 Roofline Model 未考虑到的。例如矩阵乘法，会因为 cache 大小的限制、GEMM 实现的优劣等其他限制，导致你几乎无法达到 Roofline 模型所定义的边界（屋顶）。\n1. 为什么不能达到屋顶上方？因为不是顺序访问， 屋顶是内存并行访问计算出来的，访问内存有利用率. cache 有命中率，（物理能达到最大带宽 逻辑不能达到，搬了那么多数，利用的不多，跳着访问，浪费了带宽）\n1. Roofline 模型是一个可视化工具，`用于表示计算系统的性能上限`，并帮助识别应用程序的性能瓶颈。它通过绘制两个关键性能指标（算术密集度和峰值性能）来显示应用程序或某一计算部分的性能相对于理论峰值性能的位置。\n1. 可以实测最大 FLOPS，最大主存带宽, 得到最大计算访存比；当程序计算访存比小于最大计算访存比时，则是访存密集型，大于则是计算密集型。`并不是访存量大于计算量才是访存密集型`。\n1. 应用所在的点不能超过屋顶\n1. 其中 x 轴表示`算术密度`（每个访问的字节所做的浮点操作数），y 轴表示`性能`（通常是 FLOP/s， Tops/s，即每秒浮点操作数）。如果代码在带宽屋顶下方但接近于它，那么可能会受到内存带宽的限制。相反，如果代码在计算屋顶下方但接近于它，那么它可能受到计算能力的限制。\n1. `how to opt gemm` 只需计算性能就能矩阵乘优化好坏，因为 roof line 目标就是提高计算性能, 计算性能才是关键，计算量一定，计算越快，延迟越少，访存只是完成计算需要的步骤，目的是完成计算\n1. `Arithmetic Intensity`算术密度(FLOPs/Byte), (速率或总量， 知道总操作数和总访问量也可以)\n1. 注意 FLOPS 和 Flops 区别。 FLOPS = Flops/s\n1. 转折点为最大性能，最大带宽（可由计算密度算）\n1. 程序算术密度点大于转折点所在算术密度就是计算密集型程序。小于则是访存密集型。\n1. 和 IPC 有何关联？\n1. [MegPeak——让你更懂你的处理器](https://zhuanlan.zhihu.com/p/522007924)\n1. [Accelerating HPC Applications with NVIDIA Nsight Compute Roofline Analysis](https://developer.nvidia.com/blog/accelerating-hpc-applications-with-nsight-compute-roofline-analysis/)\n1. 确定应用程序或代码段的总浮点操作次数和所访问的内存量?\n1. `likwid-bench`可以算 roofline\n1. [Roofline Model 与深度学习模型的性能分析](https://zhuanlan.zhihu.com/p/34204282)\n1. 内存最大带宽不是一个一个字节搬的速度 有位宽，增大 cache 命中率防止一个个搬\n1. 放在 cache 增加计算访主存储器比，斜向上\n1. 总访存量没变化；从 cache 里更快，如果计算强度不变，屋顶模型直接向上\n1. 应用计算强度如果不变，充分利用计算能力会减少延迟， 增大吞吐\n1. 拿到一个系统可以先制作 proofline, 跟据 proofline 指导优化\n1. 降低重复访存能增加计算密度， 斜向上，更好的利用硬件资源(性能提升)\n1. 如果只用 cache， 计算密度不变，向上\n1. Achieved Value - The achieved value represents the performance of the profiled kernel. If baselines are being used, the roofline chart will also contain an achieved value for each baseline.\n1. The distance from the achieved value to the respective roofline boundary (shown in this figure as a dotted white line), represents the opportunity for performance improvement. The closer the achieved value is to the roofline boundary, the more optimal is its performance. An achieved value that lies on the Memory Bandwidth Boundary but is not yet at the height of the ridge point would indicate that any further improvements in overall FLOP/s are only possible if the Arithmetic Intensity is increased at the same time. 到达点到 roofline 边界距离显示了优化机会，越接近边界表示越接近最佳性能，如果接近边界但还没到脊点，那么只有计算密度增加才能提高计算性能。\n1. memory bound 步骤：先向上， 在向右\n   - 向上：提高带宽(提升 cache 命中率)\n   - 向右：提高计算密度(算法层面优化：减少访存) Reducing data movement increases AI\n1. 系统和应用在存储不同层级上有不同的脊点和 AI\n   ![pic](https://i.ibb.co/8PQq9fy/Cs-Me-Agns06.png)\n\n### [berkeleylab/cs-roofline-toolkit](https://bitbucket.org/berkeleylab/cs-roofline-toolkit/src/master/)\n\n1. voltar gpu 使用的 tensor core(mma.h)测试的性能\n1. 得到数据\n\n```shell\ngit clone https://bitbucket.org/berkeleylab/cs-roofline-toolkit.git\ncd cs-roofline-toolkit/Empirical_Roofline_Tool-1.1.0/\ncp Config/config.voltar.uoregon.edu Config/config.orin\nvi Config/config.orin\n./ert Config/config.orin\n```\n\n1. 生成图片\n\n```shell\nvi Results.Orin/Run.001/roofline.gnu\n# 修改gpu配置\n# set terminal pngcairo enhanced font \"Arial,10\" fontscale 1.0 size 500, 350\n# set output 'roofline.png'\ngnuplot Results.Orin/Run.001/roofline.gnu\n```\n\n1. 加点\n\n```shell\n# Operational intensity\nop_ins = 1.32\n# Performance value\napp_perf = 77\napplication = \"Testapplication\"\napplication_offset = 1\nset object circle at first op_ins,app_perf radius char 0.5 fillcolor rgb 'red' fillstyle solid\nset label 1 at op_ins+0.2,app_perf+application_offset sprintf(\"%s (%.2f, %.2f)\", application, op_ins, app_perf)\n```\n\n1. 生成的 ps 文件, 可以直接显示\n   - [ps file onlineviewer](https://psviewer.org/onlineviewer.aspx)\n1. [Tutorial Empirical-Roofline-Model](https://github.com/RRZE-HPC/likwid/wiki/Tutorial%3A-Empirical-Roofline-Model)\n   - `sudo apt intall gnuplot`\n   - `gnuplot plotscript.gp`\n\n### [timemory](https://github.com/NERSC/timemory)\n\n1. [getting_started/roofline](https://timemory.readthedocs.io/en/develop/getting_started/roofline.html)\n1. `echo 1 | sudo tee /proc/sys/kernel/perf_event_paranoid`\n\n## 编译器选项\n\n1. [100-gcc-tips](https://github.com/hellogcc/100-gcc-tips/blob/master/src/index.md)\n1. `gcc -Q --help=optimizers -O1`打印优化级别的对应选项\n1. gcc 选项顺序很重要，如果两个选项冲突，则以后一个为准。可以用`-O3 -fno-inline-functions`既使用-O3 的功能又关闭函数内嵌功能。\n\n   - `-O3 -O0` O0 级别\n   - `-O0 -O3` O3 级别\n   - `-O3 -O` O3 级别\n   - `-O0 -O` O0 级别, If you use multiple -O options, with or without level numbers, the last such option is the one that is effective.\n\n1. `-O3` 是编译器的优化级别选项，表示进行高级优化。这个选项告诉编译器进行更多的优化，以提高代码的执行效率。`-O3` 是最高级别的优化选项，它会启用多种优化技术，包括循环展开、函数内联、向量化等。使用 `-O3` 可以显著提高代码的执行速度，但可能会增加编译时间和可执行文件的大小。\n1. `-Ofast` enables all -O3 optimizations. It also enables optimizations that are not valid for all standard-compliant programs. It turns on -ffast-math and the Fortran-specific -fstack-arrays, unless -fmax-stack-var-size is specified, and -fno-protect-parens\n1. `-LNO:simd` 是针对 SIMD（Single Instruction Multiple Data）指令集的优化选项。SIMD 是一种并行计算的技术，它允许在同一时间执行多个数据元素的相同操作，以提高程序的并行性和性能。`-LNO:simd` 告诉编译器使用 SIMD 指令集进行优化，以利用硬件的并行能力。这可以在循环、向量操作和并行计算等方面提高程序的性能。\n1. `-fopenmp`可以在编译时启用 OpenMP 并行编程的支持，从而实现多线程并行执行，提高程序的性能， 需要注意的是，使用 OpenMP 进行并行编程需要在代码中添加适当的并行指令，如 #pragma omp parallel，来标识需要并行执行的代码块。同时，也需要注意线程间的同步和数据共享，以避免并发冲突和数据竞争的问题。\n   - `#pragma omp parallel for reduction(+ : sum)`\n   - `#pragma omp parallel`\n   - 多线程访问同时写一个地址会 core， 需要先处理数据依赖关系，比如先放到 vector 中，最后再合并处理\n   - `export OMP_NUM_THREADS=4`设置运行时线程数量\n1. `sudo apt install libomp-dev`\n1. `-fopenmp-simd -DSIMDE_ENABLE_OPENMP`\n1. `-funroll-loops`强制编译器展开循环。这可能会提高某些循环密集型程序的性能，但也可能使代码体积增大。\n1. `-fprefetch-loop-arrays` 如果目标平台支持，这会在循环中为数组引用生成预取指令。\n1. `-march=native` 为当前运行编译命令的机器优化代码。这会使 GCC 产生针对你的特定 CPU 类型的代码，使用所有可用的指令集和优化。\n1. `-flto` 启用链接时间优化。这在链接时进行全程序分析，可能会产生更好的优化代码，但会增加链接时间。\n1. `-mfpu=neon` 此选项告诉编译器要为 NEON 浮点单元生成代码。这适用于较老的 ARM 架构和编译器版本。\n1. `-march` 使用此选项指定目标架构，例如`-march=armv8-a`。这可以确保编译器针对特定的 ARM 版本生成优化代码。\n1. `-mfloat-abi` 该选项定义了浮点数应该如何在函数调用中传递。有三个选项可以选择：soft、softfp 和 hard。使用 NEON 时，通常建议使用-mfloat-abi=hard。\n1. `-ftree-vectorize` 这是一个优化选项，允许编译器自动将循环转换为使用向量指令。虽然这不是直接与 NEON 相关的，但它可以帮助自动利用 NEON 功能。\n1. `-fprofile-generate / -fprofile-use`用于基于真实数据的反馈指导优化（PGO）。首先使用-fprofile-generate 编译和运行程序来收集数据，然后使用-fprofile-use 再次编译以使用该数据进行优化。\n\n```\nrm -rf *.gcda\ng++ -Ofast -fprofile-generate -o prefetch prefetch.cpp && ./prefetch\ng++ -Ofast -fprofile-use -o prefetch prefetch.cpp && ./prefetch\n```\n\n1. `-DCMAKE_CXX_FLAGS=\"-fprofile-generate\"` 然后运行生成 gcda, 然后 `-DCMAKE_CXX_FLAGS=\"-fprofile-use\"`\n1. `-fauto-profile` 使用 perf 来优化\n1. `软件预取技术`：预取技术可以减少由于缓存未命中而导致的延迟。`__builtin_prefetch` 是 GCC 的内置函数，用于预取数据到缓存。其他编译器可能有类似的指令。\n1. `-ftree-vectorize -funroll-loops`可以组合用\n\n### links\n\n1. [gcc.gnu.org/onlinedocs](https://gcc.gnu.org/onlinedocs/)\n1. [gcc-9.5.0 Optimize-Options](https://gcc.gnu.org/onlinedocs/gcc-9.5.0/gcc/Optimize-Options.html#Optimize-Options)\n\n## 程序性能分析步骤\n\n1. `cpufreq-set -g performance` 保持最大频率(频率固定好分析)\n1. `taskset -c 0 ./test` 固定核\n1. `perf stat -ddd ./test`\n1. `perf top -p` 快速的定位热点函数\n1. 火焰图看各函数耗时占比\n1. perf report and annotate 查看热点函数的热点代码\n1. profiling table 统计代码具体时间\n   - (profiling table + csv server)[nmon](https://nmon.sourceforge.io/pmwiki.php?n=Main.HomePage)\n   - [Caliper](https://github.com/LLNL/Caliper) 类似 Caliper\n1. flamescope 看 hot map\n1. speedscope\n\n## 60s 操作 bpf\n\n1. uptime -----> `load averages` [link](https://zhuanlan.zhihu.com/p/75975041) [link1](https://blog.51cto.com/u_15077533/4173309)\n\n   - Linux load averages 不仅可以用于追踪 runnable 任务，还可以跟踪不间断睡眠状态下的任务（uninterruptible sleep state）。\n   - 如果 1min 平均值高于 5min 或 15min 平均值，则负载正在增加, 如果 1min 平均值低于 5min 或 15min 平均值，则负载正在减少\n   - 如果它们高于系统 CPU 的数量，那么系统很可能会遇到性能问题（视情况而定）\n   - 当 load averages 首次出现在 Linux 中时，就像其他操作系统一样，它们反映了 CPU 的需求。但后来在 Linux 上将它们改为不仅包括 runnable 任务，还包括处于不间断状态的任务（TASK_UNINTERRUPTIBLE 或 nr_uninterruptible）\n   - 这证实了故意改变 load averages 以反映对其他系统资源的需求，而不仅仅针对于 CPU 资源。Linux 从 “CPU load averages” 变为 “system load averages”。\n\n1. dmesg -T | tail ------> `kernal erros`\n1. vmstat 1 ------> `overall stats by time`\n1. mpstat -P ALL 1 -----> `CPU balance`\n   - `sudo apt install sysstat`\n   - 可以查看多核心 cpu 中每个计算核心的统计数据；\n   - Report CPU statistics.\n1. pidstat 1 -----> `process usage`\n   - 可以看进程分配在哪一个 cpu 核上\n1. iostat -xz 1 -----> `iostat -xz 1`\n1. free -m -----> `memory usage`\n   - man free 查看各个字段含义\n   - cat /proc/meminfo 有更详细内存占用\n1. sar -n DEV 1 -----> `network I/O`\n1. sar -n TCP,ETCP 1 -----> `TCP stats`\n1. top -----> `check overview`\n\n## [simd library](https://www.reddit.com/r/cpp/comments/106ivke/simd_intrinsics_and_the_possibility_of_a_standard/)\n\n1. [highway cpu 向量加速库 SIMD](https://github.com/google/highway)\n   - [获取支持的加速硬件](https://github.com/google/highway/blob/f86369577e9f884f9156dddbf03ee786493c67f9/hwy/examples/benchmark.cc#LL245C5-L245C31)\n   - `cat /proc/cpuinfo` 看 flags\n1. [eve](https://github.com/jfalcou/eve)\n1. [xsimd](https://github.com/xtensor-stack/xsimd)\n1. [图像处理与机器学习 Simd](https://github.com/ermig1979/Simd)\n1. [parallelism TS v2](https://en.cppreference.com/w/cpp/experimental/simd)\n1. [simde](https://github.com/simd-everywhere/simde)\n\n## 性能基准测试\n\n1. [spec](https://www.spec.org/) 需要 ios 文件\n1. [SpecCPU2017 测试 cpu 性能](https://www.cnblogs.com/xiaoqi-home/p/15981359.html)\n\n## [sysstat](https://github.com/sysstat/sysstat)\n\n## [top](https://www.jianshu.com/p/af584c5a79f2)\n\n1. man top\n1. ? help\n1. RES=CODE+DATA 进程使用的、未被换出的物理内存大小\n1. 内存主要关注物理内存, 虚拟内存表示程序文件及链接库大小，并不是所有的都加载到内存\n1. used mem = total - free - buffers - cache （man free 可以看计算公式),\n1. 主要看 `avaliable` < free + buff/cache\n1. 纳入内核管理的内存不见得都在使用中，还包括过去使用过的现在可以被重复利用的内存，内核并不把这些可被重新使用的内存交还到 free 中去，因此在 linux 上 free 内存会越来越少。\n1. %MEM -- Memory Usage (RES) A task's currently used share of available physical memory. 当前使用的物理内存 / 总的物理内存\n\n## proc 获取信息\n\n1. `/proc/[PID]/stat`\n\n## Basic\n\n1. `Autotuning`（自动调优）在计算机科学中是一种优化技术，用于自动找到最佳的软件配置或参数设置，以提高应用程序的性能或资源使用效率。这种方法在多种计算领域中都有应用，尤其在高性能计算（HPC）、数据库管理、机器学习和深度学习等领域中非常重要。Autotuning 通常涉及一系列的探索和评估过程，可能包括启发式搜索、贝叶斯优化、进化算法等策略。这些过程旨在自动探索大量可能的配置组合，并评估它们在特定应用和硬件环境中的性能，以找到最佳配置。\n   - Autotuning 可以自动搜索最优的算法参数或配置设置，以最大化应用程序的性能。例如，在高性能计算中，它可以确定最佳的并行化策略、内存分配和计算资源使用方式。\n   - 不同的硬件平台（如 CPU、GPU、TPU）有不同的性能特点和优化要求。Autotuning 能够根据目标平台的特性自动调整算法和参数，以实现最佳性能。\n   - 手动调优通常需要深厚的专业知识和大量的时间。Autotuning 通过自动化这一过程，降低了对专业知识的依赖，同时提高了效率。\n   - 在机器学习和深度学习中，autotuning 可用于自动寻找最优的模型超参数，如学习率、网络层数、批量大小等。\n   - 在数据库系统中，autotuning 可以帮助自动优化查询计划、索引管理和内存分配等，以提高查询效率和数据处理速度。\n1. `tiling`（瓦片化）通常指的是一种优化技术，用于提高数据处理效率，特别是在高性能计算和图形处理领域。这种技术的核心思想是将大的数据集或计算任务分割成更小的、更易于管理和处理的单元或“瓦片”（tiles）。分治思想\n   - 在高性能计算和深度学习中，tiling 被用于优化矩阵运算，特别是在使用 GPU 或其他并行计算设备时。通过将大矩阵分割成更小的块，可以更有效地利用缓存，减少内存访问的延迟和带宽需求。这样可以提高数据的局部性，从而提升计算效率。\n   - 在内存密集型应用中，tiling 可以减少对大块连续内存的需求。这种方式有助于减少内存碎片化，提高内存利用率。\n   - 在数据传输密集型操作中（如从 CPU 到 GPU 的数据传输），tiling 可以减少每次传输的数据量，从而降低延迟和带宽需求。\n1. 估计模型运行帧率时需考虑： 内存耗时和计算耗时； 要搬运的内存和内存读写速度可以算出一帧的耗时\n1. `性能分析` 和 `性能优化` 两大部分。\n1. eigen 矩阵可以向量化，Eigen::Map 问 chatgpt\n1. 矩阵乘法是计算密集型运算(会变)。为取得良好的 CPU 性能，有两个重要的优化： [link](https://tvm.hyper.ai/docs/tutorial/tensor_expr#%E7%A4%BA%E4%BE%8B-2%E4%BD%BF%E7%94%A8-te-%E6%89%8B%E5%8A%A8%E4%BC%98%E5%8C%96%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95)\n   - `提高内存访问的缓存命中率`。高缓存命中率可以加速复杂的数值计算和热点内存访问。这需要将原始内存访问模式转换为适合缓存策略的模式。\n   - `SIMD（单指令多数据）`，又称向量处理单元。在每个循环中，SIMD 可以处理一小批数据，而不是处理单个值。这需要将循环体中的数据访问模式转换为统一模式，以便 LLVM 后端可将其降低到 SIMD。\n1. `效率 = 有效量 / 理论量 = 有效量 / (理论峰值量 * 时间) = 有效量 * fps / 理论峰值量`\n1. op 计算效率：根据 op 算法算出计算量 C（可用 highway 函数统计 cycle)， 测出实际耗时 t, 已知理论峰值算力 O/s; `计算效率 = 计算量 / 理论计算量(峰值算力 * 实际耗时) = C / (O * t) = C * fps / O` ？\n1. op io 效率 `io效率 = io量 / 理论io量(峰值IO * 实际耗时)`\n1. 模型算力利用率： `利用率 = 模型计算量 / 理论计算量(峰值算力 * 实际耗时) = 模型计算量 * fps / 峰值算力`\n1. 提高 cache 命中率软件方法：\n   - 连续访存\n   - 分块\n   - 循环置换\n1. 向量处理器可以设置步幅，访问步幅确定的非连续内存也有比较高的性能\n1. 在栈上申请内存比堆快, 不是运算快 [link](https://zhuanlan.zhihu.com/p/481687008);\n1. MIPS: 每秒执行了多少百万条指令。DMIPS：D 是 Dhrystone 的缩写，在 MIPS 前面加上 Dhrystone （整数运算），用于测整数计算能力。进程占用 DMIPS = 总的 DMIPS \\* 进程占用 CPU 百分比\n   - [ARM 各内核系列整型运算能力对比---DMIPS / MHz](https://blog.csdn.net/qq_21475601/article/details/106564903)\n   - A55 单核算力： 2.7DMIPS/MHz\\*1.2GHz=3.24 KDMIPS\n   - 评估算力： 某个平台 top（隔几秒统计一次）看各进程 cpu 占比(max, mean)，根据 cpu 占比计算进程占 DMIPS， 所有进程加起来看是否超过另外一个平台总算力\n1. 函数计算量评估方法：\n   1. 假设平台的最大计算量是 M，函数运行时的 CPU 占用率是 p%，函数的计算需求 C 可以大致估计为 C = M \\* p%。 然后通过函数的帧率 f 来估计每帧的计算需求：C_frame = C / f。\n1. ![cpu gpu npu算力单位](https://mmbiz.qpic.cn/sz_mmbiz_png/2icOarNW84W7BnJpVvdnUeOgJYHibeWJbd4z0KJu556ykzgnjl7MHm5YWyjWDqR7eMPmXIWSWCFy8KG3dJQD8O8A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n1. [抛弃 ARM，高通第五代汽车芯片](https://mp.weixin.qq.com/s/uTp9DXsG0pzTbyux1JgHMA)\n1. 芯片指令解码位宽对 IPC 有直接影响\n1. 提高性能方法： 充分利用并行， 局部性原理\n\n1. 并行加速：\n\n   - `数据级并行（DLP）`，它的出现是因为可以同时操作许多数据项。\n\n     - `指令级并行`: 在编译器的帮助下，利用流水线之类的思想适度开发数据级并行，利用推理执行之类的思想以中等水平开发数据级并行。\n     - `SIMD`： 向量体系结构和图形处理器（GPU）将单条指令并行应用于一个数据集，以开发数据级并行。\n\n   - `任务级并行（TLP）`，它的出现是因为创建了一些能够单独处理但大量采用并行方式执行的工作任务。\n\n     - `线程并行`：线程级井行在一种紧耦合硬件模型中开发数据级并行或任务级并行，这种模型允许在并行线程之间进行交互。\n     - `请求级并行`: 在程序员或操作系统指定的大量去耦合任务之间开发并行。\n\n1. 局部性原理\n\n   - 时间局部性：最近访问过的内容很可能会在短期内被再次访问\n   - 空间局部性：地址相互临近的指令或数据很可能会在短时间内被用到\n\n1. 加速比(归一化)： 原执行时间 / 新执行时间 = 1 / ( (1 - 升级比例) + (升级比例/升级加速比))；新执行时间 = 不能加速部分 + 加速后部分 = 不能加速部分 + 加速部分/加速部分加速比\n1. Amdahl 定律：根据加速比， 可以得到\n\n   - 如果仅改进一部分计算的性能，在增加改进是，所获得的加速比增量会逐渐减小\n   - 加速比有最大值： 1 / ( 1 - 升级比例)\n\n1. 感知系统优化方案（从系统上分析)：\n\n   - 整体 pipeline 调度优化: 线程优先级， 各模块多线程，异步\n   - 整体数据流优化： 合并前处理操作， rgba 在 rgb 转 nchw 时来做， memory_pool, zero copy\n   - 模型+后处理联合优化：sigmoid 在后处理做, 反算阈值，sigmoid 之前先过滤， 优化 cpu 热点代码\n   - cuda 算子， neon 加速\n\n1. 关注数据流，具体哪一步耗时\n\n1. data packing:\n   - 注意 struct 成员变量顺序，尽量减少 padding\n1. 访存优化：\n\n   - 连续访问\n   - 不要重复访问（可以先用临时变量存下来)\n\n1. 软硬算法联合优化\n1. 编译器：\n\n   - 打开编译器优化报告\n   - 编译器参数：-O3(启动与机器无关的优化功能) -march=armv8.2-a(启动针对特定 CPU 系列的优化功能) -flto(启动过程间优化功能)\n   - gcc -fopt-info 输出优化报告\n\n1. 技术栈:\n\n   - 第一个就是编译器，你可能不需要更深入理解编译器的具体的原理，但是你要了解编译器通用的编译优化手段，以及它有比较通用的一些编译优化的选项。\n   - 第二个就是 OS 的一个调度，还有一个可能 CPU 绑核。在手机上的话，绑核还是很明显的，如果是在小核上和大核，要是中核，它们也差距很多。硬件的限制。如果你想你的任务要跑特别快，比如假如一个特别重要的前台功能，你需要把你的主线程一个界面相关的线程可能就要绑定到大核上，让他跑这么快。\n   - 第三个在硬点上，我可能我们要比较了解 CPU 的微架构是什么样的， CPU 微架构什么样的，为什么我的代码跑的时候它就慢，慢又拆解为几类，怎么去分析它。第二个你要去可能要去尝试的去学习，怎么去读或者改这些汇编的一些指令。\n\n1. 系统级性能优化通常包括两个阶段：性能剖析（performance profiling）和代码优化。\n1. [性能静态分析（定量）](https://zhuanlan.zhihu.com/p/419859575)\n\n   - [Parallelware Analyzer](https://www.jianshu.com/p/a0296d5b91bc)\n\n1. [性能指标](https://zhuanlan.zhihu.com/p/343408130)\n   - latency(延迟或响应时间)\n   - throughput()\n   - 资源使用率 (资源：处理器，内存，IO)\n     - cpu(不大于 75%)\n     - 内存(不大于 80%)\n     - IO(磁盘, 网络)\n   - 错误率\n1. latency 和 throughput (延迟和吞吐量)\n   - throughput(速度？)：吞吐量一般指相当一段时间内测量出来的系统单位时间处理的任务数或事务数。如：1s 处理 10 帧图片 (10FPS)\n   - latency: 执行一次任务需要的时间。如：处理一帧图片耗时 10ms\n   - latency 最大值：每个节点最大耗时叠加在一起，就是整体 pipeline 的 latency 最大值，即一帧最大需要多少时间输出结果，这个 latency 值尽量越小越好；\n   - 目标：低延迟，高吞吐\n   - 提高吞吐：提升 node 处理速度或异步流水线， 异步流水可能会增加延迟（较好提升, 有两种方法)\n   - 降低延迟：提升 node 处理速度， 优化关键点，也会增加吞吐(容易遇见瓶颈)\n   - 提升 node 处理速度能够同时提高吞吐和降低延迟。\n1. 流水线深度(硬件执行单元个数)，buffer 数量(处理多少帧)关系\n\n   - 如果完全流水起来， buffer size 不小于流水线深度，流水线帧率由耗时最大的 node 决定: 1 / max(node time)\n   - (异步处理比较重要 queue size)\n   - node 之间相互不影响，buffer size 大于流水线深度通常作用不大， 同一时刻所有 node 只能处理流水线深度的 buffer，但如果 node 之间相互影响，不好推测 buffer size 大小。\n   - node 之间相互影响：buffer size 不影响其他功能越大越好\n   - [流水线深度](https://zhuanlan.zhihu.com/p/66933636)\n\n1. 提升 node 处理速度方法：\n   - 减少运算量(降低算法复杂度)\n   - 加速库(如：硬件加速) 或更换更强硬件\n   - 多线程\n   - 减少资源申请与释放(如 memory pool, thread pool)\n   - 减小内存拷贝(如：零拷贝)\n   - 内存使用优化：如模型多个输出，申请一次大内存，偏移量来寻址。\n   - 空间换时间(如查表法)\n   - 语言特性：内联函数\n   - 提高 cache 命中率(如：矩阵变数组，连续访问会提高 cache 命中率)\n   - 改变运算方法(如 sigmoid 后再做处理，减少指数运算)\n1. 异步方法(流水线)\n   - 多线程\n   - 增加 buffer 数\n   - 调度优化\n   - 多流水线(pingpang)\n1. [定位和优化程序 CPU、内存、IO 瓶颈](https://bbs.huaweicloud.com/blogs/detail/222032)\n   - 两个方向：`提⾼运⾏速度 + 减少计算量`\n   - 先建立测试`baseline`\n   - 瓶颈点可能有多个，如果不解决最狭窄的瓶颈点\n1. 调研性能瓶颈，如何对系统性能进行分析。\n1. 单例 perf table\n1. 计算密集型\n   - 减少不必要计算\n1. 访存密集型\n   - 提高 cache 命中率\n   - 减少不必要访存\n1. IO 密集型\n\n## 理论性能评估\n\n1. (假设串行执行)理论 GPU 使用时间计算：各模型的推理耗时是单模型实测值，GPU 时间片按照各自的设计帧率（如 15Hz (fps)）× 推理耗时得到，是个纯理论计算的值。总和为各部分理论值相加，820 为理论值，这个值如果大于 1000 则系统无法达成。\n1. 单模型推理时间 x fps = 模型占用 gpu 时间, 所有模型时间不能超过 1000ms;\n1. 有了各个模型推理时间也可以算理论最大帧率；（未考虑模型对计算资源的抢占，锁）1000 / 所有模型加一起耗时;\n\n## [推理优化](https://www.cnblogs.com/Matrix_Yao/p/13181778.html)\n\n1. [深度模型推理加速的术与道](https://www.6aiq.com/article/1662281420188)\n1. 减少 sigmoid 的使用(非常耗时)，可以放到后处理来做，根据 sigmoid 公式， 反算阈值，过滤之后再进行计算\n\n### 算子融合\n\n1. 算子融合基于对深度学习拓扑结构模式的观察。深度学习算子按其对资源的需求可以分为两类：\n\n   - 计算密集型算子，这些算子的时间绝大部分花在计算上，如卷积、全连接等。\n   - 访存密集型算子，这些算子的时间绝大部分花在访存上，他们大部分是 element-wise 算子， ReLU，eltment-wise sum。\n\n## cpu time and off-cpu time\n\n1. [profiling 与性能优化总结](https://zhuanlan.zhihu.com/p/362575905)\n\n## 性能调优方法\n\n### PMC(Performance Monitoring Counters)\n\n### [循环展开](https://zhuanlan.zhihu.com/p/553771789)\n\n1. 计算能力（GFlops）和访存速度（MB/s）的矛盾，在汇编语言中就是取指令（load store）和计算（ALU）的矛盾。访存和计算天然是两种独立的资源，两者可以独立运行互不影响。\n1. 当循环次数变少，循环体增多的时候，`CPU在执行循环体的过程中就有了更大的操作空间进行指令调度(inline也有这个作用)`，在计算 a[i]的时候就可以去调度 a[i+1]相关的指令（取数据）和 a[i-1]相关的指令（存数据），因此 CPU 流水线更满载，也就提高了性能。\n1. 循环展开可以减少分支预测的次数\n1. 一般不需要手动优化，-O2 会自动展开， -O1 不会\n1. perf annotate 可以看出循环有开销(判断，跳转或 index 操作)， cpu 流水线并行性低\n\n### 分支预测\n\n1. 推理：在还不确切知道是否真的需要某一条指令时，就先执行改指令。如果预测错误，就需要清理流水线。\n\n### 预取\n\n1. 数据和指令预取\n1. 硬件和软件预取\n\n### [IPC (instruction per cycle)](https://zhuanlan.zhihu.com/p/385519404)\n\n1. `tiptop`\n1. `perf stat` 可以查看\n1. 当比较不同的指令集时，与使用相同芯片技术实现更复杂的指令集相比，更简单的指令集可能会导致更高的 IPC 数字。但是，更复杂的指令集可能能够以更少的指令实现更多有用的工作。\n1. 确定： `计算密集型或访存密集型`\n1. 如果 IPC < 1.0, 很可能是 Memory stall 占主导，可从软件和硬件两个方面考虑这个问题。软件方面：减少不必要的访存操作，提升 cache 命中率，尽量访问本地节点内存；硬件方面：增加 cache 容量，加快访存速度，提升总线带宽。\n   如果 IPC > 1.0, 很可能是计算密集型的程序。可以试图减少执行指令的数量：消除不必要的工作。火焰图 CPU flame graphs，非常适用于分析这类问题。硬件方面：尝试超频、使用更多的 core 或 hyperthread。作者根据 PMU 相关的工作经验，设定了 1.0 这个阈值，用于区分访存密集型(memory-bound)和计算密集型(cpu-bound)程序。读者可以根据自己的实际工作平台，合理调整这个阈值。\n1. [cpu-utilization-is-wrong](https://www.brendangregg.com/blog/2017-05-09/cpu-utilization-is-wrong.html)\n1. [CPU 最高性能预估之“理论最大 IPC”](https://zhuanlan.zhihu.com/p/364661188?utm_source=zhihu&utm_medium=social&utm_oi=26748628500480)\n1. [IPC 测试，可以预估最大 IPC](https://zhuanlan.zhihu.com/p/385519404)\n1. ipc 是指每个 core 的 IPC\n1. [用 CPI 火焰图分析 Linux 性能问题](https://cloud.tencent.com/developer/article/1517979) 使用 CPI 火焰图，可以很方便地做 CPU 利用率的分析，找到和定位引发 CPU 停顿的函数。一旦找到相关的函数，就可以通过 perf annotate 命令对引起停顿的指令作出进一步确认。\n\n### CPI(Cycle Per Instruction)\n\n### 绑核\n\n1. taskset\n1. `taskset -c 3,5 firefox`\n1. taskset 超出核心数程序可能更快， why?\n\n### memory bound\n\n### SSE（Streaming SIMD Extensions）\n\n### SVE(Scalable Vector Extension)\n\n1. 是 arm AArch64 架构下的下一代 SIMD 指令集，旨在加速高性能计算，SVE 引入了很多新的架构特点\n\n### 多发射处理器\n\n1. 多发射处理器相对于向量处理器的潜在优势在于它们能够从结构化程度较低的代码中提取某些并行，以及能够很轻松地缓存所有形式的数据。因为这些原因，`多发射方法已经成为利用指令级并行的主要方法`，而向量主要作为这些处理器的扩展。\n1. 目标就是允许在一个时钟周期中发射多条指令。多发射处理器主要有以下 3 类。\n\n#### 静态调度超标量处理器\n\n1. 由于静态调度超标量的收益会随着发射宽度的增长而逐渐减少，所以静态调度超标量主要用于发射宽度较窄的情况，通常仅有两条指令\n1. 大多属于嵌入式领域， MiPS 和 ARM ，包括 ARM Cortex-A8\n\n#### VLIW( 超长指令字)处理器\n\n1. [计算机指令集架构 – 超长指令字（VLIW）](https://zhuanlan.zhihu.com/p/337749676)\n1. 大多数属于信号处理领域，比如 TI C6x\n\n#### 动态调度超标量处理器\n\n1. Intel Core i3 、i5 ， i7. AMD Phenom, IBM Power7\n\n### cache\n\n1. LLC: last level cache 最后一级缓存，比较关键\n1. cache locality 表示程序对缓存中数据的访问 locality 和重用情况。如果同一个 warp 中的线程迭代访问同一缓存线(cache line)上的数据(比如遍历一个数组),那么可以最大化利用 cache,称之为良好的 cache locality。\n\n### 提高 cache 性能\n\n处理器的缓存（通常是 L1、L2 和 L3）旨在减少从主内存访问数据所需的时间。它们存储了最近或经常访问的数据副本，从而提高了数据访问速度。为了充分利用缓存并提高程序性能，你可以遵循以下一些方法和建议：\n\n1. **局部性原理**：\n   - **时间局部性**：如果某个数据项被访问，那么在近期它可能再次被访问。\n   - **空间局部性**：如果某个数据项被访问，那么它附近的数据项也可能会很快被访问。\n     为了利用这种局部性，尝试在连续的内存地址上组织和访问数据（例如，使用数组而不是链表）。\n2. **避免伪共享**：确保并发运行的线程访问的数据不位于同一缓存行上，因为这可能会导致缓存无效并减慢性能。\n3. **数据对齐**：确保数据结构按照缓存行大小进行对齐，这可以减少跨多个缓存行的数据访问。\n4. **预取策略**：如果可能，尝试预测下一次需要的数据并预先加载它。一些编译器和硬件支持数据预取指令。\n5. **减少缓存失效**：\n   - 将常用的数据放在一起。\n   - 避免大的数据结构，它们可能会频繁地导致缓存失效。\n   - 在可能的情况下，使用更小的数据类型。\n6. **循环分块/循环重排**：这是一种技术，可以将大的循环重组为更小的块，从而更好地适应缓存。这尤其适用于多层嵌套的循环。\n7. **避免不必要的写回操作**：只在数据确实更改时写入，以减少不必要的写回到主内存。\n8. **使用只读和常量数据**：如果数据不需要更改，将其标记为常量可以帮助编译器进行优化。\n9. **控制数据的分配和布局**：例如，在 C++中，你可以使用自定义分配器来控制对象在内存中的位置。\n10. **利用非统一内存访问 (NUMA) 架构**：在多处理器系统上，确保线程尽可能在与其数据最近的处理器上运行。\n11. **监控和剖析**：使用工具（如`perf`、`VTune`等）来分析你的程序，找出缓存失效的热点，并对它们进行优化。\n\n#### [cache line](https://plantegg.github.io/2021/05/16/CPU_Cache_Line%E5%92%8C%E6%80%A7%E8%83%BD/)\n\n1. cpu 一般 size 64 byte (512bit)\n1. `getconf -a | grep CACHE` 查看各级 cache line size\n1. `lscpu -C`\n1. `likwid-topology`显示有关硬件拓扑的详细信息，例如 CPU、缓存、NUMA 域等。\n\n#### cache 一致性\n\n1. [在线体验 MESI 协议状态转换](https://www.scss.tcd.ie/Jeremy.Jones/VivioJS/caches/MESIHelp.htm)\n1. [cache 相关动画](https://www.scss.tcd.ie/Jeremy.Jones/VivioJS/caches/MESIHelp.htm)\n\n#### cache miss\n\n1. 应用程序的读占比越高，对缓存越友好；访问的字长越大对预取越友好；同等数量的指令内存依赖越低，CPI 会越高。\n1. `sudo perf stat -d -d -d -a -- sleep 10`\n1. `sudo perf stat -e cache-misses ls`\n1. `sudo cachestat-bpfcc 2`\n1. `sudo cachetop-bpfcc`\n\n#### prefetch\n\n1. 预取是一种优化技术，它允许 CPU 提前加载将要访问的数据到缓存中，以便在实际访问数据时可以更快地获得它。预取可以减少由于缓存未命中引起的等待时间。减少指令延迟\n1. 硬件预取：现代 CPU 已经有自己的硬件预取策略。在某些情况下，手动的预取可能会与硬件的预取策略发生冲突，导致性能下降。\n1. ` __builtin_prefetch(&data[i + 16], 0, 1);  // 预取下一个将要访问的数据`\n\n#### 伪共享\n\n1. 伪共享发生在以下情况下：\n   - 两个或多个处理器核心在它们各自的本地缓存中读/写不同的变量。\n   - 这些变量恰好位于同一缓存行中。\n1. 避免：\n   - 数据对齐：确保经常由不同线程访问的数据在不同的缓存行上。许多编译器和平台都提供数据对齐指令或属性。\n   - 填充：在数据结构中添加填充，使每个线程访问的数据元素都有一个完整的缓存行。\n   - 局部变量：尽量使用线程的局部变量，因为它们通常存储在栈上，并且不太可能与其他线程的变量共享缓存行。\n   - 避免细粒度并行：如果任务太小，线程间的同步和伪共享可能会抵消并行处理的好处。\n1. sample, 避免伪共享更快\n\n```c++\n#include <chrono>\n#include <iostream>\n#include <thread>\n\nusing Clock = std::chrono::steady_clock;\n\n#define __TIC__(tag) auto __##tag##_start_time = Clock::now();\n\n#define __TOC__(tag)                                                           \\\n  auto __##tag##_end_time = Clock::now();                                      \\\n  std::cout << #tag << \" : \"                                                   \\\n            << std::chrono::duration_cast<std::chrono::microseconds>(          \\\n                   __##tag##_end_time - __##tag##_start_time)                  \\\n                   .count()                                                    \\\n            << \"us\" << std::endl;\n#define ITERATIONS 1000000000\n\n// 伪共享情况\nstruct SharedData {\n  long value1;\n  long value2;\n};\n\n// 避免伪共享的情况\nstruct PaddedData {\n  long value1;\n  char padding[64]; // 通常缓存行的大小为64字节\n  long value2;\n};\n\nvoid updateValue1(SharedData *data) {\n  for (long i = 0; i < ITERATIONS; i++) {\n    data->value1 += i;\n  }\n}\n\nvoid updateValue2(SharedData *data) {\n  for (long i = 0; i < ITERATIONS; i++) {\n    data->value2 += i;\n  }\n}\n\nint main() {\n  SharedData data;\n  __TIC__(TEST)\n  std::thread t1(updateValue1, &data);\n  std::thread t2(updateValue2, &data);\n  t1.join();\n  t2.join();\n  __TOC__(TEST)\n\n  PaddedData paddedData;\n  __TIC__(TEST_WITH_PADDING)\n  std::thread t3([&paddedData] {\n    for (long i = 0; i < ITERATIONS; i++) {\n      paddedData.value1 += i;\n    }\n  });\n  std::thread t4([&paddedData] {\n    for (long i = 0; i < ITERATIONS; i++) {\n      paddedData.value2 += i;\n    }\n  });\n  t3.join();\n  t4.join();\n  __TOC__(TEST_WITH_PADDING)\n\n  return 0;\n}\n```\n\n## Notes\n\n1. Release 模式编译\n1. 看 cpu 使用率 log 重定向到/dev/null。 `> /dev/null 2>&1`\n1. log 重定向到文件, 不要打印到 stdout\n   - `> output.log 2>&1` this will redirect both STDOUT and STDERR to the file.\n   - `2>&1 > output.log` the old STDOUT will be saved (copied) in STDERR and then STDOUT will be redirected to file. So, stdout will go to file and stderr will go to console.\n   - `2>&1 | tee output.log` both streams will be redirected to tee. Tee will duplicate any input to its stdout (the console in your case) and to file (output.log).\n1. GPU 函数耗时统计不能只记录一次的，GPU 可能做一些准备工作，教训： nppiResize_8u_C3R 不管大小第一次运行耗时都很大 The cuda context is lazily initialized\n1. -O3 测试\n1. perf stat ./xxx 测试程序运行时间\n\n## Tools\n\n### [arm map](https://developer.arm.com/documentation/101136/22-1-3/MAP/Performance-analysis-with-Caliper-instrumentation/Analyze-your-program)\n\n### [palanteer](https://github.com/dfeneyrou/palanteer)\n\n1. [c++ doc](https://dfeneyrou.github.io/palanteer/index.html#)\n1. [getting_started](https://dfeneyrou.github.io/palanteer/getting_started.md.html)\n\n### [njmon](https://nmon.sourceforge.io/pmwiki.php?n=Site.Njmon)\n\n1. 注意可以看视频教程\n1. [njmon-intro-and-update](https://www.ibm.com/support/pages/njmon-intro-and-update)\n1. `wget http://sourceforge.net/projects/nmon/files/njmon_linux_v83.zip` tar and make\n1. [njmon-grafana-dashboard-json](https://github.com/chhanz/njmon-grafana-dashboard-json)\n\n### [nmon](https://nmon.sourceforge.io/pmwiki.php?n=Main.HomePage)\n\n1. [nmon2influxdb](https://github.com/adejoux/nmon2influxdb)\n1. Nigel's Monitor(nmon)\n1. 可以记录 cpu, memory， io 等资源，用 nmonchart 显示图表\n1. 建议您将快照数量保持在 800 - 1000 左右，以避免文件过大出现问题。\n1. nmon -f -s 10 -c 360 (snapshot the statistics every 10 seconds for an hour)\n1. (very good)nmon -fT -s 5 -c 720 非常好用，top 信息, html 中显示 top timeline, execl 中显示 top summary，包括 cpu、memory 各进程使用情况\n   - 包含各进程 cpu mem 使用信息\n1. -a 包含 gpu 信息\n1. [nmonchart](https://nmon.sourceforge.io/pmwiki.php?n=Site.Nmonchart)\n   - sampleD 可以看详细信息\n   - 下方 download 下载\n   - `wget -c http://sourceforge.net/projects/nmon/files/nmonchart42.tar`\n   - sudo apt install ksh\n   - ./nmonchart \\*.nom 生成 html\n   - JFS 可以显示文件系统使用百分比历史\n   - cpu util 中 idle + wait + sys + user = 100%\n   - top command 中与 cpu util 表示方法不同，top command 不叠加，cpu util 叠加\n1. [Nmon-Analyser](https://nmon.sourceforge.io/pmwiki.php?n=Site.Nmon-Analyser)\n   - 下方下载，注意是 excel，需要解除宏限制 [link](https://support.microsoft.com/zh-cn/topic/%E5%B7%B2%E9%98%BB%E6%AD%A2%E6%9C%89%E6%BD%9C%E5%9C%A8%E5%8D%B1%E9%99%A9%E7%9A%84%E5%AE%8F-0952faa0-37e7-4316-b61d-5b5ed6024216)\n   - 使用 excel 生成各种图表，包含各个 cpu 和 cpu_all 信息\n1. [easyNmon](https://github.com/mzky/easyNmon?tab=readme-ov-file)\n   - 对 nmon 的扩展\n1. [nmonvisualizer](https://github.com/nmonvisualizer/nmonvisualizer)\n\n### [top-analyzer](https://gceasy.io/yc-top-analyzer.jsp)\n\n### [netdata](https://github.com/netdata/netdata)\n\n### ui\n\n1. [Linux perf Profiler UIs](https://www.markhansen.co.nz/profiler-uis/)\n\n### [firefox-devtools/profiler](https://github.com/firefox-devtools/profiler)\n\n1. 可以打开 perf script 之后的数据，类似 flamegraph\n1. [live web](https://profiler.firefox.com/)\n1. 点击上方线程对应的图片，可以显示具体函数调用，注意会高亮所有相同调用的部分\n\n### [tracy](https://github.com/wolfpld/tracy)\n\n### [uftrace](https://github.com/namhyung/uftrace)\n\n1. uftrace record ./test\n1. uftrace report\n1. uftrace info\n1. uftrace dump --flame-graph > test.fold\n1. flamegraph.pl test.fold > test.svg\n\n### [orbit](https://github.com/google/orbit)\n\n1. Orbit is a CPU profiler that enables you to identify CPU-related performance issues in your application. It allows you to take system traces and detailed CPU performance data using callstack sampling, dynamic instrumentation, memory tracing and GPU driver tracing (AMD only).\n\n### [likwid](https://github.com/RRZE-HPC/likwid.git)\n\n1. `sudo apt install likwid`\n1. [AddARMSupport](https://github.com/RRZE-HPC/likwid/wiki/AddARMSupport)\n1. `likwid-topology -g`\n1. `likwid-topology -V 3`打开 debug 信息\n1. [各平台各性能指标计算方法](https://github.com/RRZE-HPC/likwid/tree/master/groups) 需要配合[event](https://github.com/RRZE-HPC/likwid/wiki/ARM-A57)\n1. `likwid-perfctr -e` 查看 event\n1. `likwid-perfctr -a`查看支持的 group\n1. [userguide](https://docs.nersc.gov/tools/performance/likwid/)\n1. [tutorial1](https://blog.rwth-aachen.de/hpc_import_20210107/attachments/20056127/24117298.pdf)\n1. [tutorial](https://www.vi-hps.org/cms/upload/material/tw09/vi-hps-tw09-VI-HPS_likwid.pdf)\n1. 源码编译时需要修改 config.mk 的 COMPILER 字段; For ARM builds, the COMPILER flag in config.mk needs to changed to GCCARMv8 or ARMCLANG (experimental).\n1. `likwid-topology`显示有关硬件拓扑的详细信息，例如 CPU、缓存、NUMA 域等。\n1. `likwid-bench`可以算 roofline; likwid-bench: Micro benchmarking platform for CPU architectures\n1. `likwid-perfscope`: Frontend to the timeline mode of likwid-perfctr, plots live graphs of performance metrics using gnuplot\n1. [Tutorial Empirical-Roofline-Model](https://github.com/RRZE-HPC/likwid/wiki/Tutorial%3A-Empirical-Roofline-Model)\n   - `sudo apt intall gnuplot`\n   - `gnuplot plotscript.gp`\n1. `cat /proc/cpuinfo` part 不一定支持， topology.h 中查看\n\n### [gprof](http://www.tastones.com/stackoverflow/c++/profiling/profiling_with_gcc_and_gprof/)\n\n1. [gprof2dot](https://github.com/jrfonseca/gprof2dot)\n1. [link](https://jasonblog.github.io/note/ncku_embededd/tong_xue_li_yong_gprof2dot_gong_ju_ff0c_jiang_zui_.html)\n\n```\n- g++ -pg ... -o xxx (Note: do not use -g)\n- ./xxx\n- gprof -b ./xxx gmon.out > xxx.log\n-  gprof ./xxx gmon.out | less\n\nsudo pip install gprof2dot\ngprof ./xxx | gprof2dot | dot -Tpng -o report.png\ngprof ./xxx | gprof2dot | dot -Tsvg -o report.svg\n\nSET(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -pg\")\nSET(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -pg\")\nSET(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -pg\")\n```\n\n### google-perftools\n\n```\n- g++ -O3 -o test test.cc -lprofiler\n- LD_PRELOAD=/usr/local/lib/libprofiler.so CPUPROFILE=test.prof ./test\n- google-pprof ./test test.prof --svg > test.svg\n```\n\n### valgrind\n\n1. [valgrind + massif-visualizer 看内存占用](https://www.zhihu.com/question/31382177)\n\n#### cachegrind\n\n1. cachegrind：分析 CPU 的 cache 命中率、丢失率，用于进行代码优化。\n1. `valgrind --tool=cachegrind --branch-sim=yes ls`\n1. `cg_annotatel(callgrind_annotate) cachegrind.out.88431` 各模块 cache 统计\n1. `cg_annotatel(callgrind_annotate) cachegrind.out.88431 file` 统计 file 每一行 cache 情况, 可以看出 O3 比 O1 cache 优化了\n1. [qcachegrindwin 分析生成文件](https://jaist.dl.sourceforge.net/project/qcachegrindwin/0.7.4/)\n1. Kcachegrind GUI 显示分析生成的文件\n\n#### [callgrind](https://learnku.com/articles/46663)\n\n1. callgrind\n\n```sh\nvalgrind –tool=callgrind ./tmp\ncallgrind_annotate callgrind.out.XXX\ngprof2dot.py -f callgrind callgrind.out.XXX |dot -Tpng -o report.png\n```\n\n### [pmu-tools](https://github.com/andikleen/pmu-tools)\n\n### [(good)pprof](https://github.com/google/pprof)\n\n1. [perf_data_converter](https://github.com/google/perf_data_converter)\n1. `-g`和`sudo`可能有问题\n\n```sh\nsudo perf record -F 99  -g ./bandwidth64\npprof -svg perf.data\n```\n\n1. perf 结果可视化\n1. [readme](https://github.com/google/pprof/blob/main/doc/README.md)\n1. 与 google-pprof 不是一个工具\n1. [需要编译 perf_data_converter](https://github.com/google/perf_data_converter)\n   - `cp  bazel-bin/src/perf_to_profile ~/go/bin`\n\n### [gprof2dot](https://github.com/jrfonseca/gprof2dot)\n\n1. 可类似 gprof 将 perf 结果生成调用图\n1. `perf script | c++filt | gprof2dot.py -f perf | dot -Tpng -o output.png`\n\n## [simpleperf](https://android.googlesource.com/platform/system/extras/+/master/simpleperf/doc/README.md)\n\n## [perf](https://billtian.github.io/digoal.blog/2016/11/27/01.html)\n\n1. `-vvv`打印调试信息\n1. `perf stat --all-cpus --no-aggr ls`查看每一个 cpu 统计\n1. [使用 perf 进行性能分析时如何获取准确的调用栈](https://gaomf.cn/2019/10/30/perf_stack_traceback/)\n1. [good examples](https://www.brendangregg.com/perf.html)\n1. perf 是性能分析的必备工具, 它最核心的能力是能访问硬件上的 Performance Monitor Unit (PMU), 对分析 CPU bound 的问题很有帮助, 当然 perf 也支持各种软件 event.\n\n### 源码安装\n\n1.\n\n```sh\ngit clone --depth=1 https://github.com/torvalds/linux.git\ncd tools/perf/\nmake -j4\n```\n\n2. [docxgen](https://www.cs.rice.edu/~la5/doc/perf-doc/de/df8/builtin-record_8c.html)\n1. [原理解析](https://github.com/freelancer-leon/notes/blob/master/kernel/profiling/perf.md)\n\n### perf_event\n\n1. `strace perf stat ./test`strace 跟踪`perf_event_open`看 type 和 config\n1. 可 c++编程\n1. 在 Linux perf events 中，确实存在对同时使用的 raw events 的数量限制。这是因为硬件计数器的数量是有限的。当超过硬件性能计数器的数量时，需要使用复用(multiplexing)来统计所有事件。\n1. raw counter 不管是 bash 还是 c++都有个数限制，需要分时复用，有没有方法计算到利用的时间？\n1. `perf stat -vvv ./test`可以看 perf_event_attr 配置\n\n### timechart\n\n1. `sudo perf timechart record` or `sudo perf timechart record sleep 1`\n1. `sudo perf timechart`\n1. 图上进程显示时间表示占用 cpu 时间, 注意有多核\n   - 一个空格 100ms\n1. `/sys/kernel/debug/tracing/events/sched/sched_wakeup` 需要/sys/kernel/debug/tracing 文件夹\n\n### list\n\n1. [arm55 pmu 手册](https://developer.arm.com/documentation/100442/0100/debug-descriptions/pmu/pmu-events?lang=en)\n1. 列出 event\n1. `likwid-perfctr -e` 列出所有 PMC\n1. `likwid-perfctr -e | grep MEM`列出所有内存相关 PMC, `perf stat -e r013,r066,r067` r + mask + eventsel 监控对应事件\n\n```sh\nnvidia@miivii-tegra:~$ likwid-perfctr -e | grep MEM\nMEM_ACCESS, 0x13, 0x0, PMC\nMEMORY_ERROR, 0x1A, 0x0, PMC\nMEM_ACCESS_LD, 0x66, 0x0, PMC\nMEM_ACCESS_ST, 0x67, 0x0, PMC\n\nnvidia@miivii-tegra:~$ perf stat -e r013,r066,r067 -- ls\nDesktop  Documents  Downloads  Music  Pictures  Public  Templates  Videos\n\n Performance counter stats for 'ls':\n\n           156,045      r013:u\n           118,079      r066:u\n            37,966      r067:u\n\n       0.003111539 seconds time elapsed\n\n       0.003294000 seconds user\n       0.000000000 seconds sys\n```\n\n#### record\n\n1. `perf record ./test`\n1. `perf record -e LLC-load-misses` 只记录某个热点，可生成火焰图\n1. perf record 对可以同时记录的事件数量有限制。这个限制由硬件性能监视器计数器（hardware performance monitor counters）决定。\n1. 多进程也不行， 开多个实例记录会不准，flopsc, bandwidth 可以用来验证是否准确\n1. `likwid-perfctr -e`可以查看支持的 PMC 个数\n\n#### report\n\n#### script\n\n1. `perf script -v` -v 查看调试信息\n\n#### [stat](https://zhuanlan.zhihu.com/p/385519404)\n\n1. `perf stat -I 1000 -a sleep 10` -I 1000 每隔 1000ms 输出一次\n1. `CPUs utilized = task_clock / 总时间` task_clock 统计所有核的无意义，肯定是 1; 统计单个程序的才有意义; 可以测量一个程序对 cpu 利用率， sleep 的时间会去掉\n1. `频率 = cpu_cycles / cpu_clock or task_clock time`多核不准\n1. `perf stat -ddd -- ./test` 可以查看 cpu 运行频率，运行 cycle 数，指令数, 程序运行时间\n1. 性能有一些损耗，影响不大（-ddd 打开细节越多损失越大）\n1. `sudo perf stat -a -- sleep 10`\n1. 不计算分布，仅仅进行统计, 可以看总体性能(IPC 等)\n1. `sudo perf stat -d -d -d -a -- sleep 10`\n1. `sudo perf stat -e cache-misses ls`\n1. 可以查看 CPU 真实利用率： CPUs utilized, 不包括等待时间，自旋时间等。真实利用率比 top 查看的利用率要低很多。\n1. top 查看的利用率： 如果 CPU 没有运行在内核的 idle 线程，就认为 CPU 在被使用。当 cpu 阻塞并等待内存访问时，也可能显示较高的利用率。\n\n#### [annotate](https://developer.ibm.com/tutorials/l-analyzing-performance-perf-annotate-trs/)\n\n1. 将热点函数精确到代码行，用于对问题进行精确定位。\n1. perf report 可以选中 sympol, 按快捷键 a\n1. h 显示快捷键\n1. k 显示行号\n1. <tab> Cycle thru hottest instructions\n1. t Circulate percent, total period, samples view\n1. Cross-arch annotate: 需要 vmlinux\n1. [build-kernel](https://ctf-wiki.org/pwn/linux/kernel-mode/environment/build-kernel/)\n1. [build-linux-kernel](https://phoenixnap.com/kb/build-linux-kernel)\n1. [cross arch](https://patchwork.ozlabs.org/project/linuxppc-dev/patch/1474472876-2706-2-git-send-email-ravi.bangoria@linux.vnet.ibm.com/)\n\n#### list\n\n1. 查看支持的 events\n1. `perf stat -e`查看 events\n1. `strace perf stat`strace 跟踪系统调用\n\n#### top\n\n1. 我们知道 perf top 是通过读取 PMU 的 PC 寄存器来获取当前执行的指令进而根据汇编的 symbol 信息获得是执行的哪条指令。\n1. 能够快速的定位热点函数\n1. (只统计栈顶)可以看到具体函数占用 cpu 比例，与火焰图从上往下看，越宽的函数占比越高，被遮盖的函数占比少\n1. lib 不能被 strip， 否则函数名显示地址\n1. perf top -p xxxx\n   - perf top -p 23015,32476 // 查看这两个进程的 cpu cycles 使用情况\n   - perf top // 默认配置\n   - perf top -G // 得到调用关系图\n   - perf top -e cycles // 指定性能事件\n   - perf top -s comm,pid,symbol // 显示调用 symbol 的进程名和进程号\n   - perf top --comms nginx,top // 仅显示属于指定进程的符号\n   - perf top --symbols kfree // 仅显示指定的符号\n\n#### mem\n\n### [perfetto](https://perfetto.dev/docs/)\n\n1. [tracing-101](https://perfetto.dev/docs/tracing-101)\n   - Logging vs tracing vs profiling vs metrics\n   - profiling 使用采样技术，tracing 会跟踪每个细节\n   - Traces cannot feasibly capture execution of extreme high frequency events e.g. every function call. Profiling tools fill this niche: by sampling, they can significantly cut down on how much information they store.\n   - 跟踪无法捕获高频时间， profiling 弥补了空白\n   - Traces are excellent at providing this exact context: application instrumentation and low-level kernel events together provide deep insight into why code was run in the first place.\n   - 跟踪非常适合提供这种精确的上下文：应用程序检测和低级内核事件一起提供了对代码最初运行原因的深入了解。\n   - metrics: why bother with high level metrics at all? Why not instead just use tracing and compute metrics on resulting traces? In some settings, this may indeed be the right approach. In local and lab situations using trace-based metrics, where metrics are computed from traces instead of collecting them directly, is a powerful approach. If a metric regresses, it's easy to open the trace to root cause why that happened.\n   - 指标是跟踪系统随时间变化的性能的数值。通常指标映射到高级概念。指标的示例包括：CPU 使用率、内存使用率、网络带宽等。指标是在程序运行时直接从应用程序或操作系统收集的。\n   - metrics 比 tracing 要轻量级很多\n\n### [hotspot](https://github.com/KDAB/hotspot)\n\n1. the Linux perf GUI for performance analysis\n1. `sudo apt install hotspot`\n1. `sudo strip --remove-section=.note.ABI-tag /usr/lib/x86_64-linux-gnu/libQt5Core.so.5`\n1. `sudo perf record -F 99 -p 81163 --call-graph dwarf -- sleep 60`\n\n### [pyroscope](https://github.com/grafana/pyroscope)\n\n### [speedscope](https://github.com/jlfwong/speedscope)\n\n1. [在线网页](https://www.speedscope.app/)\n1. 需要 perf script 后的 unfold(可以显示各个线程结果) 或 fold(显示整体结果, 有些信息缺失) 文件， 不是原始文件\n1. 页面上边中间可选择具体线程来看占用时间\n1. time order: 时间变化\n1. left heavy: 类似火焰图\n1. Sandwidth: 占用时间比例，及调用栈\n1. 点击符号可以看到在哪个动态库中，助于调试\n1. 调用堆栈按照它们在输入文件中出现的顺序从左到右排列，这通常是安排它们被记录的时间顺序。这个视图对于理解应用程序随时间变化的行为非常有帮助，比如 \"首次从数据库获取到数据，然后为序列化准备数据，数据被序列化为 JSON\"。\n1. 可以用来看调用关系\n1. 调用栈过长可以上下滚动滑轮\n\n### [flamescope](https://github.com/Netflix/flamescope)\n\n1. [flamescope recognition](https://www.brendangregg.com/blog/2018-11-08/flamescope-pattern-recognition.html)\n1. [netflix-flamescope](https://netflixtechblog.com/netflix-flamescope-a57ca19d47bb)\n1. heatmap + flamegraph\n1. 需要 npm install\n1. 将生成的 perf 文件 copy 到 examples 目录下\n1. 可以看 pagefault\n1. perf 可以看 all， 也可以看单独进程\n1. 斜线可以大概分辨出程序帧率，因为周期性活动\n1. 为什么以 49 赫兹采样？因为 50 赫兹可能会与定时活动同步进行采样，并且计数过高或过低。为什么一开始大约是 50 赫兹？它不太慢也不太快。太慢了，我们没有足够的样本来绘制 FlameScope 的 50 行热图（行数可以更改）。太快和采样的开销会减慢应用程序。\n1. 注意显示火焰图时 url 会显示时间区间\n\n### [火焰图](https://github.com/brendangregg/FlameGraph)\n\n1. [用 CPI 火焰图分析 Linux 性能问题](https://cloud.tencent.com/developer/article/1517979) 使用 CPI 火焰图，可以很方便地做 CPU 利用率的分析，找到和定位引发 CPU 停顿的函数。一旦找到相关的函数，就可以通过 perf annotate 命令对引起停顿的指令作出进一步确认。\n1. [作者 slice](https://www.slideshare.net/brendangregg/usenix-atc-2017-visualizing-performance-with-flame-graphs)\n1. 可以跑多次，看看优化效果\n1. 开发板交叉编译 perf，或者找相同 aarch 机器，copy。perf 之后将结果 copy 到开发机生成火焰图\n1. release 模式显示不全，debug 模式更全,可能是 release 符号被保护了？因为 release 模式用到的库被 strip 了\n1. 库不能 strip\n1. 函数名可能被 mangle，需要 demangle `./stackcollapse-perf.pl data.unfold | c++filt > data.folded`\n1. If you are profiling C++ code, you may want to pipe stacks through c++filt to get readable frames.\n1. [flamegraph-rust](https://github.com/flamegraph-rs/flamegraph)\n1. [火焰图类型](https://www.infoq.cn/article/a8kmnxdhbwmzxzsytlga)\n   - on-cpu(默认): 分析 cpu 瓶颈\n   - off-cpu: 阻塞时间\n   - 内存火焰图：内存问题\n   - Hot/Cold 火焰图： on-cpu and off-cpu 结合\n1. [性能分析之 off-cpu](https://zhuanlan.zhihu.com/p/162375221)\n1. Top edge shows who is running on cpu and how much(width)\n1. `flamegraph.pl --color=java` C++和系统调用显示颜色不一样,\n1. 平顶山的火苗是瓶颈，perf top 也可以看出来 1. `flamegraph.pl --color=java` C++, 注意 C++ 普通函数会被认为是其他，比较傻瓜，索引::\n1. 第一要想到的是优化平顶\n1. 栈上空白处是没有函数调用，栈上没信息，所以不显示，也在计算，比如 for\n\n1. 为什么要叫火焰图，形象的理解：整个图形看起来就像一团跳动的火焰，这也正是其名字的由来。燃烧的火苗尖部就是 CPU 正在执行的操纵，图纵向表示调用栈的深度，横向表示消耗的时间。一个格子的宽度越大越说明其可能是瓶颈。当我们分析火焰图时，主要就是看那些比较宽大的火苗，`特别留意那些类似平顶山的火苗`。\n   火焰图是基于 stack 信息生成的 SVG 图片，用来展示 CPU 的调用栈。\n   y 轴表示调用栈，每一层都是一个函数，调用栈越深，火焰图的层数就越高，火焰就看起来越高，顶部就是正在执行的函数，下方就是它的父函数。\n   x 轴表示抽样数，表示一个函数在 x 轴占据的宽度越宽，就表示它被抽到的次数多，即执行的时间长，注意，x 轴不代表时间，而是所有的调用栈合并后，按字母顺序排列的。\n   火焰图是 SVG 图片，可以很方便的使用：\n   （1）鼠标悬浮\n   火焰的每一层都会标注函数名，鼠标悬浮时会显示完整的函数名、抽样抽中的次数、`占据总抽样次数的百分比`；\n   （2）点击放大\n   在某一层点击，火焰图会水平放大，该层会占据所有宽度，显示详细信息；\n   左上角会同时显示“Reset Zoom”，点击该链接，图片就会恢复原样；\n   （3）搜索\n   按下 Ctrl+F 会显示一个搜索框，用户可以输入关键词或正则表达式，所有复合条件的函数名会`高亮显示`。\n\n1. 火焰图生成\n   生成和创建火焰图需要以下几个步骤：\n   （1）捕获堆栈\n   使用 perf/systemtap/dtrace 等工具抓取程序的运行堆栈。\n   （2）折叠堆栈\n   trace 工具抓取的系统和程序运行每一时刻的堆栈信息，需要对他们进行分析组合，将重复的堆栈累计在一起，从而体现出负载和关键路径。\n   （3）生成火焰图\n   分析 stackcollapse 输出的堆栈信息生成火焰图。\n1. static 函数可能不显示，会展开\n1. 火焰图结合 perf(on-cpu) 实际使用方法\n\n```sh\n# sudo apt install -y linux-tools-generic\n# sudo rm /usr/bin/perf\n# sudo ln -s /usr/lib/linux-tools/4.15.0-162-generic/perf /usr/bin/perf\n# git clone https://github.com/brendangregg/FlameGraph.git\n# cd FlameGraph\n# perf record -F 99 -p ${PID} -a -g -- sleep 60\n# perf record -m 1 -F 99(出现mmap问题)\n# perf script > out.perf\n# ./stackcollapse-perf.pl out.perf > out.folded\n# ./stackcollapse-perf.pl out.perf | c++flit > out.folded\n# ./flamegraph.pl out.folded > out.svg\n```\n\n1. 很轻松的分析出，哪些代码会经常性的触发 pagefault，以及比重。\n1. [page-faults](https://github.com/lrita/lrita.github.io/blob/master/_posts/2019-09-27-systemtap-profiling-pagefault.md#L29) flamegraph 作者给的例子有些错误,\n   map, vector clear 会调用 explicit_bzero，容易产生 pagefault\n\n```sh\n# perf record -e page-faults -p $1 -g -- sleep 60\n# perf script > out.perf\n# ./stackcollapse-perf.pl out.perf > out.folded\n# ./flamegraph.pl out.folded > out.svg\n```\n\n1. [调用关系混乱](https://seekstar.github.io/2021/09/20/linux-perf%E5%BE%97%E5%88%B0%E5%AE%8C%E6%95%B4%E8%B0%83%E7%94%A8%E5%85%B3%E7%B3%BB/)\n1. [出现 e 大量 unknown](https://blog.csdn.net/bandaoyu/article/details/108765693) 加`--call-graph dwarf`, 而 -g 就相当于 --call-graph fp. fp 就是 Frame Pointer，即 x86 中的 EBP 寄存器，fp 指向当前栈帧栈底地址，此地址保存着上一栈帧的 EBP 值\n\n```sh\nsudo perf record -F 99 -p 81163 --call-graph dwarf -- sleep 60\n```\n\n1. 问题：\n   - perf 还有一个问题是对中断的要求，perf 很多事件都依赖中断，但 Linux 内核是可以关中断的，关中断以后，你就无法击中关中断的点了，你的中断会被延迟到开中断的时候，所以，在这样的平台上，你会看到很多开中断之后的函数被密集击中。但它们是无辜的。但更糟糕的是，如果在关中断的时候，发生了多个事件，由于中断控制器会合并相同的中断，你就会失去多次事件，让你的统计发生错误。现代的 Intel 平台，基本上已经把 PMU 中断都切换为 NMI 中断了（不可屏蔽），所以前面这个问题不存在。但在大部分 ARM/ARM64 平台上，这个问题都没有解决，所以看这种平台的报告，都要特别小心，特别是你看到\\_raw_spin_unlock()一类的函数击中极高，你就要怀疑一下你的测试结果了（注意，这个结果也是能用的，只是看你怎么用）。\n   - 你每次看 perf report 的报告，首先要去注意一下总共收集了多少个点，如果你只有几十个点，你这个报告就可能很不可信了。\n2. 哪些场景下可以利用火焰图来帮助我们处理问题\n   （1）版本更新时，对比前后两个版本的火焰图差别，有助于我们分析新旧两个版本的异同；\n   （2）CPU 异常占用时（CPU 异常打满或者 CPU 利用率一直上不去），通过火焰图，有助于我们定位异常代码；\n   （3）性能分析时，通过火焰图来了解程序实际执行时各模块对 CPU 资源的占用情况；\n   （4）问题排查，通过分析火焰图的调用栈是否符合预期，排查程序逻辑执行是否符合预期；\n\n## benchmark\n\n1. [nanobench](https://github.com/martinus/nanobench)\n   - 可以测 IPC, cache miss\n\n## 总结(搜索 speed up C++)\n\n1. 内存连续读写速度远快于随机读写。(尽可能顺序访问内存)\n1. 内存的写入速度比读取慢不少(减少内存写操作)\n1. 用连续内存(多线程分配内存？)\n1. 少分配内存, 尽量原位操作\n1. 函数满足 RVO(return value optimization)\n1. 不满足 RVO, 用 move, 例如返回 tuple, 构造时元素要用 move\n1. 多维 vector 内存分配比较耗时\n1. 尽量用一维 vector 或 array，多维 vector 慢很多\n1. transform 不要用 back_inserter,先分配好，然后用 begin()\n1. lamda 是内联函数，比普通函数快\n1. 多用 for, transform, 少用 push_back, emplace_back\n1. transform 应用到连续内存会比较快， 比如 array, 应用到 vector 比 array 慢很多\n1. range 应用到连续内存比较快，array 比 vector 快很多，array 的 range 比 for 快很多，vector 的 range 比 for 慢\n1. range 比 transform 快\n1. 矩阵运算用一维矩阵比二维矩阵快很多。\n1. vector: resize 会调用构造函数，reserve 不会调用。reserve 比 resize 快很多。需要 empalce_back 的要先 reserve。初始化为 0 比较慢？\n1. 动态内存分配、STL 容器、string 都是一些常容易 cache 不友好的场景，核心代码处尽量不用\n\n## cpu mem 实时利用率\n\n```python\ndef ResouceUsageCatch():\n    print('ResouceUsageCatch runing ...')\n    msg = os.popen('ps -aux | grep -v grep | grep  \\'PID\\'').read()\n    msg_head = msg.split()\n    print(msg_head)\n    print(msg_head.index('%CPU'))\n    print(msg_head.index('RSS'))\n    cpu_val_list = []\n    mem_val_list = []\n\n    time.sleep(20) # wait the demo run stable\n    with open('cpu_mem.csv','w+') as file_out:\n        run_times = 0\n        cpu_mem_writer = csv.writer(file_out)\n        cpu_mem_writer.writerow(['cpu', 'mem'])\n        while True:\n            msg = os.popen('ps -aux | grep -v grep | grep  \\'visual_radar_by_camera_demo\\'').read()\n            if(len(msg) == 0):\n                break\n            msg_list = msg.split()\n            # print(msg_list[msg_head.index('%CPU')])\n            # print(msg_list[msg_head.index('RSS')])\n\n            cpu_mem_writer.writerow([float(msg_list[msg_head.index('%CPU')]), float(msg_list[msg_head.index('RSS')])])\n\n            cpu_val_list.append(float(msg_list[msg_head.index('%CPU')]))\n            mem_val_list.append(float(msg_list[msg_head.index('RSS')]))\n\n            run_times = run_times + 1\n            print(\"run times : \", run_times)\n            if run_times > 3600:\n                break\n\n            time.sleep(1)\n        os.system('killall visual_radar_by_camera_demo')\n        time.sleep(2)\n\n    with open('test_report.csv','w+') as file_out:\n        csv_writer = csv.writer(file_out)\n        csv_writer.writerow(['', 'max', 'average', 'min', 'data_unit'])\n        print('=************************= cpu usage =************************=')\n        if (len(cpu_val_list) > 0):\n            cpu_max = max(cpu_val_list)\n            cpu_min = min(cpu_val_list)\n            cpu_ave = sum(cpu_val_list)/len(cpu_val_list)\n            print(\"cpu max : %d\", cpu_max)\n            print(\"cpu min : %d\", cpu_min)\n            print(\"cpu ave : %d\", cpu_ave)\n            csv_writer.writerow(['cpu', round(cpu_max, 1), round(cpu_ave, 1), round(cpu_min, 1), '%'])\n        print('---------------------------------------------------------------')\n\n        print('=************************= mem usage =************************=')\n        if (len(mem_val_list) > 0):\n            mem_max = max(mem_val_list)/1000.\n            mem_min = min(mem_val_list)/1000.\n            mem_ave = sum(mem_val_list)/len(mem_val_list)/1000.\n            print(\"mem max : \", mem_max)\n            print(\"mem min : \", mem_min)\n            print(\"mem ave : \", mem_ave)\n            csv_writer.writerow(['mem', round(mem_max, 1), round(mem_ave, 1), round(mem_min, 1), 'M'])\n        print('---------------------------------------------------------------')\n\n```\n\n## profiling result\n\n1. 多线程分配内存(申请内存足够大，且时间大于线程开销)（或申请内存时可以做其他操作）\n\n```\nTEST : 4955us\nTEST_MT : 2786us\n\nvector<int> test1(1000000, 100);\n  vector<int> test2(1000000, 100);\n  vector<int> test11(1000000, 100);\n  __TOC__(TEST)\n\n  __TIC__(TEST_MT)\n  std::vector<int> test3;\n  std::vector<int> test4;\n  std::vector<int> test5;\n  auto fun = [](std::vector<int> &v, int num) { v.resize(num); };\n  auto f1 = async(fun, ref(test3), 1000000);\n  auto f2 = async(fun, ref(test4), 1000000);\n  auto f3 = async(fun, ref(test5), 1000000);\n  f1.get();\n  f2.get();\n  f3.get();\n  __TOC__(TEST_MT)\n\n```\n\n1. vector construct, reserve, resize （resize 会调用构造函数，reserve 不会调用。需要 empalce_back 的要先 reserve。初始化为 0 比较慢？）\n\n```\nTEST_RESERVE : 6275us\nTEST_NOT_RESERVE : 10727us\nTEST : 3740us\nTEST_zero : 3763us\nTEST_one : 3292us\nTEST_RESIZE : 3575us\n\n  __TIC__(TEST_RESERVE)\n  vector<int> test0;\n  test0.reserve(2000000);\n  for (auto i = 0u; i < 2000000; ++i) {\n    test0.emplace_back(0);\n  }\n  __TOC__(TEST_RESERVE)\n\n  __TIC__(TEST_NOT_RESERVE)\n  vector<int> test1;\n  for (auto i = 0u; i < 2000000; ++i) {\n    test1.emplace_back(0);\n  }\n  __TOC__(TEST_NOT_RESERVE)\n\n  __TIC__(TEST)\n  vector<int> test(2000000);\n  __TOC__(TEST)\n\n  __TIC__(TEST_zero)\n  vector<int> test2(2000000, 0);\n  __TOC__(TEST_zero)\n\n  __TIC__(TEST_one)\n  vector<int> test4(2000000, 1);\n  __TOC__(TEST_one)\n\n __TIC__(TEST_RESIZE)\n  vector<int> test3;\n  test3.resize(2000000);\n  __TOC__(TEST_RESIZE)\n\n```\n\n1. range and transform (array 的 transform 比 vector 的快)\n\n```\nARRAY_RANGE : 1084us\nVECTOR_RANGE : 6421us\nARRAY_TRANSFORM : 1603us\nVECTOR_TRANSFORM : 8252us\n\n  __TIC__(ARRAY)\n  array<int, 1000000> a;\n  for (auto i = 0u; i < 1000000; ++i) {\n    a[i] = i;\n  }\n  __TOC__(ARRAY)\n\n  __TIC__(ARRAY_TRAN)\n  array<int, 1000000> a1;\n  transform(a1.begin(), a1.end(), a1.begin(), [](auto &a) { return a + 1; });\n  __TOC__(ARRAY_TRAN)\n\n  __TIC__(VECTOR)\n  vector<int> v(1000000);\n  for (auto i = 0u; i < 1000000; ++i) {\n    v[i] = i;\n  }\n  __TOC__(VECTOR)\n\n  __TIC__(VECTOR_TRAN)\n  vector<int> v1(1000000);\n  transform(v1.begin(), v1.end(), v1.begin(), [](auto &v) { return v + 1; });\n  __TOC__(VECTOR_TRAN)\n\n  __TIC__(VECTOR_TRAN2)\n  vector<int> v2(1000000);\n  transform(v1.begin(), v1.end(), v2.begin(), [](auto &v) { return v + 1; });\n  __TOC__(VECTOR_TRAN2)\n\n```\n\n1. 一维 vector 和二维 vector(一维比多维快很多)\n\n```\nVECTOR2 : 3770us\nVECTOR1 : 554us\nARRAY : 627us\n\n __TIC__(VECTOR2)\n  vector<vector<int>> v(10000, vector<int>(5));\n  for (auto i = 0u; i < v.size(); ++i) {\n    for (auto j = 0u; j < 5; ++j) {\n      v[i][j] = 1;\n    }\n  }\n  __TOC__(VECTOR2)\n\n  __TIC__(VECTOR1)\n  vector<int> v1(10000 * 5);\n  for (auto i = 0u; i < 10000 * 5; ++i) {\n    v1[i] = 1;\n  }\n  __TOC__(VECTOR1)\n\n  __TIC__(ARRAY)\n  array<int, 10000 * 5> a;\n  for (auto i = 0u; i < 10000 * 5; ++i) {\n    a[i] = 1;\n  }\n  __TOC__(ARRAY)\n\n\n```\n\n1. array and vector, transform and for(array 配合 transform 比较快， vector 配合 transform 很慢)\n\n```\nARRAY : 1372us\nARRAY_TRAN : 1078us\nVECTOR : 1219us\nVECTOR_TRAN : 4595us\n\n  __TIC__(ARRAY)\n  array<int, 100000> a;\n  for (auto i = 0u; i < 100000; ++i) {\n    a[i] = i;\n  }\n  __TOC__(ARRAY)\n\n  __TIC__(ARRAY_TRAN)\n  array<int, 100000> a1;\n  transform(a1.begin(), a1.end(), a1.begin(), [](auto a) { return a + 1; });\n  __TOC__(ARRAY_TRAN)\n\n  __TIC__(VECTOR)\n  vector<int> v(100000);\n  for (auto i = 0u; i < 100000; ++i) {\n    v[i] = i;\n  }\n  __TOC__(VECTOR)\n\n  __TIC__(VECTOR_TRAN)\n  vector<int> v1(100000);\n  transform(v1.begin(), v1.end(), v1.begin(), [](auto v) { return v + 1; });\n  __TOC__(VECTOR_TRAN)\n\n```\n\n1. function return\n\n```\nstd::vector<int> m1() { return std::vector<int>(3000000, 1); }\n\nstd::vector<int> m2() { return std::move(std::vector<int>(3000000, 1)); }\n\n// fastest\nstd::vector<int> m3() {\n  std::vector<int> result(3000000, 1);\n  return result;\n}\n\nstd::vector<int> m4() {\n  std::vector<int> result(3000000, 1);\n  return std::move(result);\n}\n\nM1 : 16538117ns\nM2 : 15113944ns\nM3 : 14415912ns\nM4 : 15932611ns\n```\n\n1. range: vector and array(array 的 range 比 for 快很多，vector 的 range 比 for 慢)\n\n```\nVECTOR : 648us\nVECTOR_RANGE : 1784us\nARRAY : 743us\nARRAY_RANGE : 177us\n\n__TIC__(VECTOR)\n  vector<int> v1(10000 * 5);\n  for (auto i = 0u; i < 10000 * 5; ++i) {\n    v1[i] = 1;\n  }\n  __TOC__(VECTOR)\n\n  __TIC__(VECTOR_RANGE)\n  vector<int> v2(10000 * 5);\n  for (auto &i : v2) {\n    i = 1;\n  }\n  __TOC__(VECTOR_RANGE)\n\n  __TIC__(ARRAY)\n  array<int, 10000 * 5> a;\n  for (auto i = 0u; i < 10000 * 5; ++i) {\n    a[i] = 1;\n  }\n  __TOC__(ARRAY)\n\n  __TIC__(ARRAY_RANGE)\n  array<int, 10000 * 5> a1;\n  for (auto &i : a) {\n    i = 1;\n  }\n  __TOC__(ARRAY_RANGE)\n\n```\n\n2. range and for\n\n```\n std::vector<int> vec(500000, 0);\n\n  __TIC__(M1);\n  for (int i = 0; i < vec.size(); ++i) {\n    vec[i]++;\n  }\n  __TOC__(M1);\n\n  __TIC__(M2);\n  for (auto item : vec) {\n    item++;\n  }\n  __TOC__(M2);\n\n  __TIC__(M3);\n  for (auto& item : vec) {\n    item++;\n  }\n  __TOC__(M3);\n\n\nM1 : 5885us\nM2 : 12406us\nM3 : 11534us\n```\n\n1. transform and for\n\n```\n  std::vector<float> vec(10000, 1.111);\n  std::vector<float> vec1(10000, 2.111);\n  std::vector<float> result;\n  // 操作一个vector\n  std::transform(vec.begin(), vec.end(), std::back_inserter(result),\n                 [](const auto item) -> float { return item + 100; });\n  // for_each(vec.begin(), vec.end(), [](auto &item) { item += 100; })\n\n // 性能好很多，不要用back_inserter\n  std::vector<float> result5(vec.size(), 0);\n  std::transform(vec.begin(), vec.end(), result5.begin(),\n                 [](const auto &item) -> float { return item + 100; });\n\n\n  // 操作两个vector NOTE: 使用前需要检查vec, vec1 size是否匹配。\n  // 性能好 797us 13us(-O3)\n  std::vector<float> result1(10, 0.0);\n  std::transform(vec.begin(), vec.end(), vec1.begin(), result1.begin(),\n                 [](const auto &item1, const auto &item2) -> float {\n                   return item1 + item2;\n                 });\n\n  // 性能差 2605us 173us(-O3)\n  std::vector<float> result2;\n  std::transform(vec.begin(), vec.end(), vec1.begin(),\n                 std::back_inserter(result2),\n                 [](const auto &item1, const auto &item2) -> float {\n                   return item1 + item2;\n                 });\n  // for 463us 70us(-O3)\n  std::vector<float> result3(vec.size(), 0);\n  for (auto i = 0u; i < result3.size(); ++i) {\n    result[i] = vec[i] + vec1[i];\n  }\n\n```\n\n3. lambda and fun\n\n```\n\n// 322ns\nint add(int &a, int &b) {\n  auto result = a + b;\n  return result;\n}\n\n// 233ns\ninline int add_inline(int &a, int &b) {\n  auto result = a + b;\n  return result;\n}\n\n// 233ns\nauto l = [](int &a, int &b) { return a + b; }; // 使用lambda函数的效率与使用函数对象是一样的，都要快于函数指针。他们都能够在编译期将代码内联展开，减少函数调用的时间。\n// 编译为类\nclass __lambda_6_11\n  {\n    public:\n    inline /*constexpr */ int operator()(int a, int b) const\n    {\n      return a + b;\n    }\n\n    using retType_6_11 = int (*)(int, int);\n    inline /*constexpr */ operator retType_6_11 () const noexcept\n    {\n      return __invoke;\n    };\n\n    private:\n    static inline int __invoke(int a, int b)\n    {\n      return a + b;\n    }\n  }\n```\n\n4 for and memset\n\n```\n  int a[800000];\n  __TIC__(M1);\n  memset(a, 0, 800000 * sizeof(int)); // memset 只能用于连续内存，不能用于vector\n  __TOC__(M1);\n\n  __TIC__(M2);\n  for (auto i = 0u; i < 800000; ++i) {\n    a[i] = 0;\n  }\n  __TOC__(M2);\n\nM1 : 1401us\nM2 : 1908us\n```\n\n5. for, transform, push_back, emplace_back(少用 push_back, emplace_back)\n\n```\nvector<float> vec(1000000, -1235.23);\n\n  __TIC__(FOR)\n  vector<float> result_for(1000000, 0);\n  for (auto i = 0u; i < result_for.size(); ++i) {\n    result_for[i] = vec[i];\n  }\n  __TOC__(FOR)\n\n  __TIC__(TRANSFORM)\n  vector<float> result_trans(1000000, 0);\n  std::transform(vec.begin(), vec.end(), result_trans.begin(),\n                 [](auto i) -> float { return i; });\n  __TOC__(TRANSFORM)\n\n  __TIC__(PUSH_BACK)\n  vector<float> result0;\n  for (auto i : vec) {\n    result0.push_back(i);\n  }\n  __TOC__(PUSH_BACK)\n\n  __TIC__(PUSH_BACK1)\n  vector<float> result;\n  for (auto i : vec) {\n    result.push_back(std::move(i));\n  }\n  __TOC__(PUSH_BACK1)\n\n  __TIC__(EMPLACE_BACK)\n  vector<float> result1;\n  for (auto i : vec) {\n    result1.emplace_back(i);\n  }\n  __TOC__(EMPLACE_BACK)\n\n  __TIC__(EMPLACE_BACK2)\n  vector<float> result2;\n  for (auto &i : vec) {\n    result2.emplace_back(i);\n  }\n  __TOC__(EMPLACE_BACK2)\n\n  __TIC__(EMPLACE_BACK3)\n  vector<float> result3;\n  for (auto &i : vec) {\n    result3.emplace_back(std::move(i));\n  }\n  __TOC__(EMPLACE_BACK3)\n\n  cout << vec[0] << endl;\n\n  for (auto &i : vec) {\n    // cout << i << \" \";\n  }\n  return 0;\n\nFOR : 2042us\nTRANSFORM : 2161us\nPUSH_BACK : 4369us\nPUSH_BACK1 : 5835us\nEMPLACE_BACK : 5175us\nEMPLACE_BACK2 : 5075us\nEMPLACE_BACK3 : 5364us\n\n```\n\n1. struct bindings(多用引用)\n\n```\n  vector<float> vec(1000000, -1235.23);\n  auto tup = std::make_tuple(vec, vec, vec);\n  __TIC__(TUP1)\n  auto [x, y, z] = tup;\n  __TOC__(TUP1)\n\n  __TIC__(TUP2)\n  auto &[l, m, n] = tup;\n  __TOC__(TUP2)\n\nTUP1 : 6940us\nTUP2 : 0us\n\n```\n\n```\n// 三种方式等价\n  vector<float> vec(1000000, -1235.23);\n  __TIC__(TUP1)\n  auto tup = std::make_tuple(vec, vec, vec);\n  auto [x, y, z] = std::move(tup);\n  __TOC__(TUP1)\n\n  __TIC__(TUP2)\n  auto tup2 = std::make_tuple(vec, vec, vec);\n  auto &[x2, y2, z2] = tup;\n  __TOC__(TUP2)\n\n  __TIC__(TUP3)\n  auto [x1, y1, z1] = std::make_tuple(vec, vec, vec);\n  __TOC__(TUP3)\n\nTUP1 : 10087us\nTUP2 : 10126us\nTUP3 : 10127us\n```\n\n```\n// 构造时能用Move用move\ntuple<vector<float>, vector<float>, vector<float>> get_tup() {\n  vector<float> vec1(1000000, -1235.23);\n  vector<float> vec2(1000000, -1235.23);\n  vector<float> vec3(1000000, -1235.23);\n  return {vec1, vec2, vec3};\n}\n\ntuple<vector<float>, vector<float>, vector<float>> get_tup1() {\n  vector<float> vec1(1000000, -1235.23);\n  vector<float> vec2(1000000, -1235.23);\n  vector<float> vec3(1000000, -1235.23);\n  return {std::move(vec1), std::move(vec2), std::move(vec3)};\n}\n\n  __TIC__(TUP3)\n  auto [x1, y1, z1] = get_tup();\n  __TOC__(TUP3)\n\n  __TIC__(TUP4)\n  auto [x4, y4, z4] = get_tup1();\n  __TOC__(TUP4)\n\nTUP3 : 18522us\nTUP4 : 6687us\n```\n\n1. if and std::max (简单比较用 if，无函数调用)\n\n```\nvector<int> test(100000, 0);\n  for (auto i = 0u; i < 100000; i++) {\n    test[i] = rand();\n  }\n\n  __TIC__(TEST)\n  int max_value = 0;\n  for (auto i = 0u; i < 100000; ++i) {\n    if (test[i] > max_value) {\n      max_value = test[i];\n    }\n  }\n  __TOC__(TEST)\n\n  __TIC__(TEST2)\n  int max_value2 = 0;\n  for (auto i = 0u; i < 100000; ++i) {\n    max_value2 = std::max(max_value2, test[i]);\n  }\n  __TOC__(TEST2)\n\nTEST : 570us\nTEST2 : 994us\n\n```\n\n## Links\n\n1. [C++性能优化](https://zhuanlan.zhihu.com/p/33638344)\n1. [C++性能调优](https://segmentfault.com/a/1190000040755126)\n\n## 优化的问题\n\n1. j5 模型定点对齐输出， 后处理在需要的地方进行定浮点转换与取有效数据\n1. 模型输出 chw， 按 c 取最大值，形成 h\\*w mask；如果按 for i j k 来取，不连续。解决方法：设置第一层为最大值，后续 k-1 层没一层和第 k-1 层做对比，最大值保存在第一层，max 对应的 index 存起来\n\n```\n  // net_output: NCHW; net_output_shape: NHWC\n  auto frame_w = net_output_shape[0][2];\n  auto frame_h = net_output_shape[0][1];\n  auto channel_num = net_output_shape[0][3];\n  // cv::Mat mask_mat(frame_h,frame_w, CV_8UC1);\n  cv::Mat mask_mat = cv::Mat::zeros(frame_h, frame_w, CV_8UC1);\n  unsigned char *maskdata = mask_mat.data;\n  // for(int i=0; i<frame_h; i++){\n  //   for(int j=0; j<frame_w; j++){\n  //     int index = i*frame_w+j;\n  //     float tmp = net_output[index];\n  //     int maxk = 0;\n  //     for(int k=1; k<channel_num; k++){\n  //       int index1 = k*frame_h*frame_w+index;\n  //       if (tmp < net_output[index1]) {\n  //         tmp = net_output[index1];\n  //         maxk = k;\n  //       }\n  //     }\n  //     maskdata[index] = maxk;\n  //   }\n  // }\n\n  // net_output 和 maskdata 都连续访存, 提高cache命中率\n  int wxh = frame_h *frame_w;\n  // set layer 0 to max_value;\n  std::vector<float> max_value(wxh, 0);\n  memcpy(&max_value[0], net_output, wxh * sizeof(float));\n  // get max index from layer 1\n  int index = wxh;\n  for (auto k = 1u; k < channel_num; ++k) {\n    for (auto offset = 0u; offset < wxh; ++offset) {\n      if (net_output[index] > max_value[offset]) {\n        max_value[offset] = net_output[index];\n        maskdata[offset] = k;\n      }\n      index++;\n    }\n  }\n\n  // // not copy: 7ms\n  // int index = wxh;\n  // for (auto k = 1u; k < channel_num; ++k) {\n  //   for (auto offset = 0u; offset < wxh; ++offset) {\n  //     if (net_output[index] > net_output[offset]) {\n  //       net_output[offset] = net_output[index];\n  //       maskdata[offset] = k;\n  //     }\n  //     index++;\n  //   }\n  // }\n```\n","tags":["Profiling","Perf"],"categories":["Program","Cpp"]},{"title":"Programming-Paradigm","url":"/2021/08/22/Programming-Paradigm/","content":"\n## 概念\n1. 编程范式的实际含义：限制和规范程序员的能力。\n2. 每个编程范式的目的都是设置限制，这些编程范式主要告诉我们不能做什么，而不是可以做什么。\n3. 语法，features和范式的关系？语言的语义和句法决定了语言的features? features决定了语言的编程范式？\n4. 限制：\n   - 结构化编程：对程序控制权的直接转移进行了限制和规范， 限制goto语句， 因为if, while等包装了goto\n   - 面向对象编程：对程序控制权的间接访问进行了限制和规范， 限制函数指针, 因为多态包装了函数指针。\n   - 函数式编程：对程序中的赋值进行了限制和规范， 限制赋值语句，因为包装了变量\n5. `关系： 多态是跨越架构边界的手段，函数式编程时我们规范和限制数据存放位置和访问权限的手段，结构化编程则是个模块的算法实现基础。这和软件架构的三大关注重点不谋而合：功能性，组件独立性，数据管理`\n6. 多态归根结底不过就是函数指针的一种应用。\n\n## Procedural/Structured programming 面向过程/结构化编程\n1. 结构化程序设计方法SP(Structured Programming)的着眼点是“面向过程”。　　\n2. 它采用子程序、块结构、for循环以及while循环等结构，来取代传统的 goto。goto语句的某些用法会导致某个模块无法被递归拆分成更小的，可证明的单元，会导致无法采用分解法来将大型问题进一步拆分成更小的，可证明的部分。\n3. 可以用`顺序结构`，`分支结构`和`循环结构`这三种结构构造出任何程序。\n4. 科学和数学： 数学是要将可证明的结论证明，科学则是要将可证明的结论证伪。科学理论和定律特点：它们可以被证伪，但没有办法证明。\n3. 软件开发像是一门科学研究学科：可通过无法证伪来证明软件的正确性。测试的作用。\n4. 结构化编程最有价值的地方在于它赋予我们创造可证伪程序单元的能力。\n5. 无论在哪一个层面上，从最小的函数到最大的组件，软件开发过程都和科学研究非常类似，他们都是由证伪驱动的。软件架构师选哟定义可以方便地进行证伪的模块,组件以及服务。\n6.\n\n## Object Oriented Programming 面向对象编程\n1.面向对象编程就是以多态为手段来对源代码中的依赖关系进行控制的能力，这种能力让软件架构师可以构建出某种插件式架构，让高层策略性组件与底层是线性组件相分离，底层组件可以编译成插件，实现独立于高层组件的开发和部署。\n2.`依赖反转`： 通过接口，多态来使`源代码上的依赖关系方向`与`控制流方向`相反。 中间加了接口，只有被控制端的源代码依赖关系方向反转。 面向接口编程就是利用了依赖反转来使得模块分离。\n3.依赖反转可用来打破组件间循环依赖。\n\n## Functional Programming 函数式编程\n1. 最大特点是不可变性——某个符号所对应的值永远不变，从理论上讲，函数式编程语言应该是没有赋值语句的。\n2. 如果由足够大的存储量和处理能力，应用程序就可以用完全不可变的，纯函数式的方式来编程。\n3. 一个架构设计良好的应用程序应该将状态可修改的部分和不需要修改的部分隔离成单独的组件，然后用合适的机制来保护可变量。可变状态组件的逻辑越少越好。\n4. 好处：\n   - 没有可变状态，函数就是引用透明(referential transparency)的和没有有副作用(no side effect)\n   - 可以解决并发竞争，死锁等问题\n   - 代码容易进行推理，不容易出错。\n   - 由于函数式语言是面向数学的抽象，更接近人的语言，而不是机器语言，代码会比较简洁，也更容被理解。\n5. 面向数学的抽象，将计算描述为一种表达式求值，一句话，函数式程序就是一个表达式。\n6. 函数式编程中的函数不是指计算机中的函数，而是指数学中的函数，即自变量的映射。也就是说一个函数的值仅仅取决于函数参数的值。\n7. 函数式编程语言中的变量不是命令式编程语言中的变量（存储状态的单元），而是代数中的变量，即一个值的名称。变量的值是不可变的。命令式编程中的\"x = x + 1\"（赋值运算），拿给程序员看说是对的，拿给数学家看，却被认为这个等式为假。\n\n\n## Imperative programming 指令式编程\n1. 面向计算机硬件的抽象\n   - 变量： 存储单元\n   - 赋值语句： 获取，存储指令\n   - 表达式： 内存引用和算数运算\n   - 控制语句： 跳转指令\n1. 冯诺依曼机的指令序列。\n\n\n## Reference\n1. <<架构整洁之道>>\n","tags":["Paradigm"],"categories":["Program","PLT"]},{"title":"Awesome Books","url":"/2021/08/16/Awesome-Books/","content":"\n## Awesome Books\n\n1. [深入理解计算机系统 Computer Systems:A Programmer's Perspective Computer Systems:A Programmer's Perspective CSAPP](https://hansimov.gitbook.io/csapp/publish-info/publisher-words)\n1. [计算机程序的构造和解释 Structure and Interpretation of Computer Programs SICP](https://github.com/sarabander/sicp-pdf)\n1. 计算机体系结构 量化研究方法\n1. 计算机组成与设计 硬件软件接口\n1. 重构\n1. 架构整洁之道\n1. 代码整洁之道\n1. 设计模式\n1. 编译原理\n1. 软件建模与设计\n1. C++ Primer\n1. [Software-Engineering-at-Google 中文版本](https://github.com/qiangmzsx/Software-Engineering-at-Google)\n1. [Software Engineering at Google](https://abseil.io/resources/swe-book)\n1. [Category Theory for Programmers](https://github.com/hmemcpy/milewski-ctfp-pdf)\n\n## links\n\n1. [cs-books/influential-cs-books](https://github.com/cs-books/influential-cs-books)\n"},{"title":"Modern Cmake","url":"/2021/07/28/Cmake/","content":"\n## cmake base\n1. `g++ -o test test.cpp -Iglog/include -lglog -Lglog/lib`\n   - `-I` 表示从后面路径找头文件\n   - `-l` 表示链接库，glog -> libglog.so/libglog.a, 只需要指定glog，libglog.so会自动补全\n   - `-L` 表示从后面路径找链接库\n   - `-l` 和 `-L` 配合找到要链接的库\n1. 常见的两个问题：\n   - `No such file or directory` 找不到头文件或共享库，需要指定-I或-L\n   - `undefined reference to xxx` 未定义符号xxx, 需要-l链接xxx定义所在的共享库\n1. `cmake .. -DCMAKE_VERBOSE_MAKEFILE=ON` 加参数显示详细编译链接信息\n   - 出问题可以把编译或者链接信息单独拿出来执行或修改，方便定位和修复问题\n1. cmake基础命令\n   - `target_include_directories` 相当于加`-I`\n   - `target_link_directories` 相当于加`-L`\n   - `target_link_libraries` 相当于加`-l`\n\n\n## code generate\n\n1. vitis ai library overview generate test_performance.... [link](https://github.com/Xilinx/Vitis-AI/blob/v2.5/src/Vitis-AI-Library/overview/cmake/XilinxTest.cmake#L346)\n\n## FetchContent\n\n```cmake\ninclude(FetchContent)\nFetchContent_Declare(backward\n    GIT_REPOSITORY https://github.com/bombela/backward-cpp\n    GIT_TAG master  # or a version tag, such as v1.6\n    SYSTEM          # optional, the Backward include directory will be treated as system directory\n)\nFetchContent_MakeAvailable(backward)\ntarget_link_libraries(mytarget PUBLIC Backward::Interface)\n```\n\n## 版本升级\n\n1. 下载 sh 脚本，本地升级。[link](https://github.com/Kitware/CMake/releases)\n1. `wget -c https://cmake.org/files/v3.28/cmake-3.28.1-linux-x86_64.sh && chmod +x cmake-3.28.1-linux-x86_64.sh && ./cmake-3.28.1-linux-x86_64.sh`\n1. `wget -c https://cmake.org/files/v3.28/cmake-3.28.1-linux-aarch64.sh && chmod +x cmake-3.28.1-linux-aarch64.sh && ./cmake-3.28.1-linux-aarch64.sh`\n\n## 变量\n\n1. `CMAKE_SOURCE_DIR`: project\n1. `CMAKE_CURRENT_SOURCE_DIR` 当前 CMakeLists.txt 所在目录\n1. `CMAKE_BINARY_DIR`: 编译目录\n1. `CMAKE_CURRENT_BINARY_DIR`: 指向当前正在处理的 CMakeLists.txt 文件对应的构建目录。\n\n## Notes\n1. cmake 和 make的区别\n   - 工程设计师\tCMake\t设计蓝图（生成构建脚本，比如 Makefile）\n   - 工地工人\tMake\t根据蓝图（Makefile）来干活（编译）\n1. (good)编译 log 太多可以把 error 重定向到文件 `command 2> error_log.txt`\n1. include_directories 加 SYSTEM 防止第三方库报 warning [link](https://cmake.org/cmake/help/latest/command/include_directories.html)\n1. makefile 传参\n\n```shell\nSRC_DIR ?= /src\nBUILD_DIR ?= /build\n$ make SRC_DIR=/tmp/src BUILD_DIR=/tmp/build\n```\n\n1. `target_compile_definitions` 导出定义，供下游使用，`add_definitions`不会被导出\n1. `-DCMAKE_TOOLCHAIN_FILE=path/to/toolchain-clang.cmake` 设置 toolchain\n1. `-DCMAKE_BUILD_TYPE=RelWithDebInfo` release 模式编译，带 debug info\n1. `-DCMAKE_VERBOSE_MAKEFILE=ON` 显示详细信息\n1. `-DCMAKE_FIND_DEBUG_MODE=ON`\n1. `cmake -S . -B build --trace-source=CMakeLists.txt` trace CMakeLists.txt\n1. `cmake -S . -B build --debug-find` trace find\n1. osqp\n\n```cmake\nif(CMAKE_BUILD_TYPE STREQUAL \"Debug\")                                                                                                                                                                                                                             set(OSQP_ENABLE_DEBUG ON)\n  set(CMAKE_VERBOSE_MAKEFILE ON)\n  set(CMAKE_EXPORT_COMPILE_COMMANDS ON)\nendif()\n```\n\n1. (good)打印所有变量\n\n```cmake\nget_cmake_property(_variableNames VARIABLES)\nforeach (_variableName ${_variableNames})\n    message(STATUS \"${_variableName}=${${_variableName}}\")\nendforeach()\n```\n\n1. `-DCMAKE_CXX_FLAGS=\"-g\"` cmake 编译选项添加 debug info； release 模式也适用\n1. 参考官方写 CMakeLists.txt: [cmake](https://gitlab.kitware.com/cmake/cmake) [opencv](https://github.com/opencv/opencv)\n1. `Call stack`: 看出问题的顺序\n1. CMake 3.13 now allows the target_link_libraries() command to be called from any directory to add link dependencies and link interface libraries to targets created in other directories.\n1. cmake 2.8 3.1 对 string(REGEX MATCH \"lib.\\*\\.so$\" LIB ${LIB}) 处理是不同的， 2.8 需要加`\\\\.`\n1. get_filename_component(LIB ${LIB} NAME_WE) 获取文件名，无后缀\n1. 参考 opencv 等库的 CMakeLists.txt\n1. 对外 include 里如果包含，需要 PUBLIC 传递出去，如果只在实现里用则用 PRIVATE\n1. `RUNTIME_DEPENDENCIES` 3.21 之后可以安装依赖库\n1. `#cmakedefine USE_MYMATH` 条件编译能看到定义的配置\n1. 增量编译：是在源程序已经完成第一次编译的基础上再次编译时采取的一种增量性编译技术。只重新编译必须的部分，即通过依赖分析，只编译修改过的部分及其影响的路径。\n1. debug CMakeLists.txt `cmake -S . -B build --trace-source=CMakeLists.txt`\n1. 处理依赖关系时区别名，例如导入 gflags.so `add_library(gflags SHARED IMPORTED GLOBAL) add_library(gflags::gflags ALIAS gflags)`来区别 libgflags.so\n1. 注意区分`add_library(gflags SHARED IMPORTED GLOBAL)`和`libgflags.so`的区别，前者是 target, 后者是 so。为了区分两者， 可以使用`add_library(gflags SHARED IMPORTED GLOBAL) add_library(gflags::gflags ALIAS gflags)` target::target 来区分。\n1. `cmake` + `ninja` + `clang` 组合加速编译\n1. bazel、GYP、CMake 和 Meson 是文件生成器(build system generator)， make, ninja 是构建系统。\n1. `cmake ..`: configure the project and generate a native build system\n1. cmake 修改编译选项时需要删除 build 目录下的 `CMakeCache.txt` 否则编译选项不 work, 或者重新`cmake ..` or run script [link](https://stackoverflow.com/a/42922588)\n1. **加速编译**： `configure the project and generate a native build system`后直接执行`make -C $BUILD_DIR -j4` or `cmake --build ${BUILD_DIR}`，不用每次都重新构建编译系统\n1. [CMake] Debug build compiles and links, but Release build fails to link?\n\n```cmake\nif (CMAKE_BUILD_TYPE MATCHES Release)\n    set(CMAKE_SHARED_LINKER_FLAGS \"-s -Wl,--version-script=${PROJECT_SOURCE_DIR}/symbol.lst\")\nendif()\n```\n\n1. -DCMAKE_BUILD_TYPE=Debug: `-fPIC -O2 -Wall`\n1. -DCMAKE_BUILD_TYPE=Release: `-fPIC -O2 -DNDEBUG`\n1. `enable_testing()` 需要在第一级 CMakeLists.txt 里，否则`make test`不生效\n1. make test 中程序 pwd 路径为 build 目录程序所在路径。\n1. `Testing/Temporary/LastTest.log` 为`make test`结果\n1. `set(CMAKE_VERBOSE_MAKEFILE on)` make 时显示详细信息，等效为`make VERBOSE=1`\n1. set(CMAKE_C_COMPILER /opt/gcc-linaro-6.5.0-2018.12-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc)， s\u0011et(CMAKE_CXX_COMPILER /opt/gcc-linaro-6.5.0-2018.12-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-g++) 要在 project 前， 会根据编译器选择环境， 无这条则会选择宿主机， 地平线 demo 问题。\n1. option 和 c++里的 definition 是不一样的 definition 需要 add_definitions()来添加\n\n## functions\n\n### add test\n\n### cpm add lib\n\n## cmake 变量\n\n### 作用域\n\n## cmake component\n\n## 依赖管理\n\n1. [CPM.cmake](https://github.com/cpm-cmake/CPM.cmake)\n1. [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html)\n1. CPM.cmake 封装了 FetchContent\n1. 导入已经编译好的库\n1. 导入三方库参考 cmake export 出的 cmake file。参见 cpp_sandbox/cmake/export; 查看生成的 cmake 文件，主要两个步骤\n   - add_library(xxx::xxx STATIC IMPORTED) 导入库\n   - set_target_properties 设置 include 和 link 依赖\n1. 参考 vitis ai 导入 xrt\n\n```cmake\nif(gflags_ADDED)\n  add_library(gflags INTERFACE IMPORTED GLOBAL) ## global导入， 解决cpm local scope include_directories不工作问题\n  add_library(gflags::gflags ALIAS gflags)\n  set_target_properties(\n    gflags PROPERTIES IMPORTED_LOCATION \"${gflags_SOURCE_DIR}/lib/libgflags.so\")\n  target_include_directories(gflags INTERFACE \"${gflags_SOURCE_DIR}/include\")\n  # target_link_directories(gflags INTERFACE \"${gflags_SOURCE_DIR}/lib\")\nendif()\n```\n\n## cpack\n\n### build 完之后 make package\n\n```cmake\nset(CPACK_GENERATOR \"TGZ\")\nset(CPACK_PACKAGE_FILE_NAME \"cpack-test\")                                                                                                                                                                           include(CPack)\n```\n\n## color\n\n1. [cmake 自带打印颜色](https://stackoverflow.com/a/36233927) `cmake -E cmake_echo_color --green \">>>>> Download doxygen\"`\n1. put make function in .bashrc\n\n```sh\nmake()\n{\n  pathpat=\"(/[^/]*)+:[0-9]+\"\n  ccred=$(echo -e \"\\033[0;31m\")\n  ccyellow=$(echo -e \"\\033[0;33m\")\n  ccend=$(echo -e \"\\033[0m\")\n  make \"$@\" 2>&1 | sed -E -e \"/[Ee]rror[: ]/ s%$pathpat%$ccred&$ccend%g\" -e \"/[Ww]arning[: ]/ s%$pathpat%$ccyellow&$ccend%g\"\n  return ${PIPESTATUS[0]}\n}\n```\n\n## basic\n\n1.  内置函数获取名字\n\n````cmake\nstring(REGEX MATCH \"lib.*\\.so$\" LIB ${LIB})\nif(LIB)\n    get_filename_component(LIB ${LIB} NAME_WE)\n    string(REPLACE \"lib\" \"\" LIB \"${LIB}\")\nendif()\n```\n\n1.  获取库列表名字\n\n```cmake\nexecute_process(\n  COMMAND ls\n  COMMAND sh -c \"awk '/.so\\$|.a\\$/{print \\$0}'\"\n  COMMAND sh -c \"awk -F '.' '{print substr($1,4)}'\"\n  WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\n  OUTPUT_VARIABLE LIBS\n  OUTPUT_STRIP_TRAILING_WHITESPACE)\nmessage(STATUS, \"LIBS=${LIBS}\")\nstring(REPLACE \"\\n\" \";\" LIBS \"${LIBS}\")\nlist(LENGTH LIBS len)\nmessage(STATUS, \"len=\\\"${len}\\\"\")\nstring(REPLACE \" \" \";\" SEXY_LIST ${LIBS})\nforeach(LIB ${LIBS})\n  message(STATUS, \"LIB=\\\"${LIB}\\\"\")\nendforeach()\n\n````\n\n1. `include(CMakePrintHelpers) cmake_print_variables(MY_VARIABLE)\" 打印信息\n1. `-DCMAKE_EXPORT_COMPILE_COMMANDS=ON` 代码补全使用\n1. `cmake -LA` after build in build dir:list a set of available options in the cmake\n1. 如果 CMAKE_BUILD_TYPE 的值为\"Debug\"就采用 debug 模式编译；如果 CMAKE_BUILD_TYPE 的值为\"Release\"就采用 release 模式编译；如果 CMAKE_BUILD_TYPE 的值为空，默认采用 release 模式\n1. debug release set\n\n1. CMakeLists.txt\n\n```cmake\nSET(CMAKE_BUILD_TYPE \"Debug\")\nor\nSET(CMAKE_BUILD_TYPE \"Release\")\n```\n\n1. build script\n\n```sh\nmkdir Release\ncd Release\ncmake -DCMAKE_BUILD_TYPE=Release ..\nmake\n```\n\n1. `set(CMAKE_CXX_COMPILER \"g++\")` 选择编译器\n1. 查看 target properties\n\n```cmake\n# Get all propreties that cmake supports\nif(NOT CMAKE_PROPERTY_LIST)\n    execute_process(COMMAND cmake --help-property-list OUTPUT_VARIABLE CMAKE_PROPERTY_LIST)\n\n    # Convert command output into a CMake list\n    string(REGEX REPLACE \";\" \"\\\\\\\\;\" CMAKE_PROPERTY_LIST \"${CMAKE_PROPERTY_LIST}\")\n    string(REGEX REPLACE \"\\n\" \";\" CMAKE_PROPERTY_LIST \"${CMAKE_PROPERTY_LIST}\")\nendif()\n\nfunction(print_properties)\n    message(\"CMAKE_PROPERTY_LIST = ${CMAKE_PROPERTY_LIST}\")\nendfunction()\n\nfunction(print_target_properties target)\n    if(NOT TARGET ${target})\n      message(STATUS \"There is no target named '${target}'\")\n      return()\n    endif()\n\n    foreach(property ${CMAKE_PROPERTY_LIST})\n        string(REPLACE \"<CONFIG>\" \"${CMAKE_BUILD_TYPE}\" property ${property})\n\n        # Fix https://stackoverflow.com/questions/32197663/how-can-i-remove-the-the-location-property-may-not-be-read-from-target-error-i\n        if(property STREQUAL \"LOCATION\" OR property MATCHES \"^LOCATION_\" OR property MATCHES \"_LOCATION$\")\n            continue()\n        endif()\n\n        get_property(was_set TARGET ${target} PROPERTY ${property} SET)\n        if(was_set)\n            get_target_property(value ${target} ${property})\n            message(\"${target} ${property} = ${value}\")\n        endif()\n    endforeach()\nendfunction()\n\nprint_target_properties(xxx)\n```\n\n1. 查看 cmake property\n\n```\nget_cmake_property(_variableNames VARIABLES)\nforeach (_variableName ${_variableNames})\n    message(STATUS \"${_variableName}=${${_variableName}}\")\nendforeach()\n```\n\n1. 查看 include_director\n\n```\nget_property(dirs DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} PROPERTY INCLUDE_DIRECTORIES)\nforeach(dir ${dirs})\n  message(STATUS \"include dir='${dir}'\")\nendforeach()\n\nget_property(dirs DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} PROPERTY LINK_DIRECTORIES)\nforeach(dir ${dirs})\n  message(STATUS \"link dir='${dir}'\")\nendforeach()\n```\n\n1. `ctest -N` 查看 test 列表\n1. target_compile_definitions(full_flow PRIVATE \"-DGIT_INFO=\\\"${GIT_BRANCH}-${GIT_COMMIT_HASH}\\\"\") 代替 add_definitions(-DGIT_INFO)\n1. add_subdirectory(protobuf) 添加子文件夹，子文件夹下 CmakeLists.txt 管理相应文件夹。\n1. cmake_minimum_required(VERSION 3.8） 版本要求\n1. project(HELLO) 定义工程名\n1. message(STATUS, \"“） 打印消息\n1. aux_source_directory(. DIR_SRCS) 查找指定目录下的所有源文件并将结果存入变量名\n1. add_excutable(hello, ${DIR_SRCS}) 定义了工程生成的可执行程序文件名，源文件为 DIR_SRCS 变量中定义的源文件列表\n1. add_library(libname [SHARED | STATIC | MODULE] source_files) 定义了库文件名 静态库，动态库不能重名\n1. add_library(hello_static STATIC ${LIBHELLO_SRC}) set_target_properties(hello_static PROPERTIES OUTPUT_NAME \"hello\") 有 libhello.so 的情况下生成 libhello.a\n1. `CMAKE_INSTALL_PREFIX` 变量类似于 configure 脚本的 – prefix,常见的使用方法看 `cmake -DCMAKE_INSTALL_PREFIX=/usr .`\n1. `make install PREFIX=your_installation_directory`\n1. install(TARGETS name [ARCHIVE|LIBRARY|RUNTIME] [DESTINATION <dir>] ...) 安装库和可执行程序\n1. install(FILES include/quicksort.hpp DESTINATION include) 安装头文件\n1. include_directories(/usr/include/hello) 添加头文件搜索路径\n1. link_directories(dir1 dir2...) 添加链接库路径\n1. 官网不推荐使用 link_directoris，而是推荐使用 find_package 和 find_library 寻找共享库的绝对路径，再传给 target_link_libraries 使用\n1. find_library(LIBQUICKSORT_PATH quicksort ./lib)\n1. find_path(GLOG_INCLUDE_DIR glog/logging.h PATHS ${GLOG_ROOT_DIR}) 先查找路径再查找库 find_library(GLOG_LIBRARY glog PATHS ${GLOG_ROOT_DIR} PATH_SUFFIXES lib lib64)\n1. target_link_libraries(main hello) 为可执行程序添加 libhello.so 链接库\n1. add_definitions(\"-DNDEBUG\" \"-O3\") 向编译器添加编译选项\n1. set(CMAKE_CXX_FLAGS \"$(CMAKE_CXX_FLAGS) -std=c++11\") 设置 C++编译选项 等价于 add_definitons(\"-std=c++11)\n1. option 选项，让你可以根据选项值进行条件编译 option(DEBUG \"Debug mode.\" OFF)\n1. cmake --help-module FindProtobuf ；cmake --help-module FindThreads 查看模块的 cmake 用法\n1. list\n1. file\n1. string\n1. foreach\n1. if 用法\n\n```\noption(DEBUG \"Debug mode.\" OFF)\nif(DEBUG)\n    message(STATUS \"Use Debug mode\")\nelse()\n    message(STATUS \"Don't use Debug mode\")\nendif\n\n```\n\n## Concepts\n\n1. CMake is not a build system. It's the build system generator.\n2. add_subdirectory 最好只有一层，避免嵌套，只添加顶层文件夹\n\n## Target and Property\n\n1. 现代化的 CMake 是围绕 Target 和 Property 来定义的，并且竭力避免出现变量 variable 的定义。Variable 横行是典型 CMake2.8 时期的风格。现代版的 CMake 更像是在遵循 OOP 的规则，通过 target 来约束 link、compile 等相关属性的作用域。\n\n### Target\n\n1. 如果把一个 Target 想象成一个对象（Object），会发现两者的组织方式非常相似：\n\n```\n构造函数：\nadd_executable\nadd_library\n成员函数：\nget_target_property()\nset_target_properties()\nget_property(TARGET)\nset_property(TARGET)\ntarget_compile_definitions()\ntarget_compile_features()\ntarget_compile_options()\ntarget_include_directories()\ntarget_link_libraries()\ntarget_sources()\n成员变量\nTarget properties（太多）\n```\n\n### Properties\n\n1. target_xxx 命令 PRIVATE/PUBLIC...后为 property，例如\n\n```\ntarget_source(MyEXE PRIVATE \"main.cpp\")  ## main.cpp为source属性\ntarget_link_library(MyEXE PRIVATE Poco::Net Poco::Util) ## Poco:Net Poco::Util为link属性\ntarget_compile_definition(MyEXE PRIVATE std_cxx_14) ## std_cxx_14为编译属性\n```\n\n## Build-Requirements and Usage-Requirements\n\n1. `Build-Requirements`： 包含了所有构建 Target 必须的材料。如源代码，include 路径，预编译命令，链接依赖，编译/链接选项，编译/链接特性等。\n1. `Usage-Requirements`：包含了所有使用 Target 必须的材料。如源代码，include 路径，预编译命令，链接依赖，编译/链接选项，编译/链接特性等。这些往往是当另一个 Target 需要使用当前 target 时，必须包含的依赖\n\n## PRIVATE/INTERFACE/PUBLIC\n\n1. 没有指定 PUBLIC 或 PRIVATE 关键字时,CMake 会把这个链接库的属性默认设置为 PRIVATE。\n1. 定义了`Target属性`的传递范围。\n1. `PRIVATE`: 表示 Target 的属性只定义在当前 Target 中，任何依赖当前 Target 的 Target 不共享 PRIVATE 关键字下定义的属性。Build-Requirements\n1. `INTERFACE`：表示 Target 的属性不适用于其自身，而只适用于依赖其的 Target。 Usage-Requirements\n1. `PUBLIC`：表示 Target 的属性既是 build-requirements 也是 usage-requirements。凡是依赖。凡是依赖于当前 Target 的 Target 都会共享本属性。\n\n## Modern Cmake Using Steps\n\n1. Always create targets with no sources first.\n1. Use `target...` commands to add build-/usage-requirements\n1. Use `IMPORTED` targets for external libraires. But, prefer `find_package` or `EXPORTED` targets ro creating them yourself.\n\n## 3.13 之默认生成绝对路径\n\n1. 3.13 之前\n\n```\nadd_library( MyTarget SHARED )\ntarget_sources ( MyTarget\n    PRIVATE    src/A.cpp\n               src/B.cpp\n               headers/B.hpp\n    PUBLIC     ${CMAKE_CURRENT_SOURCE_DIR}/headers/A.hpp\n    INTERFACE  ${CMAKE_CURRENT_SOURCE_DIR}/headers/C.hpp\n)\n```\n\n1. 3.13 之后生成绝对路径\n\n```\nadd_library( MyTarget SHARED )\ntarget_sources ( MyTarget\n    PRIVATE    src/A.cpp\n               src/B.cpp\n               headers/B.hpp\n    PUBLIC     headers/A.hpp\n    INTERFACE  headers/C.hpp\n)\n```\n\n## CMakeLists.txt\n\n1. top level CMakeLists.txt begining:\n\n```\ncmake_minimum_required( VERSION 3.15...3.17 )\n\nset( CMAKE_PROJECT_INCLUDE_BEFORE\n     \"${CMAKE_CURRENT_LIST_DIR}/common-project-info.in\" )\n# include( \"${CMAKE_CURRENT_LIST_DIR}/common-project-include.in\" )\n\nproject (MyRootProject\n    VERSION ${project_version}\n    DESCRIPTION ${project_description}\n    HOMEPAGE_URL ${project_homepage}\n    LANGUAGES C CXX )\n\n```\n\n```\n# common-project-info.in\n\nset ( project_version 1.2.3 )\nset ( project_description \"test....\" )\nset ( project_homepage \"https://www....\" )\n```\n\n## [find_package](https://zhuanlan.zhihu.com/p/658727351)\n\n1. [path 问题](https://www.cnblogs.com/lidabo/p/16635310.html)\n1. `set(xxx_DIR ${}/lib/cmake)`需要指定 cmake 位置\n1. `find_package(xxx REQUIRED HINTS /usr/local/lib)`\n1. `export CMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu:/usr/local/lib/x86_64-linux-gnu` or `set(CMAKE_FIND_ROOT_PATH /usr/lib/x86_64-linux-gnu /usr/local/lib/x86_64-linux-gnu)`设置查找路径\n1. find_package 采用两种模式搜索库：The “better” way(Config 模式) to do things is to have an installed package provide its own details to CMake; [link](https://hsf-training.github.io/hsf-training-cmake-webpage/09-findingpackages/index.html#findpackage)\n\n   - `Module模式`：搜索 CMAKE_MODULE_PATH 指定路径下的 FindXXX.cmake 文件，执行该文件从而找到 XXX 库。其中，具体查找库并给 XXX_INCLUDE_DIRS 和 XXX_LIBRARIES 两个变量赋值的操作由 FindXXX.cmake 模块完成。\n   - `Config模式`：搜索 XXX_DIR 指定路径下的 XXXConfig.cmake 文件，执行该文件从而找到 XXX 库。其中具体查找库并给 XXX_INCLUDE_DIRS 和 XXX_LIBRARIES 两个变量赋值的操作由 XXXConfig.cmake 模块完成\n\n1. 两种模式看起来似乎差不多，不过 cmake 默认采取 Module 模式，如果 Module 模式未找到库，才会采取 Config 模式。如果 XXX_DIR 路径下找不到 XXXConfig.cmake 文件，则会找/usr/local/lib/cmake/XXX/中的 XXXConfig.cmake 文件。总之，Config 模式是一个备选策略。通常，库安装时会拷贝一份 XXXConfig.cmake 到系统目录中，因此在没有显式指定搜索路径时也可以顺利找到。\n\n## find_path and include\n\n```\nfind_path(XRT_INCLUDE_DIRS\n    NAMES xrt.h\n    PATH_SUFFIXES xrt\n    )\nmessage(\"config types: ${XRT_INCLUDE_DIRS}\")\n```\n\n## pkg-config\n\n1. 如果 find_package 找不到库，如果系统中有库的.pc 配置文件，可以使用 pkg-config 替代。\n\n```cmake\nfind_package(glog QUIET)\nif (NOT glog_FOUND)\n    find_package(PkgConfig)\n    pkg_search_module(PKG_GLOG REQUIRED IMPORTED_TARGET GLOBAL libglog)\n    add_library(glog::glog ALIAS PkgConfig::PKG_GLOG)\nendif(NOT glog_found)\n```\n\n## 组件变量\n\n## samples\n\n1. 例子 1：最基础例子\n\n```\ncmake_minimum_required(VERSION 3.8)\nproject(QUICKSORT)\naux_source_directory(. DIR_SRCS)\nadd_definitions(\"-DNDEBUG\" \"-O3\")\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11\")\nadd_executable(quicksort ${DIR_SRCS})\n```\n\n2. 例子 2： 使用静态库并安装\n\n```\ncmake_minimum_required(VERSION 3.8)\nproject(QUICKSORT)\n#aux_source_directory( DIR_SRCS)\n#add liabrary\nadd_library(quicksort\n    include/quicksort.hpp\n    src/quicksort.cpp)\ninstall(TARGETS quicksort DESTINATION lib)  #安装静态库\n\n#add target\nset(SRC_LIST src/main.cpp)\nadd_definitions(\"-DNDEBUG\" \"-O3\" \"-std=c++11\")\n#set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11\")\nadd_executable(main ${SRC_LIST})\n\n#link library\ninclude_directories(include)\ntarget_link_libraries(main quicksort)\n\n#install program\ninstall(TARGETS main RUNTIME DESTINATION bin)\n```\n\n1. 例子 3 只使用动态库\n\n```\ncmake_minimum_required(VERSION 3.8)\nproject(QUICKSORT)\n#aux_source_directory( DIR_SRCS)\n#add shared liabrary\nadd_library(quicksort SHARED\n    include/quicksort.hpp\n    src/quicksort.cpp)\ninstall(TARGETS quicksort LIBRARY DESTINATION lib)\ninstall(FILES include/quicksort.hpp DESTINATION include)\n\n#add target\nset(SRC_LIST src/main.cpp)\nadd_definitions(\"-DNDEBUG\" \"-O3\" \"-std=c++11\")\n#set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11\")\nadd_executable(main ${SRC_LIST})\n\n#link library\ninclude_directories(include)\ntarget_link_libraries(main quicksort)\n```\n\n1. 例子 4 安装静态库和动态库\n\n```\ncmake_minimum_required(VERSION 3.8)\nproject(QUICKSORT)\n#aux_source_directory( DIR_SRCS)\n#add shared liabrary\nadd_library(quicksort SHARED\n    include/quicksort.hpp\n    src/quicksort.cpp)\n#add static liabrary\nadd_library(quicksort_static STATIC\n    include/quicksort.hpp\n    src/quicksort.cpp)\nset_target_properties(quicksort_static PROPERTIES OUTPUT_NAME \"quicksort\")\n\nSET_TARGET_PROPERTIES(quicksort PROPERTIES CLEAN_DIRECT_OUTPUT 1)\nSET_TARGET_PROPERTIES(quicksort_static PROPERTIES CLEAN_DIRECT_OUTPUT 1)\ninstall(TARGETS quicksort LIBRARY DESTINATION lib)\ninstall(TARGETS quicksort_static ARCHIVE DESTINATION lib)\ninstall(FILES include/quicksort.hpp DESTINATION include)\n\n#add target\nset(SRC_LIST src/main.cpp)\nadd_definitions(\"-DNDEBUG\" \"-O3\" \"-std=c++11\")\n#set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11\")\nadd_executable(main ${SRC_LIST})\n\n#link library\ninclude_directories(include)\ntarget_link_libraries(main quicksort)\n\n# 参考 https://www.cnblogs.com/zhoug2020/p/5904206.html\n```\n\n1. 例子 5 添加 glog 库\n\n```\ncmake_minimum_required(VERSION 3.8)\nproject(QUICKSORT)\n#aux_source_directory( DIR_SRCS)\n#add shared liabrary\nadd_library(quicksort SHARED\n    include/quicksort.hpp\n    src/quicksort.cpp)\n#add static liabrary\nadd_library(quicksort_static STATIC\n    include/quicksort.hpp\n    src/quicksort.cpp)\nset_target_properties(quicksort_static PROPERTIES OUTPUT_NAME \"quicksort\")\n\nSET_TARGET_PROPERTIES(quicksort PROPERTIES CLEAN_DIRECT_OUTPUT 1)\nSET_TARGET_PROPERTIES(quicksort_static PROPERTIES CLEAN_DIRECT_OUTPUT 1)\ninstall(TARGETS quicksort LIBRARY DESTINATION lib)\n#install(TARGETS quicksort_static ARCHIVE DESTINATION lib)\ninstall(FILES include/quicksort.hpp DESTINATION include)\n\n#add target\nset(SRC_LIST src/main.cpp)\nadd_definitions(\"-DNDEBUG\" \"-O3\" \"-std=c++11\")\n#set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11\")\nadd_executable(main ${SRC_LIST})\n\n#link library\ninclude_directories(include /home/jiaxiyang/workspace/glog/lib/include) #添加glog include目录\nlink_directories(/home/jiaxiyang/workspace/glog/lib/lib) #添加glog库安装路径\ntarget_link_libraries(main quicksort glog)\ninstall(TARGETS main RUNTIME DESTINATION bin)\n```\n\n1. 例子 6 链接自己编译的库\n\n```\ncmake_minimum_required(VERSION 3.8)\nproject(QUICKSORT)\n\n#SET_TARGET_PROPERTIES(quicksort PROPERTIES CLEAN_DIRECT_OUTPUT 1)\n#SET_TARGET_PROPERTIES(quicksort_static PROPERTIES CLEAN_DIRECT_OUTPUT 1)\n#install(TARGETS quicksort LIBRARY DESTINATION lib)\n#install(TARGETS quicksort_static ARCHIVE DESTINATION lib)\n#install(FILES include/quicksort.hpp DESTINATION include)\n\n#add target\nset(SRC_LIST src/main.cpp)\nadd_definitions(\"-DNDEBUG\" \"-O3\" \"-std=c++11\")\n#set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11\")\nadd_executable(main ${SRC_LIST})\n\n#link library\ninclude_directories(/home/jiaxiyang/workspace/algorithm/quicksort1/include /home/jiaxiyang/workspace/glog/lib/include) #添加glog include目录\nlink_directories(/home/jiaxiyang/workspace/glog/lib/lib) #添加glog库安装路径\n#link_directories(/home/jiaxiyang/workspace/algorithm/quicksort1/lib) #添加glog库安装路径   如果没有find_library，出现在add_executable后面会出错 建议用find_library\nfind_library(LIBQUICKSORT_PATH quicksort ./lib)\nMESSAGE(STATUS ${LIBQUICKSORT_PATH} \" found\")\n\ntarget_link_libraries(main ${LIBQUICKSORT_PATH} glog)\ninstall(TARGETS main RUNTIME DESTINATION bin)\n\n```\n\n## Links\n\n1. [More Modern CMake](https://hsf-training.github.io/hsf-training-cmake-webpage/)\n1. [参考文档: 构建百科全书， 测试百科全书，命令行参考文档，查询参考文档](https://bazel.build/reference?hl=zh-cn)\n1. [Cmake Tutorial](https://cmake.org/cmake/help/latest/guide/tutorial/index.html)\n1. [Cmake Forum](https://discourse.cmake.org/)\n1. [Professional CMake pdf](https://github.com/jiaxiyang/documents/blob/master/pcmake.pdf)\n1. [Cmake Buildsystem](https://cmake.org/cmake/help/latest/manual/cmake-buildsystem.7.html)\n1. [Effective Modern Cmake](https://gist.github.com/mbinna/c61dbb39bca0e4fb7d1f73b0d66a4fd1)\n1. [Effective Modern Cmake video](https://www.youtube.com/watch?v=bsXLMQ6WgIk)\n1. [Deniz Bahadir 2019](https://www.youtube.com/watch?v=y9kSr5enrSk)\n1. [Deniz Bahadir 2018 traditional and modern camke](https://www.youtube.com/watch?v=y7ndUhdQuU8)\n1. [Beniz Bahadir PPT](https://github.com/Bagira80/More-Modern-CMake)\n1. [OO Cmake](https://zhuanlan.zhihu.com/p/76975231)\n1. [Cmake Concept](https://ukabuer.me/blog/more-modern-cmake)\n","tags":["Cmake"],"categories":["Tools","Cmake"]},{"title":"Cpp-concurrency-in-action","url":"/2021/07/20/Cpp-concurrency-in-action/","content":"\n## Notes\n\n1. [C++ Concurrency in action 2](https://b-ok.lat/book/3688262/d57395?dsource=recommend&regionChanged=&redirect=4729105)\n1. [C++ Concurrency in action Chinese](https://github.com/xiaoweiChen/CPP-Concurrency-In-Action-2ed-2019)\n1. 多线程写共享数据时，需要加锁`const std::lock_guard<std::mutex> lock(results_len_mutex_);`\n1. 异步程序并行度跟有几个耗时的模块相关，例如：yolox 推理和后处理最耗时，异步并行度设为 2，做当前帧的后处理同时做下一帧的模型推理。\n1. 异步需要考虑 timeout 问题， 可以使用 `condition_variable`\n1. . 流控：\n\n```c++\nconst auto time_start = std::chrono::steady_clock::now();\nconst auto time_end = std::chrono::steady_clock::now();\nauto cost_ms =\n    std::chrono::duration_cast<std::chrono::milliseconds>(time_end - time_start);\nstd::this_thread::sleep_for(\n    std::chrono::milliseconds(static_cast<int>(1000.0 / fps)) - cost_ms);\n```\n\n## 1. Hello, world of concurrency in C++\n\n### 1.1 并发是什么\n\n1. 多进程并发优点： 操作系统在进程间提供的保护操作和更高级别的通信机制，可以更容易编写安全的并发代码。还有一个优势，可以使用远程连接的方式，在不同的机器上运行独立的进程。\n\n### 1.2 为什么使用并发\n\n1. `分离程序`： 将相互独立的部分分开就行操作。如 DVD 用户界面和播放功能应分离。\n1. `性能`：两种方式：任务并行（将单一任务分成几部分并行）和数据并行（每个线程在不同的数据块上执行相同的操作）\n\n#### 1.2.1 什么时候不使用并发\n\n1. 收益比不上成本，除非潜在性能足够大，否则不使用并发，需要脑力成本和维护成本，代码更复杂，难以理解。\n1. 资源有限： 太多线程的会耗尽资源。可以使用线程池进行优化。太多的线程上下文切换导致的性能损耗可能比增加线程的收益更多。\n\n### 1.3 并发和多线程\n\n#### 1.3.1 C++多线程历史\n\n1. C++98 标准不支持多线程，没有内存模型。\n1. 各种非标准多线程库：MFC，Boost，大部分使用的是 RAII\n\n#### 1.3.2 支持并发\n\n1. C++11: `线程管理，共享数据保护，线程间同步操作，原子操作`\n1. C++11 之后，C++标准支持多线程，也就是说可以跨平台编写高效，可移植的代码。编译器可以搞定具体平台，用户无需担心。\n\n#### 1.3.4 C++线程库的效率\n\n1. 高级工具（使用高级 API, 抽象高）和低级工具（使用低级 API，抽象低)有开销差，即抽象代价（abstraction penalty)。C++标准库设计时尽量使得高级 API 和低级 API 具有相同的性能。\n1. 低级工具：为了达到终极性能，需要提供给硬件打交道的低级 API。为了这个目的，形成了原子操作库。\n1. 高级工具：为了使得编写多线程代码更简单。因为有额外的代码需要执行，这些工具会带来性能开销。\n1. 如果很看重性能或者高级工具开销过高，可以通过低层工具来实现。绝大多数情况，有过高的复杂度和过大的出错率，来交换小幅度的性能收益是不划算的。\n\n## 2. Managing threads\n\n1.  文件锁用在进程之间，线程之间锁不住，文件锁是应用于进程中的一种锁，由于多线程是通过修改文件描述符指向的文件结构体中的成员变量实现，所以文件锁不能用于线程之间。 见[fslock](https://docs.rs/fslock/0.1.6/fslock/struct.LockFile.html)\n\n### 2.5 线程标识\n\n1. `std::this_thread::get_id()`获取当前线程的 ID.\n\n## 3. Sharing data between threads\n\n1. 错误的数据共享是多线程产生 bug 的主要原因。\n1. `data race`: the specific type of race condition that arises because of concurrent modification to a single object\n1. 数据竞争时间敏感，Bug 可能很难复现，很难查找。\n\n### 3.2 使用互斥量\n\n1. 访问共享数据前将数据锁住，访问结束将数据解锁。\n1. 互斥量是 C++保护数据最通用的机制。\n1. 互斥量问题： 死锁，对数据保护太多。\n1. 一个互斥量只能用于一个资源的互斥访问。\n1. 互斥量(互斥锁)本质上是一把锁。\n\n#### 3.2.1 互斥量\n\n1. `std::mutex`创建互斥量，`lock`对互斥量上锁，unlock 为解锁。\n1. RAII 管理互斥量：C++标准库为互斥量提供了 RAII 模板类`std::lock_guard`，在构造时提供已锁的互斥量（lock_guard 对象在构造时对传进来的 mutex 上锁），并在析构时解锁，从而保证互斥量被正确的解锁。\n1. 大多数情况下，互斥量通常会与需要保护的数据放在同一类中，而不是定义成全局变量。\n1. C++17 提供`std::scoped_lock`加强版的`std::lock_guard`, 可以接受多个参数。\n\n#### 3.2.4 死锁\n\n1. 死锁一般解决方法：按顺序上锁。要么将两个都锁住，要么一个都不锁。\n1. `std::lock`能同时锁住多个互斥量。\n1. C++17 中的`std::scoped_lock<>`是一种新的 RAII 模板类型。能接受不定数量的互斥量类型作为模板参数。在构造 scoped_lock 对象时对传入的 mutex 上锁，析构时解锁。\n1. `std::lock`锁上 mutex 之后不负责释放，还需要配合 lock_guard 或者手动释放 mutex。`std::scoped_lock`生命周期结束后会自动释放锁。\n1. C++17 特性：`自动推导模板参数`。可以简写为`std::scoped_lock lk(m1, m2);` 而不用写为`std::scoped_lock<std::mutex, std::mutex> lk(m1, m2);`\n\n### atomic\n\n1. 从效率上来说，原子操作要比互斥量的方式效率要高。\n1. 互斥量的加锁一般是针对一个代码段，而原子操作针对的一般都是一个变量。\n1. store 和 load 来赋值和读取， 除了初始化不要使用赋值\n1. [atomic 不一定免锁](https://zh.cppreference.com/w/cpp/atomic/atomic/is_lock_free)\n1. 对结构体需要链接 `-latomic`\n1. `-fsanitize=thread`可以检测数据竞争\n\n## 4. Synchronizing concurrent operations on atomic types\n\n### 4.1 等待时间或条件\n\n1. 当一个线程等待另一个线程完成时，有几种选择：\n   - 持续检查共享数据标志，浪费资源查询\n   - 在等待线程时，使用`std::this_thread::sleep_for()`进行周期性间歇，节省执行时间，问题：很难确定休眠时间\n   - 通过标准库提供的工具。例如：`condition variable`\n\n#### [condition_variable](https://blog.csdn.net/wxj1992/article/details/116888582)\n\n1. condition*variable 是一个类，搭配互斥量 mutex 来用，这个类有它自己的一些函数，这里就主要讲 wait 函数和 notify*\\*函数，故名思意，wait 就是有一个等待的作用，notify 就是有一个通知的作用。主要用法这里就不再赘述了，简而言之就是程序运行到 wait 函数的时候会先在此阻塞，然后自动 unlock，那么其他线程在拿到锁以后就会往下运行，当运行到 notify_one()函数的时候，就会唤醒 wait 函数，然后自动 lock 并继续下运行。\n1. wait 线程：当 condition 不满足的时候需要等待，直到条件满足了才做自己的事情。\n1. notify 线程：做完自己的事情后修改 condition 变量并唤醒等待线程。\n1. wait 时未持有锁，wait 之后占有锁\n1. wait 会先判断条件，如果满足，直接返回。\n1. wait_for() 解阻塞时获得 lock， 无论是 notify_all() 或 notify_one() 还是超时都会解阻塞。\n1. `wait_for() 若经过 rel_time 时限后谓词 pred 仍求值为 false 则为 false ，否则为 true` 中途被通知后 pred 为 true 则直接返回 true，如果为 false，继续等待。\n1. 先获得锁，再检测条件？\n1. 解锁之后再 notify， 否则可能出现当前线程还未释放锁，被接收的线程被唤醒后拿不到锁立即就又休眠。\n\n### 4.2 future\n\n1. async（sender)函数是一种 promise,负责发送数据， future（receiver)负责接收数据。`future fut = async([]{return 3+4;}); cout << fut.get() << endl;`\n\n## 5. The C++ memory model and operations on atomic types\n\n### 5.1 内存模型\n\n#### 5.1.1 对象和内存模型\n\n1. C++程序中数据都是由对象构成。\n1. 每个变量都是对象\n1. 每个对象至少占有一个内存位置。\n\n#### 5.1.2 对象，内存位置和并发\n\n1. 当多个线程访问同一内存位置时，如果只读数据，不需要保护，如果有线程对数据进行修改，就可能会产生条件竞争。\n1. 为了避免条件竞争，两个线程就要以一定的`顺序执行`。有两种方式：`互斥量`和`原子操作`\n\n### 5.2 原子操作和原子类型\n\n1. `原子操作`是不可分割的操作。系统的所有线程中，不可能观察到原子操作完成了一半，要么是做了，要么是没做，只有两种可能。非原子操作可能会被另一线程观察到只完成一半，如果是一个存储操作，那么其他线程看到的值，可能既不是存储前的值，也不是存储后的值。\n1. 原子操作是一种操作（函数），互斥量（一种锁）是实现这种操作的一种方式。\n\n#### 5.2.1 标准的原子类型\n\n1. 标准的原子类型定义在头文件`<atomic>`中，这些类型操作都是原子的。\n1. 可以使用互斥量来模拟原子操作。\n1. 原子操作可以替代互斥量来完成同步操作。如果操作内部使用互斥量实现，那么不可能有性能提升，可以使用无锁数据结构来实现。\n1. 通常标准的原子类型是不能进行拷贝和赋值的，他们没有拷贝构造函数和拷贝赋值函数。但是可以隐式的转化为内置类型进行赋值。\n\n## 6. Designing lock-based concurrent data structures.\n\n## 7. Designing lock-free concurrent data structures\n\n## 8. Designing concurrent code\n\n## 9. Advanced thread managment\n\n## 10. Parallel algorithms\n\n## 11. Testing and debugging multithreaded applications\n\n## others\n\n### 什么是临界区\n\n1. 临界区(critical section)是指包含有共享资源的一段代码，这些代码可能被多个线程访问 或修改。 临界区的存在就是为了保证当有一个线程在临界区内执行的时候，不能有其他任何 线程被允许在临界区执行。\n","tags":["Cpp","Concurrency"],"categories":["Program","Cpp"]},{"title":"Rust grammar","url":"/2021/07/11/Rust-grammar/","content":"\n## [顶层概念](https://www.infoq.cn/article/Uugi_eIJusEka1aSPmQM)\n\n![概念层次图](https://static001.infoq.cn/resource/image/2d/91/2d36b45f6905a13a310e6447778ca391.jpg)\n\n<!-- more -->\n\n## Grammar\n\n1. unimplemented! and todo!来表明函数未实现，先跳过\n1. indent with four spaces, not a tab\n1. Most lines of Rust code end with a semicolon(;)\n1. `println!()` ! means call macro, not a function\n1. Cargo is Rust's build system and package manager.\n1. Cargo expects your source files to live inside the src directory. The top-level project directory is just for README files, license information, configuration files, and anything else not related to your code.\n1. `cargo run` Compile the code and then run the resulting executable all in one command.\n1. `cargo check` is much faster than `cargo build`\n1. `cargo build --release` compile the project with optimizations. Run faster but compile slower.\n1. The `main` function is the entry point into the program.\n1. The `fn` syntax declares a new function, the parentheses, `()`, indicate there are no parameters, and the curly bracket, `{`, starts the body of the function.\n1. `let` is used to create a variable. In Rust, variables are immutable by default.\n1. `String::new()` An associated function is implemented on a type, in this case String, rather than on a particular instance of a String. Some languages call this a static method. This new function creates a new, empty string. You’ll find a new function on many types, because it’s a common name for a function that makes a new value of some kind.\n1. `{}` is a place holder when using `println!`\n1. Crate is a coleection of Rust source code files.\n1. Cargo.toml dependencies rand = \"0.5.5\"; In this case, we’ll specify the rand crate with the semantic version specifier 0.5.5. Cargo understands Semantic Versioning (sometimes called SemVer), which is a standard for writing version numbers. The number 0.5.5 is actually shorthand for ^0.5.5, which means “any version that has a public API compatible with version 0.5.5.”\n1. `cargo doc --open` It will build documentation provided by all of your dependencies locally and open it in your browser.\n1. `match` is like `switch` in C++\n1. shallow copy, deep copy(clone), move(let s2 = s1; // s1 is invailid)\n1. Differences Between Immutable Variables and Constants:\n   - You declare constants using the const keyword instead of the let keyword, and the type of the value must be annotated.\n   - Constants can be declared in any scope, including the global scope\n   - Constants may be set only to a constant expression, not the result of a function call or any other value that could only be computed at runtime.\n1. trait can add some useful functions for user's own struct.\n1. trait 可以与泛型结合来将泛型限制为拥有特定行为的类型，而不是任意类型。\n1. Rust 编译时会将泛型代码单态化(monomorphization)来保证效率，单态化是指编译时用具体类型来填充泛型。\n1. 用户会将重复代码泛化（抽象），编译器会将泛化代码具体化。\n1. trait 类似其他语言中接口(interfaces)功能，虽然有些不同。\n1. 只有当 trait 或者要实现 trait 的类型位于 crate 的本地作用域时，才能为该类型实现 trait。\n1. 在 release 构建中，Rust 不检测溢出，相反会进行一种被称为二进制补码包装（two’s complement wrapping）的操作。当在 debug 模式编译时，Rust 检查这类问题并使程序 panic\n1. Rust 的浮点数默认类型是 f64。数字类型默认是 i32。\n1. Rust 的 char 类型的大小为四个字节(four bytes)，并代表了一个 Unicode 标量值（Unicode Scalar Value），这意味着它可以比 ASCII 表示更多内容。在 Rust 中，拼音字母（Accented letters），中文、日文、韩文等字符，emoji（绘文字）以及零长度的空白字符都是有效的 char 值。\n1. Rust 有两个原生的复合类型：元组（tuple）和数组（array）。\n1. `tuple:` `let tup: (i32, f64, u8) = (500, 6.4, 1); let (x, y, z) = tup; //结构 let five_hundred = tup.0;`\n1. `struct:`\n\n```rust\nstruct User {\n    username: String,\n    email: String,\n}\n\nlet mut user1 = User {\n    email: String::from(\"someone@example.com\"), //实例中字段的顺序不需要和它们在结构体中声明的顺序一致\n    username: String::from(\"someusername123\"),\n};\n\nuser1.email = String::from(\"anotheremail@example.com\");\n```\n\n1. 我们也可以定义一个没有任何字段的结构体！它们被称为 类单元结构体（unit-like structs）因为它们类似于 ()，即 unit 类型。类单元结构体常常在你想要在某个类型上实现 trait 但不需要在类型中存储数据的时候发挥作用。\n1. 方法与函数类似：它们使用 fn 关键字和名称声明，可以拥有参数和返回值，同时包含在某处调用该方法时会执行的代码。不过方法与函数是不同的，因为它们在结构体的上下文中被定义\n\n```rust\n#[derive(Debug)]\nstruct Rectangle {\n    width: u32,\n    height: u32,\n}\n\nimpl Rectangle {\n    fn area(&self) -> u32 {\n        self.width * self.height\n    }\n}\n```\n\n1. impl 块的另一个有用的功能是：允许在 impl 块中定义 不 以 `self` 作为参数的函数。这被称为 `关联函数（associated functions）`，因为它们与结构体相关联。它们仍是函数而不是方法，因为它们并不作用于一个结构体的实例。你已经使用过 String::from 关联函数了。使用结构体名和 `::` 语法来调用关联函数。\n\n## Project Manage\n\n1. 包`packages`: Cargo 的一个功能，允许你构建、测试和分享 crate\n1. `crates`: 一个模块的`树形`结构，它形成了库或二进制项目。\n1. 模块`modules and use`: 允许你控制作用域和路径的私有性。\n1. 路径`path`: 一个命名机结构体、函数或模块等项的方式。\n1. 在 Rust 中，代码包也被称为 crates\n1. 一个包中可以包含多个二进制 crate 和一个可选的 crate 库。\n1. 包中至少包含一个 crate，无论是库还是二进制，至多包含一个库 crate\n1. `cargo new`会创建一个包。\n1. Cargo 准守一个约定： `src/main.rs`就是一个与包同名的二进制 crate 的 crate 根。同理，`src/lib.rs`是库 crate 的 crate 根。如果同时包含，则它有两个 crate。\n1. 通过将文件放在`src/bin`目录下，一个包可以拥有多个二进制 crate：每个`src/bin`下的文件都会被编译成一个独立的二进制 crate。\n1. 模块让我们可以将一个 crate 中的代码进行分组，以提高重用性。\n1. `模块的定义`是以`mod`(module)关键字为起始，然后指定模块的名字，并且用或花括号包围模块的主题。\n1. `模块的声明`是以`mod`为起始，指定模块名字，以`;`结尾，不带有花括号。在与模块名同名的.rs 文件中定义该模块。\n1. 模块一般是在上一级文件声明，不是在本文件中声明，如可以在 lib.rs 中声明模块，然后在相应.rs 文件中定义该模块。\n1. crate 的组织结构类似于文件系统的目录，被称为模块树(module tree)。\n1. Rust 通过路径来查找一个项的位置。\n1. `绝对路径(absolute path)` 从 crate 根开始，以 crate 名或者字面值`crate`开头。\n1. `相对路径(relative path)` 从当前模块开始，以`self`、`super`或当前模块的表示符开头。\n1. 只有在同一 crate 中才能使用`crate::`关键字为起始的绝对路径。\n1. 模块不仅对你组织代码很有用。它们还定义了 Rust 的私有性边界：这条界限不允许外部代码了解、调用和依赖被封装的实现细节。\n1. Rust 中默认所有项(函数、方法、结构体、枚举、模块、和常量）都是私有的。\n1. 项是指：函数、方法、结构体、枚举、模块和常量。\n1. 父模块中的项不能使用子模块中的私有项，但是子模块可以使用它们父模块中的项。\n1. Rust 通过这种方式来实现模块系统功能，因此隐藏内部实现细节，这样你就知道可以更改内部代码的哪部分而不会破坏外部代码。\n1. 可以使用`super`开头来构建父模块开始的相对路径。类似于文件系统的`..`语法。\n1. 如果请偶们将枚举设置为公有，则它的所有成员都将变为共有。\n1. 在作用域中增加`use`类似于在文件系统中创建软连接。\n1. 使用`use`引入结构体、枚举和其他项时，习惯是指定它们的完整路径。\n1. 使用`use`将两个同名类型引入同意作用域的一个解决办法：在类型后面使用`as`指定一个新的本地名称或者别名。\n1. `use std::{cmp::Ordering, io};`使用嵌套路径减少 use 的使用。\n1. `use std::io:*` 将`std::io`中定义的所有公有项引入当前作用域。使用时要多加小心，常用于测试模块`tests`中。\n1. Rust 将 package 分成多个 crate，将 crate 分成模块，通过绝对或相对路径从一个模块引用另一个模块。\n1. Cargo 提供了叫工作空间`workspaces`的功能，它可以帮助我们管理多个相关的协同开发的包。\n1. 工作空间是一系列共享同样的 cargo.lock 和输出目录的包。\n1. 工作空间顶级目录中的 Cargo.toml 中不包含`[package]`等信息，相反，它以`[workspace]`部分作为开始。\n1. 工作空间在顶级目录有一个`target`目录，`member`并没有自己的 target 目录。通过共享的 target 目录，工作空间可以避免其他 crate 多余的重复构建。\n1. cargo 不假定工作空间中的 crates 包会相互依赖，所以需要明确表明工作空间中 crate 包的依赖关系。一个包用到了其他包，需要在该包的 Cargo.toml 文件`[dependencies]`域中加入依赖\n1. 工作空间只在根目录有一个 Cargo.lock，而不是在每一个 crate（就当是 packge)目录都有 Cargo.lock。这确保了所有的 crate 都使用完全相同版本的依赖。也节省了空间，避免多个拷贝。\n1. Cargo 有一个机制来确保任何人在任何时候重新构建代码，都会产生相同的结果：Cargo 只会使用你指定的依赖版本，除非你又手动指定了别的。\n\n## 类型系统\n\n1. [类型大小](https://zhuanlan.zhihu.com/p/21820917)\n2. Rust 中有一个重要的 trait Sized，可以用于区分一个类型是不是 DST。所有的 DST 类型都不满足 Sized 约束。我们可以在泛型约束中使用 Sized、!Sized、?Sized 三种写法。\n3. Rust 还支持 0 大小类型（Zero Sized Type）。 () 类型和空结构体类型，都是 0 大小类型。\n\n## Ownership 所有权\n\n1. 区分值(内存)和变量。\n1. 所有权是指： 每一个值都只有一个所有者（变量）, 当变量离开作用于时，值被丢弃。\n1. 当变量离开作用域，Rust 为我们调用一个特殊的函数 drop。在 C++ 中，这种 item 在生命周期结束时释放资源的模式有时被称作 资源获取即初始化（Resource Acquisition Is Initialization (RAII)）。\n1. `move:` `let s1 = String::from(\"hello\"); let s2 = s1;` 这个操作被称为移动（move），而不是浅拷贝。Rust 则认为 s1 不再有效，因此 Rust 不需要在 s1 离开作用域后清理任何东西。\n1. Rust 永远也不会自动创建数据的 “深拷贝”。因此，任何 自动 的复制可以被认为对运行时性能影响较小。\n1. `clone:` 当出现 clone 调用时，你知道一些特定的代码被执行而且这些代码可能相当消耗资源\n1. `copy:` `let x = 5; let y = x;` x 在栈上，copy 操作。Rust 有一个叫做 Copy trait 的特殊注解，可以用在类似整型这样的存储在栈上的类型上。\n1. 如果一个类型拥有 Copy trait，一个旧的变量在将其赋值给其他变量后仍然可用。如 String 没有实现 Copy trait, i32 实现了。\n1. Copy and Clone: Copy 是隐性调用，Clone 显性。 Copies happen implicitly, for example as part of an assignment y = x. The behavior of Copy is not overloadable; it is always a simple bit-wise copy. Cloning is an explicit action, x.clone()\n1. [link](https://doc.rust-lang.org/beta/std/marker/trait.Copy.html) Clone is a supertrait of Copy, so everything which is Copy must also implement Clone. If a type is Copy then its Clone implementation only needs to return \\*self\n1. Rust 不允许自身或其任何部分实现了 Drop trait 的类型使用 Copy trait。\n1. 任何简单标量值的组合可以是 Copy 的，不需要分配内存或某种形式资源的类型是 Copy 的。\n\n```rust\nfn main() {\n    let s = String::from(\"hello\");  // s 进入作用域\n\n    takes_ownership(s);             // s 的值移动到函数里 ...\n                                    // ... 所以到这里不再有效\n\n    let x = 5;                      // x 进入作用域\n\n    makes_copy(x);                  // x 应该移动函数里，\n                                    // 但 i32 是 Copy 的，所以在后面可继续使用 x\n} // 这里, x 先移出了作用域，然后是 s。但因为 s 的值已被移走，\n  // 所以不会有特殊操作\n\nfn takes_ownership(some_string: String) { // some_string 进入作用域\n    println!(\"{}\", some_string);\n} // 这里，some_string 移出作用域并调用 `drop` 方法。占用的内存被释放\n\nfn makes_copy(some_integer: i32) { // some_integer 进入作用域\n    println!(\"{}\", some_integer);\n} // 这里，some_integer 移出作用域。不会有特殊操作\n```\n\n1. 变量的所有权总是遵循相同的模式：将值赋给另一个变量时移动它。当持有堆中数据值的变量离开作用域时，其值将通过 drop 被清理掉，除非数据被移动为另一个变量所有。\n1. 将获取引用作为函数参数称为 借用（borrowing）。\n1. 可变引用有一个很大的限制：在特定作用域中的特定数据只能有一个可变引用。\n1. 也不能在拥有不可变引用的同时拥有可变引用。\n1. Rust 中编译器确保引用永远也不会变成悬垂状态：当你拥有一些数据的引用，编译器确保数据不会在其引用之前离开作用域。\n1. 在任意给定时间，要么 只能有一个可变引用，要么 只能有多个不可变引用。\n1. `另一个没有所有权的数据类型是 slice(部分引用 &[T])。slice 允许你引用集合中一段连续的元素序列，而不用引用整个集合`。\n1. “字符串 slice” 的类型声明写作 &str, 字符串字面值就是 slice\n1. [胖指针](https://zhuanlan.zhihu.com/p/76945648)\n\n## Enums 枚举 and 模式匹配\n\n1. 枚举允许你通过列举可能的 成员（variants） 来定义一个类型\n1. 用枚举替代结构体还有另一个优势：每个成员可以处理不同类型和数量的数据。\n\n```rust\nenum IpAddr {\n    V4(u8, u8, u8, u8),\n    V6(String),\n}\n\nlet home = IpAddr::V4(127, 0, 0, 1);\n\nlet loopback = IpAddr::V6(String::from(\"::1\"));\n```\n\n1. 结构体和枚举还有另一个相似点：就像可以使用 impl 来为结构体定义方法那样，也可以在枚举上定义方法\n1. `Option` 类型应用广泛因为它编码了一个非常普遍的场景，即一个值要么有值要么没值。\n1. Rust 并没有空值，不过它确实拥有一个可以编码存在或不存在概念的枚举。这个枚举是 Option<T>，而且它定义于标准库中，如下:\n\n```rust\nenum Option<T> {\n    Some(T),\n    None,\n}\n```\n\n1. Option<T> 枚举是如此有用以至于它甚至被包含在了 prelude 之中，你不需要将其显式引入作用域。另外，它的成员也是如此，可以不需要 Option:: 前缀来直接使用 Some 和 None。即便如此 Option<T> 也仍是常规的枚举，Some(T) 和 None 仍是 Option<T> 的成员。\n1. 只要一个值不是 Option<T> 类型，你就可以安全的认定它的值不为空。\n1. Rust 有一个叫做 match 的极为强大的控制流运算符，它允许我们将一个值与一系列的模式相比较，并根据相匹配的模式执行相应代码。模式可由字面值、变量、通配符和许多其他内容构成\n\n```rust\nenum Coin {\n    Penny,\n    Nickel,\n    Dime,\n    Quarter,\n}\n\nfn value_in_cents(coin: Coin) -> u8 {\n    match coin {\n        Coin::Penny => 1,\n        Coin::Nickel => 5,\n        Coin::Dime => 10,\n        Coin::Quarter => 25,\n    }\n}\n```\n\n1. 对于 if, 表达式必须返回一个布尔值，而 match 可以是任何类型的。\n1. 一个分支有两个部分：一个模式和一些代码。第一个分支的模式是值 Coin::Penny 而之后的 => 运算符将模式和将要运行的代码分开。这里的代码就仅仅是值 1。每一个分支之间使用逗号`,`分隔。\n1. 如果想要在分支中运行多行代码，可以使用大括号`{}`。\n1. 匹配 Option<T>:\n\n```rust\nfn plus_one(x: Option<i32>) -> Option<i32> {\n    match x {\n        None => None,\n        Some(i) => Some(i + 1),\n    }\n}\n\nlet five = Some(5);\nlet six = plus_one(five);\nlet none = plus_one(None);\n```\n\n1. Rust 中的匹配是 穷尽的（exhaustive）：必须穷举到最后的可能性来使代码有效。\n1. `_`通配符：\n\n```rust\nlet some_u8_value = 0u8;\nmatch some_u8_value {\n    1 => println!(\"one\"),\n    3 => println!(\"three\"),\n    5 => println!(\"five\"),\n    7 => println!(\"seven\"),\n    _ => (),\n}\n```\n\n1. if let 语法让我们以一种不那么冗长的方式结合 if 和 let，来处理只匹配一个模式的值而忽略其他模式的情况。\n\n```rust\n\nlet some_u8_value = Some(0u8);\nmatch some_u8_value {\n    Some(3) => println!(\"three\"),\n    _ => (),\n}\n\nif let Some(3) = some_u8_value {\n    println!(\"three\");\n}\n```\n\n1. 使用 if let 意味着编写更少代码，更少的缩进和更少的样板代码。然而，这样会失去 match 强制要求的穷尽性检查。match 和 if let 之间的选择依赖特定的环境以及增加简洁度和失去穷尽性检查的权衡取舍。\n1. 可以在 if let 中包含一个 else。else 块中的代码与 match 表达式中的 \\_ 分支块中的代码相同\n\n```rust\nlet mut count = 0;\nif let Coin::Quarter(state) = coin {\n    println!(\"State quarter from {:?}!\", state);\n} else {\n    count += 1;\n}\n```\n\n1. 如果你的程序遇到一个使用 match 表达起来过于啰嗦的逻辑，记住 if let 也在你的 Rust 工具箱中。\n\n## 常见集合 collections\n\n1. 不同于内建的数组和元组类型，这些集合指向的数据是储存在堆上的，这意味着数据的数量不必在编译时就已知，并且还可以随着程序的运行增长或缩小。\n1. vector 允许我们在一个单独的数据结构中储存多于一个的值，它在内存中彼此相邻地排列所有的值。vector 只能储存相同类型的值。\n1. `let v: Vec<i32> = Vec::new();` 创建空的 vector\n1. 为了方便 Rust 提供了 vec! 宏。这个宏会根据我们提供的值来创建一个新的 Vec。`let v = vec![1, 2, 3];`\n1. 对于新建一个 vector 并向其增加元素，可以使用 push 方法\n\n```rust\nlet mut v = Vec::new();\n\nv.push(5);\nv.push(6);\n```\n\n1. 访问 vector 中一个值的两种方式，索引语法或者 get 方法：\n\n```rust\nlet v = vec![1, 2, 3, 4, 5];\n\nlet third: &i32 = &v[2];\nprintln!(\"The third element is {}\", third);\n\nmatch v.get(2) {\n    Some(third) => println!(\"The third element is {}\", third),\n    None => println!(\"There is no third element.\"),\n}\n```\n\n1. 使用 & 和 [] 返回一个引用；或者使用 get 方法以索引作为参数来返回一个 Option<&T>。\n\n## Error Handling 错误处理\n\n1. 当函数返回为 Result 时: `?`运算符放在 Result 之后的含义：如果 Result 的值是 Ok，这个表达式将会返回 Ok 中的值而程序继续执行，如果是 Err，Err 中的值将作为整个函数的返回值，就好像使用了 return 关键字一样，这样错误值就被传递给调用者。\n1. `cargo clippy` 报更多的错误\n1. Rust 将错误组合成两个主要类别，可恢复错误和不可恢复错误。\n1. 可恢复错误通常代表向用户报告错误和重试操作是合理的情况，比如未找到文件。\n1. 不可恢复错误通常是 Bug 的同义词，比如访问超过数组结尾的位置。\n1. 大部分语言并不区分这两类错误，并采用类似异常这样方式统一处理他们。\n1. Rust 没有异常，但是有可恢复错误`Result<T, E>`和不可恢复错误`panic!`\n1. 执行 Rust 的`panic!`宏时，程序会打印出一个错误信息，展开并清理栈数据，然后接着退出。\n1. 出现 panic 时，程序默认是`展开(unwinding)`，这意味着 Rust 会回溯栈并清理它遇到的每一个函数的数据。另一种选择是`终止(abort)`，这回不清理数据就退出程序。可以在 Cargo.toml 的[profile]部分增加`panic = 'abort'`，可以由展开切换为终止。\n1. 使用`RUST_BACKTRACE`环境变量运行程序会得到一个 backtrace，backtrace 是一个执行到目前位置所有被调用的函数的列表。\n1. Rust 的 backtrace 跟其他语言一样：阅读 backtrace 的`关键`是`从头开始读直到发现你自己编写的代码`，这就是问题的根源。`这一行往上是你的代码所调用的代码，往下则是调用你的代码的代码(栈)`\n1. 为了获取带有详细信息的 backtrace，`必须是debug模式`\n1. Result 枚举的定义。`enum Result<T, E> { Ok(T), Err(E), }`，其中`T`代表返回的`Ok`成员中的`数据类型`，而`E`代表失败是返回`Err`成员中的错误的类型。\n1. Result 常与 match 进行联合使用\n\n```rust\nuse std::fs::File;\n\nfn main() {\n    let f = File::open(\"hello.txt\"); // f值是Ok(file)或者是Err(error)\n\n    let f = match f {\n        Ok(file) => file,\n        Err(error) => {\n            panic!(\"Problem opening the file: {:?}\", error)\n        },\n    };\n}\n```\n\n1. `unwrap`函数作用于 Result，如果 Result 的值是成员 OK，unwrap 会返回 Ok 中的值，如果是成员 Err，unwrap 会为我们调用`panic!`\n1. `expect`与`unwrap`使用方式一样，`expect`用来调用`panic!`的错误信息将会作为参数传递给`expect`，而不像`unwrap`那样使用默认的`panic!`信息。`expect`更容易查找错误信息位置。\n1. 每一个 Result 都需要被处理，否则会出现警告。\n\n## Lifetimes 生命周期\n\n1. [link](https://skyline75489.github.io/post/2017-7-27_rust_mm.html) rust 生命周期包含变量和引用，C++ RAII 主要是变量的生命周期管理，引用除了 smart pointer C++不负责检查引用的对象是否存在\n1. 生命周期的目的是**避免悬垂指针**. 生命周期帮助编译器执行一个简单的规则：引用不应该活得比所指对象长(no reference should outlive its referent)。\n1. 生命周期之所以如此令人困惑，其部分原因在于 Rust 的很多写作中，生命周期这个词被宽泛地用来指代三种不同的东西——`变量真实的生命周期、生命周期约束和生命周期标注`。 [link](https://juejin.cn/post/6862976475860041741)\n1. 只是用来检查生命周期关系，不影响生命周期\n1. Lifetime annotations don’t change how long any of the references live. Lifetime annotations describe the relationships of the lifetimes of multiple references to each other without affecting the lifetimes.\n1. One lifetime annotation by itself doesn’t have much meaning,\n1. `longest<'a>(x: &'a str, y: &'a str) ->&'a str` 它的实际含义是 longest 函数保证返回的引用的生命周期与传入该函数的引用的生命周期的较小者一致（x 作用域和 y 作用域重叠的那一部分）。并没有改变传入值或返回值的生命周期，而是指出任何不满足这个约束条件的值都将被借用检查器拒绝。\n1. `struct Test<'a> { part: &'a str, }` Test 的实例不能比 part 字段中的引用存在的更久，Test 实例应先离开作用域。（后定义的存在的应更短）\n1. 每一个`引用`都有一个生命周期，我们需要为那些使用了引用的函数或者结构体指定生命周期。\n1. 只有引用才需要指定生命周期。\n1. 函数或方法的参数的生命周期被称为输入生命周期，二返回的生命周期被成为输出生命周期。\n1. 函数或方法生命周期省略规则，需满足下面三条且无冲突\n\n- 每一个输入参数都有自己的生命周期参数\n- 如果只有一个输入周期，那么它将被赋予所有的输出生命周期参数\n- 如果有多个输入生命周期且其中一个是`&self`或者`&mut self`(说明是个对象的方法）；那么所有的输出生命周期参数将被赋予`self`的生命周期。\n\n12. 生命周期省略简记：\n\n- 只有一个输入参数\n- 有多个输入且其中一个是`&self`\n\n13. Rust 中 struct 和其方法未封装到一起，C++相当于将 struct 和方法封装到类中。C 也有 struct 和函数，但不具有 trait 或接口功能。\n14. 方法签名中，`&self` 来替代 `rectangle: &Rectangle`，因为该方法位于`impl Rectangle`上下文中，所以 Rust 知道`self`类型是`Rectangle`\n15. `impl`不以`self`作为参数的函数，被称为关联函数。是函数不是方法。(类似 C++的静态函数）)\n16. `self`是 keyword. 方法中如果想改名字，可以这样传参`self1: &Rectangle`\n17. 方法定义中的生命周期注解应用于关联函数，方法（含有`&self`)可省略。\n18. 静态生命周期： `'static`，其生命周期能够存活于整个程序期间。所有的字符串字面值都拥有`'static`生命周期。\n19. 生命周期也是泛型`test<'a, T>(x: &'a str, y: &'a str, ann: T) -> &'a str where T: Display` 因为生命周期也是泛型，所以`'a`和泛型参数`T`都位于函数名后的同一尖括号列表中。\n20. PhantomData Zero-sized type used to mark things that \"act like\" they own a T. 用在 ffi 封装指针上。\n21. `fn longest<'a>(x: &'a str, y: &'a str) -> &'a str ` 当具体的引用被传递给 longest 时，被 'a 所替代的具体生命周期是 x 的作用域与 y 的作用域相重叠的那一部分。\n\n## Traits\n\n1. trait object 用来实现泛型函数里动他分发\n2. Trait Object，表现为 &dyn Trait (无所有权） 或者 Box<dyn Trait>（有所有权）。\n3. 在 Rust 里，类型的行为是通过 Trait 来赋予的，几乎所有的数据结构都实现了一个到多个 Trait。泛型编程时只有指定 trait bound(限制)才能使用接口。\n4. Rust 可以同时支持“静态分派(static dispatch)”和“动态分派(dynamic dispatch)”\n5. 利用泛型：编译器会根据实际调用参数的类型不同，直接生成不同的函数版本，类似 C++ 的 template 一样。通过泛型函数实现的“多态”，是在编译阶段就已经确定好了调用哪个版本的函数，因此被称为“静态分派”。\n6. Rust 中的“动态分派”靠 Trait Object 来完成。\n7. 指向 trait 的指针就是 Trait Object。如 Box<dyn Bird>.\n8. [Object Safe](https://zhuanlan.zhihu.com/p/23791817): 以下条件下 Trait Object 是无法构造出来：\n   - 当 trait 有 Self:Sized 约束时\n   - 当函数中有 Self 类型作为参数或者返回类型时\n   - 当函数第一个参数不是 self 时\n   - 当函数有泛型参数时\n9. 使用工厂模式时， 如果 trait 返回 Self，例如：fn get(&self) -> Self; `fn get(self: Box<Self>) -> Box<dyn InfoManager>; ` [link](https://users.rust-lang.org/t/boxed-self-in-a-trait-method/1742/2)\n10. Rust 的动态分派，和 C++的动态分派，内存布局有所不同。在 C++里，如果一个类型里面有虚函数，那么每一个这种类型的变量，内部都包含一个指向虚函数表的地址。而在 Rust 里面，对象本身不包含指向虚函数表的指针，这个指针是存在于 trait object 指针里面。`如果一个类型实现了多个 trait，那么不同的 trait object 指向的虚函数表也不一样`。\n11. You can implement traits for types you didn't define, whereas interfaces can only be implemented for your own classes.\n12. `孤儿规则`(orphan rule)，简称 OR：\"当你为某类型实现某 trait 的时候，必须要求类型或者 trait 至少有一个是在当前 crate 中定义的。 你不能为第三方的类型实现第三方的 trait 。\n13. Trait Object 的底层逻辑不过就是胖指针（fat pointer） —— 一个包含两个指针的数据结构。其中，一个指针指向数据本身，另一个则指向虚函数表（vtable）。在这张表里，包含具体类型的一些信息，如 size，aligment 以及一系列函数指针：[link](https://mp.weixin.qq.com/s?__biz=MzA3NDM0ODQwMw==&mid=2649829038&idx=1&sn=b9459537ff93d2aee56b822d09d6a80a&chksm=8704acb2b07325a4a9c46f0bbac3df2db2fadeb2b7928e3949072f01df0d975eaecb36a6e8dd&scene=178&cur_album_id=1701598373151047686#rd)\n\n## Generic Types 泛型\n\n3. [透过 Rust 探索系统的本原：泛型](https://mp.weixin.qq.com/s?__biz=MzA3NDM0ODQwMw==&mid=2649829038&idx=1&sn=b9459537ff93d2aee56b822d09d6a80a&chksm=8704acb2b07325a4a9c46f0bbac3df2db2fadeb2b7928e3949072f01df0d975eaecb36a6e8dd&scene=178&cur_album_id=1701598373151047686#rd)\n4. 泛型编程，与其说是一种技术，不如说是一种思想。它不仅仅是类型参数化，函数泛型化那么简单，背后体现的是程序员的抽象思维能力。抽象思维的能力，和语言无关，和工具无关，它是一种在实践中不断探索，不断学习，不断历练得出的能力。支持泛型的语言并不能帮助你更好地做泛型编程，就好比给我一台斯坦威钢琴，并不意味着我就具备了演奏李斯特《钟》的能力。\n5. 定义数据结构时： 参数化类型（Parameterized Type）\n6. 定义函数时： 泛型函数（Generic Function）\n7. 泛型函数: 静态分派（static dispatch 泛型为具体类型 T）和动态分派（dynamic dispatching 泛型为 trait object）\n8. 编译器在编译时就能捕获到使用的类型，从而进行对应的处理，这被称之为静态分派（static dispatch）。\n9. ![静态分派和动态分派](https://mmbiz.qpic.cn/mmbiz_png/SER9L29WQ0icrEMnzjvMNTzSZbaPboUnMUFBia3wHebkuX9e6Ah70g3CKC9v8bVicGeGJEq0Dd38AnesVU5ffXmdg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)\n\n## Iterators and Closures 迭代器与闭包\n\n1. 函数式编程：包含将函数作为参数值或其他函数的返回值，将函数赋值给变量以供之后执行等等。\n1. 闭包：一个可以储存在变量里的类似函数的结构\n1. 迭代器：一种处理元素序列的方式。\n1. Rust 的闭包是可以保存进变量或作为参数传递给其他函数的匿名函数（可以捕获环境的匿名函数）。可以在一个地方创建闭包，然后在不同的上下文中执行闭包运算。\n1. 不同于函数，闭包允许捕获调用者作用域的值。\n1. 在程序的一个位置指定某些代码，并只在程序的某处实际需要结果的时候执行这些代码。这正式闭包的用武之地。\n1. 闭包通常很短，并只关联于小范围的上下文而非任意情境。在有限的上下文中，编译器能可靠的推断参数和返回值的类型。\n1. 闭包语法：`let test = |num1, num2| { println(\"num1: {}\",num1); num2 + 1 };` `|num1, num2|`是参数，`num2+1`是返回值。`let`表明`tet`是一个匿名函数的定义。不是调用匿名函数的返回值。`test`存储的是代码。\n1. 带类型注解的闭包 `let test = |num1: u32, num2: u32| -> u32 { println(\"num1: {}\",num1); num2 + 1 };`\n1. 如果闭包体只有一行则大括号可以省略。例如： `let add_one = |x| x+1;`\n1. 如果调用多次闭包，编译器会根据第一次使用时的参数类型，如果对同一闭包使用不同类型会得到类型错误。\n1. ...\n1.\n1.\n1. 迭代器负责遍历序列中的灭一项和决定序列何时结束的逻辑。\n1. 迭代器是惰性的，在调用方法使用之前它都不会有效果。\n1. 迭代器的实现方式提供了对多种不同的序列使用相同逻辑的灵活性，减少了重复代码并消除了潜在的混乱。\n1. 迭代器都实现了定义于标准库的 trait `Iterator`。包含`next`方法。\n1. 迭代器的`sum`方法返回迭代的次数，会消费适配器。迭代器调用`sum`方法后不再允许使用迭代器，因为`sum`会获取迭代器所有权。\n1. 迭代器的方法：`迭代器适配器(iterator adaptors)` 允许将迭代器变为不同类型的迭代器。\n1. `v1.iter().map(|x| x+1);`迭代器适配器方法`map`使用闭包来调用每个元素以生成新的迭代器。这里的闭包创建了一个新的迭代器，对其中 vector 中的每个元素都被加 1。因为迭代器适配器是惰性的，这里需要消费迭代器。\n1. `collect`方法会消费迭代器并将结果收集到一个数据结构中。\n1. 下面例子会调用`map`方法创建一个新的迭代器，接着调用`collect`方法消费新迭代器并创建一个 vector。\n\n```rust\nlet v1: Vec<i32> = vec![1, 2, 3];\nlet v2: Vec<_> = v1.iter().map(|x| x + 1).collect();\nassert_eq!(v2, vec![2, 3, 4]);\n```\n\n## Smart Pointers 智能指针\n\n1. 指针是一个包含内存地址的变量的通用概念，这个地址指向一些其他数据。\n1. Rust 最常见的指针是引用，他们没有任何额外的开销，所以应用的最多。\n1. `智能指针(smart pointers)`是一类数据结构，它们的表现类似指针，但是也拥有额外的元数据和功能。\n1. `引用计数(reference counting)` 是智能指针类型，其允许数据有多个所有者。引用计数智能指针记录总共有多少个所有者，并当没有任何所有者时负责清理数据。\n1. 在 Rust 中，普通引用和智能指针的一个额外的区别是引用是一类只借用数据的指针；相反，在大部分情况下，智能指针拥有他们指向的数据。\n1. 智能指针通常使用结构体实现。\n1. 智能指针区别于常规结构体的显著特性在于其实现了`Deref`和`Drop`trait。\n1. `Deref` trait 允许智能指针结构体实例表现的像引用一样，这样就可以编写既用于引用，又用于智能指针的代码。\n1. `Drop` trait 允许我们自定义当智能指针离开作用域时运行的代码。\n1. 很多库都拥有自己的智能指针而你也可以编写属于你的智能指针。\n1. `Box<T>`用于在堆上分配值。\n1. `Rc<T>`一个引用计数类型，其数据可以有多个所有者。\n1. `Ref<T>`和`RefMut<T>`通过`RefCell<T>`访问。\n1. 最简单直接的智能指针是`box`，其类型是`Box<T>`，box 允许你将一个值放在堆上而不是栈上，留在栈上的则是指向堆数据的指针。\n1. 除了数据被存储在堆上而不是栈上之外，box 没有性能损失，不过也没有很多额外的功能。多用于\n   - 当有一个在编译时未知大小的类型，而又想要在需要确切大小的上下文中使用这个类型值的时候。（box 作为链表的指针实现链表）\n   - 当有大量数据并希望在确保数据不被拷贝的情况下转移所有权的时候。\n   - 当希望拥有一个值并只关心它的类型是否实现了特定 trait 而不是具体类型的时候\n1. `Box<T>`类型是一个只能智能指针，因为它实现了`Deref`trait，它允许`Box<T>`的值被当做引用对待。\n1. 变量的所有权总是遵循相同的模式：将值赋给另一个变量时移动它。当持有堆中数据值的变量离开作用域时，其值将通过 drop 被清理掉，除非数据被移动为另一个变量所有。\n1. & 符号就是 引用，允许你使用值但不获取其所有权。\n1. 我们将获取引用作为函数参数称为 借用（borrowing）。\n\n## AsRef and Deref\n\n1. Deref is different from the other two: one type can only be derefed to one target type, and \\*d always has the same type.\n2. Borrow and AsRef both give a reference to the underlying data, but Borrow requires that the original type and the borrowed type have the same behavior, while AsRef does not have the same requirement.\n3. 当我们在示例中输入 _y 时，Rust 事实上在底层运行了如下代码：`_(y.deref()) ` Rust 将 \\* 运算符替换为先调用 deref 方法再进行普通解引用的操作，如此我们便不用担心是否还需手动调用 deref 方法了。Rust 的这个特性可以让我们写出行为一致的代码，无论是面对的是常规引用还是实现了 Deref 的类型。\n4. 解引用强制多态（deref coercions）是 Rust 在函数或方法传参上的一种便利。其将实现了 Deref 的类型的引用转换为原始类型通过 Deref 所能够转换的类型的引用。当这种特定类型的引用作为实参传递给和形参类型不同的函数或方法时，解引用强制多态将自动发生。这时会有一系列的 deref 方法被调用，把我们提供的类型转换成了参数所需的类型。\n5. 解引用强制多态的加入使得 Rust 程序员编写`函数和方法`调用时无需增加过多显式使用 & 和 \\* 的引用和解引用。这个功能也使得我们可以编写更多同时作用于引用或智能指针的代码。\n6. [deref](https://wiki.jikexueyuan.com/project/rust-primer/intoborrow/deref.html)\n\n## Concurrency 并发\n\n## Asynchronous Programming\n\n1. async function 是一种 promise(sender). promise 调用 await 时，执行并返回 future(reciever).\n\n## 异步编程\n\n1. 主要作用： makes it possible for a thread to continue doing useful work while it waits for the value to become available. 例如：IO DMA\n1. [深入浅出 Tokio](https://zhuanlan.zhihu.com/p/107820568)\n1. [同步异步，阻塞非阻塞区别](https://www.cnblogs.com/mhq-martin/p/9035640.html)\n1. 同步异步区别是在调用一个功能时，没得到`结果`之前，是否继续后续操作\n1. 阻塞非阻塞区别是`线程/程序`在等待调用结果时，是否一直等待（挂起当前线程）\n1. 阻塞和非阻塞关注的是程序在`等待调用结果`（消息，返回值）时的`状态`.\n1. 进程线程锁是用来同步的，lock()函数是阻塞式，不拿到锁不返回，线程被阻塞。try_lock()是非阻塞式的，如果拿不到锁也返回。\n1. 异步是目的，而多线程是实现这个目的的方法\n1. Async/Await is a paradigm that is orthogonal to threading, which means that it has the potential to run tasks on a single thread OR on multiple threads depending on the executor that is used.\n1. async/await 与多线程 [link](https://qvault.io/rust/concurrency-in-rust-can-it-stack-up-against-gos-goroutines/)\n\n```\nuse async_std::task;\nuse futures::executor::block_on;\nuse std::time::Duration;\n\nfn main() {\n    let future = async_main();\n    block_on(future); // 执行器单线程执行，总共需要15秒，多线程可能就快很多\n}\n\nasync fn async_main() {\n    print_for_five(\"await\").await; // await: function 阻塞\n\n    let async_one = print_for_five(\"async 1\");\n    let async_two = print_for_five(\"async 2\");\n\n    futures::join!(async_one, async_two);\n}\n\nasync fn print_for_five(msg: &str) {\n    for _ in 0..5 {\n        // task::sleep(Duration::from_secs(1)).await; //async 1, 2 循环执行，task await会异步阻塞\n        std::thread::sleep(std::time::Duration::from_secs(1)); //执行器单线程时顺序执行，因为不会阻塞\n        println!(\"one second has passed: {}\", msg)\n    }\n}\n\n```\n\n## Object Oriented 面向对象\n\n## Macros 宏\n\n1. 从根本上来说，宏是一种为写其他代码而写代码的方式，即所谓的 元编程（metaprogramming）\n2. 元编程对于减少大量编写和维护的代码是非常有用的，它也扮演了函数扮演的角色。但宏有一些函数所没有的附加能力。\n3. derive 属性，其生成各种 trait 的实现\n4. 一个函数标签必须声明函数参数个数和类型。相比之下，宏能够接受不同数量的参数：用一个参数调用 println!(\"hello\") 或用两个参数调用 println!(\"hello {}\", name) 。\n5. 宏可以在编译器翻译代码前展开，例如，宏可以在一个给定类型上实现 trait 。而函数则不行，因为函数是在运行时被调用，同时 trait 需要在编译时实现。\n6. 实现一个宏而不是函数的消极面是宏定义要比函数定义更复杂，因为你正在编写生成 Rust 代码的 Rust 代码。由于这样的间接性，宏定义通常要比函数定义更难阅读、理解以及维护。\n7. hello_macro(不是宏) 中定义的 trait(可包含多个接口）， hello_macro_derive 中实现的过程式宏。在一个类型上指定 #[derive(HelloMacro)] 时，hello_macro_derive 函数将会被调用。\n\n## Program with C/C++\n\n1. [Rustonomicon](https://doc.rust-lang.org/nomicon/index.html)\n1. [rust bindgen](https://github.com/rust-lang/rust-bindgen)\n1. [rust with C](https://doc.rust-lang.org/stable/embedded-book/interoperability/index.html)\n\n## 日志\n\n1. [env_logger](https://docs.rs/env_logger/0.8.3/env_logger/)\n2. actix_web 使用了 env_logger [actix_web::middleware::Logger](https://github.com/actix/actix-web/blob/master/src/middleware/logger.rs#L51)\n\n## packages\n\n1. [prettytable-rs](https://github.com/phsym/prettytable-rs)\n","tags":["Rust"],"categories":["Program","Rust"]},{"title":"Tools and Linux Command","url":"/2021/06/29/Bash-usage/","content":"\n## Useful Tools (can install from github release)\n\n### windows\n\n1. 滴答清单 windows + phone\n1. sticky 便利贴 `win+alt+t` always on top\n1. [files 文件管理器](https://files.community/)\n1. [PowerToys](https://github.com/microsoft/PowerToys)\n   - color pick: win + shift + c\n   - 鼠标荧光笔\n1. 罗技 master3 鼠标， 大拇指 + 左右， workspace 切换\n1. [PowerToys/run 快速启动](https://docs.microsoft.com/zh-cn/windows/powertoys/run)\n   - alt + space\n   - search startup 开机启动\n   - `ctl + win + A` app 置顶，超有用，配合任务栏使用, 点击任务栏最小化，置顶和最小化切换\n1. [windows terminal](https://github.com/microsoft/terminal)\n   - win + ` 上半部分显示\n   - [docs](https://learn.microsoft.com/zh-cn/windows/terminal/)\n1. [windows quicklook 预览](https://github.com/QL-Win/QuickLook)\n1. [quicklook plugins](https://github.com/QL-Win/QuickLook/wiki/Available-Plugins)\n1. [CopyQ 粘贴板](https://github.com/hluk/CopyQ)\n   - F6\n   - win + v(系统自带)\n   - 固定常用的命令\n1. [snipaste 截图](https://zh.snipaste.com/index.html)\n1. [ShareX](https://github.com/ShareX/ShareX)\n   - tools OCR 图片文字识别\n1. [flameshot 截图工具](https://github.com/flameshot-org/flameshot)\n1. [screentogif](https://www.screentogif.com/)\n1. [zoomit 投影演示 倒计时](https://docs.microsoft.com/en-us/sysinternals/downloads/zoomit)\n1. [WSL2](https://docs.microsoft.com/zh-cn/windows/wsl/install)\n1. [Dadroit JSON Viewer](https://dadroit.com/)\n1. [Windows 优化器](https://github.com/hellzerg/optimizer)\n1. [TrafficMonitor](https://github.com/zhongyang219/TrafficMonitor)\n\n<!-- more -->\n\n#### [shortcuts](https://support.microsoft.com/en-us/windows/keyboard-shortcuts-in-windows-dcc61a57-8ff0-cffe-9796-cb9706c75eec)\n\n1. `win + f` 全屏\n1. `win + v`历史复制\n1. `win + 1` 设置为打开 vscode\n\n### linux\n\n1. [onefetch](https://github.com/o2sh/onefetch/tree/main)\n1. [sshx web 连接，可分享](https://github.com/ekzhang/sshx)\n1. [inshellisense](https://github.com/microsoft/inshellisense)\n1. [screenshot-to-code](https://github.com/abi/screenshot-to-code)\n1. [autocomplete](https://github.com/withfig/autocomplete)\n1. [cfonts](https://github.com/dominikwilkowski/cfonts)\n1. [neofetch](https://github.com/dylanaraps/neofetch) and [asciiart](https://www.asciiart.eu/vehicles/cars)\n1. [tmux](https://github.com/tmux/tmux/wiki)\n1. [z.lua(zh, zb)](https://github.com/skywind3000/z.lua)\n1. [zoxide](https://github.com/ajeetdsouza/zoxide)\n1. [fzf(C-r, M-c, kill -9 <tab>)](https://github.com/junegunn/fzf)\n   - vi realpath + C-t 直接找到文件\n1. [rg](https://github.com/BurntSushi/ripgrep)\n1. [fd](https://github.com/sharkdp/fd)\n1. [bat](https://github.com/sharkdp/bat)\n1. [shfmt](https://github.com/mvdan/sh)\n1. [gping](https://github.com/orf/gping)\n1. [commitizen](https://github.com/commitizen/cz-cli)\n1. [zsh](https://github.com/ohmyzsh)(.zsh_history 保存常用命令)\n1. [exa](https://github.com/ogham/exa)\n1. [hexyl](https://github.com/sharkdp/hexyl)\n1. [tldr](https://github.com/tldr-pages/tldr)\n1. [vimium](https://github.com/philc/vimium)\n1. [htop](https://htop.dev/)\n1. [doxygen](https://docxgen.readthedocs.io/en/latest/)\n1. [samba](https://www.samba.org/)\n1. [hexo](https://hexo.io/zh-cn/)\n1. [docusaurus](https://github.com/facebook/docusaurus)\n1. [mkdocs](https://github.com/mkdocs/mkdocs/)\n1. [sphinx-doc](https://github.com/sphinx-doc/sphinx)\n1. [snap 通用的 linux 软件包](https://cn.ubuntu.com/blog/what-is-snap-application)\n1. [snap store](https://snapcraft.io/store)\n1. [7yuv linux](https://vball.fun/2021/11/23/build-7yuv-snap-package/)\n1. [imgcat](https://github.com/eddieantonio/imgcat)\n1. [watchman](https://github.com/facebook/watchman)\n1. [日志查看 lnav](https://github.com/tstack/lnav)\n1. [日志分析 miller](https://github.com/johnkerl/miller)\n1. [termius](https://termius.com/)\n\n### 文件传输\n\n1. A从B传到C: `ssh  -p 8327 xiyang@219.135.228.244  \"cd /data/xiyang/siorigin_triton_bk/ && tar -cf - precommit_hooks.tgz\" | ssh yxsqw@10.193.64.65 \"tar -xvf - -C ~/\"`\n   - B和C都不能连A\n1. mount or pipe + rsync\n1. scp 适用于文件较少\n1. rsync 适用于大量文件，增量传输\n   - rsync 同步数据， 可以启服务备份数据 `rsync -avzr --progress test --exclude=test/5 10.10.0.98:/home/jiaxiyang/` 多用 rsync，少用 scp\n   - `sshpass -p \"\" rsync -avz -e \"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\" test root@10....` sshpass + rsync\n1. pipe 适用于大量文件，第一次传输\n   - `tar cf - read_shape | ssh root@10.31.1.153 \"tar xf - -C ~/xiyang/\"`通过管道传输，免去了单独操作麻烦, 用管道来压缩和解压，降低了磁盘 IO(两次), 直接内存里操作\n\n### ssh\n1. 别名: ~/.ssh/config; ssh cpu_58\n\n``` shell\nHost cpu_58\n    HostName 10.193.64.58\n    User yxsqw\n    Port 10086\n```\n\n### tmux\n\n1. c-m 或 enter： 换行\n1. `tmux send-keys -t .3 \"ls -al\" c-m` 向当前 windows pannel 3 发送 ls 命令\n1. `tmux list-panes | wc -l` 当前 panel 个数\n1. 创建固定个数 panel，每个 panel 执行不同命令\n\n```sh\n#!/bin/bash\n\nPANEL_NUM=6\nwhile [ $(tmux list-panes | wc -l) -lt ${PANEL_NUM} ]; do\n\ttmux split-window \\; #select-pane -L\n    tmux select-layout tiled\ndone\n\n# even-horizontal even-vertical main-horizontal main-vertical tiled\n# tmux select-layout even-horizontal\ntmux select-layout tiled\n\nCURRENT_PANEL=$(tmux display -pt \"${TMUX_PANE:?}\" '#{pane_index}')\nfor i in $(seq 1 ${PANEL_NUM}); do\n\techo ${i}\n\tif [ ${i} -ne ${CURRENT_PANEL} ]; then\n\t\ttmux send-keys -t .${i} \"echo ${i}; ls -al\" c-m\n\tfi\ndone\n\n```\n\n## 查看系统系统\n\n1. `top` and `htop`\n   - top 按 t 显示百分比, 按多次\n1. `free -m`查看内存使用\n1. `sudo iotop`\n1. `iftop`\n1. `iostat -x 1 10` 查看 io 情况\n1. `sudo watch pidstat -d` 看哪个进程占用 IO 多。\n1. `who`, `w`, `last` and `lastlog` 查看最后用户登录\n\n## [文件权限](https://www.runoob.com/linux/linux-file-attr-permission.html)\n\n1. never use 777\n1. u,g,o,a 分别代表 属主，数组，其他用户，所有用户\n1. +,-,= 分别代表 增加，减去，设置一个权限\n1. `chmod o+w` 给其他用户添加写权限\n1. `o+w`会高亮文件夹\n1. mount 问题解决`chmod o+w -R .` `find . -type f | xargs chmod o+w` `chmod o+w $(find . -type f)` 给其他用户加写权限， 不会有 git 历史，umask 022\n1. `o-r` 不让其他用户看\n\n## Add User and Group\n\n```sh\n# 查看用户和组id\n% id -u && id -g\n\n# 添加组\n% sudo groupadd xiyang -g 1022\n\n# 添加用户到组，zsh作为默认shell\n% sudo useradd -g xiyang -ms /bin/zsh xiyang -u 2022\n\n# 删除用户密码\n% passwd -d xiyang\n\n# 查看当前用户group，注意添加之后重新开terminal才生效\n% groups\n\n# 添加到sudo group\n% sudo  usermod -aG sudo xiyang\n\n# xiyang作为main group\n% sudo usermod -g xiyang xiyang\n\n# 改变用户id\n% sudo usermod -u 1022 xiyang\n\n# 改变组id\n% sudo groupmod -g 1022 xiyang\n\n# 删除用户 -r 删除目录\n% userdel -r xxx\n\n# 查看所有用户uid\n% cat /etc/passwd\n\n# sample\n% sshpass -p nvidia ssh nvidia@10.31.1.187\n% sudo groupadd xiyang -g 1022\n% sudo useradd -g xiyang sudo  -ms /bin/bash xiyang -u 1022\n% sudo passwd -d xiyang\n% sudo su - xiyang\n% passwd\n```\n\n## awk\n\n1. 列出.so 或.a 库 `ls | awk '/.so$|.a$/{print $0}'`\n1. 分割字符串 注意从$1 开始，$0 表示输入 `echo test.so | awk -F '.' '{print $1}'`\n1. 获取 so 库的名字 `echo libtest.so | awk -F '.' '{print substr($1,4)}'`\n1. 获取库名字 `ls | awk '/.so$|.a$/{print $0}' | awk -F . '{print substr($1, 4)}'`\n1. 获取第一个匹配行 `cat file | grep -n \"#i[fn]\\|#pragma\" | awk -F ':' '{print $1}' | awk 'NR==1'`\n1. `ldd ${BINS} | grep \"=> /\" | awk '{print $3}' | xargs -I '{}' cp -v '{}' ${OUTPUT_DIR}/lib` copy all link libs\n1. 查看库链接关系 `for i in $(ldd test | grep \"=> /\" | awk '{print $3}'); do echo \"***************$i **************\"; ldd $i | grep flowsdk; done`\n1. 查看加载用到的库 `readelf -d build/dog/dog_test1 | grep 'NEEDED'`\n\n## [sed](https://www.gnu.org/software/sed/manual/html_node/Command_002dLine-Options.html#Command_002dLine-Options)\n\n1. 双引号才能使用变量\n1. 删除匹配： `sed -i \"/fancyfoot/d\" latex/refman.tex`\n1. 在匹配后加入一行 `sed -i '/{fancyplain}/a\\\\\\rhead{}' latex/refman.tex`\n1. 在匹配前加一行 `sed -i '/doxysection/i\\\\\\newpage' latex/refman.tex`\n1. 删除 1 到 n 行： `sed -i \"1, ${n}d\" file`\n1. 匹配字符所在行替换：`sed -i '/config/c\\This line is relace.' test.json`\n\n## curl\n\n1. `curl -fsSL https://mirrors.xxx.xx/xx.sh | bash -ex` 一键运行\n1. curl -L http:xxx 不加 -L 会自动转换为 https\n1. -s, --silent Silent mode\n1. -S, --show-error Show error even when -s is used\n1. -f, --fail Fail silently (no output at all) on HTTP errorsc 出错退出\n1. -O, --remote-name Write output to a file named as the remote file 保存为文件名\n1. -o, --output <file> Write to file instead of stdout\n\n## Useful Commond\n\n1. `timeout 3s xxxx` 超时退出; 可用于无法停止程序测试；不用 ctrl-c\n1. `alias mc='cmd(){ date;pwd;who|wc -l;};cmd'` alias 执行多条命令\n1. `pidof firefox | xargs kill -9 ` or `kill $(pidof firefox)` kill process by name\n1. `nl file` 显示行号\n1. `ftp`: 搭建 ftp 服务器，脚本 put:`scp -r $@ user@IP:path`上传文件，脚本 get:`for i in $@; do scp -r user@IP:path/${i}; done`获取文件。get-ls:`ssh xiyang@xsjsda153 \"cd /var/ftp/pub/xiyang/ && ls $@\"`, get-list-all:`ssh user@IP \"cd path && ls -alhrt\"`, get-list:`ssh user@IP \"cd path && ls -lhrt | tail -n 10\"`获取 ftp 服务器列表。ftp 服务器上可以放常用文件，各服务器方便取。也可用于服务器(包括 windows)间数据交换。\n\n## Miscellaneous\n1. `kill -9 %%`  终止最近的后台作业\n1. `bash -v`查看bash执行时哪里设置了环境变量\n1. `alias` 查看alias\n1. `sudo passwd user`linux passwd 强制设置简单密码, 需要 sudo\n1. `sed -i s@/archive.ubuntu.com/@/mirrors.tuna.tsinghua.edu.cn/@g /etc/apt/sources.list && apt-get update -y` apt 切换清华源\n1. `dpkg-query -l`查看安装库版本\n1. `nice -n 10 ./test` 调整程序优先级(不设置默认是 0), nice 值越高，优先级越低。top 或 ps -l 可查看优先级（NI: nice）Niceness values range from -20 (the highest priority) to 19 (the lowest).\n1. `renice -n 10 -p <pid>` 重新设置 nice 值\n1. (good) `hostname -I` 查看 IP 地址\n1. htop 显示 io,需要新版本； 配置文件`~/.config/htop/htoprc` [link](https://superuser.com/a/1730191) [changelog](https://github.com/htop-dev/htop/blob/main/ChangeLog)\n\n```sh\nwget -c https://github.com/htop-dev/htop/releases/download/3.2.2/htop-3.2.2.tar.xz && tar xf htop-3.2.2.tar.xz && cd htop-3.2.2\n./autogen.sh && ./configure && make -j4\n```\n\n1. htop `S -> Meters添加修改监控内容` 可添加 CPU average， disk io 等，也可修改布局和 style， 都该成 bar；\n1. htop `S -> Display Option` Also show CPU frequency 显示 cpu 频率\n1. htop `S -> Screens`可以添加 IO_RATE, IO_READ_RATE, IO_WRITE_RATE, CWD\n1. `gdb attach <pid> =>  info proc all` 查看进程是否设置了 coredump\n1. `gcore $(pidof processname)` 命令行直接执行产生 coredump `-a`产生更详细信息\n1. json config tree `main_file=test.json; parse_json() { local file=\"$1\" indent=\"$2\"; if [[ ! -f \"$file\" ]]; then echo \"${indent}$file (not found)\"; return; fi; local json_refs=$(grep -oE '\"[^\"]+\\.json\"' \"$file\" || echo \"\"); local count=0; local total=$(echo \"$json_refs\" | grep -c .); for ref in $json_refs; do count=$((count + 1)); local ref_file=$(echo $ref | tr -d '\"'); if [[ $count -eq $total ]]; then echo -n \"${indent}└── \"; local new_indent=\"${indent}    \"; else echo -n \"${indent}├── \"; new_indent=\"${indent}│   \"; fi; if [[ -f \"$ref_file\" ]]; then echo \"$ref_file\"; parse_json \"$ref_file\" \"$new_indent\"; else echo \"$ref_file (not found)\"; fi; done; }; echo; echo \"$main_file\"; parse_json \"$main_file\" \"\"; echo`\n1. `./pg_perf_monitor_threads.sh 535  | sed 's/\\x1b\\[[0-9;]*m//g' | tee output.txt` 去除彩色\n1. `stdbuf -oL command | sed 's/\\x1b\\[[0-9;]*m//g' > output.txt` 修改缓存行为，以行为单位写入到文件\n1. `ls /proc/[PID]/task/` or `ps -T -p [PID]`获取进程线程号\n1. `echo 1 | sudo tee /proc/sys/kernel/perf_event_paranoid` sudo echo 写文件\n1. `pidof test`根据 name 查看 pid\n1. `ls -sh`\n1. `du -sh * | sort -h` 按大小排序 sort -h enables human-readable sort, so it understands sizes like K, M, and G.\n1. `du -sh .[!.]* * | sort -h`查看隐藏文件夹大小\n1. `update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1` 修改默认程序版本\n1. `cat test* > test.bin` 合并文件\n1. `split -d -n 3 -a 1 total_dummy_input@0.bin00 total_dummy_input@0_ && ls total_dummy_input@0_* | xargs -I {} mv {} {}.bin` 统一重命名\n1. `split -d -n 5 xxx xxxx` slipt 文件\n1. `split -b 262144 total_flow_predictions.bin total_flow_predictions_split.bin` slipt 文件为两部分，第一部分 262144bytes\n1. `grep -v grep` -v 显示不包含的行， 显示不包含 grep 的行\n1. `tar -C build -zcf install.tgz output` 打包 build/output, 不保留 build 文件夹\n1. `pwdx` - report current working directory of a process 查看进程工作路径, 有用，可用来看机器被谁占用, `ll /proc/<pid>/` 查看 cwd\n1. `for pid in $(ps -e -o pid=); do cmd=$(ps -p $pid -o args=); dir=$(sudo pwdx $pid 2>/dev/null | grep -v \": /$\" | cut -d ' ' -f2-); if [[ ! -z $dir ]]; then echo -e \"\\033[31m$pid: $dir\\033[0m\\n        ------  \\033[34m$cmd\\033[0m\"; fi; done` 显示所有进程 pwd 及命令\n1. `pid=PID; cmd=$(ps -p $pid -o args=); dir=$(sudo pwdx $pid 2>/dev/null | grep -v \": /$\" | cut -d ' ' -f2-); if [[ ! -z $dir ]]; then echo -e \"\\033[31m$pid: $dir\\033[0m\\n        ------  \\033[34m$cmd\\033[0m\"; fi;` 查看单个 pid 信息\n1. `for pid in $(ps -e -o pid=); do sudo pwdx $pid; done | grep -v \": /$\"` 查看所有进程路径\n1. `for pid in $(ps | awk '{print $1}'); do /userdata/pwdx $pid; done | grep -v \": /$\"`j5 查看所有进程路径\n1. `ps -eo pid,%mem,cmd --sort=-%mem | awk 'NR>1 {print $1, $2\"%\", $3; s+=$2} END {print \"Total:\", s \"%\"}'` 查看所有进程占用内存百分比及其和\n1. `cat -n test.cpp` or `less -N test.cpp` 打印带行数\n1. `readelf -a $(which find)` 命令里可以调用其他命令\n1. `lsblk` List information about block devices.查看有哪些磁盘可挂载\n1. 解决 ssh 经常断开问题 [link](https://blog.csdn.net/qq_15505637/article/details/80881770)\n   - `sudo /etc/init.d/ssh restart`\n1. `stat`: 获取文件各种信息\n1. `fd -e py -e h -e hpp -e cpp -e cc -e png -e jpg -e txt -e md -e cmake -e json | xargs chmod 644` 统一改变文件权限\n1. `fd --type f '.*\\.(c|cpp|h|hpp|hxx|cxx)' . -x clang-tidy {}`\n1. `/etc/profile` sh shell ssh 执行配置\n1. `echo $SHELL` 查看系统用的 shell\n1. `file`查看编码格式\n1. `nohup` no hang up（不挂起），用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行。\n1. `less` is more\n1. nv12 to jpg: `ffmpeg -s 1920x1080 -pix_fmt yuv420p -i nv12_1080p.yuv test-1920-1080.jpg`\n1. png to nv12: `ffmpeg -i input.png -pix_fmt nv12 1920x1080.yuv`\n1. `(ldd ${BINS} | grep \"=> /\" | awk '{print $3}' | xargs -I '{}' cp -v '{}' ${OUTPUT_DIR}/lib` copy all link libs\n1. `wget -rcN -np -nH -q --cut-dirs=100 -R \"index.html*\" https://mirrors.phigent.io/xxx/xx -P output_dir` wget 下载文件夹， `--cut-dirs=n` 从/开始往后忽略几级文件夹， `-q` quite\n1. `wget -r -nH --cut-dirs=100 --reject \"index.html*\" --no-parent https://mirrors.phigent.io/xxx/xx -P output_dir`\n1. `ls -Al --si --time-style long-iso` `-A`:不显示.和.., `--si` human readable `--time-style long-iso`时间显示为 2022-06-10\n1. `truncate --size 1M file_name` 创建大小为 1M 的文件 file_name\n1. `dd if=/dev/urandom of=random_input.bin bs=983040 count=1` 创建大小为 bs 的随机文件\n1. [imagemagick](https://guides.wp-bullet.com/batch-resize-images-using-linux-command-line-and-imagemagick/): 命令行 resize 图片\n1. `watch cat /proc/sys/fs/file-nr` get the current number of open files\n1. `pidof firefox | xargs kill -9 ` or `kill $(pidof firefox)` kill process by name\n1. `pkg-config --cflags --libs gstreamer-1.0`\n1. `export PKG_CONFIG_PATH=${PIXI_PATH}/env/share/pkgconfig:${PIXI_PATH}/env/lib/pkgconfig:${PKG_CONFIG_PATH}`设置 pkgconfig 查找路径\n1. `pkg-config --modversion poppler-data` 查看库版本 , pkg-confg 配置文件在/usr/share/pkgconfig 下(pkg-config 一般为开发者使用，文件系统中勾选-dev)\n1. `ldconfig -p `查看系统安装的库\n1. `ldconfig -p | grep pcap` 查看系统是否安装 pcap\n1. 打洞： A1, A2, A3..., B, C; A 能连接 B, B 能连接 C, A 不能连接 C。C 上配置`~/.ssh/config`:\n\n```\nconfig\n```\n\nB 上执行`ssh `登录到 C，再打开一个 C 连接，就能和 A 之间使用 scpc 传数据。如果 A 连网，C 如果想连网，B 执行完 ssh 命令后，登录到 C 上，再执行`ssh `后登录到 A 上，再打开一个 C 连接，就能上网了。\n\n1. ssh -L 8000:10.31.1.177:80 xiyang@10.31.1.225\n1. [link](https://github.com/damaainan/ceshisina/blob/master/Linux/blog/linux%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4/06linux%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%E4%BB%8B%E7%BB%8D%E5%85%AD%EF%BC%9A%E7%BD%91%E7%BB%9C.md#L454)\n\n   - 假设 A 为 windows 机器，打开 mobarxterm 本地执行 ssh -L 2222:C_IP:22 xiyang.jia@B_IP 登录 B, 将所有发送 2222 端口的数据通过中间主机`B`转发至目标主机`C`的`22`端口，\n   - A 再开一个窗口，执行 ssh -p 2222 root@localhost，A 直接连接 C\n\n1. sed -i \"s/查找字段/替换字段/g\" `grep 查找字段 -rl 路径`\n1. `cat url-list.txt | xargs wget -c`xargs 将参数列表转换成小块分段传递给其他命令\n1. `fd pdf | xargs -n1 -I {} cp {} pdf/` cp 所有 pdf 到 pdf/文件夹\n1. ssh scp 免密 登录`ssh-keygen -t rsa && ssh-copy-id -i ~/.ssh/id_rsa.pub root@10.10.129.25` 或将一台机器的 id_rsa.pub 复制到另一台机器~/.ssh/authorized_keys 文件中\n1. `tree -L 2` 查看二级目录结构\n1. `lsb_release -a` 查看操作系统版本，是 Ubuntu 还是 CentOS，是 14.04 还是 16.04\n1. [link](https://blog.csdn.net/u011630575/article/details/52151995)\n   `./test.sh 2>&1 | tee test.log` log 同时输出到前台和文件中 可以不带 2>&1(将标准错误输出到标准输出)\n   `./test.sh &>test.log` 将标准输出, 标准错误重定位到 test.log, &>file 意思是把标准输出 和 标准错误输出 都重定向到文件 file 中, 在 Bash 中，&>与 2>&1 的含义相同。\n   `./test.sh 2>test.log 1>&2` 将标准输出重定位到标准错误， 标准错误重定位到 test.log\n   `./test.sh >test.log 2>&1` 将标注错误重定位到标准输出，标准输出重定位到 test.log\n   ` ls a.txt b.txt 1>file.out 2>file.err` \"1>\" 通常可以省略成 \">\"\n1. IP 地址别名，如服务器别名 `/etc/hosts 10.10.0.61 dg16`\n1. `ssh -v root@10.10.129.25` 通过[-v]参数，查看 ssh 连接的具体过程\n1. `echo123456 | sudo -S ./winless.sh` 以 root 权限来执行文件 这里 123456 是密码，参数-S 专门为执行 sudo 命令的时候要输入密码而准备的，表示标准输入。\n1. ssh 打洞，中间机器运行 `ssh -R10115:${IP1}:22 ${IP2}`\n   然后运行 `rsync -avz --progress -e 'ssh -p 10115' ${FILES} localhost:${PATH}`\n1. w 查看登录的用户 who 向登录用户发消息 `echo jia > /dev/pts/20 ` `for i in $(who | awk '{print $2}'); do echo \"${i}\" > /dev/${i}; done`\n1. `nslookup www.baidu.com` 域名查询命令\n1. split 将一个大文件分割成多个小文件\n1. `chmod 777 -R work_space` 改变目录下所有文件权限\n1. `time program` 测试程序运行时间\n1. `ssh -XY jiaxiyang@10.10.0.61` 需要的时候显示图形界面\n1. `arp -na` 查看网络中连接情况\n1. `find . -name '*' -exec touch {} \\;` 修改当前目录及子目录中所有文件的时间\n1. `chattr` 修改文件（夹）属性 不能被删除\n1. `figlet \"jia\" -f scrpt(ls /usr/share/figlet)` 改变输出字体大小\n1. `ln -s source target` 创建软连接\n1. ln 非常有用，可以使用 ln 来重新命名文件，可以提供一层虚拟层，文件名字路径改变只需重新链接一下，上层代码不用改变。例如 xilinx 所有硬盘都连接到 /proj/rdi/staff/xiyang 方便操作\n1. `watch -d -n 1 ./Re reg` 每隔 1 秒查看程序变化\n1. `axel -n 15（线程数）URL` 多线程下载\n1. `apt-cache search trash` 搜索安装包\n1. `sudo dpkg --install atom-amd64.deb` dpkg 安装.deb 软件包\n1. `export PATH=\"/home/zxy/Desktop:$PATH\"` 添加环境变量\n1. `linuxlogo` 命令行显示 linux logo\n1. `sudo mount -t tmpfs -o size=8G tmpfs ramdisk/` 创建内存文件系统， 可以加快程序运行时间。\n1. `hostname` 查看主机名\n1. `tar -zxvf aa.tar.gz BOOT.BIN` 单独解压 某个文件 `tar -tf` 压缩包名称，可以查看压缩包内容 https://www.cnblogs.com/manong--/p/8012324.html\n1. `speedtest-cli` 测下载上传网速\n1. `curl -O -u jiaxiyang:jiaxiyang -s[-#]`\n1. `wget -c url` 支持断点续传 好用，可以用来下载大文件，比如： petalinux cache 30G\n1. `crontab -e` 定时执行脚本 cat /var/mail/jiaxiyang 查看结果\n1. `syncthing` 文件同步神器\n1. `realpath` 查看文件全路径\n1. `shotwell` \\*.jpg 打开图片\n1. `vimdiff dp` 快速移动不同\n1. `stat - display file or file system status`\n1. `stat -c %s file` 查看文件长度\n1. `stat -c %z file` 查看文件修改时间\n1. `rm file || true` 如果不存在执行 true，还是会报错，但$?返回为 0 使用，使用 bash -ex 时不会退出。\n1. `cat file | grep -v '^#'` 输出不以#号开头的行\n1. `SCRIPT_PATH=$(dirname $(realpath xx.sh)) ` 比 pwd 要好，pwd 显示的是虚拟链接地址。\n1. `PROJECT_NAME=$(basename $(SCPRIT_PATH))` 当前脚本所在的工程名字\n1. sshpass 如果没带 nocheck 需要 ssh 先登录一次, 或者 StrictHOstkeyChecking=no\n1. `sshpass -p root（密码） ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@10.10.129.22 '/etc/init.d/led start'` 跨机器执行命令 在机器 A 上调用，在机器 B 上执行，将结果输出到 A 上。\n1. `sshpass -p root（密码） ssh(scp) -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@10.10.129.22 ssh -y -y root@zu9-2 \" ' cd /home && ls'\" `zu9 上执行 cd /home && ls 注意 要同时加双引号和单引号。`sshpass -p jiaxiyang scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null static_ip.txt jiaxiyang@10.10.0.96:~`\n1. `scp -o ProxyJump=nvidia@10.31.1.12 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -r dir_test  root@10.31.1.180:/userdata/mount/xiyang/`跨机器传文件\n1. `ssh -o ProxyJump=nvidia@10.31.1.12 root@10.31.1.180 'ls /userdata'` 跨机器执行， host -> 12 ->180 执行\n1. `tar -cf - zu9_test | sshpass -p root ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@10.10.129.133 \"tar -xvf - -C /var\"` 跨机器 cp 文件并免密\n1. `rm -rf /etc/localtime && ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime` 设置北京时区\n1. 多网卡不能同时配置一个网关?\n\n## Links and Reference\n\n1. [The art of command line](https://github.com/jlevy/the-art-of-command-line/blob/master/README-zh.md)\n1. [Awesome shell](https://github.com/alebcay/awesome-shell)\n1. [Awesome bash](https://github.com/awesome-lists/awesome-bash)\n1. [Bash cheatsheet](https://github.com/zhouyiqi91/awesome-cheatsheets/blob/master/languages/bash.sh)\n","tags":["Bash"],"categories":["Tools","Bash"]},{"title":"Elisp","url":"/2021/06/27/Elisp/","content":"\n## Useful Command\n\n1. C-u before command to insert the result into the current buffer.\n2. crux-eval-and-replace 将表达式替换为结果， 例如(+ 1 1) -> 2\n3. 创建新 buffer: switch-to-buffer\n4. 需要到命令行工作时，不用关闭 emacs, 使用 suspend-frame(C-z) 回到命令行，使用 fg 命令返回 emacs\n\n## grammer\n\n1. (car list) first element\n1. (nth n list) nth element\n1. (car (last list)) last element\n\n```emacs-lisp\n(car (list \"a\" \"b\" \"c\") )   ; \"a\"\n(nth 1 (list \"a\" \"b\" \"c\") ) ; \"b\"\n(car (last (list \"a\" \"b\" \"c\")) )   ; \"c\"\n```\n\n1. `(setq temp (append temp (list 1)))` addpend to list temp\n1. loop list\n\n```emacs-lisp\n  (dolist (element (extract-rectangle start end)) ;; extract-rectangle to lists\n    (setq LIST (append LIST (split-string element)))) ;; apend to list\n```\n\n## functions\n\n1. `(message \"%s=%d\" \"test\" 1)` minibuf 打印\n1. ` (split-string \"1 2 3 4\")` => (\"1\" \"2\" \"3\" \"4\")\n1. ` (seq-map 'string-to-number (split-string \"1 2 3 4\"))` => (1 2 3 4)\n1. `(vconcat [] (split-string \"1.2.3\" \"\\\\.\"))` => [\"1\" \"2\" \"3\"]\n1. max, min, sum and mean\n1. `(extract-rectangle start end)` extract-rectangle to lists\n\n```emacs-lisp\n(defun count-of-nums (xs)\n  (length xs))\n\n(defun min-of-nums (xs)\n  (apply #'min xs))\n\n(defun max-of-nums (xs)\n  (apply #'max xs))\n\n(defun sum-of-nums (xs)\n  (apply #'+ xs))\n\n(defun mean-of-nums (xs)\n  (float (/ (float (sum-of-nums xs))\n            (count-of-nums xs))))\n```\n\n1. 对 region 进行统计，注意不能是 rectangle [http://sugano-nu.github.io/emacs-lisp-stat-attitude/](link)\n\n```emacs-lisp\n(defun statistics-in-region (start end)\n  (interactive \"r\")\n  (setq LIST (split-string (buffer-substring start end)))\n  (setq LIST (mapcar* #'(lambda (X) (math-read-number X)) LIST))\n  (message \"length:%s, mean:%s, variance:%s, standard deviation:%s, meidan:%s, max:%s, min:%s\"\n           (length LIST)\n           (math-format-number (calcFunc-vmean (cons 'vec LIST)))\n           (math-format-number (calcFunc-vvar (cons 'vec LIST)))\n           (math-format-number (calcFunc-vsdev (cons 'vec LIST)))\n           (math-format-number (calcFunc-vmedian (cons 'vec LIST)))\n           (math-format-number (calcFunc-vmax (cons 'vec LIST)))\n           (math-format-number (calcFunc-vmin (cons 'vec LIST)))))\n\n;; not chose a string with character, it will be as 0\n(defun statistics-in-rectangle (start end)\n  \"statistics-in-rectangle.\"\n  (interactive \"r\")\n  (setq LIST nil)\n  (dolist (element (extract-rectangle start end)) ;; extract-rectangle to lists\n    (setq LIST (append LIST (split-string element)))) ;; apend to list\n  ;; (setq LIST (seq-map 'string-to-number LIST)) ;;  convert to num\n  (setq LIST (mapcar* #'(lambda (X) (math-read-number X)) LIST))\n  (message \"length:%s, mean:%s, variance:%s, standard deviation:%s, meidan:%s, max:%s, min:%s\"\n           (length LIST)\n           (math-format-number (calcFunc-vmean (cons 'vec LIST)))\n           (math-format-number (calcFunc-vvar (cons 'vec LIST)))\n           (math-format-number (calcFunc-vsdev (cons 'vec LIST)))\n           (math-format-number (calcFunc-vmedian (cons 'vec LIST)))\n           (math-format-number (calcFunc-vmax (cons 'vec LIST)))\n           (math-format-number (calcFunc-vmin (cons 'vec LIST)))))\n```\n\n1. toggle function: cycle status\n\n```emacs-lisp\n;; http://xahlee.info/emacs/emacs/elisp_toggle_command.html\n(defun company-toggle-delay (@n)\n  \"Company toggle idle delay.\"\n  (interactive \"p\")\n  (let* (\n         ($values [0.2 nil])\n         ($index-before\n          (if (get 'company-toggle-delay 'state)\n              (get 'company-toggle-delay 'state)\n            0))\n         ($index-after (% (+ $index-before (length $values) @n) (length $values)))\n         ($next-value (aref $values $index-after)))\n\n    (put 'company-toggle-delay 'state $index-after)\n    (setq company-idle-delay  $next-value)\n    (message \"company idle delay to %s\" $next-value)))\n```\n\n1. if mode then\n\n```emacs-lisp\n(if (derived-mode-p 'c++-mode)\n    (message \"The mode is on\")\n    (message \"The mode is off\")\n)\n```\n\n1. yes or no\n\n```emacs-lisp\n(if (yes-or-no-p \"Do you really want to remove everything?\")\n    (message \"yes\")\n    (message \"no\"))\n```\n\n1. auto add, commit and push\n\n```emacs-lisp\n(defun aborn/simple-git-commit-push ()\n  \"Simple commit current git project and push to its upstream.\"\n  ;; (interactive \"sCommit Message: \")\n  (interactive)\n  (when (and buffer-file-name\n             (buffer-modified-p))\n    (save-buffer))                   ;; save it first if modified.\n  (magit-diff-unstaged)\n  (when (yes-or-no-p \"Do you really want to commit everything?\")\n    (magit-stage-modified)\n    ;; (magit-mode-bury-buffer)\n    (magit-diff-staged)\n    (setq msg (read-string \"Commit Message: \"))\n    (when (= 0 (length msg))\n      (setq msg (format-time-string \"commit by magit in emacs@%Y-%m-%d %H:%M:%S\"\n                                    (current-time))))\n    (message \"commit message is %s\" msg)\n    ;;(magit-commit (list \"-m\" msg))\n    (magit-call-git \"commit\" \"-m\" msg)\n    (magit-push-current-to-upstream nil)\n    (message \"now do async push to %s\" (magit-get \"remote\" \"origin\" \"url\")))\n  (magit-mode-bury-buffer))\n```\n\n## Reference\n\n1. [Xah 教程](http://ergoemacs.org/emacs/elisp_basics.html)\n1. [BuildYourOwnLispCn](https://github.com/ksco/BuildYourOwnLispCn)\n"},{"title":"Haskell","url":"/2021/06/22/Haskell/","content":"\n\n\n## Higher Order Functions\n1. Haskell functions can take functions as parameters and return functions as return values. A function that does either of those is called a higher order function.\n1. Every function in Haskell officially only takes one parameter.\n1. `map` takes a function and a list and applies that function to every element in the list, producing a new list.\n1. `filter` is a function that takes a predicate (a predicate is a function that tells whether something is true or not, so in our case, a function that returns a boolean value) and a list and then returns the list of elements that satisfy the predicate.\n1. `folds` They're sort of like the map function, only they reduce the list to some `single value`.\n1.[map, filter, fold 示意图](https://excalidraw.com/#json=5033777575231488,ixM2dP13DMyzxCvIDEpzqg)\n1. map是一种抽象，屏蔽了对list操作的具体实现，不用关心是迭代或是递归实现的，只能感受到list每个元素都经过了转化，不用考虑控制结构，只需要考虑策略。(link: SICP)\n\n## Functors, Applicatives And Monads\n1. defination\n\n```\nclass Functor f where\n    fmap :: (a -> b) -> f a -> f b\n\nclass (Functor f) => Applicative f where\n    pure :: a -> f a\n    (<*>) :: f (a -> b) -> f a -> f b\n\nclass (Applicative m) => Monad m where\n    return :: a -> m a\n    (>>=) :: m a -> (a -> m b) -> m b\n    (>>) :: m a -> m b -> m b\n    x >> y = x >>= \\_ -> y\n    fail :: String -> m a\n    fail msg = error msg\n\nclass Monoid m where  -- 注意是Monoid不是Monad\n    mempty :: m\n    mappend :: m -> m -> m\n    mconcat :: [m] -> m\n    mconcat = foldr mappend mempty\n```\n1.  type of fmap, (<\\*>) and (>>=)\n\n```\nfmap :: (Functor f) => (a -> b) -> f a -> f b           -- 先解f a封装到a, a -> b, b再封装到f b; 操作2次封装\n(<*>) :: (Applicative f) => f (a -> b) -> f a -> f b    -- 先解f a封装到a, 解f(a -> b)封装， a -> b, b再封装到f b; 操作3次封装\n(>>=) :: (Monad m) => m a -> (a -> m b) -> m b          -- 先解m a封装到a, a -> mb; 操作1次封装\n```\n\n3. Functor, Applicative and Monad都是封装，可以封装原始类型。\n4. <$> 是 fmap 的中缀表达式版本\n5. functors: you apply a function to a wrapped value using fmap or <$>\n6. applicatives: you apply a wrapped function to a wrapped value using <*> or liftA\n7. monads: you apply a function that returns a wrapped value, to a wrapped value using >>= or liftM\n8.  conclusion\n  - A functor is a data type that implements the Functor typeclass.\n  - An applicative is a data type that implements the Applicative typeclass.\n  - A monad is a data type that implements the Monad typeclass.\n  - A Maybe implements all three, so it is a functor, an applicative, and a monad.\n9. 区别\n## Reference\n1. [Learn you haskell book](http://learnyouahaskell.com/chapters)\n3. [Functors, Applicatives, And Monads In Pictures](https://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html)\n"},{"title":"Chrome shortcuts","url":"/2021/05/21/Chrome-config/","content":"\n## base\n\n1. 对视频右键两次，选择画中画模式，方便记笔记\n1. [番茄时钟画中画](https://lazy-guy.github.io/tomodoro/index.html)\n1. 选择右击`复制指向突出显示的内容的链接`\n\n## [shortcuts](https://support.google.com/chrome/answer/157179?hl=zh-Hans&co=GENIE.Platform%3DDesktop)\n\n1. `C-l`或`M-d`: 地址栏\n1. `M-Enter`: open another tab when search: it's very usefull\n1. `C-k` 或 `C-e`: 搜索\n1. `Tab`: 下一个可点击项\n1. `M-left`: 后退浏览历史\n1. `M-right`: 前进浏览历史\n1. `C-r`: 刷新网页\n1. `Space`: pgDn\n1. `S-Space`: pgUp\n1. `C-t`: 新标签\n1. `C-n`: 新窗口\n1. `C-w`: 关闭当前标签\n1. `C-Tab`: 切换标签\n1. `C-S-t`: 恢复关闭的标签\n1. `C-h`: history\n1. `C-j`: download\n1. `C-1`: 跳转到第一个标签\n1. `C-9`: 跳转到第一个标签\n1. `C-Backspace`: 删除一个单词\n1. `S-Esc`: 任务管理器\n1. `C-S-b`: 显示或隐藏书签栏\n1. `C-f` + `C-g`: 查找与跳转\n\n## [自定义 shortcuts](https://chrome.google.com/webstore/detail/shortkeys-custom-keyboard/logpjaacgmcbpdkdchjiaagddngobkck?hl=zh-CN)\n\n1. `M-h`：Select previous tab\n1. `M-l`：Select next tab\n1. `M-k`：Select last tab\n1. `M-b`: 后退浏览历史\n1. `M-f`: 前进浏览历史\n1. `M-s`: fullscreen\n1. `M-3`：new tab\n1. `M-8`：close tab\n1. `M-w`：Duplicate current tab\n\n## [vimium 快捷键](https://vimium.github.io/)\n\n1. `yy`: copy URL to clipboard\n1. `/`: find\n1. `j`: Scroll down\n1. `k`: Scroll up\n1. `h`: Scroll left\n1. `l`: Scroll right\n1. `gg`: Scroll to the top of the page\n1. `G`: Scroll to the bottom of the page\n\n## 搜索设置\n\n1. chrome 快捷搜索设置\n\n```\ngitlab l https://gitlab.phigent.io/?sort=name_asc&name=%s&sort=name_asc\ngithub g https://github.com/search?q=%s\ncppreference cpp https://duckduckgo.com/?sites=cppreference.com&q=%s&ia=web\ntranslate t https://translate.google.cn/?source=osdd&who=test&sl=en&tl=zh-CN&text=%s&op=translate\nblog blog https://github.com/jiaxiyang/blog/tree/master/source/_posts\nlinks links https://jiaxiyang.github.io/2020/06/29/Links/\n100ms 100ms https://github.com/jiaxiyang/100ms_dot_emacs/blob/master/init.el\nlead lead https://github.com/jiaxiyang/leader-key-mode/blob/master/leader-key-mode.el\nmelpa m https://melpa.org/#/?q=test&sort=downloads&asc=false\nexcalidraw e https://excalidraw.com/\nbaidu b https://www.baidu.com/#ie={inputEncoding}&wd=%s\n```\n\n## 插件\n\n1. [onetab 节省内存](https://chrome.google.com/webstore/detail/onetab/chphlpgkkbolifaimnlloiipkdnihall) 快捷键`alt+shift+1` 改为 `alt+1`。 `alt+2`发送当前标签页至 OneTab\n1. [广告屏蔽](https://chrome.google.com/webstore/detail/adblocker-ultimate/ohahllgiabjaoigichmmfljhkcfikeof/related?hl=zh-CN)\n1. [sourcegraph 插件](https://chrome.google.com/webstore/detail/sourcegraph/dgjhfomjieaadpoljlnidmbgkdffpack)\n1. [octotree](https://chrome.google.com/webstore/detail/octotree-github-code-tree/bkhaagjahfmjljalopjnoealnfndnagc?hl=zh-CN)\n1. [gitzip 插件](https://chrome.google.com/webstore/detail/gitzip-for-github/ffabmkklhbepgcgfonabamgnfafbdlkn)\n1. [vimiu](https://vimium.github.io/)\n1. [shortkeys](https://chrome.google.com/webstore/detail/shortkeys-custom-keyboard/logpjaacgmcbpdkdchjiaagddngobkck/related?hl=zh-CN)\n1. [magical](https://chrome.google.com/webstore/detail/magical-text-expansion/iibninhmiggehlcdolcilmhacighjamp/related?hl=zh-CN)\n1. [colorzilla](https://chrome.google.com/webstore/detail/colorzilla/bhlhnicpbhignbdhedgjhgdocnmhomnp?hl=zh-CN)\n1. （good)[沉侵式翻译](https://immersivetranslate.com/)\n   - arxiv 的文章也可以用\n   - youtube 双语字幕\n   - `Alt A`\n1. [ImTranslator: 翻译，字典，声音](https://chromewebstore.google.com/detail/imtranslator-%E7%BF%BB%E8%AF%91%EF%BC%8C%E5%AD%97%E5%85%B8%EF%BC%8C%E5%A3%B0%E9%9F%B3/noaijdpnepcgjemiklgfkcfbkokogabh)\n1. (good)[侧边翻译](https://chromewebstore.google.com/detail/%E4%BE%A7%E8%BE%B9%E7%BF%BB%E8%AF%91/bocbaocobfecmglnmeaeppambideimao?hl=zh-CN)\n1. [google translate](https://chrome.google.com/webstore/detail/google-translate/aapbdbdomjkkjkaonfhkkikfgjllcleb/related?hl=en)\n1. [omni](https://github.com/alyssaxuu/omni)\n1. [easy-auto-refresh](https://chrome.google.com/webstore/detail/easy-auto-refresh/aabcgdmkeabbnleenpncegpcngjpnjkc/related)\n1. [markdown-diagrams](https://chrome.google.com/webstore/detail/markdown-diagrams/pmoglnmodacnbbofbgcagndelmgaclel)\n1. [svg-export](https://chrome.google.com/webstore/detail/svg-export/naeaaedieihlkmdajjefioajbbdbdjgp/related?hl=zh-CN)\n1. [whatfont](https://chrome.google.com/webstore/detail/whatfont/jabopobgcpjmedljpbcaablpmlmfcogm/related?hl=zh-CN)\n1. [fireshot 截屏工具](https://chromewebstore.google.com/detail/%E6%8D%95%E6%8D%89%E7%BD%91%E9%A1%B5%E6%88%AA%E5%9B%BE-fireshot%E7%9A%84/mcbpblocgmgfnpjjppndjkmgjaogfceg?hl=zh-CN&utm_source=ext_sidebar)\n","tags":["Tools"],"categories":["Tools","Chrome"]},{"title":"Jetson test","url":"/2021/05/14/Jetson-test/","content":"\n## basic\n1. 使用TensorRT中的trtexec测性能\n2. 模型转化为engine后运行\n\n## links\n1. https://github.com/NVIDIA-AI-IOT/jetson_benchmarks\n1. https://developer.nvidia.com/embedded/jetson-benchmarks\n1. https://github.com/dusty-nv/jetson-inference\n1. https://github.com/NVIDIA/TensorRT/tree/release/7.1/samples/opensource/trtexec\n","tags":["Jetson"],"categories":["Computer","GPU"]},{"title":"3d Detection","url":"/2021/05/10/3d_detection/","content":"\n## lidar data\n\n1. [format](https://www.shuangyi-tech.com/news_161.html)\n2. pts、LAS、PCD、.xyz 和.pcap\n\n## PCL\n\n1. install pcl and pcl-dev or install from source\n2. [dependency](https://pcl.readthedocs.io/projects/tutorials/en/latest/compiling_pcl_posix.html)\n3. pcl version > 1.10 否则匹配不上 vtk\n4. notes: update yum source when dependency error\n5. [pcd](https://blog.csdn.net/a464057216/article/details/54864591)\n6. yum search boost; yum infoe boost;\n7. yum install boost169 boost169-devel libusbx-devel\n8. `cmake -DBOOST_INCLUDEDIR=/usr/include/boost169 -DBOOST_LIBRARYDIR=/usr/lib64/boost169 ..`\n9. rm find_package(openni) and rm with_openni\n10. [passthrough 例子](https://pointclouds.org/documentation/tutorials/passthrough.html)\n\n## ouster lidar\n\n1. [forum](https://forum.ouster.at/)\n1. 不支持 pcd 格式输出，支持 pcap 格式\n\n## links\n\n1. [survey](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9127813)\n2. [A_Survey_on_3D_Object_Detection_Methods_for_Autonomous_Driving_Applications](A_Survey_on_3D_Object_Detection_Methods_for_Autonomous_Driving_Applications)\n3. [SECOND](https://github.com/traveller59/second.pytorch)\n4. [pointpillars](https://blog.csdn.net/W1995S/article/details/114498082?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0.no_search_link&spm=1001.2101.3001.4242)\n5. [点云目标检测之 VoxelNet、SECOND、PointPillars、PV-RCNN](https://zhuanlan.zhihu.com/p/75103748)\n","tags":["AI"],"categories":["Algorithm","AI"]},{"title":"TensorRT","url":"/2021/04/13/TensorRT/","content":"\n## NOTE\n\n1. 多看开发文档 [developer guide](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work-with-loops)\n1. [data layout](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#data-layout)\n   - While optimizing the network, TensorRT performs transformations internally (including to HWC, but also more complex formats) to use the fastest possible CUDA kernels. In general, formats are chosen to optimize performance, and applications have no control over the choices. However, the underlying data formats are exposed at I/O boundaries (network input and output, and passing data to and from plugins) to allow applications to minimize unnecessary format transformations.\n   - 模型内部只支持 hwc layout（cuda kernel 效率更高), 如果输入是 chw, tensort 会内部做转换成 hwc\n   - 输入输出都是 hwc，会更高效一些\n1. tensorrt log 设置，打印出 warning 和 error: [link](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#build-phase-c), trt 内部代码会使用 ILogger 的 log 函数，内部有各种级别的 log\n\n```c++\nclass Logger : public ILogger\n{\n    void log(Severity severity, const char* msg) noexcept override\n    {\n        // suppress info-level messages\n        if (severity <= Severity::kWARNING)\n            std::cout << msg << std::endl;\n    }\n} logger;\n```\n\n## 转 trt engine 方法\n\n1. [torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt)\n1. TensorRT creates an optimized engine for each profile, choosing CUDA kernels that work for all shapes within the [minimum, maximum] range and are fastest for the optimization point - typically different kernels for each profile. You can then select among profiles at runtime.\n   - 为了优化 dynamic shape 需要提高最大最小 shape\n1. python 用 trt 部署验证模型更有效率\n1. pytorch2.0 之后有了原生的 IR, 可以修改和运行 IR, 也可以直接支持转换成 trt 模型，简化了 torch -> onnx model -> trt engine -> load to torch 的流程\n1. [Torch-TensorRT](https://pytorch.org/TensorRT/) [github](https://github.com/pytorch/TensorRT)\n1. [官方两种转 trt 工具](https://github.com/onnx/onnx-tensorrt?tab=readme-ov-file#executable-usage)\n   - trtexec `trtexec --onnx=model.onnx`\n   - polygraph `polygraphy run model.onnx --trt`\n1. [onnx-tensorrt](https://github.com/onnx/onnx-tensorrt?tab=readme-ov-file#executable-usage) python 接口，直接导出 trt\n1. onnx 转 trt 两种方式：\n   - onnx --> engine --> c++ inference\n   - onnx --> c++ convert and inference\n1. 使用 trtexec 先将 ONNX 转换成 engine\n   - 优点：\n     - 简单性：trtexec 是一个命令行工具，可以很容易地用于转换模型，并且不需要编写任何额外的代码。\n     - 灵活性：trtexec 提供了许多命令行选项，用于优化和调试转换过程。\n     - 性能测试：您可以使用 trtexec 来测试模型的性能，这在评估优化策略时很有用。\n     - 重用：一旦创建了 engine 文件，您可以在不同的应用程序或设备上多次重用它，而不必再次进行优化。\n   - 缺点： -两步流程：首先需要使用 trtexec 创建 engine，然后在 C++程序中加载它。这使得整体流程分为两个步骤，可能不如单步流程简洁。\n1. 直接使用 C++ 接口转换 ONNX 然后进行推理\n   - 优点：\n     - 集成：整个流程，从加载 ONNX 模型到执行推理，都在一个 C++程序中完成。这对于集成到大型系统或自动化流程中很有用。\n     - 动态性：可以根据需要进行动态优化或调整，例如基于不同的输入尺寸或条件。\n   - 缺点：\n     - 复杂性：与使用命令行工具相比，需要编写和维护更多的代码。\n     - 优化时间：每次运行程序时，如果不保存和重用 engine，都可能需要重新进行模型的优化步骤。\n\n## base\n\n1. 使用 cuda graph 加速 trt 模型 trtexec --useCudaGraph\n1. onnx 转 trt 时假如输入部分和输出部分有些不好支持，可以提到模型外面去做\n1. 模型转换后输出 tensor 顺序和 onnx 顺序不一定对应 [onnx-model-and-tensorrt-engine-gives-different-output](https://forums.developer.nvidia.com/t/onnx-model-and-tensorrt-engine-gives-different-output/193061)\n1. 推理是使用训练好的模型进行预测的过程，而部署是将这个模型集成到最终应用环境的过程。\n1. [推理系统优化目标](https://microsoft.github.io/AI-System/SystemforAI-8-Inference.pdf)\n   - 延迟(Latency): 满足服务等级协议的延迟\n   - 吞吐量(Throughputs): 暴增负载的吞吐量需求\n   - 效率(Efficiency): 高效率，低功耗使用 GPU，CPU\n   - 灵活性(Flexibility): 支持多种框架，提供构建不同应用的灵活\n   - 扩展性(Scalability): 扩展支持不断增长的用户或设备\n1. 推理考虑延迟、吞吐量和效率，部署考虑灵活性和可扩展性\n1. 提高 batch size 用于提高吞吐\n1. 等待浪费 GPU, 造成低效率\n1. `PyTorch-Quantization` is a toolkit for training and evaluating PyTorch models with simulated quantization. Quantization can be added to the model automatically, or manually, allowing the model to be tuned for accuracy and performance. The quantized model can be exported to ONNX and imported to an upcoming version of TensorRT. PyTorch-Quantization 是一个用于通过模拟量化来训练和评估 PyTorch 模型的工具包。量化可以自动或手动添加到模型中，从而可以调整模型的准确性和性能。量化模型可以导出到 ONNX 并导入到即将推出的 TensorRT 版本。\n1. [work with loop](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work-with-loops)\n1. rnn 在 9.0 被移除，使用 loop op 替代\n1. NVIDIA TensorRT is an SDK for optimizing trained deep learning models to enable high-performance inference. TensorRT contains `a deep learning inference optimizer for trained deep learning models`, and `a runtime for execution`.\n1. [转换模型步骤](https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-861/quick-start-guide/index.html#ecosystem)\n1. [多种转换和部署方式](https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-861/quick-start-guide/index.html#conv-deploy-opt)\n1. all operations in your model must be supported by TensorRT (or you must provide custom plug-ins for unsupported operations)\n1. at inference, we pick a small batch size when we want to prioritize latency and a larger batch size when we want to prioritize throughput.\n1. TensorRT supports TF32, FP32, FP16, and INT8 precisions.\n1. One of the most performant and customizable options for `both model conversion and deployment` are to use the TensorRT API, which has both C++ and Python bindings.\n1. TensorRT C++ API 中的接口类以前缀 I(interface 的意思) 开头，例如 ILogger 、 IBuilder 等。\n1. 同步推理: `executeV2`方法; 异步推理: `enqueueV2`\n1. `enqueue`异步接口没有很快返回的原因：\n   - 有插件会强制 stream 同步？\n   - nsight system 上看 tensorrt node 调用， 对应 cuda api 里有 cudaStreamSync()函数， 会阻塞 cpu 导致 enqueue 不返回\n   - 有多个 stream sync, 每个 stream sync 执行之后之前通过 cuda api 调用的 kernel 都已执行完\n   - 为什么要多个 stream？node 不相关可以并行加速, 可以看到 kernel 执行时间有并行， 为什么要 sync? 后面的节点需要前面的节点都执行完，有关联\n   - [控制单模型 stream 个数 --maxAuxStreams](https://github.com/NVIDIA/TensorRT/issues/2828#issuecomment-1488371967) Set maximum number of auxiliary(辅助) streams per inference stream.\n   - [within-inference-multi-streaming](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#within-inference-multi-streaming)\n     - In the context of TensorRT and inference, each layer of the optimized final network will require work on the GPU. However, not all layers will be able to fully use the computation capabilities of the hardware. Scheduling requests in separate streams allows work to be scheduled immediately as the hardware becomes available without unnecessary synchronization. Even if only some layers can be overlapped, overall performance will improve.\n     - 如果放在单独流中调度，不能充分利用硬件调度能力，将 request 放在多个流调度时，硬件准备好就能使用，不用额外的 sync, 即使只有少数的 layer 并行，总体结果也会提高\n   - [trtexec-flags 搜--maxAuxStreams](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#trtexec-flags)\n   - enqueue 可能不是真异步，内部有同步\n   - 想并行运行多个模型需要使用多线程，net1.enqueueV2(stream1); net2.enqueueV2(stream2);\n1. 注意 event sync 和 stream sync 的区别; event sync 在 enqueue 之后执行，enqueue 执行时可能会多次调用 stream sync\n1. ![NVIDIA 推理堆栈](https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/nvidia-inference-stack.png)\n1. 在 TensorRT 8.5 及更高版本中， cuDNN 和 cuBLAS 内核是可选的添加，以减少 TensorRT 库的部署大小。\n1. [Tensor Layouts In Memory: NCHW vs NHWC](https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#tensor-layout)\n1. 因为为 Tensor Core 实现的卷积需要 NHWC 布局，并且当输入张量以 NHWC 布局时速度最快\n1. NHWC 格式的输入非常适合 NVIDIA 上的 Tensor Core GPU 。由于 ONNX 仅支持 NCHW 格式(?)，因此必须使用技巧启用 NHWC 作为输入张量。将输入维度设置为 NHWC ，并在 CUDA 或 TensorRT EP 删除的输入之后插入 Transpose 操作\n1. tensor core 用 Implicit GEMM 实现的卷积不用生成中间的矩阵，直接原位计算， 不需要 im2col [link](https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#tc-usage)\n1. The process of identifying the sequences of operations that can be fused is called `pattern-matching`. TensorRT has a powerful pattern-matching algorithm that can identify a lot of possible fusions.\n1. 可以使用 python API 来调用转换后的 engine 来和 torch 结果作比较\n1. workspace\n   - 在 TensorRT 中，工作空间（Workspace）是用于存储临时数据（如中间层输出、优化过程中的临时数据）的内存区域。它主要在模型优化和执行过程中被使用。\n   - 较大的工作空间可以允许 TensorRT 执行更复杂的优化，可能带来更高的运行效率。然而，过大的工作空间可能导致内存资源的浪费或者在内存受限的设备上无法运行。\n   - 在使用 TensorRT API 时，可以通过编程方式配置工作空间的大小。这通常在`构建优化引擎（Optimization Engine）时设置`\n   - [Q: How do I choose the optimal workspace size?](https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-722/developer-guide/index.html#troubleshooting)。\n\n## [Using the TensorRT Runtime API](https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html#runtime)\n\n### runtime\n\n1. `std::unique_ptr<nvinfer1::IRuntime> runtime{nvinfer1::createInferRuntime(sample::gLogger.getTRTLogger())};`\n\n### engine\n\n1. `std::unique_ptr<nvinfer1::ICudaEngine> mEngine(runtime->deserializeCudaEngine(engineData.data(), fsize, nullptr));` runtime deserialize 到 engine\n1. `auto input_idx = mEngine->getBindingIndex(\"input\");` 得到 input idx\n1. `assert(mEngine->getBindingDataType(input_idx) == nvinfer1::DataType::kFLOAT);` 判断输入类型\n1. 通过 engine 拿到输入输出信息，申请 host 和 device 内存，host 上处理完输入之后，copy to device， 执行模型，将结果从 device cp to host\n\n### context\n\n1. `IExecutionContext *context = engine->createExecutionContext();`\n1. you can have multiple contexts associated with a single engine, and run them in parallel. 多个 context 可以与一个 engine 结合，并行的运行\n   - A current exception to this is when using dynamic shapes, when each optimization profile can only have one execution context, unless the preview feature, kPROFILE_SHARING_0806, is specified. 动态 shape engine 只能有一个 context\n1. A TensorRT execution context encapsulates execution state such as persistent device memory for holding intermediate activation tensors during inferenc\n1. Inference execution is kicked off using the context’s executeV2 or enqueueV2 methods. After the execution is complete, we copy the results back to a host buffer and release all device memory allocations\n   - executeV3\n   - enqueueV3 `bool status = context->enqueueV3(bindings, stream, nullptr);`\n1. `void* bindings[] = {input_mem, output_mem};` input_mem and output_mem 在 gpu device 上\n1. `auto output_buffer = std::unique_ptr<int>{new int[output_size]};` on cpu\n1. `cudaMemcpyAsync(output_buffer.get(), output_mem, output_size, cudaMemcpyDeviceToHost, stream);` device to host\n1. `cudaStreamSynchronize(stream);`\n\n## [NVIDIA Multi-Instance GPU User Guide](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html)\n\n1. 多实例 GPU (MIG) 是采用 NVIDIA Ampere 架构或更高版本架构的 NVIDIA GPU 的一项功能，可实现用户控制的将单个 GPU 划分为多个较小的 GPU。物理分区提供专用计算和内存切片，具有 QoS 以及在部分 GPU 上独立执行并行工作负载。对于 GPU 利用率较低的 TensorRT 应用程序，MIG 可以在对延迟影响很小或没有影响的情况下产生更高的吞吐量。最佳分区方案是特定于应用程序的。\n\n## [ecosystem](https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html#ecosystem)\n\n1. workflow\n   - 导出 onnx 模型\n   - 选择 batch size\n   - 选择量化精度\n   - 转换模型\n   - 发布模型\n1. 有多种方式可以转换模型\n1. 有多种方式可以部署模型\n\n## [fusion](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#enable-fusion)\n\n## [dla](https://developer.nvidia.com/deep-learning-accelerator)\n\n1. TensorRT allows you to execute part of the network on the DLA and the rest on GPU; engine 可以一部分在 dla 上执行，另外的在 cuda 上\n1. [jetson_dla_tutorial](https://github.com/NVIDIA-AI-IOT/jetson_dla_tutorial)\n1. 只有 edge 平台才有\n1. jtop -> ENG 上方显示有几个 dla core\n\n## quantization\n\n1. FP32, FP16, BF16: unquantized floating point types\n1. fp16 不算量化\n1. low precision quantized types (INT8, FP8, INT4)\n1. TensorRT supports quantized floating point, where floating-point values are `linearly` compressed and rounded to low precision quantized types (INT8, FP8, INT4). 线性量化\n1. TensorRT must know its dynamic range - that is, what range of values is important to represent - values outside this range are clamped when quantizing. 量化时需要知道数据的动态范围，会做截断\n1. Dynamic range information can be calculated by the builder (this is called calibration) based on representative input data (this is currently supported only for INT8). activation 量化时， 校准时会收集 activation 的动态范围\n1. TensorRT supports the use of low precision types to represent quantized floating point values. The quantization scheme is symmetric quantization - quantized values are represented in signed INT8, FP8E4M3 (FP8 for short), or signed INT4, and the transformation from quantized to unquantized values is simply a multiplication. In the reverse direction, dequantization uses the reciprocal scale, followed by rounding and clamping.\n   - 对称量化， 反量化时只是简单相乘，量化时还需要钳位\n1. TensorRT quantizes activations as well as weights to INT8 and FP8. For INT4, weight-only-quantization is supported.\n   - int4 只支持 weight 量化, 不支持 activation\n1. 对称线性量化： Q = F/s(需要钳位); s = (Fmax - Fmin) / (Qmax - Qmin); 反量化：F = Q x s\n   - s: scale 即最小刻度，Q 表示 F 有多少个最小刻度\n1. scale 是核心，ptq 和 qat 都是为了得到 scale\n1. Post-training quantization (PTQ) derives scale factors after the network has been trained. TensorRT provides a workflow for PTQ, called calibration, where it measures the `distribution of activations` within each activation tensor as the network executes on representative input data, and then uses that distribution to estimate scale values for each tensor.\n1. calibration 只是为了得到 activation 的分布来估计量化时 tensor 的 scale，weight 不需要\n1. Quantization-aware training (QAT) computes the scale factors during training, using a technique called fake-quantization which simulates the quantization and quantization process. This allows the training process to compensate for the effects of the quantization and dequantization operations.\n   - 量化感知训练 (QAT) 使用一种称为假量化的技术来模拟量化和量化过程，从而在训练期间计算比例因子。这允许训练过程补偿量化和反量化操作的影响。\n1. Implicit quantization is only supported when quantizing for INT8 只有 int8 支持隐式量化\n\n## [streaming weights](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#streaming-weights)\n\n1. TensorRT can be configured to stream the network’s weights from host memory to device memory during network execution instead of placing them in device memory at engine load time. This enables models with weights larger than free GPU memory to run, but potentially with significantly increased latency.\n   - 需要 host to device, 这使得权重大于可用 GPU 内存的模型可以运行，但延迟可能会显着增加。\n\n## plugins(custom op)\n\n1. 注意 shape 区别：固定输出 shape, shape 有输入决定(axie), 动态 shape\n1. nonzero 才是动态 shape，输出 shape 不能由输入决定, nonzero 可以由 padding 来解决\n1. 构造函数一般设置为三个。\n   - 第一个用于在 parse 阶段，PluginCreator 用于创建该插件时调用的构造函数，需要传递权重信息以及参数。\n   - 第二个用于在 clone 阶段，复制这个 plugin 时会用到的构造函数。\n   - 第三个用于在 deserialize 阶段，用于将序列化好的权重和参数传入该 plugin 并创建。\n1. TensorRT 在 fp16 运行模式下，运行到不支持 fp16 的插件 op 时，会自动切换到 fp32 模式，等插件 op 运行完再切换回来。\n1. getWorkspaceSize 这个函数需要返回这个插件 op 需要中间显存变量的实际数据大小(bytesize), 在实际运行的时候就可以直接使用 TensorRT 开辟好的空间而不是自己去申请显存空间。\n1. serialize 将 plugin 实例序列化到文件（包含参数，属性等信息）（在 plugin 里 TRTBEVPoolV2::serialize）， deserialize 通过文件发序列化到 plugin 实例(构造插件，在 creator 里做 TRTBEVPoolV2Creator::deserializePlugin;)\n1. createPlugin 用于 build 阶段，生成 plugin 实例(注意不是序列化，序列化是存文件)， deserializePlugin 用于 runtime，用于从 engine 反序列化 plugin 对象\n1. onnx 转换成 engine 存成文件，需要序列化，加载 engine 需要反序列化\n1. supportsFormatCombination 用于检测格式和类型是否支持\n1. tensorrt 支持的 op， 提供的标准 plugins, 自定义的 plugin\n1. 分为 build 期和 runtime 期，两个时期需要插件的功能不一样，build 是构建网络用的，runtime 是运行插件时用的\n1. IPluginV3OneCore (build 和 runtime 都需要的接口), IPluginV3OneBuild (build 需要的接口), and IPluginV3OneRuntime(runtime 需要的接口)\n1. core capability 是 build 和 runtime 都需要的接口\n1. [实现流程](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#add_custom_layer)\n1. [注册方法](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#register-plugin-create)\n   - [手动 initLibNvInferPlugins](https://github.com/NVIDIA/TensorRT/blob/9db1508f7479b24857d4fb81e33b6b10cede6bd0/plugin/api/inferPlugin.cpp#L180)\n   - [自动 REGISTER_TENSORRT_PLUGIN](https://github.com/NVIDIA/TensorRT/blob/9db1508f7479b24857d4fb81e33b6b10cede6bd0/plugin/geluPlugin/geluPlugin.cpp#L43) static 变量\n   - trtexec 通过 dlopen 方式加载注册，python 通过 ctypes.CDLL 方式加载注册，pgpredict 通过 link 方式启动时加载注册\n   - [using-plugin-library](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#using-plugin-library) runtime 可以手动加载， 不用 link\n     - `runtime->getPluginRegistry().loadLibrary(pluginLib.c_str())` runtime 加载\n     - `builder->getPluginRegistry().loadLibrary(pluginLibs[i]);` build 时加载\n1. onnx 和 trt plugin 可以通过属性传递参数，封装自定义算子 symbolic g.op()写入属性，trtplugin 代码可以获取属性\n1. 要在应用程序中使用 TensorRT 官方插件，必须加载 libnvinfer_plugin.so （Windows 上为 nvinfer_plugin.dll ）库，并且必须通过在应用程序中调用 initLibNvInferPlugins 来注册所有插件代码。\n1. 几个概念：\n   - plugin: 具体 plugin 实现， 需要继承 IPluginV2DynamicExt\n   - plugin creator: plugin 对象创建方法，相当于抽象工场函数的具体工厂, 改工厂的实例会被注册到全局工厂(注册表)中，用来创建 creator\n   - register: 将 plugin creator 工厂函数注册到, tensorrt sample 中注册位置[register tensorrt plugin](https://github.com/NVIDIA/TensorRT/blob/main/plugin/api/inferPlugin.cpp); 单例模式实现\n     - 需要调用 getPluginRegistry()->registerCreator(\\*pluginCreator, libNamespace)来注册\n     - REGISTER_TENSORRT_PLUGIN 宏也可以来注册：#define REGISTER_TENSORRT_PLUGIN(name) static nvinfer1::PluginRegistrar<name> pluginRegistrar##name {}; getPluginRegistry()->registerCreator(instance, \"\");\n1. 设计模式插件系统; so 加载时 static 变量初始化注册工厂实例到全局工厂；反射 由字符串来创建对象\n1. [官方 sample](https://github.com/NVIDIA/TensorRT/tree/main/samples/sampleOnnxMnistCoordConvAC)\n   - [plugin 代码位置](https://github.com/NVIDIA/TensorRT/tree/main/plugin/coordConvACPlugin)\n   - enqueue 在 cu 文件中实现\n   - 自己写的插件可能用不到 workspace, 见上述代码 enqueue 函数\n   - onnx shape_inference 在模型上显示 shape 信息\n   - 修改模型时要照着 net forward 来改 op input(tensor) output name\n   - 自定义 op 内可变参数作为输入, 不变数值直接写死, y=kx + 2; k 是参数，2 是数值, y = x + 2\n   - netron 上不变参数(weights bias div mul 需要的参数)使用 constant tensor 类型，输入是 variable tensor, attributes 是 op `初始化`用到的数值， 计算不用\n   - 参数传递与转换：在插件中，你需要将原始模型中算子的参数传递到你的插件实现中。这通常涉及到参数的序列化和反序列化。确保所有必要的参数都被正确传递，并且在需要时进行适当的数据类型转换。\n   - attention 输入为 q k v 输出 q 维度类似的结果; 参数可能为各种 shape 信息 地址 head num, scale, mask 等。如果是 paged attention 需要处理 q k v 地址不连续的情况, [link](https://zhuanlan.zhihu.com/p/638468472)\n1. [官方 plugin](https://github.com/NVIDIA/TensorRT/tree/master/plugin)\n1. ONNX 解析器会自动尝试将无法识别的节点作为插件导入。如果在插件注册表中找到与该节点具有相同 op_type 的插件\n1. python 自定义 op 用了多种内置 op 实现，在图上表现为多个内置 op，需要把 onnx 模型内置 op 转换为自定义 op 再转换模型\n1. 适用场景\n   - python 中算子 trt 不支持可以用 plugin 方式实现\n   - trt 算子性能有问题\n   - 手动算子融合(将融合算子的所有输入作为融合算子的输入， 输出同理)\n1. 合并之后的 中间 op 的参数也不用管，因为算子等价，中间参数无用\n1. op 没有权重，只是计算方法，处理输入得到输出，不用关心权重问题, 权重在输入接口,\n   - attention 插件会在接口中传递输入输出地址，kvcache 地址， 长度等信息，插件负责计算\n   - load 模型时权重会从模型 load 到 op，作为参数传递给 op；序列化与反序列化\n1. onnx 模型添加了 plugin 之后 neoton 可能显示不出 shape 信息，原因是 plugin 的存在，妨碍了自动推断;\n   - 如果 plugin 不改变 shape，直接删除掉之后，在图上就能看到 shape 信息\n1. [Extending TensorRT with Custom Layers](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#extending)\n1. 即使 TensorRT 拥有强大的模式匹配算法并支持许多可能的融合，但始终存在无法识别不常见和/或非常高级模式的风险。为了克服这一不可避免的限制，TensorRT 提供了一种称为插件的强大机制。\n1. The plugins are nodes inserted in the network graph definition that map to user-defined GPU kernels.\n\n### python plugin\n\n1. python load plugin.so [links](https://forums.developer.nvidia.com/t/loading-of-tensorrt-custom-plugin-shared-library/237504/6)\n1. [tensorrt python 使用 trt 内部结构示例](https://github.com/NVIDIA/TensorRT/blob/ba696521eed5d8b49a3c6f21ef243ca4b3474d64/samples/python/onnx_custom_plugin/test_custom_hardmax_plugin.py#L40)\n1. [python plugin 相关 bindings](https://github.com/NVIDIA/TensorRT/blob/ba696521eed5d8b49a3c6f21ef243ca4b3474d64/python/src/infer/pyPlugin.cpp#L2612)\n1. ctypes.CDLL 是 Python ctypes 库中的一个函数，用于加载动态链接库（.so 文件，DLL 文件等）。它允许你在 Python 中调用这些库中定义的函数和使用其中的资源。\n\n## 修改模型\n\n### 拆分\n\n1. [TensorRT/tools/onnx-graphsurgeon](https://github.com/NVIDIA/TensorRT/tree/master/tools/onnx-graphsurgeon)\n1. `polygraph surgeon extract`也可以提取子图\n1. 一个一个算子累加形成 grpah 可以找出出现问题的 op; 加入 polygrpah 不能用，trtexec 能用；所有 graph 设置同一个输入，和 onnx 比输出\n1. [sample](https://github.com/leimao/TensorRT-Custom-Plugin-Example/blob/main/scripts/create_identity_neural_network.py)\n\n### op 替换\n\n## build\n\n1. [源码编译](https://github.com/NVIDIA/TensorRT/tree/main?tab=readme-ov-file#optional---if-not-using-tensorrt-container-specify-the-tensorrt-ga-release-build-path)\n   - `export TRT_LIBPATH=`pwd`/TensorRT-8.6.1.6/lib`必须设置\n\n## 查看版本\n\n1. jtop -> INFO\n1. jetson_release\n1. nvidia-smi\n1. `dpkg-query -W tensorrt` tensorrt 版本\n1. `cat /proc/driver/nvidia/gpus/0000\\:02\\:00.0/information` 查看硬件型号\n\n## 查看模型信息, 对数\n\n1. 参考 python_sandbox/model_convert\n   - get_onnx_detail_info.py 能够获取算子 doc_string, 可以看堆栈，找到算子所在位置\n1. 余弦相似度来衡量两个 tensor 是否对齐\n1. pytorch 模型，onnx 模型，trt 模型都要一致\n   - pytorch 和 onnxruntime 先对数，然后 onnxruntime 和 trt 对数\n   - 有 plugin 时可以直接对 trt 和 pytorch 结果, pytorch 添加 hook 导出所有 tensor 输出，onnx 对应 pytorch 输出，onnx 切分子图，分别转成 tensorrt 模型，每一个都有输入输出，根据 onnx 模型找 pytorch 输出，看哪个算子对不上\n     - pytorch 模型 dump 数据时需要处理 sequential 和 submodule 问题，重复 layer 使用问题，onnx 模型导出时需要处理将中间层输出，名字要和 pytorch 导出的名字对上; 如果是直接 pytorch 和 trt 对数，不用对 onnx 中间结果\n     - pytorch 中间结果和 trt 中间结果做对比时， 注意 onnx 和 trt 算子可能融合，需要比较 pytorch 前后一些算子\n\n```python\ndef get_intermediate_outputs_pytorch(model, input_tensor):\n     intermediate_outputs = {}\n     hooks = []\n\n     # 处理重复使用情况\n     def hook_fn(module, input, output, name):\n        if name in intermediate_outputs:\n            count = 1\n            while name + \"_\" + conut.to_str() in intermediate_outputs：\n                count = count + 1\n            intermediate_outputs[name + \"_\" + conut.to_str()] = [output]\n        else:\n            intermediate_outputs[name] = [output]\n\n    # 递归注册钩子\n    def register_hooks(module, prefix=''):\n        for name, submodule in module.named_children():\n            submodule_name = prefix + ('.' if prefix else '') + name\n            hook_handle = submodule.register_forward_hook(lambda m, i, o, name=submodule_name: hook_fn(m, i, o, name))\n            hooks.append(hook_handle)\n            register_hooks(submodule, submodule_name)\n\n    register_hooks(model)\n\n    with torch.no_grad():\n        model(input_tensor)\n\n    # 移除钩子\n    for hook_handle in hooks:\n        hook_handle.remove()\n\n    return intermediate_outputs\n```\n\n1. [polygraphy](https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy/polygraphy/tools/)\n   - Use Polygraphy to dump layer outputs and verify there are no NaNs or Infs. The --validate option can check for NaNs and Infs. Also, we can compare layer outputs with golden values from, for example, ONNX runtime.\n   - [polygraphy-extension-trtexec](https://github.com/NVIDIA/TensorRT/tree/c5b9de37f7ef9034e2efc621c664145c7c12436e/tools/polygraphy-extension-trtexec)\n     - [位置](https://www.wheelodex.org/projects/polygraphy-trtexec/)\n   - `--mark all` is not a good practice, it will break all layer fusions. [link](https://github.com/NVIDIA/TensorRT/issues/3592#issuecomment-1888624281) [link](https://github.com/NVIDIA/TensorRT/issues/3596#issuecomment-1896408532)\n     - I think mark all is becoming increasingly unreliable. A better approach is to manually mark a subset of the intermediate tensors as outputs.\n   - `polygraphy run   sim_sparsedrive.onnx --fp16   --trt --trt-outputs /deformable_aggregation_output_0   --save-engine test.trt --plugins=/workspace/mmdeploy/mmdeploy/lib/libmmdeploy_tensorrt_ops.so   --save-outputs ./dump.json --save-timing-cache=fp16cache --load-timing-cache=fp16cache` 可以 mark 一些 output\n   - `polygraphy run  model.onnx --trt  --trt-outputs mark all    --save-outputs ./model3.json` 转完跑一遍 trt, mark all；\n     - [ 转换 onnx 模型出现 requires bool or uint8 I/O but node can not be handled by Myelin](https://www.stubbornhuang.com/2432/)\n   - `polygraphy run  sparsedrive_fp16_3090.trt --model-type=engine  --plugins=/workspace/mmdeploy/mmdeploy/lib/libmmdeploy_tensorrt_ops.so   --trt-outputs mark all --trt  --save-outputs ./dump.json` 运行 trt 模型\n   - `polygraphy run model.onnx --trt` 转 trt 模型\n   - `polygraphy inspect model model.onnx --shape-inference --show layers` 查看 onnx 模型信息\n   - `polygraphy inspect model op16_iter7_refine_filter_fb.trt --model-type=engine --show layers` 查看 trt 模型信息\n   - `polygraphy inspect capability model.onnx -v` 查看 onnx 是否被 TensorRT 支持\n     - `LD_PRELOAD=/workspace/mmdeploy/mmdeploy/lib/libmmdeploy_tensorrt_ops.so polygraphy inspect  capability sparsedrive_lantu_op13_fd.onnx -v` 有插件用 LD_PRELOAD； 还可以看用了什么插件\n   - `polygraphy run dynamic_identity.onnx --trt --onnxrt --save-engine test.trt` Comparing TensorRT And ONNX-Runtime Outputs\n   - `polygraphy run your.onnx --trt --onnxrt --onnx-outputs mark all --trt-outputs mark all` To mark all nodes in onnx as output and compare between onnxruntime and trt\n     - 只有 onnx 才能 run\n   - ` --trt --trt` trt 结果与 trt 结果比较, 跑两次比较，查看输出是否异常\n   - `polygraphy run dynamic_identity.onnx --trt --fp16 --onnxrt --input-shapes X:[1,2,4,4]` Comparing TensorRT Precisions\n   - [(good) run/05_comparing_with_custom_input_data](https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy/examples/cli/run/05_comparing_with_custom_input_data)\n   - `polygraphy surgeon sanitize model.onnx --fold-constants -o folded.onnx` 可以 fold constant, 作为 op 参数, 不用作为 input\n     - 可以看出来有多少个 node\n   - `polygraphy debug precision net_bs8.onnx --fp16 --tactic-sources cublas --check polygraphy run polygraphy_debug.engine --trt --load-outputs onnx_res.json --abs 1e-1`\n1. [polygraphy python 接口](https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy/examples/api/00_inference_with_tensorrt)\n   - 比 tensorrt python 接口好用\n1. [trt-engine-explorer](https://github.com/NVIDIA/TensorRT/tree/main/tools/experimental/trt-engine-explorer)\n\n## op 支持\n\n1. [onnx operators.md](https://github.com/onnx/onnx-tensorrt/blob/main/docs/operators.md)\n1. tensorrt 支持 rnn, lstm, gru;可能算子内部循环，外部还是 dag\n\n## TensorRT, CUDA, cuDNN\n\n1. CUDA 是`并行计算框架`，提供 API，可以运行在 Nvdia GPU 上\n2. cuDNN 是深度神经网络加速库，cuDNN 可大幅优化标准例程（例如用于前向传播和反向传播的卷积层、池化层、归一化层和激活层）的实施。世界各地的深度学习研究人员和框架开发者都依赖 cuDNN 实现高性能 GPU 加速。借助 cuDNN，研究人员和开发者可以`专注于训练神经网络及开发软件应用`，而不必花时间进行低层级的 GPU 性能调整。cuDNN 可加速广泛应用的深度学习框架，包括 Caffe2、Chainer、Keras、MATLAB、MxNet、PyTorch 和 TensorFlow。\n3. TensorRT 是用于高性能深度学习`推理`的 SDK。此 SDK 包含深度学习推理优化器和`运行时环境`，可为深度学习推理应用提供低延迟和高吞吐量。借助 TensorRT，开发者可专注于创建新颖的 AI 支持应用，无需费力调节性能来部署推理工作。\n\n## [trt-engine-explorer](https://github.com/NVIDIA/TensorRT/tree/main/tools/experimental/trt-engine-explorer)\n\n## trtexec\n\n### 转模型\n\n1. NOTE: `--noTF32 Disable tf32 precision (default is to enable tf32, in addition to fp32)`转出的模型默认是 tf32\n   - A100、H100 tf32 算力比 fp32 算力高 8 倍左右\n   - 3090 上 tf32 算力跟 fp32 算力相同\n1. `trtexec --onnx=model.onnx --saveEngine=model.trt`\n1. `trtexec --onnx=model.onnx --saveEngine=model_fp16.trt --fp16`\n1. `trtexec --onnx=model.onnx --saveEngine=model_int8.trt --int8`\n1. trtexec --onnx=./bisenetv2_310102_1280x1920.onnx --shapes=input:1x3x540x960 --saveEngine=bisenetv2-xavier-fp16-540x960.engine --allowGPUFallback --fp16 --workspace=16024\n1. onnx 模型转化为 engine or trt 后运行\n1. trtexec 在转换模型时做了一些优化，以提高模型的推理性能。这些优化包括：\n   - `算子融合：`trtexec 会尝试将多个算子融合为一个算子，以减少推理过程中的开销。\n   - `内存优化：`trtexec 会尝试将模型的权重和激活值存储在更高效的内存中，以提高推理速度。\n   - `模型裁剪：`trtexec 会尝试将模型的参数量减少，以减少推理过程中的计算量。\n   - `图优化?` [link](https://microsoft.github.io/AI-System/SystemforAI-9-Compilation%20and%20Optimization.pdf)\n     - 等价变化简化图计算; 算术表达式等价替换：a\\*0 -> 0; 公共子表达式消除, 常量折叠\n     - 子图替换，如 flash attention\n     - 包括算子融合\n     - 剪枝\n     - 调度优化，异步， 可以并行的并行做\n     - 内存优化\n     - 混合精度\n1. trtexec 执行步骤：\n   - trtexec 会将模型转换为 TensorRT 的内部表示。\n   - trtexec 会对模型进行分析，以确定可以进行优化的算子。\n   - trtexec 会对模型进行优化，以提高推理性能。\n\n#### 性能参数\n\n    - `--best               Enable all precisions to achieve the best performance (default = disabled)`\n    - `--useCudaGraph       Use CUDA graph to capture engine execution and then launch inference (default = disabled).`\n    - `--useDLACore=N       Select DLA core N for layers that support DLA (default = none)`\n    - `--useManagedMemory   Use managed memory instead of separate host and device allocations (default = disabled).`\n\n### 推理\n\n1. [Comparing TensorRT And ONNX-Runtime Outputs](https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy/examples/cli/run/01_comparing_frameworks#comparing-tensorrt-and-onnx-runtime-outputs)\n1. 推理时可以看各种信息：\n   - Build Options：\n     - 输入输出 name\n     - input output 格式(fp32, fp16...)\n     - batch\n     - layout(chw or hwc)\n   - Inference Options\n     - 推理次数\n   - Device Information\n   - Performance summary\n1. `trtexec --loadEngine=test.trt`\n1. `trtexec --loadEngine=model.trt --exportOutput=output.txt` dump model output\n1. `trtexec --loadEngine=model.trt --exportLayerInfo=LayerInfo.txt` dump layer info； 如果出现`ForeignNode[onnx::MatMul_311`表示不支持; 使用[onnx-simplifier](https://github.com/daquexian/onnx-simplifier)试试\n1. `trtexec --loadEngine=test.trt --plugins=lib/libtrtplugins.so --duration=10` 带 plugin\n1. `trtexec --loadEngine=test.trt --plugins=lib/libtrtplugins.so --iterations=N --verbose=1`\n1. 动态输入 `trtexec --onnx=fcn-resnet101.onnx --fp16 --workspace=64 --minShapes=input:1x3x256x256 --optShapes=input:1x3x1026x1282 --maxShapes=input:1x3x1440x2560 --buildOnly --saveEngine=fcn-resnet101.engine && trtexec --shapes=input:1x3x1026x1282 --loadEngine=fcn-resnet101.engine` Where --shapes sets the input sizes for the dynamic shaped inputs to be used for inference.\n1. (good) `trtexec --loadEngine=my_model.trt --dumpProfile=1`推理时显示每层耗时\n1. (good) `trtexec --loadEngine=my_model.trt --dumpProfile=1 --exportProfile=profile.json`推理时显示每层耗时\n1. 对 profile.json 进行排序\n\n```python\nimport json\n\n# 读取 JSON 文件\nwith open('profile.json', 'r') as file:\n    data = json.load(file)\n\n# 检查文件结构\nprint(type(data))  # 确保数据是一个列表\nprint(data[0])     # 查看第一个元素的结构\n\n# 根据 'percentage' 字段进行排序\nsorted_data = sorted(data, key=lambda x: x.get('percentage', 0), reverse=True)\n\n# 打印排序后的数据\n# for entry in sorted_data:\n    # print(f\"Name: {entry.get('name')}, Percentage: {entry.get('percentage')}\")\n\n# 可选：将排序后的数据写入新的 JSON 文件\nwith open('sorted_profile.json', 'w') as file:\n    json.dump(sorted_data, file, indent=4)\n\n```\n\n#### python 推理\n\n1. [onnx-tensorrt 来推理 onnx 模型](https://github.com/onnx/onnx-tensorrt?tab=readme-ov-file#executable-usage)\n   - 需要本地先 build 再 install\n   - onnx-tensorrt 与 polygraphy python 接口类似，不需要使用管理显存，但 onnx-tensorrt 需要依赖 pycuda\n1. 需要 pycuda\n\n```shell\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda/lib64\nexport CPATH=$CPATH:/usr/local/cuda/include/\nexport PATH=${PATH}:/usr/local/cuda/bin\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple pycuda\n```\n\n## links\n\n1. [官方 docker](https://docs.nvidia.com/deeplearning/tensorrt/container-release-notes/index.html#pullcontainer)\n1. [troubleshooting](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#troubleshooting)\n1. [tensorrt docs](https://docs.nvidia.com/deeplearning/tensorrt/index.html)\n1. [各种版本 doc](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html)\n1. [release notes](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html) Ensure you are familiar with the NVIDIA TensorRT Release Notes for the latest new features and known issues.\n1. [官方 c++ sample 讲解](https://developer.nvidia.com/zh-cn/blog/tensorrt-c-interface-cn/)\n1. [官方博客](https://developer.nvidia.com/zh-cn/blog/search-posts/?q=tensorrt)\n1. [c++ api](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#c_topics)\n1. [官方 sample](https://github.com/NVIDIA/TensorRT/tree/main/samples)\n1. [samples](https://github.com/wang-xinyu/tensorrtx)\n1. [trtexec](https://github.com/NVIDIA/TensorRT/tree/release/7.1/samples/opensource/trtexec)\n1. [nvidia tensorrt pdf](http://www.hpc.iitkgp.ac.in/pdfs/Nvidia-2.pdf)\n1. [plugins](https://github.com/NVIDIA/TensorRT/tree/main/plugin)\n1. [Understanding TensorRT Runtimes](https://github.com/NVIDIA/TensorRT/blob/main/quickstart/IntroNotebooks/5.%20Understanding%20TensorRT%20Runtimes.ipynb)\n1. [c++ api 教程](https://github.com/cyrusbehr/tensorrt-cpp-api)\n1. [基于 NVIDIA 的 PC 的端到端 AI ： ONNX Runtime 中的 CUDA 和 TensorRT 执行提供程序](https://developer.nvidia.com/zh-cn/blog/end-to-end-ai-for-nvidia-based-pcs-cuda-and-tensorrt-execution-providers-in-onnx-runtime/)\n\n## docker\n\n1. [官方 docker](https://docs.nvidia.com/deeplearning/tensorrt/container-release-notes/index.html#pullcontainer)\n   - docker 里有 tensorrt sample\n   - `docker pull nvcr.io/nvidia/tensorrt:24.02-py3`\n   - `docker run -itd -v /home/xiyang.jia:/home/xiyang.jia -p 10086:22 --user root --gpus all --name=xiyang_cuda --shm-size 2g --cap-add=SYS_ADMIN nvcr.io/nvidia/tensorrt:22.07-py3` 注意加--cap-add=SYS_ADMIN，ncu 收集 gpu metrics\n   - `docker exec -it xiyang_cuda  bash`\n\n## 实战\n\n1. bevfusion 遇见的问题\n\n1. onnx 转换使用心得\n\n## 常见错误\n\n1. 转换出现类型问题，可能需要对模型进行常量折叠 `polygraphy surgeon sanitize model.onnx --fold-constants -o folded.onnx`\n1. 出现 trt fuse 问题先找到 python 代码，调试一下; 更改逻辑，去除 fuse 问题\n1. nvrtc_compile.cpp:940: CHECK(false) failed. NVRTC Compilation failure [09/07/2024-16:07:22] [E] Error[10]: Could not find any implementation for node {ForeignNode[onnx::Where_4341.../Div_8]}. Timing Runner: {ForeignNode[onnx::Where_3922.../Transpose_9 + /Reshape_49]} (Myelin[0x80000023])\n   - 解决方法： 把 where 去掉\n   - [conversion-pytorch-to-tensorrt-fails-when-using-fp16-works-with-fp32-and-int8](https://forums.developer.nvidia.com/t/conversion-pytorch-to-tensorrt-fails-when-using-fp16-works-with-fp32-and-int8/285141)\n   - [Could not find any implementation for node Error while converting the instructor-large model from ONNX to TensorRT engine](https://github.com/NVIDIA/TensorRT/issues/3154)\n   - 并通过增加工作空间大小来解决 [link](https://github.com/onnx/onnx-tensorrt/issues/758#issuecomment-1095330781)\n   - 更换 opset 版本\n   - 更换 trt 版本\n1. `--workspace flag has been deprecated by --memPoolSize flag.`\n1. fp32 无问题，fp16 输出出现 inf; fp16 范围：±65504，判断 fp32 模型的中间结果是否会溢出\n   - 可能是 inverse_sigmoid 问题， `1.0 / 1.0014e-05 = 99860.19572598363`越界\n   - 解决办法，pytorch 用 fp16 finetune 一下，pytorch 会处理？\n   - 为了避免 FP16 模型出现溢出问题，可以使用 PyTorch 的 AMP（自动混合精度）工具包，它能够自动选择 FP32 和 FP16 混合精度运算，减少数值溢出风险。\n   - 查找问题，pytorch 中 model.half()还需处理输入\n   - 使用 torch.cuda.amp.autocast 来进行 fp16 推理，vscode debug 来查找第一次出现 inf 时的地方\n\n```python\n   # 使用 autocast 进行 FP16 推理\nwith torch.cuda.amp.autocast():\n    with torch.no_grad():  # 禁用梯度计算\n        output = model(input_data)\n```\n","tags":["TensorRT"],"categories":["Program","AI"]},{"title":"Actix","url":"/2021/03/22/Actix/","content":"\n\n## basic\n\n```\nuse actix_web::{get, post, web, App, HttpResponse, HttpServer, Responder};\n\n#[get(\"/\")]\nasync fn hello() -> impl Responder {\n    HttpResponse::Ok().body(\"Hello world!\")\n}\n\n#[post(\"/echo\")]\nasync fn echo(req_body: String) -> impl Responder {\n    HttpResponse::Ok().body(req_body)\n}\n\nasync fn manual_hello() -> impl Responder {\n    HttpResponse::Ok().body(\"Hey there!\")\n}\n\n#[actix_web::main]\nasync fn main() -> std::io::Result<()> {\n    HttpServer::new(|| {\n        App::new()\n            .service(hello)\n            .service(echo)\n            .route(\"/hey\", web::get().to(manual_hello))\n    })\n    .bind(\"127.0.0.1:8080\")?\n    .run()\n    .await\n}\n```\n1. Use App::service for the handlersusing routing `macros` and App::route for `manually` routed handlers, declaring the `path` and `method`.\n1. `path`: http URL,例如：http://github.com/xxx/xx/xx\n2. `method`: path的响应\n3. All actix-web servers are built around the `App` instance.\n\n## Links\n1. [official docs](https://actix.rs/docs/)\n","tags":["Rust"],"categories":["Program","Rust"]},{"title":"Web","url":"/2021/03/16/Web/","content":"\n## 前端开发\n\n1. HTML: Markup Language 构建网页，例如：建筑骨架\n2. CSS: Styling Language 网页样式: 例如： 建筑外观，墙的颜色\n3. JavaScript: Programming Language 网页功能 例如：建筑功能，电梯\n4. 常用库： REACT, ANGULAE, VUE\n\n## MVC\n\n1. [维基百科](https://zh.wikipedia.org/wiki/MVC)\n1. MVC 模式在概念上强调 Model, View, Controller 的分离，各个模块也遵循着由 Controller 来处理消息，Model 掌管数据源，View 负责资料显示的职责分离原则，因此在实现上，MVC 模式的 Framework 通常会将 MVC 三个部分分离实现\n1. View 负责格式化数据并把它们呈现给用户，业务逻辑和表示层分离，同一个 Model 可以被不同的 View 重用，所以大大提高了代码的可重用性。\n\n## Controller, Service, and Repository\n\n## 增删改查（CRUD)\n\n1. [维基百科](https://zh.wikipedia.org/wiki/%E5%A2%9E%E5%88%AA%E6%9F%A5%E6%94%B9)\n2. 增删查改（英语：CRUD[注 1]），全称增加（Create，意为“创建”）、删除（Delete）、查询（Read，意为“读取”）、改正（Update，意为“更新”），在计算机程序语言中是一连串常见的动作行为，而其行为通常是为了针对某个特定资源所作出的举动（例如：创建资料、读取资料等）。这四个行为最常见的用途是在使用 SQL 数据库与网站的 API 端口口的时候。\n\n## HTTP\n\n1. [简介](https://www.jianshu.com/p/80e25cb1d81a)\n2. [http methods](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Methods)\n\n## URI\n\n1. [维基百科](https://zh.wikipedia.org/wiki/%E7%BB%9F%E4%B8%80%E8%B5%84%E6%BA%90%E6%A0%87%E5%BF%97%E7%AC%A6)\n2. 统一资源标识符（英语：Uniform Resource Identifier，缩写：URI）在电脑术语中是一个用于标识某一互联网资源名称的字符串。\n3. URL 是一种 URI\n4. `URI = scheme “://” authority “/” path [ “?” query ][ “#” fragment ]`\n5. scheme: 指底层用的协议，如 http、https、ftp\n   host: 服务器的 IP 地址或者域名\n   port: 端口，http 中默认 80, https 中默认 443\n   path: 访问资源的路径，就是咱们各种 web 框架中定义的 route 路由\n   query: 为发送给服务器的参数\n   fragment: 锚点，定位到页面的资源，锚点为资源 id\n6.\n\n## CORS 跨域资源共享\n\n1. [所有人都应该知道的跨域及 CORS](https://zhuanlan.zhihu.com/p/53996160)\n\n## CGI 通用网关接口\n\n1. Restful 风格 API 的出现，让 CGI 获得了续命。CGI 解析前端请求，再转发给对应后端；然后从后端取回数据，给前端返回 XML 或 JSON。然后前端 JS 利用 XML/JSON 中的数据来进行填充。可以绘制出丰富的界面或用作他用。JS 可以使用 Ajax 技术来向后台 CGI 发起数据请求。Ajax 完成的是不需要刷新整个页面就可以加载后端数据（比如从数据库中取出）。\n2. 请注意区分 Web Server 和后台 Server。\n3. [图解](https://zhuanlan.zhihu.com/p/25013398)\n4. [百科](https://baike.baidu.com/item/CGI/607810)\n\n## REST API\n\n1. [postman](https://learning.postman.com/docs/getting-started/installation-and-updates/#installing-postman-on-windows)\n1. [online test](https://reqbin.com/)\n1. [youtube samples](https://www.youtube.com/watch?v=7YcW25PHnAA)\n1. [接口规范](https://i6448038.github.io/2017/06/28/rest-%E6%8E%A5%E5%8F%A3%E8%A7%84%E8%8C%83/)\n1. [github API](https://api.github.com/)\n1. [github REST API](https://docs.github.com/en/rest)\n\n## OpenAPI\n\n1. [openapi](https://swagger.io/docs/specification/about/)\n"},{"title":"Pybind11","url":"/2021/03/15/Pybind11/","content":"\n## Pybind11\n\n1. python 与 C/C++相互调用\n1. 轻量级，只含有头文件\n\n## Links\n\n1. [github](https://github.com/pybind/pybind11)\n   - 包含测试程序\n1. [doc](https://pybind11.readthedocs.io/en/stable/)\n1. [cmake example](https://github.com/pybind/cmake_example)\n1. [Setuptools example](https://github.com/pybind/python_example)\n1. [tutorial](https://pybind11.readthedocs.io/en/latest/)\n1. [python 调用 C++之 pybind11 入门](https://blog.csdn.net/fitzzhang/article/details/78988682)\n","tags":["Python"],"categories":["Program","Python"]},{"title":"First-Emacs-Plugin","url":"/2021/03/12/First-Emacs-Plugin/","content":"\n## 功能\n保存位置，高亮所在行，单个buffer跳转\n\n## 参考插件\n1. hightlight\n1. symbol-overlay\n1. dogears\n1. set-mark-command\n\n## 思路\n1. dogears加高亮\n1. symbol-overlay改造\n1. [函数修改](https://stackoverflow.com/questions/14454219/how-to-highlight-a-particular-line-in-emacs)\n\n\n## Links\n","tags":["Emacs"],"categories":["Tools","Emacs"]},{"title":"Git bisect to debug","url":"/2021/03/05/git-biset-to-debug/","content":"\n## Function\n1. Use binary search to find the commit that introduced a bug\n\n## Command\n\n```\n$ git bisect start\n$ git bisect bad                 # Current version is bad\n$ git bisect good v2.6.13-rc2    # v2.6.13-rc2 is known to be good\n$ git bisect log\n$ git bisect skip                 # Current version cannot be tested\n$ git bisect reset\n\n# run script\n$ git bisect run my_script arguments\n$ cat ~/test.sh\n#!/bin/sh\nmake || exit 125                     # this skips broken builds\n~/check_test_case.sh                 # does the test case pass?\n$ git bisect start HEAD v1.3.1 --      # HEAD is bad, v1.2 is good\n$ git bisect run ~/test.sh\n# git bisect run sh -c \"make || exit 125; ~/check_test_case.sh\"\n$ git bisect reset                   # quit the bisect session\n```\n\n\n## Links\n1. [git bisect](https://git-scm.com/docs/git-bisect)\n1. [Git 二分调试法，火速定位疑难Bug](https://juejin.cn/post/6844903537860673544)\n","tags":["Git"],"categories":["Tools","Git"]},{"title":"FFmpeg","url":"/2021/02/23/FFmpeg/","content":"\n## [视频处理基本概念](https://www.ruanyifeng.com/blog/2020/01/ffmpeg.html)\n1. `容器(封装)`：`ffmpeg-formats` 视频文件本身其实是一个容器（container），里面包括了视频和音频，也可能有字幕等其他内容。\n1. `编码格式`：`ffmpeg-codecs` 视频和音频都需要经过编码，才能保存成文件。不同的编码格式（CODEC），有不同的压缩率，会导致文件大小和清晰度的差异。\n2. `编码器`：`ffmpeg-encoders` 编码器（encoders）是实现某种编码格式的库文件。只有安装了某种格式的编码器，才能实现该格式视频/音频的编码和解码。\n\n## 工作流程\n1. ffmpeg的主要工作流程相对比较简单，具体如下。\n  - 解封装（Demuxing）\n  - 解码（Decoding）\n  - 编码（Encoding）\n  - 封装（Muxing）\n1. 其中需要经过6个步骤，具体如下。\n  - 读取输入源\n  - 进行音视频的解封装\n  - 解码每一帧音视频数据\n  - 进行音视频的重新封装\n  - 输出到目标\n1. \n                _______              ______________\n               |       |            |              |\n               | input |  demuxer   | encoded data |   decoder\n               | file  | ---------> | packets      | -----+\n               |_______|            |______________|      |\n                                                          v\n                                                      _________\n                                                     |         |\n                                                     | decoded |\n                                                     | frames  |\n                                                     |_________|\n                ________             ______________       |\n               |        |           |              |      |\n               | output | <-------- | encoded data | <----+\n               | file   |   muxer   | packets      |   encoder\n               |________|           |______________|\n\n## ffmpeg\n1. FFmpeg框架的基本组成包含AVFormat、AVCodec、AVFilter、AVDevice、AVUtil等模块库\n1. 主要命令\n  - ffmpeg主要用于音视频编解码\n  - ffprobe主要用于音视频内容分析\n  - ffplay主要用于音视频播放、可视化分析\n\n### ffmpeg\n1. 总共分为5部分：`ffmpeg {1} {2} -i {3} {4} {5}`\n\n```\n$ ffmpeg \\\n[全局参数] \\\n[输入文件参数] \\\n-i [输入文件] \\\n[输出文件参数] \\\n[输出文件]\n```\n\n### ffplay\n1. FFmpeg不但可以提供转码、转封装等功能，同时还提供了播放器相关功能，使用FFmpeg的avformat与avcodec，可以播放各种媒体文件或者流。如果想要使用ffplay，那么系统首先需要有SDL来进行ffplay的基础支撑。\n\n### ffprobe\n1. ffprobe也是FFmpeg源码编译后生成的一个可执行程序。ffprobe是一个非常强大的多媒体分析工具，可以从媒体文件或者媒体流中获得你想要了解的媒体信息，比如音频的参数、视频的参数、媒体容器的参数信息等。\n2. `./ffprobe –show_streams output.mp4`\n\n## ffmpeg硬编解码\n1. 当使用FFmpeg进行软编码时，常见的基于CPU进行H.264或H.265编码其相对成本会比较高，CPU编码时的性能也很低，所以出于编码效率及成本考虑，很多时候都会考虑采用硬编码，常见的硬编码包含Nvidia GPU与Intel QSV两种，还有常见的嵌入式平台，如树莓派、瑞芯微等\n\n## ffmpeg流媒体\n1. 随着互联网、移动互联网的发展，人们获取信息的方式开始从纸质媒体转向互联网文字媒体，又从文字媒体转向音视频流媒体。音视频流媒体又称为流媒体，而用于处理流媒体的压缩、录制、编辑操作，开源并强大的工具屈指可数，FFmpeg就是常见的流媒体处理工具。\n2. 流类型：RTMP, RTSP, HTTP, UDP/TCP, HDS, DASH\n\n## ffmpeg filter\n1. FFmpeg除了具有强大的封装/解封装、编/解码功能之外，还包含了一个非常强大的组件——avfilter。avfilter组件经常用于进行多媒体的处理与编辑。例如加水印，画中画，多宫格视频等\n\n## ffmpeg视频采集\n1. 可以使用FFmpeg采集本地的音视频采集设备的数据，然后进行编码、封装、传输等操作\n\n## Links\n1. [Official Documents](https://ffmpeg.org/documentation.html)\n1. [FFmpeg从入门到精通（书）](http://jxz1.j9p.com/pc/dgsdfhghgh.pdf)\n","tags":["FFmpeg"],"categories":["Program","FFmpeg"]},{"title":"Gstreamer","url":"/2021/02/23/Gstreamer/","content":"\n## Why Gstreamer\n\n1. 构建一个视频分析应用，需要考虑的内容有：视频流获取、视频流解码、预处理、算法推理、数据编码、数据显示、数据传输等。常常需要引入一些第三方的开发包，如 OpenCV，FFmpeg，Caffe，TensorRT，OpenGL 等。GStreamer 是一个用于开发流式多媒体应用的跨平台开源框架，应用程序可以通过管道(Pipeline)的方式，`将多媒体处理的各个步骤串联起来`，达到预期的效果。\n\n## Gstreamer\n\n1. `核心`为`Pipeline`框架以及用于扩展功能的`Plugins`。plugin 负责实现音视频传输协议、音视频输入输出源、音视频编解转码等真实的媒体处理功能，而 plugin 之间通过 pipeline 关联在一起，pipeline 负责将上一个 plugin 的数据流(data flow)传输给下一个 plugin，最终形成一个完整的多媒体处理应用；\n2. 原理：GStreamer 的程序通过连接数字媒体处理的元素注入管道（pipeline）。每个元素(element)是由一个插件提供。元素可组合为箱（bins），箱可以进一步聚合，从而形成架构图。元素沟通是透过垫（pads）。来源垫（source pad）上一个元素可以被连接到一个接收垫（sink pad）在另一个。当管道是在播放状态，数据缓冲流（data buffers flow）从来源垫（source pad）流向接收垫（sink pad）。\n3. 箱柜(bin)是由多个 element 构成的容器，同时 bin 本身也是一种 element，所以能够像操作普通 element 一样操作一个 bin，改变 bin 的状态可以改变 bin 内部所有 elements 的状态。bin 可以发送总线消息给它的子集 elements ，包括：错误消息(error messages)，标签消息(tag messages)，EOS 消息(EOS messages)。\n4. 管道(pipeline)是一个特殊的 bin，当设定管道暂停或播放状态的时候，数据流将开始流动，并且媒体数据处理也开始处理。\n5. element 被封装成 plugin 才能被 gstreamer 使用。\n\n## 3 层架构（plugin, core, app)\n\n![overview](https://gstreamer.freedesktop.org/documentation/application-development/introduction/images/gstreamer-overview.png)\n\n1. 最底层：element(plugins)\n1. 中间层：插件框架、pipeline 架构、媒体类型处理机制、等等，也就是处理 Elements 的方式方法.\n1. 最上层：tools, 多媒体应用\n\n## GStreamer plug-ins could be classified\n\n1. protocols handling\n2. sources: for audio and video (involves protocol plugins)\n3. formats: parsers, formaters, muxers, demuxers, metadata, subtitles\n4. codecs: coders and decoders\n5. filters: converters, mixers, effects, ...\n6. sinks: for audio and video (involves protocol plugins)\n\n## GStreamer tools\n\n1. gst-launch-1.0: 命令行建立和运行 pipeline, 不用编写 C 代码； `gst-launch-1.0 videotestsrc ! autovideosink`\n2. gst-inspect-1.0: 查看可用的插件和它们的相关信息； `gst-inspect-1.0 videotestsrc`\n3. gst-discoverer-1.0: 查看媒体文件的内部结构； `gst-discoverer-1.0 /root/Desktop/video.mp4`\n4. GST*DEBUG=\"GST_TRACER:7\" GST_TRACERS=\"proctime\" gst-launch-1.0 ... `查看每个element性能` [link](https://developer.ridgerun.com/wiki/index.php?title=GstShark*-\\_Processing_Time_tracer)\n5. [gst-shark](https://github.com/RidgeRun/gst-shark)\n\n## Links\n\n1. [About Gstreamer](https://gstreamer.freedesktop.org/documentation/application-development/introduction/index.html?gi-language=c)\n2. [Gstreamer tools](https://gstreamer.freedesktop.org/documentation/tutorials/basic/gstreamer-tools.html?gi-language=c)\n3. [Gstreamer tutorials](https://gstreamer.freedesktop.org/documentation/tutorials/basic/index.html?gi-language=c)\n4. [Gstreamer tutorials rust](https://github.com/sdroege/gstreamer-rs/tree/master/tutorials)\n5. [由 GStreamer 到 DeepStream](http://orangeamoy.com/2019/06/28/GStreamerAndDeepStream/)\n6. [插件例子](http://ericnode.info/post/gstreamer_intro/)\n7. [中文基础概念](https://www.shangyexinzhi.com/article/571316.html)\n","tags":["Gstreamer"],"categories":["Program","Gstreamer"]},{"title":"Docker","url":"/2021/02/02/Docker/","content":"## [vscode docker](https://code.visualstudio.com/docs/containers/overview)\n\n## [docker compose](https://code.visualstudio.com/docs/containers/docker-compose)\n1. 用于运行docker image\n\n\n## NOTES\n1. `docker system df`查看用的空间\n1. `docker pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/ubuntu:22.04`\n   - [link](https://docker.aityp.com/s/docker.io)\n1. `docker pull hello-world --registry-mirror=https://docker.mirrors.ustc.edu.cn`\n1. dockerhub 下载 images 到磁盘, 传到服务器: docker pull and docker save\n1. wsl2 中设置`--net=host`会影响 docker port 设置\n1. `docker image prune -a` 清理任何为用的的 image\n1. [docker build 以及 docker run 时使用 host 网络的方法](http://www.debugself.com/2018/01/17/docker_network/)\n1. docker 开发使用： docker image(路径无关) + docker.sh(路径相关)\n1. [tvm docker 启动 bash](https://github.com/apache/tvm/blob/main/docker/bash.sh)\n1. [windwos wsl2 + docker 开发](https://learn.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers) docker desktop 需要启动\n1. `docker attach CONTAINER`\n1. `docker info` 查看 docker 信息\n1. `docker stats` 可以看哪个 container 占用率高\n1. `sudo usermod -aG docker $USER` 将当前用户加入到 docker 组中，否则必须使用 sudo 来运行 docker\n1. `--user root --gpus all` 使用 gpu\n1. `--privileged` 使用 systemctl\n1. `--name orin`\n1. `docker save -o fedora-all.tar fedora` 保存镜像到磁盘；container 需要先 commit 才能保存到磁盘\n1. `docker commit my-container my-image` 保存 container 为镜像\n1. `docker load --input fedora.tar` or `docker load < busybox.tar.gz`\n1. `docker login --username=xiyang.jia hub.xxx.io` 登录后才能 push\n1. `docker commit efb954281fd3 j5-dsp-xiyang-dev:v1`\n1. `docker tag [ImageId] hub.xx.io/avp/[ImageName]:[镜像版本号]`\n1. `docker push docker.io/my-username/my-image:v1.0.0`\n1. `docker image rm eb3f9d403e36` 删除 image\n\n## 多阶段构建\n1. `FROM conda AS build`\n\n\n\n## docker container install ssh server\n\n```sh\n sed -i s@/archive.ubuntu.com/@/mirrors.tuna.tsinghua.edu.cn/@g /etc/apt/sources.list && apt-get update -y\n sudo apt install -y openssh-server\n PORT=22; sed -i -e \"s/^#Port 22/Port $PORT/\" -e 's/^#PermitRootLogin prohibit-password/PermitRootLogin yes/' -e 's/^#PasswordAuthentication yes/PasswordAuthentication yes/' /etc/ssh/sshd_config\n echo 'root:root' | chpasswd && /etc/init.d/ssh start\n \n echo \"$(whoami):root\" | sudo chpasswd && sudo /etc/init.d/ssh start\n # /etc/init.d/ssh start\n # vi /etc/ssh/sshd_config\n # 设置 PermitRootLogin yes和 PasswordAuthentication yes\n /etc/init.d/ssh restart\n passwd # 设置密码\n```\n\n## network=host\n1. --network=host 和 -p 选项不能同时使用\n1. 当使用 --network=host 时，容器中的端口直接暴露在宿主机上，-p 的端口映射功能就没有意义了，因此 Docker 会忽略 -p 选项。\n1. 直接在sshd_config设置port \n\n## usefull docker\n\n1. [gcc](https://hub.docker.com/_/gcc)\n\n## Command\n\n1. ![一图胜千言](https://user-gold-cdn.xitu.io/2019/4/9/16a02cdbf14142a0?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)\n\n2.\n\n```dockerfile\ndocker run -id --name gcc.xiyang -v /data0/users/teams/software/heterogeneous/xiyang.jia/:/data0/users/xiyang.jia -w /data0/users/xiyang.jia/d/working -p 10099:22 -v /etc/localtime:/etc/localtime gcc\n```\n\n1.\n\n```sh\n\n% docker pull openexplorer/ai_toolchain_centos_7:v1.12.5\n% docker run -id \\\n    --name openexplorer.xiyang \\\n    -v /data0/users/xiyang.jia:/data0/users/xiyang.jia \\\n    -w /data0/users/xiyang.jia/d/working \\\n    -p 10090:22 \\\n    --privileged \\\n    -v /etc/localtime:/etc/localtime \\\n    openexplorer/ai_toolchain_centos_7:v1.12.5\n\n% docker exec -it openexplorer.xiyang bash\n% docker ps\n% docker stop openexplorer.xiyang\n% docker rm openexplorer.xiyang\n% docker images\n% docker image rm xxx\n% docker logs openexplorer.xiyang\n% docker start <name> // start docker when it exit\n\n```\n\n### [docker load](https://docs.docker.com/engine/reference/commandline/load/)\n\n```sh\n% docker load < busybox.tar.gz\n% docker load --input fedora.tar\n```\n\n## Concept\n\nDocker 包括三个基本概念:\n\n1. 镜像（Image）\n1. 容器（Container）\n1. 仓库（Repository）\n   理解了这三个概念，就理解了 Docker 的整个生命周期。\n\n### [Image](https://yeasy.gitbook.io/docker_practice/basic_concept/image)\n\n1. Docker 镜像（Image），就相当于是一个 root 文件系统\n1. Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。\n\n### [Container](https://yeasy.gitbook.io/docker_practice/basic_concept/container)\n\n1. 镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。\n\n### [Repository](https://yeasy.gitbook.io/docker_practice/basic_concept/repository)\n\n1. `Docker Registry`: 一个集中的存储、分发镜像的服务\n1. 一个 Docker Registry 中可以包含多个 仓库（Repository）；每个仓库可以包含多个 标签（Tag）；每个标签对应一个镜像。\n1. 通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本。我们可以通过 `<仓库名>:<标签>` 的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签。\n\n## 传统虚拟化与 docker 虚拟化\n\n![传统虚拟化](https://gblobscdn.gitbook.com/assets%2F-M5xTVjmK7ax94c8ZQcm%2F-M5xT_hHX2g5ldlyp9nm%2F-M5xTdXNYDmRWNH-Lqez%2Fvirtualization.png?alt=media)\n![docker虚拟化](https://gblobscdn.gitbook.com/assets%2F-M5xTVjmK7ax94c8ZQcm%2F-M5xT_hHX2g5ldlyp9nm%2F-M5xTdXP2scg0hxytUHA%2Fdocker.png?alt=media)\n\n## VNC\n\n1. `vncserver :1`\n1. `vncserver -kill :1`\n1. `ps -ef | grep vnc`查看是否运行及端口号\n1. `cat .vnc/password.txt` 查看密码(自己设置)\n\n## Links\n\n1. [从入门到实践](https://yeasy.gitbook.io/docker_practice/introduction/why)\n1. [30 分钟快速入门](https://juejin.im/post/5cacbfd7e51d456e8833390c)\n1. [资源整理](https://juejin.cn/post/6844903450203914253)\n1. [简单聊聊 k8s，和 docker 之间的关系](https://juejin.cn/post/7224149297951195193)\n\n\n## run.sh\n\n``` shell\n#!/bin/bash\n\nCONTAINER_NAME=\"triton-xiyang\"\nIMAGE_NAME=\"triton:latest\"\n# 定义挂载路径数组\ndeclare -a MOUNT_PATHS=(\n    \"/data/\"\n    # \"/home/xiyang/\"\n    # \"/workspace/\"\n)\n# 指定工作目录\nWORKDIR=\"/data/xiyang\"\n\n# 测试是否需要sudo来运行docker命令\ncheck_docker_permission() {\n    if docker ps >/dev/null 2>&1; then\n        echo false  # 不需要sudo\n    else\n        if sudo docker ps >/dev/null 2>&1; then\n            echo true   # 需要sudo\n        else\n            echo \"错误: 无法访问docker，请确保docker已安装且用户有相应权限\"\n            exit 1\n        fi\n    fi\n}\n\n# 自动检测是否需要sudo\nUSE_SUDO=$(check_docker_permission)\n\n# 根据USE_SUDO设置docker命令前缀\nDOCKER_CMD=\"docker\"\nif [ \"$USE_SUDO\" = true ]; then\n    echo \"需要sudo权限运行docker命令\"\n    DOCKER_CMD=\"sudo docker\"\nfi\n\n\n# 定义颜色\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nBLUE='\\033[0;34m'\nCYAN='\\033[0;36m'\nNC='\\033[0m' # No Color\n\n# 生成挂载参数字符串\ngenerate_mount_args() {\n    local mount_args=\"\"\n    for path in \"${MOUNT_PATHS[@]}\"; do\n        mount_args+=\" -v ${path}:${path}\"\n    done\n    echo \"$mount_args\"\n}\n\n# 检查容器状态的函数\ncheck_container_status() {\n    # 检查容器是否存在\n    if ${DOCKER_CMD} ps -a --format '{{.Names}}' | grep -q \"^${CONTAINER_NAME}$\"; then\n        # 检查容器是否运行\n        if ${DOCKER_CMD} ps --format '{{.Names}}' | grep -q \"^${CONTAINER_NAME}$\"; then\n            echo \"容器正在运行，直接进入...\"\n            return 0  # 运行中\n        else\n            echo \"容器存在但未运行，正在启动...\"\n            return 1  # 已存在但未运行\n        fi\n    else\n        echo \"容器不存在，正在创建新容器...\"\n        return 2  # 不存在\n    fi\n}\n\n# 启动已存在的容器\nstart_container() {\n    ${DOCKER_CMD} start ${CONTAINER_NAME} >/dev/null 2>&1\n}\n\n# 创建新容器\ncreate_container() {\n    local mount_args=$(generate_mount_args)\n    ${DOCKER_CMD} run -id \\\n        --name ${CONTAINER_NAME} \\\n        ${mount_args} \\\n        -w ${WORKDIR} \\\n        --network host \\\n        --privileged \\\n        -v /etc/localtime:/etc/localtime \\\n        ${IMAGE_NAME}\n}\n\n# 显示容器信息\nshow_container_info() {\n    echo -e \"${BLUE}╔════════════════════════════════════════════════════════════╗${NC}\"\n    echo -e \"${BLUE}║${NC}                      ${YELLOW}容器信息${NC}                              ${BLUE}║${NC}\"\n    echo -e \"${BLUE}╠════════════════════════════════════════════════════════════╣${NC}\"\n    \n    # 基本信息\n    local container_id=$(${DOCKER_CMD} ps -f name=${CONTAINER_NAME} --format \"{{.ID}}\")\n    echo -e \"${BLUE}║${NC} ${CYAN}▎ 基本信息${NC}\"\n    echo -e \"${BLUE}║${NC}   容器名称: ${GREEN}${CONTAINER_NAME}${NC}\"\n    echo -e \"${BLUE}║${NC}   容器ID:   ${GREEN}${container_id}${NC}\"\n    \n    # 镜像信息\n    echo -e \"${BLUE}║${NC} ${CYAN}▎ 镜像信息${NC}\"\n    echo -e \"${BLUE}║${NC}   镜像ID:   ${GREEN}$(${DOCKER_CMD} inspect -f '{{.Image}}' ${CONTAINER_NAME} | sed 's/sha256://' | cut -c1-12)${NC}\"\n    echo -e \"${BLUE}║${NC}   镜像标签: ${GREEN}$(${DOCKER_CMD} inspect -f '{{.Config.Image}}' ${CONTAINER_NAME})${NC}\"\n    \n    # 时间信息\n    echo -e \"${BLUE}║${NC} ${CYAN}▎ 时间信息${NC}\"\n    echo -e \"${BLUE}║${NC}   创建时间: ${GREEN}$(${DOCKER_CMD} inspect -f '{{.Created}}' ${CONTAINER_NAME} | sed 's/T/ /' | cut -d. -f1)${NC}\"\n    echo -e \"${BLUE}║${NC}   启动时间: ${GREEN}$(${DOCKER_CMD} inspect -f '{{.State.StartedAt}}' ${CONTAINER_NAME} | sed 's/T/ /' | cut -d. -f1)${NC}\"\n    \n    # 运行时长\n    local start_time=$(date -d \"$(${DOCKER_CMD} inspect -f '{{.State.StartedAt}}' ${CONTAINER_NAME})\" +%s)\n    local current_time=$(date +%s)\n    local uptime=$((current_time - start_time))\n    local days=$((uptime / 86400))\n    local hours=$(((uptime % 86400) / 3600))\n    local minutes=$(((uptime % 3600) / 60))\n    echo -e \"${BLUE}║${NC}   运行时长: ${GREEN}${days}天 ${hours}小时 ${minutes}分钟${NC}\"\n    \n    # 资源使用情况\n    echo -e \"${BLUE}║${NC} ${CYAN}▎ 资源使用情况${NC}\"\n    local stats=$(${DOCKER_CMD} stats ${CONTAINER_NAME} --no-stream --format \"{{.CPUPerc}}|{{.MemUsage}}\")\n    local cpu_usage=$(echo $stats | cut -d'|' -f1)\n    local mem_usage=$(echo $stats | cut -d'|' -f2)\n    echo -e \"${BLUE}║${NC}   CPU使用率: ${YELLOW}${cpu_usage}${NC}\"\n    echo -e \"${BLUE}║${NC}   内存使用:  ${YELLOW}${mem_usage}${NC}\"\n    \n    # 网络信息\n    echo -e \"${BLUE}║${NC} ${CYAN}▎ 网络配置${NC}\"\n    echo -e \"${BLUE}║${NC}   网络模式: ${GREEN}$(${DOCKER_CMD} inspect -f '{{.HostConfig.NetworkMode}}' ${CONTAINER_NAME})${NC}\"\n    echo -e \"${BLUE}║${NC}   主机名:   ${GREEN}$(${DOCKER_CMD} inspect -f '{{.Config.Hostname}}' ${CONTAINER_NAME})${NC}\"\n    \n    # 挂载信息\n    echo -e \"${BLUE}║${NC} ${CYAN}▎ 挂载信息${NC}\"\n    ${DOCKER_CMD} inspect -f '{{range .Mounts}}{{.Source}} -> {{.Destination}}{{println}}{{end}}' ${CONTAINER_NAME} | while IFS= read -r line; do\n        if [ ! -z \"$line\" ]; then\n            echo -e \"${BLUE}║${NC}   路径: ${GREEN}${line}${NC}\"\n        fi\n    done\n    \n    # 工作目录\n    echo -e \"${BLUE}║${NC} ${CYAN}▎ 工作目录${NC}\"\n    echo -e \"${BLUE}║${NC}   ${GREEN}$(${DOCKER_CMD} inspect -f '{{.Config.WorkingDir}}' ${CONTAINER_NAME})${NC}\"\n    \n    echo -e \"${BLUE}╚════════════════════════════════════════════════════════════╝${NC}\"\n}\n\n# 进入容器\nenter_container() {\n    show_container_info\n    ${DOCKER_CMD} exec -it ${CONTAINER_NAME} bash\n}\n\n# 主函数\nmain() {\n    check_container_status\n    status=$?\n    \n    case $status in\n        0)  # 容器运行中\n            enter_container\n            ;;\n        1)  # 容器存在但未运行\n            start_container && enter_container\n            ;;\n        2)  # 容器不存在\n            create_container && enter_container\n            ;;\n    esac\n}\n\n# 执行主函数\nmain\n\n# 使用说明（注释）\n: '\n常用Docker命令：\n1. 查看运行中的容器：docker ps\n2. 停止容器：docker stop triton-xiyang\n3. 删除容器：docker rm triton-xiyang\n4. 查看镜像列表：docker images\n5. 删除镜像：docker image rm <image_id>\n'\n```\n","tags":["Docker"],"categories":["Tools","Docker"]},{"title":"Kalman Filters","url":"/2021/01/30/Kalman-Filters/","content":"\n## Concept\n1. 卡尔曼滤波是一个递归的`估计`，只要获知`上一时刻的状态估计`和`当前状态的观测`就可以计算`当前状态的估计`。\n1. 卡尔曼滤波器不需要观测/估计的历史记录。\n1. 卡尔曼滤波器是一个纯粹的`时域滤波器`，而不像低通滤波器那样，需要在频率设计，然后转到时域应用。\n1. 两个阶段：\n   - `预测`：由上一状态的估计做出对当前状态的估计\n   - `更新`：利用当前状态的观测值`优化`预测阶段的估计值，以获取一个`更精确的当前状态估计`\n1. 两种数据：\n   - `估计值`\n   - `观测值`：传感器测量值，如GPS位置。\n1. 线性系统与非线性系统：\n   - 判断标准：能不能通过上一次系统估计状态直接乘以某个矩阵得到预测状态？ 矩阵都是具体的值，与估计状态相乘后只是估计状态的线性变换。（还是测量状态不能直接与估计状态求差？）\n   - `线性系统`： 预测状态 = F * 上一次估计状态。例如：预测汽车位置，传感器获得位置值，假设很短时间内，系统是匀速运动或匀加速运动（对应两种模型）。可以通过上一时刻状态乘以矩阵得到预测值。\n   - `非线性系统`：预测状态不能直接由上一次估计状态乘以一个矩阵得到。 如雷达跟踪飞机，估计状态为(x, y). 测量状态为径向距离r和夹脚。\n\n## Formulas\n1. ![5 basic formulas](/images/kalman/formulas.jpg)\n\n## links\n1. [Kalman-and-Bayesian-Filters-in-Python](https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python)\n1. [Using in car](https://blog.csdn.net/codesamer/article/details/81191487)\n1. [基于Kalman滤波器的进行物体的跟踪](https://www.jianshu.com/p/d51a3a7736ca)\n1. [图说卡尔曼滤波](https://zhuanlan.zhihu.com/p/39912633)\n1. [Youtube Matlab tutorial](https://www.youtube.com/watch?v=VFXf1lIZ3p8)\n","tags":["Filters"],"categories":["Algorithm","Kalman"]},{"title":"Redis","url":"/2021/01/11/Redis/","content":"\n## 锁设计原则\n1. 所有操作都可分为增删改查，改可用删除和增加来实现，但改更快并且具有原子性。\n1. 只有获得锁才能对锁进行删和改。\n1. 注意区分又状态和无状态。凡是涉及到有状态的API(lock, unlock)要特别小心。要与无状态API(圆的面积)模块区分开。\n1. 系统状态尽可能的少，太多状态容易出问题。\n\n\n## 基于Redis的分布式锁\n1. Redis Lua脚本具有原子性\n\n### 实现思想：\n1. 获取锁的时候，使用setnx加锁，并使用expire命令为锁添加一个超时时间，超过该时间则自动释放锁，锁的value值为一个随机生成的UUID，通过此在释放锁的时候进行判断。\n1. 获取锁的时候还设置一个获取的超时时间，若超过这个时间则放弃获取锁。\n1. 释放锁的时候，通过UUID判断是不是该锁，若是该锁，则执行delete进行锁释放。\n\n### 超时后又多个程序获得锁解决方法\n1. 将过期时间设置足够长，确保代码逻辑在锁释放之前能够执行完成\n1. `为获取锁的线程增加守护线程，为将要过期但未释放的锁增加有效时间`\n\n### [使用守护线程特点](https://segmentfault.com/a/1190000022935064)\n1. `一定要用SET key value NX PX milliseconds 命令`:如果不用，先设置了值，再设置过期时间，这个不是原子性操作，有可能在设置过期时间之前宕机，会造成死锁(key永久存在)\n1. `value要具有唯一性`:这个是为了在解锁的时候，需要验证value是和加锁的一致才删除key。这是避免了一种情况：假设A获取了锁，过期时间30s，此时35s之后，锁已经自动释放了，A去释放锁，但是此时可能B获取了锁。A客户端就不能删除B的锁了。\n\n\n\n## Redis server\n\n``` shell\ngit clone https://github.com/redis/redis.git\ncd redis && make\ncd src\n./redis-server\n```\n\n## Redis client\n\n``` shell\n./redis-cli\n\n## cmd\nset foo bar\nget foo\n\n## change dump.rdb path\nconfig set dir /home/xxx\n\n# save data to dump.rdb\nsave\n```\n\n## hiredis (just a C client; not include server)\n\n``` shellp\ngit clone https://github.com/redis/hiredis.git\ncd hiredis\nmkdir build; cd build; cmake ..; make;\nsudo make install\n./hiredis-test\n\n## NOTE: sample are easier than hiredis-test\n```\n\n## Links\n1. [redis lock华为漫画](https://bbs.huaweicloud.com/blogs/209955)\n1. [Zookeeper 华为漫画](https://bbs.huaweicloud.com/blogs/209954)\n1. [redis实现分布式锁](https://blog.csdn.net/xlgen157387/article/details/79036337)\n1. [小米解决方案](https://xiaomi-info.github.io/2019/12/17/redis-distributed-lock/)\n1. [redis分布式锁](https://juejin.cn/post/6844903830442737671)\n1. [redislock C++ sample](https://github.com/yuhanfang/redislock)\n1. [Redis set command(note: NX)](https://redis.io/commands/set)\n1. [Redisson](https://github.com/redisson/redisson)\n","tags":["Redis","Database"],"categories":["Program","Database"]},{"title":"Tmux config","url":"/2020/12/07/Tmux/","content":"\n## tmux plugin\n\n1. [tpm](https://github.com/tmux-plugins/tpm)\n1. [tmux-plgins](https://github.com/orgs/tmux-plugins/repositories?q=&type=all&language=&sort=stargazers)\n1. [tmux-resurrect](https://github.com/tmux-plugins/tmux-resurrect)\n1. prefix + I install\n\n## clear-history\n\n1. Just type`<prefix> + :`in the relevant pane and then type `clear-history` and press enter.\n\n## windows layerout\n\n1. `<prefix> + M-1` 垂直对齐\n1. `<prefix> + M-2` 水平对齐\n1. `<prefix> + M-3`\n1. `<prefix> + M-4`\n1. `<prefix> + M-5`\n\n## resize window\n\n1. :resize-window -A\n\n## Tmux config\n\n```sh\n#use mouse to scroll history\nset -g mouse on        #For tmux version 2.1 and up\n# set -g mode-mouse on   #For tmux versions < 2.1\n#set -g default-terminal \"xterm-256color\"\nset -g default-terminal \"xterm\"\nset -g status-style bg=yellow,fg=black\n\n# set history lenth\nset -g history-limit 15000\n\n# change prefix key\nset -g prefix C-q\nunbind C-b\nbind C-q send-prefix\nset -g prefix2 C-j\nbind C-j send-prefix -2\n\n# more intuitive keybindings for splitting and create window\nbind \\\\ split-window -h -c \"#{pane_current_path}\"\nbind - split-window -v -c \"#{pane_current_path}\"\nbind c new-window -c \"#{pane_current_path}\"\n\n# swap window bind\nbind M-l swap-window -t + \\; next-window\nbind M-h swap-window -t - \\; previous-window\n\n# display\nset -g base-index 1\nset-window-option -g pane-base-index 1\nset -g renumber-windows on\n\n# set terminal title\nset -g set-titles on\n\n# redraw status line every 10 seconds\nset -g status-interval 10\n\n# right status: tmux show-options -g | grep status-right\nset -g status-right \"#{=21:host_short} \"\n\n# Remove delay when sending commands\n# (default is 500 milliseconds)\nset -sg escape-time 1\n\n# reference C-t ?\nbind-key -T prefix a select-window -t :=1\nbind-key -T prefix s select-window -t :=2\nbind-key -T prefix d select-window -t :=3\nbind-key -T prefix f select-window -t :=4\nbind-key -T prefix g select-window -t :=5\nbind-key -T prefix h select-window -t :=6\nbind-key -T prefix j select-window -t :=7\nbind-key -T prefix k select-window -t :=8\nbind-key -T prefix \\; select-window -t :=1 \\; select-window -t -1\n```\n\n## Reload config\n\n```sh\ntmux source-file ~/.tmux.conf\n```\n\n## Tmux Zsh CheatSheet\n\n[https://github.com/ohmyzsh/ohmyzsh/wiki/Cheatsheet](https://github.com/ohmyzsh/ohmyzsh/wiki/Cheatsheet)\n\n```sh\nta    : tmux attach -t\ntad   : tmux attach -d -t\nts    : tmux new-session -s\ntl    : tmux list-sessions\ntksv  : tmux kill-server\ntkss  : tmux kill-session -t\n```\n","tags":["Tmux"],"categories":["Tools","Tmux"]},{"title":"Shell Script","url":"/2020/11/28/Shell-script/","content":"\n## basic\n\n1. 判断 linux 命令是否存在\n\n```sh\nBUILD_SYSTEM=Ninja\nif ! [ -x \"$(command -v ninja)\" ]; then\n    BUILD_SYSTEM='Unix Makefiles'\nfi\n\nif ! command -v <the_command> &> /dev/null\nthen\n    echo \"<the_command> could not be found\"\n    exit\nfi\n```\n\n1. 分支，循环、 case， 函数、正则表达式\n1. shell 风格指南 [link](https://zh-google-styleguide.readthedocs.io/en/latest/google-shell-styleguide/contents/)\n1. seq 产生顺序数 for i in `seq 1 10`;do echo jia; sleep 1; done\n1. cat\n\n```\ncat  >> a.txt <<EOF        追加\n    hello,world\nEOF\n```\n\n```\ncat  > a.txt <<EOF      覆盖\n    hello,world\nEOF\n```\n\n1. set -xe 显示运行的每一条命令，出错退出。 -x 显示 -e 退出\n1. echo -e \"jiaxiyang\\c\" 脚本输出不换行\n1. 16 进制转化为 10 进制 num=\"ABCD1234\" && echo $((16#${num}))\n1. 产生随机数 cat /proc/sys/kernel/random/uuid\n1. 获取本机 IP 地址： ifconfig | grep \"inet addr:\" | awk '{ print $2 }'| sed 's/addr://g'\n1. awk 按列过滤参数 ls -l test.sh | awk '{ print $5 }' 输出：454\n1. shell 小程序[link](https://www.jb51.net/article/54488.htm)\n1. shell 得到文件信息 `ll /var/log/messages | cut -d \" \" -f 5`\n1. shell 不用输入密码登录 expect [link](http://www.cnblogs.com/lixigang/articles/4849527.html) set timeout 3 ; spawn ssh jiaxiyang@10.10.0.17; expect \"\\*password:\"; send \"jiaxiyang\\r\";interact\n1. 读文件内容，记录到变量中 var=$(cat name.txt)\n1. 计时 start=$(date+%s) && sleep 2 && end=$(date+%s)&& echo $(($end - $start)) [link](http://blog.csdn.net/gengshenghong/article/details/7583580)\n1. sleep: sleep 1 睡眠 1 秒 sleep 1s 睡眠 1 秒 sleep 1m 睡眠 1 分 sleep 1h 睡眠 1 小时\n1. [ -f test ] && rm test 如果存在 test 文件 删除 test ，逻辑运算代替 if\n   对于包含至少一个其他函数的足够长的脚本，需要称为  main  的函数。main \"$@\"\n1. 比较二进制文件\n\n```\n#!/bin/bash\n\nprocess()\n{\n    file=$1\n    echo \"file  : $(realpath ${file})\"\n    echo \"date  : $(date -r ${file})\"\n    echo \"length: $(wc -c ${file} | awk '{print $1}')\"\n    echo \"md5sum: $(md5sum $(realpath ${file}) | awk '{print $1}')\"\n    xxd -l 96 ${file}\n    echo\n}\n\necho \"date now: $(date)\"\necho\nfor var in $@; do\n    process $var\ndone\n```\n\n1. function(in .bashrc)\n\n```\nfunction loop ()\n{\n    eval \"$@\";\n    while sleep 1; do\n        eval \"$@\";\n    done\n}\n```\n\n1. copy link libs\n\n```sh\nlibs() {\n    OUTPUT_DIR=$(date -u +\"%Y-%m-%d-%H:%M:%S\")\n    mkdir -p ${OUTPUT_DIR}/bin ${OUTPUT_DIR}/lib\n    BINS=\"$@\"\n    cp ${BINS} ${OUTPUT_DIR}/bin/\n    \"$(ldd ${BINS} | grep \"=> /\" | awk '{print $3}' | xargs -I '{}' cp -v '{}' ${OUTPUT_DIR}/lib)\"\n}\n```\n\n1. 创建固定个数 panel，每个 panel 执行不同命令\n\n```sh\n#!/bin/bash\n\nPANEL_NUM=6\nwhile [ $(tmux list-panes | wc -l) -lt ${PANEL_NUM} ]; do\n\ttmux split-window \\; #select-pane -L\ndone\n\n# even-horizontal even-vertical main-horizontal main-vertical tiled\n# tmux select-layout even-horizontal\ntmux select-layout tiled\n\nCURRENT_PANEL=$(tmux display -pt \"${TMUX_PANE:?}\" '#{pane_index}')\nfor i in $(seq 1 ${PANEL_NUM}); do\n\techo ${i}\n\tif [ ${i} -ne ${CURRENT_PANEL} ]; then\n\t\ttmux send-keys -t .${i} \"echo ${i}; ls -al\" c-m\n\tfi\ndone\n\n```\n\n1. 删除和添加版权信息\n\n```sh\n#!/bin/bash\n\n## remove license\n# for i in $(/usr/bin/fdfind -e h -e cpp -e hpp -e cc); do\nfor i in $(find . -regex '.*\\.\\(cpp\\|cc\\|cxx\\|hpp\\|h\\)'); do\n\tline=$(cat $i | grep -n \"#i[fn]\\|#pragma\" | awk -F ':' '{print $1}' | awk 'NR==1')\n\tline=$((${line} - 1))\n\tif ((${line} > 0)); then\n\t\tsed -i \"1, ${line}d\" $i\n\tfi\ndone\n\n## add license\ncat >pgcopyright.txt <<EOF\n/**\n * @copyright PhiGent.ai @ 2022\n */\n\nEOF\n\nfor i in $(find . -regex '.*\\.\\(cpp\\|cc\\|cxx\\|hpp\\|h\\)'); do\n\tcat pgcopyright.txt $i >$i.new && mv $i.new $i\ndone\n\nrm pgcopyright.txt\n```\n\n1. push to jfrog\n\n```sh\n#!/bin/bash\n\nJFROG_USER=xiyang.jia\nJFROG_PASSWD=\n\npush_to_jfrog() {\n        FILE=$1\n        JFROG_PATH=http://artifacts.phigent.io/artifactory/$2\n        MD5=$(md5sum $FILE | awk '{print $1}')\n        curl -u\"${JFROG_USER}:${JFROG_PASSWD}\" -T $FILE -H \"X-Checksum-MD5:${MD5}\" -L \"${JFROG_PATH}\"\n}\n\npush_dir_to_jfrog(){\n        for file in $(find $1 -type f); do\n                push_to_jfrog ${file} $2/${file}\n        done\n}\n\nif [ -d $1 ]; then\n        push_dir_to_jfrog $1 $2\nelse\n        push_to_jfrog $1 $2\nfi\n```\n","tags":["Shell"],"categories":["Program","Shell"]},{"title":"Python-grammar","url":"/2020/11/16/Python-grammar/","content":"\n## 解释器\n\n1. 官方版本的解释器:Cpython 这个解释器是用 C 语言开发的，所以叫 CPython。\n1. [其他解释器](https://devguide.python.org/#other-interpreter-implementations)\n1. CPython 解释器由一个`编译器`和一个`虚拟机`构成，编译器负责将源代码转换成字节码文件，而虚拟机负责执行字节码。所以，解释型语言其实也有编译过程，只不过这个编译过程并不是直接生成目标代码，而是中间代码（字节码），然后再通过虚拟机来逐行解释执行字节码\n1. python 解释器的编译器会将.py 源文件编译（解释）成字节码生成 PyCodeObject 字节码对象存放在内存中。python 解释器的虚拟机将执行内存中的字节码对象转化为机器语言，虚拟机与操作系统交互，使机器语言在机器硬件上运行。运行结束后 python 解释器则将 PyCodeObject 写回到 pyc 文件中。当 python 程序第二次运行时，首先程序会在硬盘中寻找 pyc 文件，如果找到，则直接载入，否则就重复上面的过程。\n1. pyc 文件其实是 PyCodeObject 的一种持久化保存方式。pyc 文件，文件中包含 python 的 magic number（来说明编译时使用的 python 版本号）、源文件的 mtime（使 pyc 和 py 文件保持同步）、编译出的 code 对象\n1. 字节码是一种中间代码，类似于汇编语言，它可以被解释器直接执行，也可以被保存到.pyc 文件中以提高下次执行的速度。\n1. [解释器基本构成](https://www.imahui.com/network/6026.html)\n\n## 加速\n\n1. sample 见 python_sandbox 中的 optimization\n1. numba jit 加速(不能使用复杂数据结构)\n1. triton 加速\n1. torch compile 加速\n1. cuda 加速\n1. 数据结构改变，优化访存\n\n## vscode test\n\n1. [link](https://code.visualstudio.com/docs/python/testing)\n\n## links\n\n1. [python tutorial](https://docs.python.org/3/tutorial/)\n\n## 加密\n\n1. [Cython](https://www.cnblogs.com/gcgc/p/16529975.html)\n   - 编译成 c，再编译成 so，strip 后再使用\n1. [在线混淆](https://www.toolfk.com/tools/online-python-confuse.html)\n1. [代码加密](https://zhuanlan.zhihu.com/p/54296517)\n\n## basic\n\n1. `pass` 语句用于指示一个空的代码块。它不执行任何操作，只是作为占位符使用，用于保持语法完整性。通常在语法上需要语句或代码块的地方，但是逻辑上不需要执行任何操作时，可以使用 pass 语句。\n1. CPython 的 Frame Evaluation API（框架评估 API）是一个高级特性，允许开发者定制 Python 解释器执行代码的方式。这个 API 提供了一种方法，能够在 Python 解释器运行时动态插入和替换代码执行的框架，从而可以进行代码插桩、动态优化或其他高级操作。\n1. print\n\n```python\n    if torch.allclose(triton_output, torch_output, atol=0.125, rtol=0):\n        print(\"✅ Triton and Torch match\")\n    else:\n        print(\"❌ Triton and Torch differ\")\n```\n\n1. `pip show numpy`查看 numpy 信息, 包括安装路径\n1. `python -v` import module 时可以查看 module path\n1. `print(os.environ)` vs code debug console 打印环境变量\n   - `print(os.environ['HOME'])`\n1. [Partial_functions](https://www.learnpython.org/en/Partial_functions) 给函数加指定参数\n   - 偏函数允许将具有 x 个参数的函数导出为具有较少参数和为更有限的函数设置的固定值的函数。\n1. [Python 中的星号，\\**和*运算符](https://sikasjc.github.io/2018/10/12/star/)\n   - 数学运算符： `*`用于乘法， `**` 用于乘方\n   - 函数参数：\n     - \\*args 可以表示任意多个参数，它本质是一个 tuple。\n     - \\*\\*kwargs 是关键字参数，它本质上是一个 dict。\n   - 解包参数列表: 在 Python 中，调用函数时，利用\\*语句可以将参数列表解包\n   - 序列解包\n   - Keyword-Only Arguments\n1. `id(a)` a 对象的地址, 用于分析对象内存\n   - Return the identity of an object.This is guaranteed to be unique among simultaneously existing objects. (CPython uses the object's memory address.)\n1. 计时\n\n```python\nstart_time = time.perf_counter()\nyour_function()\nend_time = time.perf_counter()\n```\n\n1. 在 Python 中,可以使用三个双引号(\"\"\")或三个单引号(''')来进行段落注释。\n1. `#!/usr/bin/env python3` 指定执行器\n1. python repl 里运行 py 文件\n\n```python\nexec(open(\"test.py\").read()) # python3\nexec(open(\"filename.py\").read()) # python2\n```\n\n1. `print(cls.__class__.__name__)`打印 cls 实例类的名字\n1. print 到文件\n\n```python\nwith open('output.txt', 'w') as file:\n    print('Hello, world!', file=file)\n```\n\n1. `help(torch.ones)` 显示函数 help\n1. `print(dir(list))` python 的 dir()函数可以用来获取对象的属性和方法。\n1. `unset all_proxy` 不能使用代理，可能出现 install 错误 [link](https://blog.csdn.net/whatday/article/details/109287343)\n1. [install python from deadsnakes](https://launchpad.net/~deadsnakes/+archive/ubuntu/ppa)\n1. [build python from source](https://github.com/deadsnakes/docs/blob/main/Building-Deadsnakes-Packages-from-Git.rst) 编译之后需要前一级目录 `sudo dpkg -i *.deb`\n1. `pip freeze | grep torch`: 查看库版本\n1. `python3 -c \"import torch; print(torch.__version__)\"`\n1. `python3 -c \"import tensorrt_llm; print(tensorrt_llm.__version__)\"`\n1. `print(\"Numpy version:\", numpy.__version__)`\n1. [useful-snippets](https://elpy.readthedocs.io/en/latest/ide.html#other-useful-snippets)\n1. [值语义与引用语义](https://cloud.tencent.com/developer/article/2391275)\n   - 不可变变量的值一旦创建，就不能被修改。如果你尝试修改一个不可变对象的值，Python 将会创建一个新的对象。Python 中的不可变对象包括整数（int）、浮点数（float）、字符串（str）、元组（tuple）等。\n   - 可变变量的值可以在原地修改，而不会创建一个新的对象。Python 中的可变对象包括列表（list）、字典（dict）、集合（set）等。\n1. `import copy; x = copy.deepcopy(y)`\n1. print(\"x: \", x, type(x), x.shape) [link](https://blog.csdn.net/sinat_28576553/article/details/81154912)\n1. \"%s/Kernels\" % self.exe_path 的作用是将 self.exe_path 中的值插入到字符串中 %s 的位置。这种格式化方法被称作“旧式字符串格式化”（old-style string formatting）。\n1. 现代代码中更倾向于使用 str.format() 方法或者 f-string（在 Python 3.6 及以后的版本中）。例如，使用 str.format()，上述代码可以写作 \"{}/Kernels\".format(self.exe_path)；使用 f-string，可以写作 f\"{self.exe_path}/Kernels\"。\n1. print('{:.2f} ms'.format(xxx))\n1. print(f'{xxx:.2f} ms')\n1. pytorch tensor to binary file: `tensor.cpu().numpy().astype(np.float32).tofile(\"test.bin\")`; c++ read binary file\n1. print('type : {}'.format(type(xxx)))\n1. print('size : {}'.format(len(xxx)))\n\n```\nstatic std::vector<float> read_from_bin_file(std::string file) {\n  auto result_size = std::filesystem::file_size(file);\n  std::vector<float> result(result_size / 4);\n  CHECK(std::ifstream(file).read((char*)&result[0], result_size).good())\n      << \"failed to read baseline from \" << file;\n  return result;\n}\n\ntemplate <typename T>\nvoid mywritefile(T* src, int size1, std::string filename) {\n  ofstream Tout;\n  Tout.open(filename, ios_base::out | ios_base::binary);\n  if (!Tout) {\n    cout << \"Can't open the file! \" << filename << \"\\n\";\n    return;\n  }\n  Tout.write((char*)src, size1 * sizeof(T));\n  // CHECK(std::ofstream(filename).write((char*)data.data, size).good()) <<\n}\n\ntemplate void mywritefile(int8_t* src, int size1, std::string filename);\n\n```\n\n## 多线程问题\n\n1. GIL(Global Interpreter Lock) 全局解释器锁\n1. GIL 限制了 Python 解释器同一时间只能执行一个线程的代码，因此在 CPU 密集型的任务中，多线程程序并不一定比单线程程序更快。\n\n## [Generators](https://wiki.python.org/moin/Generators)\n\n1. 生成器（Generator），是计算机科学中特殊的子程序。实际上，所有生成器都是迭代器。生成器非常类似于返回数组的函数，都是具有参数、可被调用、产生一系列的值。但是生成器不是构造出数组包含所有的值并一次性返回，而是每次产生一个值，因此生成器看起来像函数，但行为像迭代器。\n1. Generator 可以减少内存使用，用到了再产生\n1. `print(list(model.parameters())[0])` 打印 generators 第一个元素， model.parameters()是一个 generator\n1. `print([x for x in model.parameters()])`\n1. 关键字 yiled 来创建 generator\n\n## sort\n\n1. [sorted](https://www.runoob.com/python/python-func-sorted.html)\n   - sorted_data = sorted(data, reverse=True) 降序\n\n## class\n\n### 构造函数\n\n1. **init** 方法是一个特殊的方法（称为构造器），当创建类的新实例时，它会自动被调用。它用于初始化类的属性或其他必要的启动过程。\n\n```python\nclass MyClass:\n    def __init__(self, param1, param2):\n        self.attribute1 = param1\n        self.attribute2 = param2\n```\n\n### 类方法\n\n1. 在 Python 中，可以使用类方法来创建实例。这种方式通常称为“工厂方法”，它允许你以不同的方式创建类的实例。\n\n```python\nclass MyClass:\n    def __init__(self, param1, param2):\n        self.attribute1 = param1\n        self.attribute2 = param2\n\n    @classmethod\n    def from_string(cls, string):\n        param1, param2 = map(int, string.split('-'))\n        return cls(param1, param2)\n\n# 使用 __init__\ninstance1 = MyClass(10, 20)\n\n# 使用类方法\ninstance2 = MyClass.from_string(\"10-20\")\n\n```\n\n### static 方法\n\n1. 可以定义一个静态方法或一个独立的函数来处理复杂的初始化，并返回类的实例。\n\n```python\nclass MyClass:\n    def __init__(self, param1, param2):\n        self.attribute1 = param1\n        self.attribute2 = param2\n\n    @staticmethod\n    def from_list(param_list):\n        return MyClass(*param_list) # 在Python中，调用函数时，利用*语句可以将参数列表解包\n\ndef create_instance(param1, param2):\n    # 可以在这里执行更复杂的初始化\n    return MyClass(param1, param2)\n\n# 使用静态方法\ninstance1 = MyClass.from_list([10, 20])\n\n# 使用独立的函数\ninstance2 = create_instance(10, 20)\n```\n\n## Decorator 装饰器\n\n1. 在 Python 中，函数名上方的 @ 符号表示装饰器（Decorator）。装饰器是 Python 的一个高级特性，用于在不修改原函数代码的情况下增强或改变函数的功能。\n1. 装饰器`本质上是一个函数，它接受一个函数作为参数，并返回一个新的函数。`使用 @ 符号 followed by the decorator name 就可以将装饰器应用于一个函数。例如：在这个例子中，say_hello 函数被 my_decorator 装饰器装饰。当你调用 say_hello 时，实际上是在调用 my_decorator 返回的 wrapper 函数。\n\n```python\ndef my_decorator(func):\n    def wrapper():\n        print(\"Something is happening before the function is called.\")\n        func()\n        print(\"Something is happening after the function is called.\")\n    return wrapper\n\n@my_decorator\ndef say_hello():\n    print(\"Hello!\")\n\nsay_hello()\n\n```\n\n1. 装饰器的作用\n   - 日志记录：自动记录函数的调用细节。\n   - 性能测试：检测函数运行时间。\n   - 权限校验：在函数执行前进行权限检查。\n   - 缓存：存储函数的返回结果，以便后续调用时快速返回。\n   - 类型检查：检查函数的输入输出类型。\n\n## [setuptools](https://github.com/pypa/setuptools)\n\n1. 建议新项目避免 setup.py 配置\n1. In addition to specifying a build system, you also will need to add some package information such as metadata, contents, dependencies, etc. This can be done in the same `pyproject.toml` file, or in a separated one: `setup.cfg` or `setup.py`.\n1. `setup.py` pip install ./example 时执行 setup.py 来编译\n1. `python setup.py build --debug`\n1. `python setup.py install --debug`\n\n## wheel python 包\n\n1. `pip wheel .` 生成 python 包\n1. `pip install *.whl`\n\n## [argparse](https://docs.python.org/3/library/argparse.html)\n\n1. 参数传递\n\n```python\nimport argparse\n\nparser = argparse.ArgumentParser(description='Process some integers.')\nparser.add_argument('integers', metavar='N', type=int, nargs='+',\n                    help='an integer for the accumulator')\nparser.add_argument('--sum', dest='accumulate', action='store_const',\n                    const=sum, default=max,\n                    help='sum the integers (default: find the max)')\n\nargs = parser.parse_args()\nprint(args.accumulate(args.integers))\n```\n\n## [data structure](https://docs.python.org/3/tutorial/datastructures.html)\n\n1. Python 为我们提供了三种常用数据结构，列表(Lists),字典（Dictionary）,元组（Tuple）\n1. [与 c++数据结构做对比](https://blog.csdn.net/weixin_53453184/article/details/137224600)\n\n### list\n\n1. 类似 c++ vector\n1. list[-1] 最后一个元素，负号表示倒着索引\n1. squares[-3:] slice: 创建新的索引，包含最后三个 value\n1. slice 是浅拷贝\n1. list1 + list2 表示 concat\n1. letters[2:5] = ['C', 'D', 'E'] 批量赋值\n1. letters[2:5] = [] remove 第 3 个到第 5 个元素\n\n### [dict](https://www.w3schools.com/python/python_dictionaries.asp)\n\n1. `thisdict = dict(name = \"John\", age = 36, country = \"Norway\")` dict constructor\n1. `thisdict = {\"brand\": \"Ford\", \"model\": \"Mustang\",\"year\": 1964}`\n1. [Difference between dictionary and OrderedDict](https://stackoverflow.com/questions/34305003/difference-between-dictionary-and-ordereddict)\n1. dict --> c++ unordered_map\n1. OrderedDict --> c++ map ?\n1. SortedDict --> c++ map\n1. 这就是有序和排序的区别。 OrderedDict 根据插入顺序维护顺序，而 SortedDict 将根据键的排序顺序维护顺序\n1. [sortedcontainers](https://github.com/grantjenks/python-sortedcontainers)\n   - 需要安装\n1. `del dict[\"Key\"]`删除\n\n#### [dict merge](https://www.runoob.com/python3/python-merging-two-dictionaries.html)\n\n1. `dict2.update(dict1)`\n\n#### [dict_values](https://www.programiz.com/python-programming/methods/dictionary/values)\n\n```python\nmarks = {'Physics':67, 'Maths':87}\nprint(marks.values())\n\n# Output: dict_values([67, 87])\n```\n\n#### [dict_keys](https://www.programiz.com/python-programming/methods/dictionary/keys)\n\n```python\nnumbers = {1: 'one', 2: 'two', 3: 'three'}\nprint(numbers.keys())\n\n# Output: dict_keys([1, 2, 3])\n```\n\n### tuple\n\n1. c++ tuple\n\n## Comprehension\n\n1. Python 列表推导(List Comprehension)是 Python 中一个非常有用且常被使用的语法结构。它可以用简洁的语法代替循环语句,从一个可迭代对象中创建一个新的列表。\n1. `[ expr for val in collection [if condition] ]`\n\n```python\n# 简单推导\nnums = [1, 2, 3, 4]\nsquares = [x ** 2 for x in nums] # [1, 4, 9, 16]\n\n# 带判断条件\neven_squares = [x ** 2 for x in nums if x % 2 == 0] # [4, 16]\n\n# 字典推导\ndict_1 = {'a': 1, 'b': 2}\ndict_2 = {k:v**2 for k,v in dict_1.items()} # {'a': 1, 'b': 4}\n```\n\n## 虚拟环境\n\n```python\npython3 -m venv myenv\nsource myenv/bin/activate\npip install -r requirements.txt\n```\n\n## 输出表格\n\n```python\nfrom prettytable import PrettyTable\n\ntable = PrettyTable(['Name', 'Age', 'Gender'])\ntable.add_row(['Bob', 18, 'Male'])\nprint(table)\n```\n\n## Debug\n\n1. vscode debug\n1. vscode 无法跳转可能是 interpreter 问题\n1. [不跳转问题修复](https://blog.csdn.net/singxsy/article/details/123730161)\n1. lsp language server 使用 pylance, jedi\n1. `\"justMyCode\":False` 可以跳转到三方库源代码\n1. `python -i test.py` 执行之后会停下来，可以查看变量\n1. `python -m pdb test.py`\n1. `import pdb; pdb.set_trace()`\n1. `python -m trace --trackcalls pyscript.py`\n1. `python -m trace --trace pyscript.py`\n1. 打印动态库\n\n``` python\nimport sys\nfor name, module in sys.modules.items():\n    if hasattr(module, '__file__') and module.__file__:\n        if module.__file__.endswith(('.so', '.dll', '.pyd')):\n            print(f\"{name}: {module.__file__}\")\n```\n\n\n### [Debug 方法](https://blog.csdn.net/qq_38542085/article/details/78562458)\n\n1. elpy-pdb-debug-buffer\n1. logging\n1. pdb\n1. print\n\n## profiling\n\n1. [viztracer](https://github.com/gaogaotiantian/viztracer)\n   - 可用来看调用栈\n   - 适合看整体，可以看火焰图及 timeline\n   - vscode 插件：找到 result.json， 右键最后\n   1. [nsight system python-profiling](https://docs.nvidia.com/nsight-systems/UserGuide/index.html#python-profiling)\n1. [timeit](https://docs.python.org/3/library/timeit.html)\n1. [Python 装饰器实战：打造高效性能计时工具](https://www.cnblogs.com/wang_yb/p/18106433)\n1. [scalene](https://github.com/plasma-umass/scalene)\n   - 适合找具体每一行耗时及系统瓶颈\n   - a high-performance, high-precision CPU, GPU, and memory profiler for Python with AI-powered optimization proposals\n   - pip install scalene\n   - scalene --outfile result.html --html\n   - PROFILING=y scalene --outfile result5.html --html --reduced-profile --no-browser\n   - 配合 cprofile 看\n1. [pyinstrument](https://github.com/joerick/pyinstrument)\n   - pyinstrument -r html -o profile.html test.py\n   - html 右上角可以切换具体耗时及百分比\n1. [cprofile](https://zhuanlan.zhihu.com/p/24495603)\n1. [flameprof](https://github.com/baverman/flameprof)\n   - 可以将 cprofile 结果画出火焰图\n1. [vprof](https://github.com/nvdv/vprof)\n1. [line_profiler](https://github.com/pyutils/line_profiler)\n1. [tracemalloc](https://docs.python.org/3/library/tracemalloc.html)\n1. [pympler/pympler](https://github.com/pympler/pympler)\n1. [memray](https://github.com/bloomberg/memray)\n1. [memory_profiler](https://github.com/pythonprofilers/memory_profiler/tree/master)\n   `gprof2dot -f pstats mkm_run.prof | dot -Tpng -o mkm_run.png`\n1. profile 库\n\n```python\nimport profile\n\ndef test():\n    fps = profile.fps()\n    while True:\n        xxxxxxxxxxxxxxxxxxx\n        logging.info('fps:{}'.format(fps.compute()))\n```\n\n## package management\n\n1. [基于 pyproject.toml 的包管理](https://zhuanlan.zhihu.com/p/582651418)\n1. [poetry](https://github.com/python-poetry/poetry)\n1. [pixi](https://github.com/prefix-dev/pixi)\n\n## main\n\n1. 这里是一个使用`if __name__ == '__main__'`的 Python 示例:\n\n```python\n# 模块1\n\ndef foo():\n    print(\"foo\")\n\ndef bar():\n    print(\"bar\")\n\nif __name__ == '__main__':\n    foo()\n    bar()\n```\n\n```python\n# 模块2\nimport module1\n\nmodule1.foo() # 调用foo函数\n```\n\n1. 当作为脚本直接运行 module1 时,`__name__`等于`'__main__'`,if 语句块会被执行,打印 foo 和 bar。而当 module1 被导入时,`__name__`会设置成模块名,if 语句块不会执行。这种方式非常常用,它允许同一个模块既可以被导入调用,也可以直接作为脚本运行,实现代码的重用。`if __name__ =='__main__'` 是 Python 程序常见的惯用法,用于模块的导入和直接运行的区分。\n\n## [Logging](https://zhuanlan.zhihu.com/p/360306588)\n\n1. format 配置: `'[%(asctime)s] {%(pathname)s:%(lineno)d} %(levelname)s - %(message)s'` 可以显示文件绝对路径和行号，方便跳转\n1. [rich](https://github.com/textualize/rich/blob/master/README.cn.md)\n1. 基本配置：\n\n```\nimport logging\n\n#默认的warning级别，只输出warning以上的\n#使用basicConfig()来指定日志级别和相关信息\n\nlogging.basicConfig(level=logging.DEBUG #设置日志输出格式\n                    ,filename=\"demo.log\" #log日志输出的文件位置和文件名\n                    ,filemode=\"w\" #文件的写入格式，w为重新写入文件，默认是追加\n                    ,format=\"%(asctime)s - %(name)s - %(levelname)-9s - %(filename)-8s : %(lineno)s line - %(message)s\" #日志输出的格式\n                    # -8表示占位符，让输出左对齐，输出长度都为8位\n                    ,datefmt=\"%Y-%m-%d %H:%M:%S\" #时间输出的格式\n                    )\n\nlogging.debug(\"This is  DEBUG !!\")\nlogging.info(\"This is  INFO !!\")\nlogging.warning(\"This is  WARNING !!\")\nlogging.error(\"This is  ERROR !!\")\nlogging.critical(\"This is  CRITICAL !!\")\n\nlogging.info(\"timestamp {}, {}, {} \".format(i, bufs[i].timestamp.tv_sec, bufs[i].timestamp.tv_usec))\n```\n\n## 多线程\n\n1. [多线程返回值](https://blog.csdn.net/qq_20663229/article/details/94484526)\n\n```python\nimport threading as th\n\ndef print_time(arg):\n    pname = mp.current_process().name\n    pid = os.getpid()\n    tname = th.current_thread().getName()\n    LOGW('print time  [{}] {} : {} : {}'.format(pid, pname, tname, arg))\n\ndef dequeue_multibuffer1():\n    threads_test = []\n    threads_test.append(th.Thread(name=\"thread 0\", target=print_time, args=(1, ))) # 线程的args是个元组，只有一个元素要加逗号\n    threads_test.append(th.Thread(name=\"thread 1\", target=print_time, args=(2, )))\n    threads_test[0].start()\n    threads_test[1].start()\n    threads_test[0].join()\n    threads_test[1].join()\n    return 0\n\n```\n\n```python\n\n# 功能， 每个线程通过v4l2获取图片，多个线程拼接并同步，然后供其他模块使用\ndef dequeue_buffer(fd, buf, mems, shape, que):\n    while True:\n        if que.empty():\n            fcntl.ioctl(fd, v4l2.VIDIOC_DQBUF, buf)\n            frame = cv.cvtColor(mems[buf.index].reshape(shape), cv.COLOR_YUV2RGB_YUYV)\n            fcntl.ioctl(fd, v4l2.VIDIOC_QBUF, buf)\n            que.put_nowait((frame, buf.timestamp))\n\ndef dequeue_multibuffer(fds, bufs, mems_list, shape, que):\n    queues = []\n    num = len(fd2)\n    for i in range(num):\n         queues.append(queue.Queue(maxsize=10))\n\n    threads_test = []\n    for i in range(num):\n        threads_test.append(th.Thread(name=str(i), target=dequeue_buffer, args=(fds[i], bufs[i], mems_list[i], shape, queues[i])))\n    for i in range(num):\n        threads_test[i].start()\n    while True:\n        datas = []\n        for i in range(num):\n            frame, timestamp = queues[i].get()\n            logging.info(\"timestamp {}, {}, {} \".format(i, timestamp.tv_sec, timestamp.tv_usec))\n            mems = mems_list[i]\n            datas.append(frame)\n        if len(datas):\n            que.put(datas)\n\ndef main()\n    frame_queue = queue.Queue(maxsize=10)\n    get_frame_thread = th.Thread(name=\"dequeue_multibuffer\", target=dequeue_multibuffer, args=([vd_r, vd_l], [buf_r, buf_l], [mems_r, mems_l], (h, w, -1), frame_queue))\n    get_frame_thread.start()\n    while True:\n         frames = frame_queue.get()\n         do.....\n\n```\n\n## [numpy](https://numpy.org/doc/stable/index.html)\n\n1. `np.max(predictions, axis=1)` 按列求最大值(64, 1000) -> (64)\n1. `np.argmax(predictions, axis=1)` 按列求最大值下标(64, 1000) -> (64)\n1. `np.show_config()` 可以看 numpy 使用的 blas 版本, 如 openblas\n1. `np.testing.assert_allclose(out_np, out_tvm.numpy(), rtol=1e-3)` check result\n1. float_array = double_array.astype(np.float32)\n1. reshape (-1 自适应)\n\n```python\nx = x.reshape(2, -1)\nx = x.reshape(1,-1) # 展开为1行\n```\n\n1. nchw -> nhwc\n\n```python\nimport numpy as np\nimport sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\nN, C, H, W = sys.argv[3:7]\na = np.fromfile(sys.argv[1], dtype=np.float32)\na = a.reshape((int(N), int(C), int(H), int(W)))\na = a.transpose(0, 2, 3, 1)\na.tofile(output_file)\n```\n\n1. nhwc -> nchw\n\n```python\nimport numpy as np\nimport sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\nN, H, W, C = sys.argv[3:7]\na = np.fromfile(sys.argv[1], dtype=np.float32)\na = a.reshape((int(N), int(H), int(W), int(C)))\na = a.transpose(0, 3, 1, 2)\na.tofile(output_file)\n```\n\n1. 截取数据\n\n```python\nimport numpy as np\na = np.fromfile(\"inputs/total_flow_predictions.bin\", dtype=np.float32)\nprint(len(a))\na = np.resize(a, [len(a)//7])\nprint(len(a))\nb = np.fromfile(\"output/disparity.bin\", dtype=np.float32)\nprint(len(b))\nb = np.resize(b, [len(b)//2])\nprint(len(b))\ndiff1 = a - b\ndiff =  np.abs(diff1)\nprint(\"max diff is {}\".format(diff.max()))\nprint(\"mean diff is {}\".format(diff.mean()))\n```\n\n1. 误差\n\n```python\nimport numpy as np\nimport sys\n\na = np.fromfile(sys.argv[1], dtype=np.float32)\nprint(\"file1: {}, len: {}\".format(sys.argv[1], len(a)))\nb = np.fromfile(sys.argv[2], dtype=np.float32)\nprint(\"file2: {}, len: {}\".format(sys.argv[2], len(b)))\ndiff1 = a - b\ndiff =  np.abs(diff1)\nprint(\"max diff is {}\".format(diff.max()))\nprint(\"mean diff is {}\".format(diff.mean()))\n```\n\n## error\n\n1. [During handling of the above exception, another exception occurred:](https://blog.csdn.net/weixin_39514626/article/details/111839821)\n\n## [package](https://github.com/ml-tooling/best-of-python)\n\n1. [fire](https://github.com/google/python-fire)\n   - 方便解析命令行参数\n1. [rich 超强大](https://github.com/textualize/rich/blob/master/README.cn.md)\n   - 进度条\n   - table, box=box.ASCII2 设置和 prettytable 一样样式 [all format](https://rich.readthedocs.io/en/latest/appendix/box.html?highlight=box)\n   - status\n   - (good)trace back\n   - log, 可以和 logging 结合\n   - Rich is a Python library for rich text and beautiful formatting in the terminal\n1. [hydra](https://github.com/facebookresearch/hydra)\n   - 配置应用程序\n1. [tqdm](https://tqdm.github.io/)\n   - 可以展示 for 循环执行到哪里\n   - tqdm means \"progress\" in Arabic (taqadum, تقدّم) and is an abbreviation for \"I love you so much\" in Spanish (te quiero demasiado).\n1. [pillow](https://python-pillow.org/)\n1. [pydantic](https://github.com/pydantic/pydantic)\n   - 检查数据类型\n1. [pickle](https://docs.python.org/3/library/pickle.html)\n   - [sample](https://docs.python.org/zh-cn/3/library/pickle.html#examples)\n1. [(good)prettytable](https://pypi.org/project/prettytable/)\n\n```python\nfrom prettytable import PrettyTable\n\ndef transpose_table(original_table):\n    # 获取原表格的行数据\n    rows = original_table._rows\n\n    # 创建新的表格\n    transposed_table = PrettyTable()\n\n    # 获取转置后的表头（从原来的第一列数据生成）\n    transposed_table.field_names = [original_table.field_names[0]] + [row[0] for row in rows]\n\n    # 填充表格数据，取每列数据并添加到转置表中\n    for i, field in enumerate(original_table.field_names[1:], start=1):\n        transposed_row = [field] + [row[i] for row in rows]\n        transposed_table.add_row(transposed_row)\n\n    return transposed_table\n\n\ntable = PrettyTable()\ntable.field_names = [\n    'input name',\n    'type',\n]\nfor name, t in inputs_types.items():\n    table.add_row([name, t])\nprint(table)\n# print(transpose_table(table))\n```\n"},{"title":"Eclipse Cpp","url":"/2020/10/06/Eclipse-Cpp-IDE-on-Windows10/","content":"\n## install Eclipse IDE for C/C++ Developers\n\n1. [Officail Download](https://www.eclipse.org/downloads/packages/release/kepler/sr2/eclipse-ide-cc-developers)\n\n## install compiler MinGW\n\n1. [Official Install](http://mingw.org/wiki/InstallationHOWTOforMinGW)\n1. [Lose Packages](https://blog.csdn.net/done1182818968/article/details/105385382)\n","tags":["Cpp"],"categories":["Program","Cpp"]},{"title":"Emacs on Windows10","url":"/2020/09/30/Emacs-on-Windows10/","content":"\n## Install msys2 on windindows\n1. [Tsing hua source](https://mirrors.tuna.tsinghua.edu.cn/msys2/distrib/x86_64/)\n1. [Official web](https://www.msys2.org/)\n\n## Config msys2\n1. Theme: dracula\n1. Font: Consolas 14\n1. Transparency: Medium\n1. Right mouse: paste\n1. Terminal Type: xterm-256color\n1. Cursor: bold, not blinking\n\n## Install git vim oh-my-zsh emacs\n1. [Tutorial](https://blog.csdn.net/u013938484/article/details/83539008)\n1. Note: Change package source. No vim at first, you can edit the file on windows\n1. Emacs install: `pacman -S mingw-w64-x86_64-emacs`\n1. Libs install: `pacman -S mingw-w64-x86_64-libpng && pacman -S mingw-w64-x86_64-libjpeg-turbo`\n1. Config .bashrc and .zshrc\n\n## Build emacs plugins in Ubuntu on VMvare\n1. [100ms emacs](https://github.com/jiaxiyang/100ms_dot_emacs)\n1. build and copy the tar package to msys2\n1. install package on msys2\n","tags":["Emacs"],"categories":["Tools","Emacs"]},{"title":"git","url":"/2020/09/17/git/","content":"\n## 基础操作\n\n### clone 与获取\n\n```bash\n# 浅克隆指定分支\ngit clone -b llvmorg-19.1.3 --depth=1 https://github.com/llvm/llvm-project\n\n# 获取某个 commit\ngit clone --depth 1 <repository_url>\ngit fetch --depth=1 origin <commit_hash>\ngit checkout <commit_hash>\n\n# 或直接下载\nwget https://github.com/llvm/llvm-project/archive/<commit_hash>.zip\n```\n\n### 常用命令\n\n| 命令 | 说明 |\n|------|------|\n| `git add --update` | 添加所有修改的文件 |\n| `git restore --source develop src/main.cpp` | 将文件切换到某个分支内容 |\n| `git grep GK_GFX950 v3.3.1` | 从某个 tag 查找内容 |\n| `git remote set-url origin git@xxx` | 更新 remote |\n| `git config --global core.editor \"vi\"` | 设置编辑器 |\n| `git fetch --prune` | 同步并清理远程已删除的分支 |\n| `git --no-pager log -n 20` | 直接显示，不用按 q 退出 |\n\n### tag 操作\n\n```bash\ngit tag v0.0.1 && git push origin v0.0.1  # 创建并推送 tag\ngit describe --dirty --tags  # 显示当前 commit 信息，如 v1.0.1-100-gad0ddd0-dirty\ngit describe --always --dirty --match 'NOT A TAG'  # 无 tag 时显示信息\ntldr git tag  # 查看使用帮助\n```\n\n### Git LFS\n\n```bash\ngit lfs clone https://xxx           # 克隆 LFS 仓库\ngit lfs install && git lfs pull     # 已存在的 repo 拉取 LFS 文件\n\n# 代理加速\ngit config --global http.proxy http://10.31.10.80:7890\ngit config --global https.proxy https://10.31.10.80:7890\n```\n\n### 调试与统计\n\n```bash\n# 调试 clone 问题\nGIT_TRACE=1 GIT_CURL_VERBOSE=1 git clone <url>\n\n# 统计每个成员代码量\ngit log --format='%aN' | sort -u | while read name; do\n  echo -en \"作者：$name\\t\"\n  git log --author=\"$name\" --pretty=tformat: --numstat | \\\n    awk '{ add += $1; subs += $2; loc += $1 - $2 } END { printf \"添加: %s, 删除: %s, 总计: %s\\n\", add, subs, loc }'\ndone\n\n# 检查二进制文件内容\nstrings /path/to/binary | grep keyword\n```\n\n## Worktree\n\nGit worktree 允许同时检出多个分支到不同目录，无需多次 clone。\n\n### 常用命令\n\n```bash\n# 创建 worktree（新目录检出指定分支）\ngit worktree add ../feature-branch feature-branch\ngit worktree add ../hotfix -b hotfix  # 同时创建新分支\n\n# 列出所有 worktree\ngit worktree list\n\n# 删除 worktree\ngit worktree remove ../feature-branch\n# 或手动删除目录后执行\ngit worktree prune\n\n# 移动 worktree\ngit worktree move ../old-path ../new-path\n```\n\n### 使用场景\n\n| 场景 | 说明 |\n|------|------|\n| 并行开发 | 同时在多个分支工作，无需 stash 或 commit |\n| 紧急修复 | 开发中途快速切换到 hotfix 分支 |\n| 代码对比 | 在不同目录查看不同分支代码 |\n| 长时间编译 | 一个 worktree 编译，另一个继续开发 |\n| CI/CD | 并行构建多个分支 |\n\n### 示例：并行开发\n\n```bash\n# 主仓库在 ~/project，正在开发 feature-a\ncd ~/project\ngit worktree add ../project-hotfix main  # 创建 hotfix worktree\n\n# 在 hotfix 目录修复 bug\ncd ../project-hotfix\ngit checkout -b hotfix-123\n# ... 修复并提交 ...\n\n# 回到主开发\ncd ~/project\n# feature-a 的工作状态完全保留\n```\n\n### 注意事项\n\n- 每个分支只能在一个 worktree 中检出\n- 所有 worktree 共享 `.git` 目录（在主 worktree）\n- 删除 worktree 前确保没有未提交的更改\n\n## Merge\n\n```bash\n# fast-forward 合并（不产生 merge commit）\ngit merge --ff-only feature-branch\n\n# 解决冲突：接受 theirs（incoming）或 ours（current）\ntargets=\"python/test/ unittest/ README.md\"\ngit checkout --theirs -- $targets && git add $targets\n\n# 查看分支差异\ngit diff main dev -- src/main.cpp      # 两个分支某文件差异\ngit diff dev -- src/main.cpp           # 当前分支与某分支差异\n```\n\n**提示**：VSCode 插件可在文件夹右击接受 incoming\n\n## Revert\n\n```bash\ngit revert <hash>  # 撤销某个 commit\n```\n\n注意：不能 revert 某个 commit 里的单个文件\n\n## Stash\n\n```bash\ngit stash -m \"text\"                    # 保存并添加描述\ngit stash push -m \"my_feature_backup\"  # 同上\ngit stash list                         # 列出所有 stash\ngit stash apply stash@{0}              # 应用指定 stash\ngit stash show -p stash@{0}            # 查看 stash 内容\n```\n\n## Patch\n\n```bash\n# 生成 patch\ngit format-patch <start_hash>..<end_hash> -o /path/to/patches/\n\n# 应用 patch\ngit apply *.patch\n```\n\n## Submodule\n\n```bash\ngit submodule add https://github.com/NVIDIA/cutlass.git\ngit submodule update --init --recursive\n```\n\n## Commit 修改\n\n```bash\n# 修改最近一次 commit\ngit commit --amend --no-edit           # 合并 staged，message 不变\ngit commit --amend                     # 修改 message\ngit commit --amend --date=now --no-edit  # 修改时间\n\n# 交互式 rebase 修改历史 commit\ngit rebase --interactive HEAD~2\n```\n\n**Magit 快捷键**：\n- `ce` Extend：staged 合并到上次提交\n- `ca` Amend：修改上次提交的日志\n- `$` 查看具体命令\n\n## Pre-commit\n\n```bash\npip install pre-commit\npre-commit install\n\n# 运行检查\npre-commit run --all-files             # 检查所有文件\npre-commit run --from-ref origin/main --to-ref HEAD  # 检查当前分支\n\n# 只检查 staged 文件\ngit diff --name-only --cached | xargs pre-commit run --files\n\n# 配置别名\ngit config --global alias.fix-staged '!git diff --name-only --cached | xargs pre-commit run --files'\n# 之后使用 git fix-staged\n```\n\n参考：[triton precommit config](https://github.com/triton-lang/triton/blob/main/.pre-commit-config.yaml)\n\n## Workflow\n\n- dev 分支开发新功能\n- main/master 合入新功能\n- tag 确定 release\n\n参考：\n- [GitLab Flow 介绍](https://docs.gitlab.cn/jh/topics/gitlab_flow.html)\n- [GitLab Flow 的 11 条规则](http://dockone.io/article/2350)\n\n## Commit 规范\n\n参考：\n- [约定式提交](https://www.conventionalcommits.org/zh-hans/v1.0.0/)\n- [Angular Commit Guidelines](https://github.com/angular/angular/blob/22b96b9/CONTRIBUTING.md#-commit-message-guidelines)\n- [Commit Style Guide](https://github.com/feflow/git-commit-style-guide)\n\n## C++ 项目嵌入 Git 信息\n\n- [CMake 方案](http://xit0.org/2013/04/cmake-use-git-branch-and-commit-details-in-project/)\n- [无需重编译方案](https://stackoverflow.com/questions/51727566/how-to-make-git-commit-hash-available-in-c-code-without-needless-recompiling)\n\n## 常见问题\n\n### clone 失败\n\n```bash\ngit config --global http.postBuffer 5242880000\n```\n\n### 权限问题\n\n```bash\n# 给其他用户加写权限\nchmod o+w -R .\nfind . -type f | xargs chmod o+w\n\n# 不跟踪权限变化\ngit config core.fileMode false\n```\n\n参考：[git 权限问题](https://www.cnblogs.com/zqb-all/p/10631505.html)\n\n## 原理\n\nGit 是**内容寻址文件系统**，核心是键值对数据库。\n\n### .git 目录结构\n\n| 目录/文件 | 说明 |\n|-----------|------|\n| `objects/` | 存储所有数据内容（blob、tree、commit） |\n| `refs/` | 存储分支、远程、标签的引用指针 |\n| `HEAD` | 指向当前检出的分支 |\n| `index` | 暂存区信息 |\n| `config` | 项目配置 |\n| `hooks/` | 钩子脚本 |\n\n### 三种对象类型\n\n| 类型 | 说明 |\n|------|------|\n| **blob**（数据对象） | 文件内容 |\n| **tree**（树对象） | 目录结构，包含 blob 和子 tree 的引用 |\n| **commit**（提交对象） | 指向 tree，包含作者、时间、父提交信息 |\n\n### 底层命令\n\n```bash\n# 存储内容并返回 SHA-1\necho 'test' | git hash-object -w --stdin\n\n# 查看对象内容\ngit cat-file -p <sha1>\n\n# 查看对象类型\ngit cat-file -t <sha1>\n\n# 查看树对象\ngit cat-file -p master^{tree}\n```\n\n### 引用（refs）\n\n- **分支**：指向某个 commit 的指针，存储在 `.git/refs/heads/`\n- **HEAD**：符号引用，指向当前分支\n\n```bash\n# 查看 HEAD\ngit symbolic-ref HEAD\n\n# 更新引用\ngit update-ref refs/heads/master <sha1>\n```\n\n### 核心流程\n\n`git add` + `git commit` 实际做了：\n1. 将文件内容保存为 blob 对象\n2. 更新暂存区（index）\n3. 创建 tree 对象\n4. 创建 commit 对象（指向 tree + 父 commit）\n\n![Git 数据模型](https://git-scm.com/book/en/v2/images/data-model-4.png)\n\n## 参考链接\n\n- [Learn Git Branching（交互式学习）](https://learngitbranching.js.org/?locale=zh_CN)\n- [Git 可视化](https://zhuanlan.zhihu.com/p/190303151)\n- [How Does Git Work](https://github.com/ByteByteGoHq/system-design-101#how-does-git-work)\n- [A Successful Git Branching Model](https://nvie.com/posts/a-successful-git-branching-model/)\n- [Gitflow vs Github Flow vs Gitlab Flow](http://www.ruanyifeng.com/blog/2015/12/git-workflow.html)\n","tags":["git"],"categories":["Tools","git"]},{"title":"How to use org mode","url":"/2020/09/05/How-to-use-org-mode/","content":"\n## Shortkeys\n\n1. next/previous heading `C-c C-n/p`\n1. next/previous heading, same level `C-c C-f/b`\n1. `org-agenda-list` show agenda list\n1. `org-open-at-point` `C-c C-o` open links\n\n## capture -> refile -> archiving\n\n1. C-c c capture a new item (C-u C-u = goto last)\n1. C-c C-w refile subtree (C-u C-u = goto last)\n1. C-c C-x C-a archive subtree using the default command\n1. C-c C-x C-s move subtree to archive file\n1. C-c C-x a/A toggle ARCHIVE tag / to ARCHIVE sibling\n\n## code\n\n1. `<src` to insert src block\n2. `(org-babel-do-load-languages 'org-babel-load-languages '((shell . t)))` enable sh\n3. `(org-babel-do-load-languages 'org-babel-load-languages '((C . t)))` enable c++\n\n```c++\n#+begin_src cpp\n    #include<iostream>\n    int main(){\n        std::cout << \"hello world!\" << std::endl;\n        return 0;\n    }\n#+end_src\n\n#+RESULTS:\n: hello world!\n```\n\n## Table\n\n1. `|------+-----------|` 是 row 0\n1. `org-table-export` test.csv 导出 csv\n1. 公式\n   - `:=` 只对当前单元格起作用\n   - `=` 对一行或一列起作用\n   - `@` 指定行\n   - `$` 指定列\n   - `C-c '` 编辑公式\n   - `C-u C-c *` 整个表格求公式\n   - show line, column, formula reference `C-c ?`\n   - toggle table coordinate grid `C-c }`\n1. `C-c -` insert hline below (C-u : above) current row\n1. `C-c RET` insert hline and move to line below it\n1. convert region to table `C-c |`\n1. move the current column left `M-LEFT/RIGHT`\n1. move the current row up/down `M-UP/DOWN`\n1. sum numbers in current column/rectangle `C-c +`\n1. re-align the table without moving the cursor `C-c C-c`\n1. re-align the table, move to next field `TAB`\n1. move to previous field `S-TAB`\n1. re-align the table, move to next row `RET`\n\n## Tags\n\n1. set tags for current heading `C-c C-q`\n2. create sparse tree with matching tags `C-c \\`\n3. `#+FILETAGS: :work` all the item in file has work tag\n4. globally (agenda) match tags at cursor `C-c C-o`\n\n## Priority\n\n1. raise/lower priority of current item `S-UP/DOWN`\n1. select next/previous state `S-LEFT/RIGHT`\n1. `C-c , [ABC]` set the priority of the current item\n1. `C-c , SPC` remove priority cookie from current item\n\n## [checkbox](https://orgmode.org/manual/Checkboxes.html)\n\n1. `- [ ]`\n1. `C-c C-c` org-toggle-checkbox\n1. .....[/] show [1/5]\n1. .....[%] show [20%]\n\n## agenda\n\n1. `calfw-org-mode` M-c\n1. `tab` open on other window\n1. start/stop/cancel the `clock` on current item `I / O / X`\n1. view mode dispatcher `v`\n1. add/move current file to front of agenda `C-c [`\n1. remove current file from your agenda `C-c ]`\n1. cycle through agenda file list `C-’`\n1. display next/previous day,week,... `f / b`\n1. goto today / some date (prompt) `. / j`\n1. `d` org-agenda-day-view\n1. `w` org-agenda-week-view\n1. toggle entry text / clock report `E / R`\n\n## clock\n\n1. start/stop/cancel the `clock` on current item `I / O / X`\n1. start clock on current item `C-c C-x C-i`\n1. stop/cancel clock on current item `C-c C-x C-o/x`\n\n### [Calfw-org-mode](https://github.com/kiwanami/emacs-calfw)\n\n## Timestamp\n\n1. prompt for date and insert timestamp `C-c .`\n1. insert DEADLINE timestamp `C-c C-d`\n1. insert SCHEDULED timestamp `C-c C-s`. `.` goto today\n1. stampe: `+1 20:00+1` +1day and 20:00-21:00\n1. change timestamp at cursor ±1 day `S-RIGHT/LEFT`\n1. change year/month/day at cursor by ±1 `S-UP/DOWN`\n\n# [startup](https://orgmode.org/manual/Initial-visibility.html)\n\n```\n#+STARTUP: overview\n#+STARTUP: content\n#+STARTUP: showall\n#+STARTUP: show2levels\n#+STARTUP: show3levels\n#+STARTUP: show4levels\n#+STARTUP: show5levels\n#+STARTUP: showeverything\n```\n\n# Configs\n\n```\n(global-set-key (kbd \"<f9>\") 'org-clock-jump-to-current-clock)\n(global-set-key (kbd \"<f12>\") 'org-agenda-list)\n(global-set-key (kbd \"C-c l\") 'org-store-link)\n(global-set-key (kbd \"C-c a\") 'org-agenda)\n(global-set-key (kbd \"C-c c\") 'org-capture)\n\n(setq org-clock-out-remove-zero-time-clocks t)\n(setq org-startup-folded 'content\n      org-agenda-span 'day\n      org-agenda-window-setup 'current-window\n      org-agenda-start-on-weekday nil)\n(setq org-agenda-inhibit-startup t) ;; ~50x speedup\n;;(setq org-agenda-use-tag-inheritance nil) ;; 3-4x speedup\n(setq org-tags-column 80)\n(set-time-zone-rule \"Asia/Shanghai\") ;; show china time zone\n(add-hook 'org-agenda-mode-hook\n            (lambda () (add-hook 'window-configuration-change-hook\n                                 'org-agenda-align-tags nil t)))\n\n(setq org-directory \"~/org\")\n(setq org-default-notes-file (concat org-directory \"/notes.org\"))\n(setq org-agenda-files '(\"~/org/gtd.org\"\n\t\t\t \"~/org/daily.org\"\n\t\t\t \"~/org/weekly.org\"\n\t\t\t \"~/org/ideas.org\"))\n\n\n(setq org-highest-priority ?A)\n(setq org-lowest-priority  ?D)\n(setq org-default-priority ?D)\n(setq org-priority-faces\n      '((?A . (:background \"red\" :foreground \"white\" :weight bold))\n        (?B . (:background \"DarkOrange\" :foreground \"white\" :weight bold))\n        (?C . (:background \"yellow\" :foreground \"DarkGreen\" :weight bold))\n        (?D . (:background \"DodgerBlue\" :foreground \"black\" :weight bold))\n        ))\n(setq org-todo-keywords\n      '((sequence \"TODO(t)\" \"|\" \"DONE(d)\" \"CANCELED(c@/!)\")))\n        ;; (sequence \"|\" \"CANCELED(c)\")))\n(setq org-todo-keyword-faces '(\n                               ;; (\"TODO\" . org-warning)\n                               ;; (\"DONE\" . \"green\")\n                               (\"CANCELED\" . \"grey\")\n                               ))\n(setq org-agenda-prefix-format\n      (quote\n       ((agenda . \" %-10:c%?-12t% s\"))))\n(defun my:open-org-calendar ()\n  \"Open an org schedule calendar in the new buffer.\"\n  (interactive)\n  (save-excursion\n    (let* ((source1 (cfw:org-create-source))\n           (curr-keymap (if cfw:org-overwrite-default-keybinding cfw:org-custom-map cfw:org-schedule-map))\n           (cp (cfw:create-calendar-component-buffer\n                :view 'week\n                :contents-sources (list source1)\n                :custom-map curr-keymap\n                :sorter 'cfw:org-schedule-sorter)))\n      (switch-to-buffer (cfw:cp-get-buffer cp))\n      (when (not org-todo-keywords-for-agenda)\n        (message \"Warn : open org-agenda buffer first.\")))))\n\n(defun calfw-org-mode()\n  \"Open calfw org\"\n  (interactive)\n  (load-file \"~/.emacs.d/straight/build/calfw-org/calfw-org.el\")\n  (my:open-org-calendar))\n\n\n(defun get-pic-from-url (url filename)\n  (shell-command (concat \"wget -O \" filename \" \" url)))\n(defun org-pomodoro-get-pictures ()\n  \"Get pomodoro notification pictures\"\n  (interactive)\n  ;; (get-pic-from-url \"https://pic3.zhimg.com/e76fc9f4120b9bd6ca2a7ae03a447f19_1440w.jpg?source=172ae18b\" \"~/begin_short_break.jpg\")\n  (get-pic-from-url\n  \"https://previews.123rf.com/images/vaselena/vaselena1507/vaselena150700001/42287443-white-alarm-clock-with-red-clock-face-with-inscription-coffee-time-without-figures-on-white-backgrou.jpg\"\n  \"~/coffe_time.jpg\")\n  (get-pic-from-url\n  \"https://previews.123rf.com/images/vaselena/vaselena1510/vaselena151000045/47728782-white-alarm-clock-with-red-clock-face-with-inscription-work-time-without-figures-on-white-background.jpg\"\n  \"~/work_time.jpg\"))\n\n(add-hook 'org-pomodoro-finished-hook\n          (lambda ()\n            (w32-shell-execute \"open\" \"~/coffe_time.jpg\")))\n(add-hook 'org-pomodoro-break-finished-hook\n          (lambda ()\n            (w32-shell-execute \"open\" \"~/work_time.jpg\")))\n(add-hook 'org-pomodoro-long-break-finished-hook\n          (lambda ()\n            (w32-shell-execute \"open\" \"~/work_time.jpg\")))\n\n```\n\n## pdf\n\n1. [link](https://gist.github.com/melvincabatuan/350f86611bc012a5c1c6)\n2. `sudo yum -y --skip-broken install texlive-\\*`\n3. install texlive-full [link](https://baohaojun.github.io/blog/2016/12/13/0-How-to-export-Chinese-with-Org-mode-Latex-backend.html)\n\n## Links\n\n1. [cheatsheat](https://www.gnu.org/software/emacs/refcards/pdf/orgcard.pdf)\n1. [Org Mode](https://orgmode.org/)\n1. [org card](https://orgmode.org/worg/orgcard.html)\n1. [Org Guide](https://orgmode.org/orgguide.pdf)\n1. [Org Mode Chinese Tutorial](https://www.cnblogs.com/Open_Source/archive/2011/07/17/2108747.html)\n","tags":["Emacs"],"categories":["Emacs"]},{"title":"Bjarny Stroustrup cpp","url":"/2020/08/18/Bjarny-Stroustrup-Cpp/","content":"\n## What is C++20?\n\n1. The best approximation of C++'s ideals so far.\n1. As big an improvement over C++11 as C++11 was over C++98: A major \"release\"\n1. Lots of useful features\n   - Simpler, more expressive, faster code that compiles faster\n   - Modeles\n   - Concepts\n   - Coroutines\n   - Ranges\n   - Dates\n   - Span\n   - Better compile-time programing support\n   - Many \"minor features\"\n1. C++23 and C++26 is better than C++20, not the major \"release\".\n1. Diretly learn C++20. It's the outside layer of the onion.\n1. C++23:\n   - \"Completes C++20\"\n   - Plus: standard modules, library support for coroutines, executors & networking\n   - Maybe: static reflection, pattern matching\n\n## Keys C++ \"Rules of Thumb\"\n\n1. `A static type system` with equal support for build-in and user-defined types\n   - A type: specifies the set of operations that can be applied to an object and specifies how an object is laid out in memory\n   - A static type system: the base of all\n   - static mean compile time will determine all the type(not the run-time).\n   - compile-time error detection\n   - performance\n   - flexibility through compile-time resolution(overloading, generic programming, metaprogramming...)\n1. `Value and reference semantics`\n   - value types: Inegers, characters, strings, containers,...\n   - Pointers/references: T\\*, T&, unique_ptr<T>, Forward_iterator\n1. `Direct use` of machine and operating system resources\n   - bitset and span(modern c++)\n   - the onion principle: the more layers you peel off, the more control, the more error.\n1. Systematic and general `resource management`(RAII)\n   - manage heap, file...\n   - every resource must have an owner: responsible for its cleanup.\n   - control the complete object life cycle: creation, copy, move, destruction\n1. Support `composition` of software from separately developed parts.\n   - modules\n   - all major features support composition: moduls, classes, concepts, templates, functions, aliases\n1. Support for `object-oriented programming`\n1. Support for `generic programming`\n   - concepts\n1. Support `compile-time programming`\n   - Move computation from run-time to compile-time(performace, do once)\n   - It's everywhere: overloading and virtual functions, templates, variadic templates, constexp functions...\n1. `Concurency` through libraries supported by intrinsics\n1. `Libraries`\n   - A user shouldn't have to care whether a feature is implemented in the language or in a library\n\n## Philosophy (CppCoreGuidelines)\n\n1. Express ideas directly in code\n1. Write in ISO Stardard C++\n1. Express intent\n1. Ideally, a program should be statically type safe.\n1. Prefer compile-time checking to run-time checking.\n1. What cannot be checked at compile time should be checkable at run time\n1. Catch run-time errors early.\n1. Don't leak any resources.\n1. Don't waste time or space.\n1. Prefer immutable data to mutable data\n1. Encapsulate messy constructs, rather than spreading through the code.\n1. Use supporting tools as appropriate\n1. Use support libraries as appropriate\n\n## Lower levels of abstaction\n\n1. Samples:\n   - Sizes\n   - Raw pointers\n   - Allocation and deallocation\n   - Loop-control variables\n   - Casts\n   - Macros\n1. Except as implementation details and asides\n1. Don't go to lower level if you have to.\n\n## What really matters?\n\n1. People\n1. A programming language is a tool, not an end goal: peopeo want great systems, not programming languages.\n1. Software developers want great tools: not just programming language features.\n\n## Abstraction\n\n1. Often, the software is more complicated than the hardware.\n1. abstraction in code:\n   - resource mangement: vector\n   - generic programing: concepts: compile-time predicates.\n\n## The onion principle\n\n1. Mangagement of complexity: make simple thing simple.\n1. Layers of abstraction: the more layers you peel off, the more you cry.\n\n## An engineering approach\n\n1. Design c++ is not pure math.\n1. Principled and pragmatic design\n1. Progress gradually guided by feedback\n1. There are always many tradeoffs: choosing is hard\n1. Design decisions have consequences\n\n## C++ in two lines\n\n1. Direct map to hardware\n   - of instructions and fundamental data types\n   - initially from C\n   - Future: use novel hardware better(caches, multicores, GPUs, FPGAs, SIMD, ...)\n1. Zero-overhead abstraction\n   - classes, inheritance, generic programming, ...\n   - initially from Simula(where it wasn't zero-overhead)\n   - Future: Type- and resouce-safety, concepts, modules, concurrency, ...\n\n## C++'s role\n\n1. A language for\n   - writing elegant and efficiaent programs\n   - for definning and using light-weight abstractions\n   - a language for resource-constrained applications\n   - building software infrastructure\n1. Offers\n   - a direct map to hardware\n   - Zero-overhead abstraction\n1. No language is perfect\n   - for everthing\n   - for everyone\n\n## C++ is tunable\n\n1. Make simple things simple\n   - Don't make complicated tasks impossible\n   - Don't make complicated tasks unreasonable hard to do\n   - The onion principle\n1. Don't drop to lower levels of abstraction\n   - Unless you really, really need to\n   - Hide messy code behind clean interfaces\n1. Alwasy measure\n\n## Write better code\n\n1. Cleaner\n2. Simpler\n3. More readable\n4. More maintainable\n5. Faster\n6. Less clever\n7. More general\n8. More usable and re-usable\n9. Type safe\n","tags":["Cpp"],"categories":["Program","Cpp"]},{"title":"Effective modern cpp","url":"/2020/08/14/Effective-modern-cpp/","content":"\n\n## 绪论\n1. `右值`对应的是函数返回的临时对象，`左值`是可指涉的对象，可通过名字，指针等来标识。\n1. 判断左值右值方法： 判断是否能取得地址。如果能取得地址则是左值，不能则为右值。例如`Widget(Widget&& rhs);`其中rhs为左值，因为函数体内对rhs取地址没有问题。相当于rhs被右值初始化，接受了右值。\n1. 函数中，`实参`是用来初始化`形参`的。\n1. 形参都是左值，而作为其初始化依据的实参，既能是右值，也可能是左值。例：`Widget(Widget& rhs);`rhs是左值，传入的实参也是左值。`Widget(Widget&& rhs);`rhs是左值，传入的实参是右值。\n1. `声明`的作用是引入名字和类型，而不给出细节。如存储位置或具体实现。`定义`则会给存储位置和具体实现的细节。定义同时也可以当声明用。\n1. `函数的签名`：只包含形参类别和返回值类别，不包含函数名字和形参名字。例如：`bool(const Widget&)`\n\n## 第一章 类型推到\n1. C++98仅有一套类型推导规则，用于函数模板。C++11对其进行改进，并增加了两套，一套用于auto，一套用于decltype。C++14又扩展了使用语义。\n\n### 条款1： 理解模板类别推导\n1.\n\n\n## 第三章 转向现代C++\n### 在创建对象时注意区分()和{}\n1. 对象初始化的方式包括使用小括号，使用等号，或是使用大括号：`int x(0); int y = 0; int z{0};` 等号加大括号等效于大括号`int z = {0};`\n1. 大括号初始化又称为`统一初始化`。\n1. 大括号可以用来对容器进行初始化，例如: `vector<int> v{1, 2, 3, 4};`\n1. 类的成员变量在类的定义时可以用`{}`或者`=`进行初始化，不能使用`()`\n1. 不可复制对象(例如：std::atomic)可以使用`{}`或者`()`进行初始化，不能使用`=`\n1. 大括号初始化`禁止内建类型进行隐式窄化类型转化`。如：`double x, y; ... ; int sum{x+y};`编译出错，double无法用int表示。\n1. C++规定，任何能够解析为声明的都要解析为声明。大括号初始化免疫。`Widget w1();`此处被解析为函数声明，而不是初始化w1调用构造函数。`Widget w2{};`调用无形参的构造函数来构造w2。\n1. 大括号初始化时如果形参出现std::initializer_list会优先调用，此时可以小括号。\n1. 模板内容进行创建时，应该使用小括号还是大括号是一个棘手问题。\n","tags":["Cpp"],"categories":["Program","Cpp"]},{"title":"Network","url":"/2020/08/11/Network/","tags":["Network"],"categories":["Computer","Network"]},{"title":"OS","url":"/2020/08/11/OS/","content":"\n## page faults\n\n1.\n\n## C++ 变量内存分配区域\n\n1. 栈区（stack） 由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。\n2. 堆区（heap）就是那些由 malloc 等分配的内存块，他和堆是十分相似的，不过它是用 free 来结束自己的生命的。一般由程序员分配释放， 若程序员不释放，程序结束时可能由 OS 回收 。注意它与数据结构中的堆是两回事，分配方式倒是类似于链。堆可以动态地扩展和收缩。\n3. 自由存储区(也在堆区？) 就是那些由 new 分配的内存块，一般一个 new 就要对应一个 delete。\n4. 全局区/静态区（static）全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域 data 段， 未初始化的全局变量和未初始化的静态变量在相邻的另一块区域 bss 段。程序结束后有系统释放\n5. 常量存储区 存放的是常量，不允许修改。常量字符串就是放在这里的。常量字符串不能修改, 否则程序会在运行期崩溃（当然，你要通过非正当手段也可以修改，而且方法很多）.程序结束后由系统释放\n\n### [C++ 自由存储区和堆区](https://blog.csdn.net/Q_1849805767/article/details/107376682)\n\n1. 堆是操作系统维护的一块内存，而自由存储是 C++中通过 new 与 delete 动态分配和释放对象的抽象概念。堆与自由存储区并不等价\n1. new / new[]：完成两件事，先底层调用 malloc 分配了内存，然后调用构造函数（创建对象）。\n1. delete/delete[]：也完成两件事，先调用析构函数（清理资源），然后底层调用 free 释放空间。\n1. new 在申请内存时会自动计算所需字节数，而 malloc 则需我们自己输入申请内存空间的字节数。\n\n## linux 虚拟内存空间分区\n\n1. `cat /proc/pid/maps 或 pmap pid` 看进程映射区域\n1. 64 位系统虚拟内存 ![linux 64 memory](https://fanlv.wiki/2021/07/25/linux-mem/Linux-Memory-X86-64.jpg)\n1. ELF 在装载时按照 Segment 划分。一个 Segment 包含一个或多个属性类型的 Section。实际上 Segment 的概念是从装载的角度重新定划分了 ELF 的各个段。[link](https://maodanp.github.io/2019/06/02/linux-virtual-space/)， 映射的时候，这里面的 segment 会对应一个 VMA。Linux 将进程虚拟地址空间中的一个段叫做虚拟内存区域（VMA）。![示意图](https://maodanp.github.io/pic/2019/2019-06-02-linux-virtual-space-02.png)\n1. ELF 可执行文件中有两个概念，分别是段(segment)和节(section)。通过 `readelf -S name.elf` 可以查看 ELF 可执行文件的节头表，这里面有所有节的信息。使用命令 `readelf -l name.elf` 来查看 ELF 的段。而系统正是按照“segment”而不是“section”来映射可执行文件的。\n1. readelf -a test 先显示 section 信息， 然后显示 segment(Program Headers)信息，接着显示 Section to Segment mapping 信息\n1. 从装载的角度看，我们只关心\"LOAD\"型的 segment，因为只有它是需要被映射的，其他诸如“NOTE”,\"GNU_STACK\"都是在装载时起辅助作用的。[link](https://blog.csdn.net/weixin_44395686/article/details/104761488)\n1. 总体而言，Segment 和 Section 是从不同角度来划分同一个 ELF 文件。从 Section 角度来看 ELF 文件就是链接视图（Linking View），从 Segment 角度来看 ELF 文件则是执行视图（Execution View）。\n\n### 进程虚拟存储器\n\n1. 只读区(.init, .text, .rodata)\n2. 已初始化的数据(.data 段)\n3. 未初始化的数据(.bss 段)\n4. 堆\n5. 映射区(mmap, 共享库等)\n6. 栈\n\n### 内核虚拟存储器\n\n1. 内核代码和数据\n2. 物理存储器\n3. 与进程相关的数据结构(每个进程不同)\n\n## ELF\n\n1. [变量根据情况分配在.bss 段上](https://stackoverflow.com/a/16557699)\n   - int bigvar_in_bss[16300];\n   - int bigvar_in_bss[16300] = {0};\n   - int bigvar_in_data[16300] = {1};\n   - int var_in_data[5] = {1, 2, 3, 4, 5};\n1. `file -A` 查看各 section 大小\n1. `nm test` 查看变量是否在 bss 段上\n1. `readelf -s test 或 nm -S test` 查看变量大小, 在其他模块定义的符号 size 为 0\n1. `nm -S --size-sort test` 按 size 排序\n1. 初始化为 0 的在 bss 段上？[link](https://stackoverflow.com/a/16557776)\n\n## others\n\n### [同步异步，阻塞非阻塞区别](https://www.cnblogs.com/mhq-martin/p/9035640.html)\n\n1. `不同的的概念，同步与异步区别是什么时候拿到最终结果，阻塞和非阻塞区别进程线程是是否等待结果， 同步可以是阻塞的， 也可以是非阻塞的(一次非阻塞拿不全结果， 轮询try_lock或read noblock)，异步可以是阻塞的，也可以是非阻塞的，阻塞是同步的，但非阻塞不一定异步的. 比如非阻塞系统调用read，返回的是字节数，不是最终结果。`\n1. 组合：\n\n   - 同步阻塞：同步接口里使用阻塞方式拿到最终结果\n   - 同步非阻塞：同步接口里通过非阻塞方式轮询拿到最终结果\n   - 异步阻塞：异步接口里通过阻塞方式拿到最终结果\n   - 异步非阻塞：异步接口里通过非阻塞方式拿到最终结果\n\n1. 同步与异步是更高层次上的概念\n1. 同步与异步区别是什么时候拿到最终结果。如果是在调用函数就拿到最终结果，则是同步的，\n1. 同步异步区别是在调用一个功能时，没得到`结果`之前，是否继续后续操作（有问题，非阻塞也是没拿到结果，也往后继续进行， 但是非阻塞不一定是异步的）\n1. 阻塞非阻塞区别是`线程/程序`在等待调用结果时，是否一直等待（挂起当前线程）\n1. 阻塞和非阻塞关注的是程序在`等待调用结果`（消息，返回值）时的`状态`.\n1. 进程线程锁是用来同步的，lock()函数是阻塞式，不拿到锁不返回，线程被阻塞。try_lock()是非阻塞式的，如果拿不到锁也返回。\n1. 异步是目的，而多线程是实现这个目的的方法\n1. [非阻塞 I/O 和异步 I/O](https://www.zhihu.com/question/19732473/answer/241673170)\n   非阻塞 I/O 系统调用( nonblocking system call )的另一个替代品是 异步 I/O 系统调用 （asychronous system call）。 与非阻塞 I/O 系统调用类似，asychronous system call 也是会立即返回， 不会等待 I/O 操作的完成， 应用程序可以继续执行其他的操作， 等到 I/O 操作完成了以后，操作系统会通知调用进程（设置一个用户空间特殊的变量值 或者 触发一个 signal 或者 产生一个软中断 或者 调用应用程序的回调函数）。\n   此处， 非阻塞 I/O 系统调用( nonblocking system call ) 和 异步 I/O 系统调用 （asychronous system call）的区别是：\n   - 一个非阻塞 I/O 系统调用 read() 操作立即返回的是任何可以立即拿到的数据， 可以是完整的结果， 也可以是不完整的结果， 还可以是一个空值。\n   - 而异步 I/O 系统调用 read（）结果必须是完整的， 但是这个操作完成的通知可以延迟到将来的一个时间点。\n     上面提到的 非阻塞 I/O 系统调用( nonblocking system call ) 和 异步 I/O 系统调用 都是非阻塞式的行为（non-blocking behavior）。 他们的差异仅仅是返回结果的方式和内容不同。\n1. 我们所说的 “阻塞”是指进程在发起了一个系统调用（System Call） 后， 由于该系统调用的操作不能立即完成，需要等待一段时间，于是内核将进程挂起为等待 （waiting）状态， 以确保它不会被调度执行， 占用 CPU 资源。\n\n1. 在进程通信层面， 阻塞/非阻塞， 同步/异步基本是同义词， 但是需要注意区分讨论的对象是发送方还是接收方。发送方阻塞/非阻塞（同步/异步）和接收方的阻塞/非阻塞（同步/异步） 是互不影响的。\n1. 在 IO 系统调用层面（ IO system call ）层面， 非阻塞 IO 系统调用 和 异步 IO 系统调用存在着一定的差别， 它们都不会阻塞进程， 但是返回结果的方式和内容有所差别， 但是都属于非阻塞系统调用（ non-blocing system call ）\n\n1. 举例：从磁盘读 50000 字节到内存\n\n- 同步：调用 api，api 内不拿到结果或出错不返回。\n- 异步：调用 api, 直接返回，结果通过 callback 等方式通知。结果是完整的。\n- 阻塞：线程系统调用 read(), 不读 50000 字节不返回，线程挂起，内核态在执行。\n- 非阻塞：线程系统调用 read(NO_BLOCK), 达到某个条件，如出现'\\n'直接返回，结果不完整，线程要多次调用来得到结果。\n\n### [Linux 异步 I/O 框架 io_uring：基本原理、程序示例与性能压测](http://arthurchiao.art/blog/intro-to-io-uring-zh/)\n\n## CFS\n\n1. [Linux CFS 调度器：原理、设计与内核实现](http://arthurchiao.art/blog/linux-cfs-design-and-implementation-zh/)\n\n## linux src\n\n1. [apt-get source linux-source](https://askubuntu.com/a/159842)\n","tags":["OS"],"categories":["Computer","OS"]},{"title":"Cargo","url":"/2020/08/05/Cargo/","content":"\n## Build Scripts\n1. 有些库依赖第三方语言的库，例如需要C库，并且需要从源码编译，cargo目标并不是取代那些优化很好的第三方编译工具，所以cargo通过build脚本来集成其他工具。\n1. 在crate包的根路径下创建一个名为`build.sh`的脚本，Cargo会在编译crate包之前先编译`build.sh`脚本然后执行该脚本。\n1. 如果`build.sh`依赖的文件有任何改变，会被重新编译。\n1. build脚本的输入是通过环境变量传递的。\n1. 环境变量中的当前路径就是`build.sh`源码所在路径。\n1. build脚本会将输出放入到`OUT_DIR`中的各个路径中。所以build脚本不应该改变这些输出路径中的文件。\n1. Cargo会解析那些以`cargo:`开头的行，其他行被忽略。\n1. build脚本的输出默认被隐藏，可以使用`-vv`(very verbose)来查看输出。如果依赖的文件没有被改变，什么都不会输出，因为不会重新执行编译脚本。\n1. build脚本的编译输出都会存在类似于`target/debug/build/<pkg>/output`文件中。\n1. ...\n1. `build.sh`中依赖的crates需要在Cargo.toml中的`[build-dependencies]`中指定依赖，`build.sh`可用的依赖见[crates.io build-dependencies crates](https://crates.io/keywords/build-dependencies)\n","tags":["Rust","Cargo"],"categories":["Program","Rust"]},{"title":"Software design","url":"/2020/08/03/Software-design/","content":"\n## [roadmap](https://github.com/stemmlerjs/software-design-and-architecture-roadmap)\n\n1. [system-design-101](https://github.com/ByteByteGoHq/system-design-101)\n1. [project-based-learning like build-your-own-x](https://github.com/practical-tutorials/project-based-learning#cc)\n1. [developer-roadmap](https://github.com/kamranahmedse/developer-roadmap)\n1. [build-your-own-x](https://github.com/codecrafters-io/build-your-own-x) 实现 os, git, c++编译器等 sample\n1. [system-design-primer(200K star)](https://github.com/donnemartin/system-design-primer)\n1. [awesome-design-patterns](https://github.com/DovAmir/awesome-design-patterns)\n<!-- more -->\n\n1. Summary\n   ![Summary](https://user-images.githubusercontent.com/6892666/65833569-bb34fc00-e29f-11e9-8516-79cbd9f8f07b.png)\n1. [programming-principles](https://github.com/webpro/programming-principles)\n1. Map\n   ![map](https://user-images.githubusercontent.com/6892666/65896069-834eb700-e37a-11e9-95be-7ae2300d5d50.png)\n1. [分析模板](https://github.com/joelparkerhenderson/architecture-decision-record)\n1. [github](https://github.com/search?q=software+architecture)\n1. [SoftwareArchitect](https://github.com/justinamiller/SoftwareArchitect)\n   ![roadmap2](https://github.com/justinamiller/SoftwareArchitect/raw/master/src/archRoadmap.jpg)\n\n## good design\n\n1. `高内聚，低耦合`\n1. (代理模式？) opencv 接口使用代理类 _InputArray，This is the proxy class for passing read-only input arrays into OpenCV functions. \\_InputArray is a class that can be constructed from Mat, Mat_<T>, Matx<T, m, n>, std::vector<T>, std::vector<std::vector<T> >, std::vector<Mat>, std::vector<Mat\\_<T> >, UMat, std::vector<UMat> or double. It can also be constructed from a matrix expression.\n   [link](https://docs.opencv.org/4.6.0/d4/d32/classcv_1_1__InputArray.html)\n\n## 架构风格、架构模式、设计模式 [link](https://cloud.tencent.com/developer/article/1627409)\n\n1. 架构风格是在最高抽象层次上的应用程序设计；\n1. 架构模式是实现架构风格的一种方式；\n1. 设计模式是解决局部问题的一种方法。\n1. [10 种常见的软件架构风格？](https://www.cnblogs.com/IcanFixIt/p/7518146.html)\n\n## Architecture Style\n\n### 分层 Layered Architecture\n\n### 管道和过滤器\n\n### Client/Server\n\n### 事件驱动 Message Bus\n\n1. ![Architectural diagrams for the various IoT protocols.](https://marvel-b1-cdn.bc0a.com/f00000000264121/www.rti.com/hs-fs/hubfs/Assets%202017/communicationsevolution.jpg?width=720&height=405&name=communicationsevolution.jpg)\n\n### 发布订阅风格\n\n- 事件总线是对发布-订阅模式的一种实现。 [事件总线（发布订阅模式）](https://juejin.cn/post/6949929918138417165)\n\n### p2p\n\n1. RPC? 为了允许不同的客户端均能访问服务器，许多标准化的 RPC 系统应运而生了。其中大部分采用接口描述语言（Interface Description Language，IDL），方便跨平台的远程过程调用。\n\n### Service-Oriented Architecture (SOA)\n\n## Architecture Pattern\n\n## [programming-principles](https://github.com/webpro/programming-principles)\n\n1. `Don't Make Me Think`\n1. `开闭原则` 软件中的对象（类，模块，函数等等）应该对于扩展是开放的，但是对于修改是封闭的\n1. `单一职责原则` 一个类只做一件事\n1. `里氏替换原则` 子类应该可以完全替代父类，也就是说在使用继承时，只扩展新功能，不要破坏父类原有的功能。\n1. `依赖倒置原则` 细节应该依赖与抽象，抽象不应该依赖于细节。把抽象层放在程序设计的高层，并保持稳定，程序的细节变化由低层的实现层来完成。\n1. `迪米特法则/最少知道原则` 一个类不应该知道自己操作类的细节，换言之，只和朋友谈话，不和朋友的朋友谈话。\n1. `接口隔离原则` 客户端不应该依赖它不需要的接口。如果一个接口在实现时，部分方法由于冗余被客户端空实现，则应该将该接口拆分，让实现类只依赖自己需要的接口。\n\n1. 稳定抽象原则\n1. [vitis ai library 为例](https://excalidraw.com/#json=5093806722514944,Fbhi7FIo_fywgxy1glxm3w)\n1. `控制翻转`\n1. `依赖注入`\n\n## Design Pattern\n\n### [工厂模式](https://blog.csdn.net/qq_52670477/article/details/126629335)\n\n1. [C++ 基于模板的自动注册工厂模式](https://zhuanlan.zhihu.com/p/268462046)\n1. [示意图](https://excalidraw.com/#json=XTOYCW9APeNtW9s_GEcAP,fKiAKGMSFLUlct6H4HAQ6Q)\n1. 简单工厂模式： 不符合开闭原则\n1. 工厂方法模式\n1. 抽象工厂模式\n1. 从代码的实现复杂度来看，简单工厂模式最简单，工厂方法模式次之，抽象工厂模式最复杂。如果将简单工厂模式的代码修改得符合“开闭原则”，就变成了工厂方法模式；如果修改工厂方法模式的代码，使得一个工厂支持多个产品的生产，那就成了抽象工厂模式。\n1. 原来由用户直接对对象进行管理，变成由工厂(ioc 容器)对对象进行管理，然后注入到应用程序中。实现了应用和用户与对象具体实现的松耦合。\n1. [稳定抽象原则](https://excalidraw.com/#json=5093806722514944,Fbhi7FIo_fywgxy1glxm3w): 依赖关系指向更稳定的方向，工厂接口比较稳定，应用和对象具体实现都依赖工厂接口。\n1. 目的：将对象的创建与对象的使用解耦。\n1. `简单工厂函数` 将对象的创建放入到统一工厂函数中，根据类型判断具体创建哪一种类型对象。相当于将耦合问题从使用中转移到工厂函数。扩展性差，每增加一个产品就要修改工厂函数。\n1. `工厂方法模式` 每个产品都有一个工厂函数，相当于将耦合从总的工厂函数中转移到各个产品的工厂函数中，问题：使用时需要包含各个工厂头文件。\n1. `抽象工厂模式` 同工厂方法模式，只不过每一个具体工厂可以可以调不同接口（不是同一个接口传参数）创建不同的产品。\n1. `反射，依赖注入` 由类名来创建对象。相当于工厂方法模式+单例模式。全局有一个总的工厂，工厂里有保存产品类型及其工厂函数的 map 表(使用到函数指针)，每个产品都要有一个工厂，并且需要注册（依赖注入）到总的工厂 map 表中。解决了工厂方法模式中使用问题。map 可以使用全局变量，注册函数写成类的静态函数，就不需要专门设计一个总的工厂类。总工厂生产的产品不由总工厂控制，而是交给子工厂控制，这叫`控制反转`，将子工厂通过注册接口传入到总工厂中，这叫`依赖注入`\n1. `模板工厂模式`\n\n#### Reference\n\n1. [factory method](https://www.cnblogs.com/xiaolincoding/p/11524401.html)\n1. [reflection](https://blog.csdn.net/K346K346/article/details/51698184)\n\n### 单例模式\n\n1. 注意单例模式静态链接问题，如果链接两次，可能会有两个单例，有多份静态程序。如注册机制未加 PRIVATE 限制，导致静态库被链接两次， 可以用 gdb 调试，注册函数加断点, 看看调用堆栈。\n1. 类比： 政府是单例模式的一个很好的示例。 一个国家只有一个官方政府。 不管组成政府的每个人的身份是什么， ​ “某政府” 这一称谓总是鉴别那些掌权者的全局访问节点。\n1. 目的： 保证一个类只有一个实例，并且提供一个访问它的全局访问点，该实例被所有程序模块共享\n1. [reference](https://zhuanlan.zhihu.com/p/37469260)\n1. code\n\n```c++\nclass Singleton\n{\n private:\n\tSingleton() { };  // 私有构造函数，拷贝构造函数和赋值函数，防止创建对象。\n\t~Singleton() { };\n\tSingleton(const Singleton&);\n\tSingleton& operator=(const Singleton&);\n public:\n\tstatic Singleton& get_instance()\n    {\n\t\tstatic Singleton instance;  // 使用local static对象，只在第一次访问get_instance才创建\n\t\treturn instance;\n\t}\n};\n```\n\n### [装饰器](https://refactoring.guru/design-patterns/decorator)\n\n1. [Python-like C++ decorators](https://stackoverflow.com/questions/30679445/python-like-c-decorators)\n1. 其别名也可以称为包装器(Wrapper)\n1. 俄罗斯套娃\n1. [Python 装饰器实战：打造高效性能计时工具](https://www.cnblogs.com/wang_yb/p/18106433)\n1. [例子](https://developer.aliyun.com/article/1354526)\n   - 注意红烧肉可以换成其他菜品\n1. 性能问题：增加了函数调用，icache 和 data cache\n\n### 消费者模式\n\n1. 类比： 如果你订阅了一份杂志或报纸， 那就不需要再去报摊查询新出版的刊物了。 出版社 （即应用中的 “发布者”） 会在刊物出版后 （甚至提前） 直接将最新一期寄送至你的邮箱中。出版社负责维护订阅者列表， 了解订阅者对哪些刊物感兴趣。 当订阅者希望出版社停止寄送新一期的杂志时， 他们可随时从该列表中退出。\n\n### 订阅发布模式\n\n1. 消息中间件经常使用， 例如 ZeroMQ(libzmq)\n1. MQ(message queue)\n\n### 订阅发布模式和观察者模式\n\n1. [link1](https://segmentfault.com/a/1190000020169229)\n1. [观察者模式 vs 发布订阅模式](https://zhuanlan.zhihu.com/p/51357583)\n1. [fastdds](https://blog.csdn.net/briblue/article/details/124081170)\n\n### 生产者消费者模式\n\n### 代理模式\n\n1. 现实类比： 信用卡代理现金\n1. opencv\n1. 消息中间件\n1. ![接口图](https://www.runoob.com/wp-content/uploads/2014/08/20211025-proxy.svg)\n\n### [组合模式](https://refactoringguru.cn/design-patterns/composite)\n\n1. 画图，可以有 subgraph\n1. tree 也是一种组合？\n1. 组合模式是一种结构型设计模式， 你可以使用它将对象组合成树状结构， 并且能像使用独立对象一样使用它们。\n1. 类成员变量中有该类数组\n\n## Software Design\n\n1. ![MVC、RPC、SOA和微服务架构区别](/images/software-diagram/architechture.png)\n\n## Reflection\n\n1. 反射字面意思：由类型名映射到类型对象？\n1. 反射技术以其明确分离描述系统自身结构、行为的信息与系统所处理的信息，建立可动态操纵的因果关联以动态调整系统行为的良好特征，已经从理论和技术研究走向实用化，使得动态获取和调整系统行为具备了坚实的基础。当需要编写扩展性较强的代码、处理在程序设计时并不确定的对象时，反射机制会展示其威力，反射技术以其明确分离描述系统自身结构、行为的信息与系统所处理的信息，建立可动态操纵的因果关联以动态调整系统行为的良好特征，已经从理论和技术研究走向实用化，使得动态获取和调整系统行为具备了坚实的基础。当需要编写扩展性较强的代码、处理在程序设计时并不确定的对象时，反射机制会展示其威力，这样的场合主要有：\n   - 序列化（Serialization）和数据绑定（Data Binding）\n   - 远程方法调用（Remote Method Invocation RMI）\n   - 对象/关系数据映射（O/R Mapping）。\n1. 由于 c++中的类结构可读性差，难以调试，协议升级困难等缺点，导致 xml 与 json 等自注释文本协议普及。但`文本协议`（文件里方便阅读理解）与`内存模型`（内存上）存在差异，需要有序列化与反序列化对象的操作。序列化：将类对象转化为文本，反序列化：将文本转化为类对象。开发者需要写大量重复代码去进行序列化与反序列化操作。Java 为解决这个问题添加了反射机制:将类型信息编译到 class 文件中，并利用这些信息提供统一的序列化与反序列化功能。\n1. 反射基本功能之一：如何通过类的名称来生成新的对象？例：`ClassXX object = new \"ClassXX\"`, C++使用：`ClassXX object = new ClassXX(x)` 工厂函数是通过在工厂函数里指定 tpye 来生成，不是通过类名。`ClassXX object = ClassXX::create(x)`\n1. 实现方法：\n   - 每一个类都创建一个产生对象的函数\n   - 设计一个总的工厂类，类中使用 map 保存(类名，函数)。通过共产类创建对象。因为全局只需要一个工厂类的对象，因此使用单例模式设计工厂类。\n1. 编程语言的反射机制所能实现的功能还有通过类名称字符串获取类中属性和方法，修改属性和方法的`访问权限`等，系统运行起来之后可修改类属性方法权限，厉害。\n\n### Reference\n\n1. [concept](https://zhuanlan.zhihu.com/p/70044481)\n1. [sample](https://blog.csdn.net/K346K346/article/details/51698184)\n\n## [Dependency Injection (DI)依赖注入 Inversion of Contol (IOC)控制反转](https://blog.csdn.net/weixin_43862847/article/details/122341510)\n\n1. [fruit DI 框架](https://github.com/google/fruit)\n1. [boost di](https://github.com/boost-ext/di)\n1. [c++相关依赖注入框架](https://stackoverflow.com/q/56091707)\n1. 解决依赖类之间的紧耦合问题，假设 a 依赖 b（b 的实例可能是 a 的成员变量），实现了 b 类实例的创建与 a 无关。\n1. 通常由调用者创建被调用者的实例（调用者依赖被调用者，调用者会在自己函数体内创建被调用者），当创建被调用者不再由调用者来完成，而是通过容器来创建被调用者实例，然后注入调用者。因此被成为依赖注入。\n1. a 依赖 b，但 a 不控制 b 的创建和销毁，仅使用 b，那么 b 的控制权交给 a 之外处理，这叫`控制反转`，而 a 要依赖 b，必然要使用 b 的 instance，那么通过 a 的接口把 b 传入，或通过 a 的构造把 b 传入，或通过设置 a 的属性把 b 传入，这个过程叫做`依赖注入`。\n1. 随着 DI 的频繁使用，要实现 IOC(相当于工厂模式中总的工厂)，会有很多重复代码，有人将实现 IOC 的代码打包成组件或框架，来避免人么重复造轮子。所以实现 IOC 组件或框架，我们可以叫它`IOC Container`。\n1. DI 是 IOC 实现方法之一。\n1. 构造方法注入： a 构造函数中将 b 作为参数传递。\n1. 工厂模式注入： 将 b 的工厂传递给 a。\n1. `类型擦除`指将原有类型消除或者隐藏，因为很多时候我们并不关心具体类型是什么，我们只需要去使用就可以了，提高提高代码的简洁性。是不是很耳熟，对，抽象继承的多态其实就是比较传统、常见及简单的类型擦除。我们使用的只是抽象接口，而不关心具体实现类的类型。类型擦除常用做法是`多态、模板、通用类型（C++17 std::any|std::variant）、闭包`\n1. [对 IOC 和 DI 的通俗理解](https://blog.csdn.net/fuzhongmin05/article/details/55802816) good\n1. 传统应用程序是由我们自己在对象中主动控制去直接获取依赖对象，也就是正转；而反转则是由容器来帮忙创建及注入依赖对象；为何是反转？因为由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象，所以是反转；哪些方面反转了？依赖对象的获取被反转了。\n\n## Links\n\n1. [design-patterns](https://refactoring.guru/design-patterns/)\n1. [Refactoring Guru](https://refactoring.guru/)\n1. [Picture of Design Patterns](https://design-patterns.readthedocs.io/zh_CN/latest/index.html)\n1. [Awesome Design Patterns](https://github.com/DovAmir/awesome-design-patterns)\n1. [微服务架构设计模式](https://item.jd.com/12595796.html) [book pdf](https://blog.csdn.net/flyzer0/article/details/108574150)\n1. [Microsoft Application Architecture Guide, 2nd Edition](<https://docs.microsoft.com/en-us/previous-versions/msp-n-p/ff650706(v=pandp.10)>)\n1. [系统架构——MVC、RPC、SOA 和微服务架构](https://www.cnblogs.com/shoshana-kong/p/14943755.html)\n1. [MVC、 RPC、SOA 和 微服务架构异同](https://www.jianshu.com/p/48839b7d0050)\n1. [程序设计语言分类](https://zh.wikipedia.org/wiki/Template:%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%AF%AD%E8%A8%80)\n1. [计算机科学](https://zh.wikipedia.org/wiki/Template:%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6)\n1. [10 architectures](https://www.cnblogs.com/IcanFixIt/p/7518146.html)\n1. [CS-Notes](https://github.com/CyC2018/CS-Notes/blob/master/notes/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%20-%20%E7%9B%AE%E5%BD%95.md)\n1. [Ruanyifeng](http://ruanyifeng.com/blog/2016/09/software-architecture.html)\n1. [Software architecture patterns](https://www.oreilly.com/programming/free/files/software-architecture-patterns.pdf)\n1. [Software-Engineering-at-Google 中文版本](https://github.com/qiangmzsx/Software-Engineering-at-Google)\n1. [java-design-patterns stars hight](https://github.com/iluwatar/java-design-patterns)\n","tags":["Design"],"categories":["Program","Design"]},{"title":"Cpp to c interface","url":"/2020/07/27/Cpp-to-c-interface/","content":"\n## base\n\n1. 如果给 c++程序 dlopen 用，只需要 create 和 release 接口，c++程序可以拿到 C++头文件定义，可以将 create 指针强转成类对象，用完之后调用 free。\n1. shared_ptr 两种解决方法(堆上手动管理智能指针，栈由系统自动管理)\n   - struct 存 graph\n   - 创建 shared_ptr 的指针，指针不释放 (已经创建栈上的 shared_ptr,没法调用 new)\n   - [自建 sample](https://github.com/jiaxiyang/cpp_sandbox/tree/master/cpp2c)\n\n### [extern \"C\" 作用](https://www.cnblogs.com/xiangtingshen/p/10980055.html)\n\n1. extern \"C\"的主要作用就是为了能够正确实现 C++代码调用其他 C 语言代码。加上 extern \"C\"后，会指示编译器这部分代码按 C 语言（而不是 C++）的方式进行编译。由于 C++支持函数重载，因此编译器编译函数的过程中会将函数的参数类型也加到编译后的代码中，而不仅仅是函数名；而 C 语言并不支持函数重载，因此编译 C 语言代码的函数时不会带上函数的参数类型，一般只包括函数名。\n1. `extern \"C\" {}` 作用是阻止 C++ name mangling, 生成 C 格式符号表\n\n## method.hpp\n\n```c++\n#pragma once\n\nnamespace jia {\nClass Method {\n public:\n    static std::unique_ptr<Method> create(std::string name);\n\n    virtual const std::string get_name() const = 0;\n}\n}\n```\n\n## method_imp.hpp\n\n```c++\n#pragma once\n#include \"method.hpp\"\n\nnamespace jia {\nClass MethodImp : public Method {\n public:\n    MethodImp(std::string name);\n    const std::string get_name() const override;\n\n private:\n    int name_;\n    int value_;\n    friend class c_api;  // the purpose is to get the private member which can not get from the public interface\n}\n}\n\n```\n\n## method.cpp\n\n```c++\n#include \"method.hpp\"\n#include \"method_imp.hpp\"\n\nnamespace jia {\n\nstd::unique_ptr<Method> Method::create(std::string name) {\n    return std::unique_ptr<Method>{static_cast<Method*>(new Method{name})};\n}\n\n}\n\n```\n\n## method_imp.cpp\n\n```c++\n#include \"method_imp.hpp\"\n#include \"c.h\"\n\nnamespace jia {\n\nMethodImp::MethodImp(std::string name)\n    : name_{name},\n      value_{1}{};\n\nconst std::string MethodImp::get_name() const {\n    return name_;\n}\n\n}  // namespace jia\n\n/* C API implementations */\n#include \"c.h\"\n\nnamespace jia {\n\nclass c_api {\n public:\n    static int get_value(c_method_t method) {\n        auto self = static_cast<jia::Method*>(method);\n        return self->value_;  // to get value_; c_api need to be friend class of MethodImp\n    }\n}\n\nextern \"C\" c_method_t c_create(const char* name) {\n    auto m = jia::Method::create(std::string(name));\n    return static_cast<c_method_t>(m.realse());  // release will move unique_ptr ownership\n}\n\nextern \"C\" int c_destroyed(c_method_t method) {\n    auto m = static_cast<jia::Method*>(method);\n    delete m;\n    return 0;\n}\n\nextern \"C\" const char* c_get_name(c_method_t method) {\n    auto self = static_cast<jia::Method*>(method);\n    return self->get_name().cstr();\n}\n\nextern \"C\" int c_get_value(c_method_t method) {\n    return jia::c_api::get_value(method);\n}\n\n}\n\n```\n\n## c.h\n\n```c++\n#pragma once\n\n#ifdef __cplusplus  // c++ will use and c not use\nextern \"C\" {\n#endif\n\ntypedef void* c_method_t;\n\nc_method_t c_create(const char* name);  // create an instance\n\nint c_destroyed(c_method_t);            // destroy the instance\n\nconst char* c_get_name(c_method_t method);\n\nint c_get_value(c_method_t method);\n\n#ifdef __cplusplus\n}\n#endif\n```\n\n## test.c\n\n```c++\n#include \"c.h\"\n\nint main() {\n    c_method_t m = c_create(\"c_test\");\n    printf(\"name:  %s\", m->c_get_name());\n    printf(\"value: %d\", m->get_value());\n    c_destroyed(m);\n}\n```\n","tags":["Cpp"],"categories":["Program","Cpp"]},{"title":"Memory concept and issues","url":"/2020/07/26/Memory-concept-and-issues/","content":"\n## 内存层次结构\n\n1. [示意图](https://excalidraw.com/#json=-Jyt8n2_jgQDOsSsLHgc-,7u5WEbNh-76yPmNJw4AUqQ)\n   ![示意图](https://i.ibb.co/M2KRtV9/6-Lx-Su-Yv-CJI.png)\n1. nsight compute 可以显示各 memory 速率\n\n## base\n\n1. [stressapptest](https://github.com/stressapptest/stressapptest)\n   - Stressful Application Test - userspace memory and IO test\n1. [stress 压力测试工具](https://blog.51cto.com/quantfabric/2593578)\n   - stress 是 Linux 的一个压力测试工具，可以对 CPU、Memory、IO、磁盘进行压力测试。\n1. `加权访存带宽计算`：只读带宽最高约 520 GB/s，L2 Cache 带宽 1.9TB/s，当 L2 cache 命中率为 30% 时，加权后的平均访存带宽为 520 x 0.7 + 1900 x 0.3 = 934 GB/s\n1. gpu sram 带宽为什么比主存大很多：\n   - 并行，多个 SM 访问 shared memory， 如有 80 个 sm，每个 sm4 个 partition， 每个 shared memory 有 32 个 bank， 等效位宽为：`80*4*32*8 = 81920`, hbm 位宽为 8192， 带宽为 1.5TB/s, sram 为 19TB/s\n1. 可以按照是否在芯片上分为片上内存(`on chip`)和片下内存(`off chip`)。\n1. [/proc/meminfo](https://github.com/freelancer-leon/notes/blob/master/kernel/mm/meminfo.md)\n1. RES=CODE+DATA 进程使用的、未被换出的物理内存大小\n1. 内存主要关注物理内存, 虚拟内存表示程序文件及链接库大小，并不是所有的都加载到内存\n1. used mem = total - free - buffers - cache （man free 可以看计算公式),\n1. 主要看 `avaliable` < free + buff/cache ` top | grep avail` or `free -s 1(100)` 间隔长好看趋势\n1. 纳入内核管理的内存不见得都在使用中，还包括过去使用过的现在可以被重复利用的内存，内核并不把这些可被重新使用的内存交还到 free 中去，因此在 linux 上 free 内存会越来越少。\n1. %MEM -- Memory Usage (RES) A task's currently used share of available physical memory. 当前使用的物理内存 / 总的物理内存\n1. 进程内存不增长， 系统整体内存一直增长可能是 log 文件写到/tmp 目录下\n\n## 内存墙\n\n1. [英伟达、高通、特斯拉、Mobileye 芯片真实算力大比拼](https://mp.weixin.qq.com/s/P6rlEFZGZ6jZqhldmH5UKg) 包含各种内存芯片参数\n   - 芯片和内存分开， 芯片有最高支持的内存类型。开发板上 ddr 类型才是确认的。所以开发板和实际用的产品可能性能不一样。\n1. [汽车领域拥抱 Transformer 需要多少算力？1200TOPS 都不够](https://mp.weixin.qq.com/s/tSTRBtdNG55COBlUWyCDIg)\n1. `内存墙`是指随着计算能力的持续增长，内存的访问速度相对于处理器的速度提升相对滞后，导致处理器频繁地等待数据。这个不平衡造成的性能瓶颈常常被称为“内存墙”。\n1. `处理器速度的增长`：在过去的几十年里，处理器的速度增长迅速，遵循摩尔定律（每 18-24 个月，晶体管的数量翻倍）。这导致了处理器每秒可以执行的指令数大幅增加。\n1. `内存速度的增长相对较慢`：尽管 DRAM 技术也在不断进步，但与处理器的增速相比，其访问速度和带宽的增长较为缓慢。\n1. `访问延迟`：当处理器需要从主内存中读取数据时，它经常需要等待几百个周期才能获取数据。由于处理器的速度比内存快得多，这种等待成为了一个瓶颈，限制了性能。\n1. `缓存的引入`：为了缓解这个问题，现代计算机系统引入了缓存（L1、L2、L3 等）来存储频繁访问的数据。这些缓存比主内存快得多，但容量较小。当缓存未命中时，处理器必须从主内存中获取数据，这会引入延迟。\n1. `带宽与延迟`：除了访问延迟外，带宽也是一个问题。`即使内存的延迟得到改善，处理器仍然可能会被大量的并发内存访问操作所限制，导致带宽成为瓶颈`\n1. `解决方案`：有许多方法被提出来试图克服内存墙的问题，包括更智能的缓存策略、预取策略、多线程、多核处理器设计等。此外，新的内存技术，如 HBM (High Bandwidth Memory) 和 3D 堆叠内存，也被引入来提供更高的带宽。\n   总的来说，内存墙描述了一个关键的挑战：随着处理器速度的增长，如何确保它们能够及时地获取所需的数据。这是计算机架构领域的一个核心问题，研究者持续致力于开发新的技术和策略来解决这个问题。\n\n## 内存类型\n\n1. HBM (High Bandwidth Memory) 是一种高性能、高带宽的堆叠式 DRAM（动态随机存取存储器）技术。HBM 是由 AMD 和 Hynix 合作开发，旨在解决高性能计算应用中带宽和功耗的瓶颈问题。\n   - 将多个 DRAM 以一种堆叠的方式嵌入到统一处理器封装内部，可降低访问延迟。从而允许处理器和 DRAM 间建立更多更快的连接。\n1. LPDDR4 是 \"Low Power Double Data Rate 4\" 的缩写，它是一种移动 DRAM（Dynamic Random-Access Memory）标准。这种类型的内存主要用于移动设备（如智能手机、平板电脑和某些高端笔记本电脑）中，因为它设计为在较低电压下运行，从而减少功耗和延长电池寿命。\n1. LPDDR4x 是 LPDDR4 的一个增强版本，它主要优化了功耗，适用于电池供电的移动设备，如智能手机和平板电脑。LPDDR4x 为 \"Low Power Double Data Rate 4x\" 的缩写。\n\n## 内存概念\n\n1. page 是磁盘和内存交换数据基本单位， cache line 是内存和缓存交换数据基本单位。\n1. <<计算机体系结构>> 第 2 章存储器层次机构设计\n1. 栈和堆都是代码在运行时可供使用的内存。\n1. 栈中的所有数据都必须占用已知且固定大小。\n1. 栈的读写比堆要快。\n1. 当代码调用一个函数时，传递给函数的值和数据的局部变量被压入栈中，当函数结束时，这些值被移除栈。\n1. Rust 所有权的存在是为了管理堆数据。\n1. 数据在堆上才需要释放\n1. 数据在堆上赋值时无指出是浅拷贝(C++)或移动(Rust，原来的变量不在有效)，而不是深拷贝。\n1. Rust 数据在栈上，赋值是都是拷贝，原来的变量依旧有效。在堆上，赋值都是移动，原来变量无效。\n1. Rust String 由三部分组成，ptr，len，capacity，这一组数据在栈上，ptr 是指向在堆上数据的指针。（数据结构的固定长度数据在栈上，可变数据在堆上）\n\n## 内存(堆栈)错误类型\n\n1. 引用空指针\n1. 使用未初始化的内存\n1. 释放后使用，也就是使用悬垂指针\n1. 缓冲区溢出，比如数组越界\n1. 非法释放已经释放过的或者未分配的指针\n\n## 内存管理方式\n\n1. 垃圾回收机制，在程序运行时不断的寻找不再使用的内存\n1. 程序员亲自分配内存和释放内存\n1. Rust 通过所有权系统管理内存，编译器在编译时会根据一系列规则进行检查，拥有数据所有者在离开作用域后自动清除其数据\n\n## [内存信息](https://www.cnblogs.com/wardensky/p/4274180.html)\n\n1. `lshw -class memory`\n1. `dmidecode|grep Speed`\n1. [windows aida64](https://www.aida64.com/downloads/YWY1MWUxM2E=)\n\n   - 工具 -> 内存缓存测试 -> 有内存类型\n   - 最下面有性能测试\n   - [破解](https://www.extremexbb.com/aida64-keys/) 使用: 6.85.6300 版本\n\n1. `DDR4-2666`\n   - `I/O 时钟频率`： 1333 MHz\n   - `数据传输速率`： 2666 MT/s， 即每秒 2666 MT/s（Mega Transfers per second）\n   - 位宽决定带宽；如果位宽为 64， 带宽为： 2666 \\* 8 = 21GB/s\n1. 内存带宽和位宽都是评估和描述内存性能的关键指标。它们涉及到数据在内存与其他组件（如 CPU）之间传输的速率以及每次传输的数据量。以下是这两个术语的解释：\n1. `内存带宽 (Memory Bandwidth)`：\n\n   - 定义：内存带宽描述了数据可以在特定时间内从内存传输到 CPU 或其他设备的最大速度。它是衡量内存性能的关键参数。\n   - 单位：常用的单位是 GB/s（吉字节每秒）。\n   - 计算方法：内存带宽 = 内存时钟频率 x 数据传输次数 x 位宽; 其中，数据传输次数通常基于内存类型，例如，DDR（Double Data Rate）内存每个时钟周期有两次数据传输。\n\n1. `位宽 (Memory Bus Width)`：\n   - 定义：位宽描述了内存总线一次能够传输的数据量。它决定了每次数据传输的宽度。\n   - 单位：位（bit）。常见的位宽有 8-bit, 16-bit, 32-bit, 64-bit, 128-bit 等。\n   - 例如，一个 64-bit 宽的内存总线意味着它可以一次性传输 64 位（8 字节）的数据。\n1. `sample`：假设我们有一个 DDR3 内存，其时钟频率为 1600 MHz，位宽为 64-bit。那么内存带宽的计算方法为：内存带宽 = 1600 MHz x 2 x 64 bits = 25600 Mbits/s = 25.6 GB/s, 因此，此内存的带宽是 25.6 GB/s。\n\n## 理论峰值带宽(compare with cpu)\n\n1. 和 cpu 比较\n\n## [内存性能测试](https://blog.csdn.net/pcokk/article/details/90733871)\n\n1. [Tools_Linux_Memory_Perf_Measure.md](https://github.com/LucaCanali/Miscellaneous/blob/master/Spark_Notes/Tools_Linux_Memory_Perf_Measure.md)\n1. [各平台各性能指标计算方法](https://github.com/RRZE-HPC/likwid/tree/master/groups)\n1. 因为 cache 比较小，需要频繁交换数据，读写大块数据几乎每次都要从 ddr 读数；例如：模型推理，每次推理都要从 ddr 重新读权重, 并不是将权重一直放在 cache 中。\n   - 从磁盘到 ddr 可能只需要加载一次\n1. [rust net bandwidth](https://github.com/imsnif/bandwhich)\n\n1. `mbw 32 -b 4096`\n1. `sysbench memory run`\n1. `lmbench`\n1. `Memtest86+`\n\n### perf\n\n1. `perf list and perf stat -e <memory_access>`获取 memory access, 根据时间，位宽算带宽\n   - `sudo perf record -F 99 -e armv8_pmuv3/mem_access/ -ag -- sleep 1`\n   - `sudo perf list -n` 还能看到带宽占用比例\n   - 可以画火焰图\n   - bandwidth = 位宽 \\* count / second; 可以与 bandwidth 对比\n\n```sh\ntimeout=2\nwhile true\ndo\n  perf record -e mem_access -ag -- sleep $timeout > /dev/null 2>&1\n  db=$(perf report | grep \"Event\" | awk '{print $NF}' | head -n 1)\n  result=$(( 8 * ${db} / ${timeout}  / 1000000 ))\n  echo \"$result MB/s\"\ndone\n```\n\n1. [arm55 pmu 手册](https://developer.arm.com/documentation/100442/0100/debug-descriptions/pmu/pmu-events?lang=en)\n1. `likwid-perfctr -e` 列出所有 PMC\n1. `likwid-perfctr -e | grep MEM`列出所有内存相关 PMC, `perf stat -e r013,r066,r067` r + mask + eventsel 监控对应事件\n\n````sh\nnvidia@miivii-tegra:~$ likwid-perfctr -e | grep MEM\nMEM_ACCESS, 0x13, 0x0, PMC\nMEMORY_ERROR, 0x1A, 0x0, PMC\nMEM_ACCESS_LD, 0x66, 0x0, PMC\nMEM_ACCESS_ST, 0x67, 0x0, PMC\n\nnvidia@miivii-tegra:~$ perf stat -e r013,r066,r067 -- ls\nDesktop  Documents  Downloads  Music  Pictures  Public  Templates  Videos\n\n Performance counter stats for 'ls':\n\n           156,045      r013:u\n           118,079      r066:u\n            37,966      r067:u\n\n       0.003111539 seconds time elapsed\n\n       0.003294000 seconds user\n       0.000000000 seconds sys\n```\n\n## [stream](https://www.cs.virginia.edu/stream/stream2/)\n\n### [BabelStream](https://github.com/UoB-HPC/BabelStream)\n\n1. use\n\n``` sh\ncmake -Bbuild -H. -DMODEL=cuda -DCUDA_ARCH=sm_86 -DCMAKE_CUDA_COMPILER=nvcc\n./cuda-stream\n\n# result\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\nUsing CUDA device Orin\nDriver: 11040\nMemory: DEFAULT\nReduction kernel config: 56 groups of (fixed) size 1024\nInit: 0.091487 s (=8802.387996 MBytes/sec)\nRead: 0.014568 s (=55279.816972 MBytes/sec)\nFunction    MBytes/sec  Min (sec)   Max         Average\nCopy        103377.883  0.00519     0.00764     0.00553\nMul         99863.044   0.00538     0.01373     0.00577\nAdd         134846.743  0.00597     0.01092     0.00642\nTriad       134462.719  0.00599     0.00829     0.00637\n\nTriad 是一个计算模式，通常在性能测试和基准测试中使用，特别是在评估计算机系统的内存带宽和处理能力时。在最简单的形式中，Triad 操作涉及到以下三个操作：\nfor (i = 0; i < N; i++) {\n    A[i] = B[i] + scalar * C[i];\n}\n```\n\n### [(good) bandwidth](https://zsmith.co/bandwidth.php#download)\n\n1. 可以抠出来源码，实时检测内存使用\n1. [不同处理器的带宽](https://zsmith.co/bw-table.php)\n1. 测试多核性能时，运行多个实例，相加结果。Is bandwidth actually showing the maximum bandwidth to and from main memory? There is an easy way to test this. We can run one instance of bandwidth on each core of a multi-core CPU (in my case, two instances, one for each core) and add up the access bandwidths to/from main memory for all instances to see whether they approach the published limits for our main memory system.\n1. 使用\n\n```sh\nwget -c https://zsmith.co/archives/bandwidth-1.12.3.tar.bz2\nsudo apt install nasm\nmake\n````\n\n1. 上图可以看出来在三个地方曲线有明显的转折。分别是 32k、256K、12MB。这分别是三级缓存的大小。就跟下图所示一样, 也就是说：当读写的 size 大小落在相应的 cache 里面的时候，吞吐量是最大的。当读写的大小的 size 超过这个 cache，而落入到下一个 cache 的时候，吞吐量就会降低。内存是最低的。[link](https://neuyilan.github.io/2017/03/07/bandwidth%E6%95%99%E7%A8%8B/)\n\n### likwid\n\n1. [测内存性能](https://github.com/RRZE-HPC/likwid/wiki/Tutorial%3A-Empirical-Roofline-Model)\n\n### [TheBandwidthBenchmark](https://github.com/RRZE-HPC/TheBandwidthBenchmark)\n\n## [内存压测](https://blog.51cto.com/u_14900374/2533353)\n\n1. memtester\n\n## 内存管理\n\n### malloc\n\n1. [memory-allocators-101-write-a-simple-memory](https://arjunsreedharan.org/post/148675821737/memory-allocators-101-write-a-simple-memory)\n1. malloc 缺点：\n   - 易内存泄漏\n   - 碎片化，影响性能\n\n## memory profiler\n\n1. jemolloc\n\n   - [内存泄漏调试 ---- jemalloc 的 heap profiling](https://www.yuanguohuo.com/2019/01/02/jemalloc-heap-profiling/)\n   - jeprof 工具不仅可以查看详细信息或者生成调用路径图(如上所示)，还可以用来比较两个 dump(显示增量部分)：\n\n1. [heaptrack](https://github.com/KDE/heaptrack)\n   - allocations 次数：\n     - ./bin/heaptrack ls\n     - ./bin/heaptrack_print heaptrack.ls.5400.gz -F stacks.txt\n     - flamegraph.pl --title \"heaptrack: allocations\" --colors mem --countname allocations < stacks.txt > heaptrack.someapp.PID.svg\n   - 内存大小（--flamegraph-cost-type 指定 allocations: number of allocations; peak: bytes consumed at highest total memory consumption ...)\n     - ./bin/heaptrack_print heaptrack.example.6433.gz --flamegraph-cost-type peak -F stacks2.txt\n     - /home/xiyang/d/working/FlameGraph/flamegraph.pl --title \"heaptrack: bytes\" --colors mem --countname bytes < stacks2.txt > heaptrack.example_peak.svg\n   - gui: `sudo apt-get install heaptrack-gui`\n     - 使用 vnc\n     - 注意 top-down 可以看堆栈\n     - caller/callee 右下角有代码具体位置\n   - 加编译 debug 能显示具体某一行分配的内存\n   - [usage](https://milianw.de/blog/heaptrack-a-heap-memory-profiler-for-linux.html)\n   - ./bin/heaptrack_print heaptrack.example.6433.gz -M ./test.txt 生成 massif 数据，可以用 massif-visualizer 可视化\n   - `sudo apt install heaptrack` 会出现 free(): invalid pointer;\n   - [源码编译](https://github.com/KDE/heaptrack?tab=readme-ov-file#compiling)\n     - `sudo apt install elfutils libdw-dev libunwind-dev`\n     - `cmake -DLIBDW_INCLUDE_DIR=/usr/include/ -DLIBDW_LIBRARIES=/lib/aarch64-linux-gnu/ ..` (不好使)\n   - `heaptrack -d ./example`debug， 会使用 gdb\n1. [FlameGraphs/memoryflamegraphs](https://www.brendangregg.com/FlameGraphs/memoryflamegraphs.html)\n1. [MTuner](https://github.com/RudjiGames/MTuner?tab=readme-ov-file)\n1. [valgrind Massif: a heap profiler](https://valgrind.org/docs/manual/ms-manual.html)\n   - valgrind --tool=massif prog\n   - ms_print massif.out.8855 (加-g 选项也可以看具体哪一行分配的内存)\n1. [massif-visualizer](https://apps.kde.org/zh-cn/massif-visualizer/)\n1. [procmem 和 procrank 查看动态库占用的内存](https://www.jianshu.com/p/34a495c3067b)\n\n## links\n\n1. [apache/arrow 内存分析工具](https://github.com/apache/arrow)\n1. [What Every Programmer Should Know About Memory](https://people.freebsd.org/~lstewart/articles/cpumemory.pdf)\n1. [每个程序员都应该了解的内存知识](https://zhuanlan.zhihu.com/p/611133924)\n1. [meminfo](https://github.com/freelancer-leon/notes/blob/master/kernel/mm/meminfo.md)\n1. [memory-allocators-101-write-a-simple-memory](https://arjunsreedharan.org/post/148675821737/memory-allocators-101-write-a-simple-memory)\n","tags":["Rust","Memory","C++"],"categories":["Program","Basic"]},{"title":"Dangling pointer","url":"/2020/07/26/Dangling-pointer/","content":"\n## Sample1\n\n``` c++\nint main() {\n    auto Name = input();\n    cout << \"Hello \" << Name << '\\n';\n    return 0;\n}\n\nconst char* input() {\n    string Line;\n    getline(cin, Line);\n    return Line.c_str();  // Line is destroyed\n}\n\n```\n\n## Sample2\n\n``` c++\n#include <iostream>\nusing namespace std;\n\nvoid test() {\n    int* p = nullptr;\n    {\n        int x = 0;\n        p = &x;\n        cout << *p;\n    }  // x is destroyed\n\n    cout << *p;\n}\n```\n\n## Sample3\n\n``` c++\n#include <iostream>\nusing namespace std;\n\nstring_view {\n    size_t len;\n    const char* str;\n}\n\nvoid test() {\n    string_view s;\n    {\n        char a[100];\n        s = a;\n        cout << s[0];\n    }  // a is destroyed and a.str become a dangling pointer\n\n    cout << s[0];\n}\n\n```\n\n## Sample4\n\n``` c++\nvoid swap(Object* a, Object* b){\n    Object* tmp;\n    tmp = a;  //tmp point to a\n    a = b;\n    b = tmp;\n}  // tmp was destroyed and b will be a dangling pointer\n\n```\n\n## Sample5\n\n``` c++\nint* a = new int(100);\n\nfor (size_t i = 0; i < 100; i++) {\n    *(a++) = i;  // a has been changed and a not point array[0] now\n}\n\nstd::cout << a[0] << std::endl;\ndelete a;\n\n```\n\n## Links\n[zhihu example](https://zhuanlan.zhihu.com/p/85200304)\n","tags":["Cpp"],"categories":["Program","Cpp"]},{"title":"Zsh config","url":"/2020/07/22/Zsh-config/","content":"\n## Settings\n\n``` shell\nexport TERM=\"screen-256color\"\n\nZSH_HIGHLIGHT_STYLES[arg0]='fg=#26ff00,bold'\nZSH_HIGHLIGHT_STYLES[command]='fg=#26ff00,bold'\n\nexport FZF_DEFAULT_OPTS=\"--color=hl:green,hl+:#00ff00\" # 设置fzf color\n\nprecmd() {                                                                                                                                          print -n '\\n'                                                                                                                                 }\n# precmd() {\n#   print -n '⚡\\n\\n'\n# }\n# precmd() {\n#   print -n '\\n'\n#   local width=$(tput cols)  # 获取终端宽度\n#   printf '%*s\\n' \"$width\" '' | tr ' ' '-'  # 打印一行 '-'\n# }\nprecmd() {\n  local width=$(tput cols)  # 获取终端宽度\n  local line=$(printf '%*s' \"$width\" '' | tr ' ' '-')  # 生成宽度为终端宽度的一行 '-'\n\n  print -n '\\n'\n  # 设置浅灰色并打印\n#   printf '\\033[90m%s\\033[0m\\n' \"$line\"\n  printf '\\033[38;2;100;100;100m%s\\033[0m\\n\\n' \"$line\"\n  }\n\n\n# bash功能\nPROMPT_COMMAND='\n  width=$(tput cols)  # 获取终端宽度\n  line=$(printf \"%*s\" \"$width\" \"\" | tr \" \" \"-\")  # 生成宽度为终端宽度的一行 \"-\"\n  printf \"\\n\\033[38;2;100;100;100m%s\\033[0m\\n\\n\" \"$line\"  # 打印浅灰色的一行 \"-\"\n'\n  \n```\n\n\n\n## NOTE\n\n1. scp not work\n\n```\n# remember to put this code to .bashrc before execute zsh\n# If not, scp can not work.\n[[ $- == *i* ]] || return\nzsh\n```\n\n2. 如果你使用的是 zsh，上面的通配符会失效，你可以切换到 bash 执行这些命令，或者使用 setopt no_nomatch 来关闭 zsh 对于通配符的拦截\n\n## oh my zsh\n\n```sh\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\n```\n\n## [microsoft/inshellisense](https://github.com/microsoft/inshellisense) [npm issue](https://stackoverflow.com/a/61404290)\n\n```sh\nnpm install -g @microsoft/inshellisense\ninshellisense bind\n```\n\n## autosuggestions\n\n```sh\ngit clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/custom/plugins/zsh-autosuggestions\nsed -i \"s/fg=8/fg=cyan/\" .oh-my-zsh/custom/plugins/zsh-autosuggestions/zsh-autosuggestions.zsh\nvi ~/.oh-my-zsh/custom/plugins/zsh-autosuggestions/zsh-autosuggestions.zsh 文件，修改 ZSH_AUTOSUGGEST_HIGHLIGHT_STYLE='fg=blue'\n# support colors: black, red, green; yellow; blue; magenta; cyan and white;\n```\n\n## zsh-syntax-highlighting\n\n```sh\ngit clone git://github.com/jimmijj/zsh-syntax-highlighting ~/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\nvi ~/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting/highlighters/main/main-highlighter.zsh #默认配色\n\nZSH_HIGHLIGHT_STYLES[arg0]='fg=yellow,bold' #命令行里先实验，让后放到.zshrc中\nZSH_HIGHLIGHT_STYLES[command]='fg=#26ff00,bold'\nZSH_HIGHLIGHT_STYLES[arg0]='fg=#26ff00,bold'\n```\n\n## .zshrc config\n\n```sh\nexport PROMPT=\"%n@%m:%1~%# \"\nplugins=(rust rustup cargo zsh-autosuggestions zsh-syntax-highlighting colored-man-pages tmux fd)\n```\n\n## Othe config\n\n```sh\ngit config oh-my-zsh.hide-status 1 --global #close git status\n```\n\n## Powlevel9k theme\n\n```\ngit clone https://github.com/bhilburn/powerlevel9k.git ~/.oh-my-zsh/custom/themes/powerlevel9k\n.zshrc：\nexport TERM=\"xterm-256color\"\nPOWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(user dir)\nPOWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(root_indicator background_jobs )\nPOWERLEVEL9K_MODE='awesome-fontconfig'\nZSH_THEME=\"powerlevel9k/powerlevel9k\"\n~/.oh-my-zsh/custom/themes/powerlevel9k/powerlevel9k.zsh-theme          修改  \"CONTENT\"             \"jia@61\" #\"$(whoami)\"\n```\n\n## awesome-terminal-fonts\n\n```sh\ngit clone https://github.com/gabrielelana/awesome-terminal-fonts\ncp -R build/* ~/.fonts/\nfc-cache -fv ~/.fonts\ncp config/10-symbols.conf ~/.config/fontconfig/conf.d\nsource ~/.fonts/*.sh\n```\n\n## autojump\n\n```sh\ngit clone git://github.com/joelthelion/autojump.git\ncd autojump\n./install.py\n```\n\n## powerline(非必须）\n\n```sh\nsudo apt-get install fonts-powerline\ngit clone https://github.com/powerline/fonts.git --depth=1\ncd fonts\n./install.sh\n```\n\n## Links\n\ninstall oh my zsh : sh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\nzsh autosuggestions https://note.youdao.com/web/#/file/WEB35aa360d3643be282ab89890d39646cf/note/WEBa075b0a3e60cbde5f506563c3c14997a/\nsudo apt-get install autojump https://note.youdao.com/web/#/file/WEB35aa360d3643be282ab89890d39646cf/note/WEB1da9f35b71c3815439e1426116e7ecf9/\nPowlevel9k theme install https://note.youdao.com/web/#/file/WEB35aa360d3643be282ab89890d39646cf/note/WEBfd1ecc9886dfcf108ee9bae6b586cab6/\nPowerline https://note.youdao.com/web/#/file/WEB35aa360d3643be282ab89890d39646cf/note/WEBac4ed9c8ab78bb09940458cfb9b1a397/\nawesome-terminal-fonts http://www.cnblogs.com/weixuqin/p/7029177.html\nzsh-syntax-highlighting https://note.youdao.com/web/#/file/WEB35aa360d3643be282ab89890d39646cf/note/WEBef4ca4fdaf7d36950545241429e3daee/\n","tags":["Zsh"],"categories":["Tools","Zsh"]},{"title":"Why Rust","url":"/2020/07/10/Why-Rust/","content":"\n## Why Rust?\n### Performance\n1. It's **`fast`** and memory efficient: with no runtime or garbage collector.\n1. It can easily integrate with other languages.\n\n### Reliability\n1. It's rich type system and ownership model guarantee <font color='red'>**memory-safety**</font> and <font color='red'>**thread-safety**</font>.\n\n### Productivity\n1. Great documentation\n1. Friendly compiler, find most errors when compiling\n1. An integrated package manager\n1. Doc system: `cargo doc --open`\n\n## Rust features\n1. Zero-cost abstractions\n1. Modern conveniences\n1. Type system: Ownership and Borrowing\n1. Sense of craftsmanship\n\n## Why Name Rust?\n1. It's a pun on \"chrome\". Rust was written by the engineer company Mozilla(firefox). And chrome is a metal.\n\n## Rust 目的\n1. 创建这个新语言的目的是为了解决一个顽疾：软件的演进速度大大低于硬件的演进，软件在语言级别上无法真正利用多核计算带来的性能提升。Rust是针对多核体系提出的语言，并且吸收一些其他动态语言的重要特性，比如不需要管理内存，比如不会出现Null指针等\n\n## Why not C++\n1. 当然C++ 也很好，因为它教会了我怎么面向搜索引擎编程。\n\n## Rust类型系统借鉴Haskell特性\n1. 没有空指针\n1. 类型默认不可变\n1. 表达式\n1. 高阶函数\n1. 代数数据类型\n1. 模式匹配\n1. 泛型\n1. trait和关联类型\n1. 本地类型推导\n\n## Rust相比Haskell独有特点\n1. 仿射变换(Affine Type)\n1. 借用，生命周期\n","tags":["Rust"],"categories":["Program","Rust"]},{"title":"Rust start","url":"/2020/07/09/Rust-start/","content":"\n![Ferris](https://www.rust-lang.org/static/images/ferris.gif)\n\n## Install on linux\n\n1. `curl https://sh.rustup.rs -sSf | sh`\n1. [Install step](https://doc.rust-lang.org/book/ch01-01-installation.html)\n\n## Config\n\n1. [wcy's config](http://wcy123.github.io/2020-04-05-setup-rust-IDE-for-emacs.html)\n\n## Local doc\n\n1. `rustup doc`\n\n## Links\n\n1. [Officail website](https://www.rust-lang.org/)\n1. [Official resource](https://www.rust-lang.org/learn)\n1. [Awesome Rust](https://github.com/rust-unofficial/awesome-rust#readme)\n1. [Rust learning](https://github.com/ctjhoa/rust-learning)\n1. [Forum](https://users.rust-lang.org/)\n1. [Packages](https://crates.io/)\n1. [STD library](https://doc.rust-lang.org/std/index.html)\n1. [Rust book](https://doc.rust-lang.org/book/)\n1. [Rust book Chinese](http://120.78.128.153/rustbook/)\n1. [Cargo book](https://doc.rust-lang.org/cargo/)\n1. [Rust examples](https://doc.rust-lang.org/stable/rust-by-example/)\n1. [Rust online programming](https://play.rust-lang.org/)\n1. [Ferris pictures](https://rustacean.net/)\n1. [Rust gameboy](https://github.com/mohanson/gameboy)\n1. [Rust zhihu](https://www.zhihu.com/topic/19674381/intro)\n1. [Rust 张汉东](https://www.infoq.cn/article/Uugi_eIJusEka1aSPmQM)\n1. [未来的人工智能语言，是 Rust 还是 Mojo ？](https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA%3D%3D&mid=2247597850&idx=1&sn=679ccdcaf76e8b6ab97ca42687769da1)\n\n## repos\n\n1. [egui](https://github.com/emilk/egui)\n1. [rerun](https://github.com/rerun-io/rerun)\n","tags":["Rust"],"categories":["Program","Rust"]},{"title":"Clang-format-usage","url":"/2020/07/01/Clang-format-usage/","content":"\n## Notes\n\n1. 不同版本的 clang-format 输出格式可能不同，需要统一团队版本\n1. 文件中有编码错误的字符不能被 format\n\n## format check\n\n1. [run-clang-format](https://github.com/Sarcasm/run-clang-format/tree/master)\n   - `-r samples` 文件夹\n   - `-i` 修改\n1. [github clang-format-check](https://github.com/marketplace/actions/clang-format-check)\n1. [enforce-code-consistency-clang-format#enforce_the_new_coding_rules](https://developers.redhat.com/articles/2022/02/25/enforce-code-consistency-clang-format#enforce_the_new_coding_rules)\n1. `clang-format --Werror -n --verbose test.cpp` 可以检测是否符合 clang-format\n1. cpplint\n1. [gitlab check](https://stackoverflow.com/a/69362305)\n1. [pre-commit](https://gitlab.com/daverona/pre-commit/cpp)\n1. [git pre commit](https://blog.csdn.net/zmlovelx/article/details/105196415)\n\n## CI format\n\n1. [使用 clang-format 在 CI 中自动格式化 C++代码](https://lrita.github.io/2022/07/30/auto-clang-format-cpp-code/)\n\n## Install clang\n\n1. [Build clang from source](http://clang.llvm.org/get_started.html)\n1. [Clang format doc](http://clang.llvm.org/docs/ClangFormat.html)\n\n## format config\n\n1. [Clang format style](http://clang.llvm.org/docs/ClangFormatStyleOptions.html)\n1. [clang format config 试验](https://zed0.co.uk/clang-format-configurator/)\n1. `AllowShortFunctionsOnASingleLine: None` 函数不在一行\n1. `SortIncludes: false` 头文件不排序\n\n## Bash Command\n\n1. generate .clang-format\n   `clang-format -style=llvm -dump-config > .clang-format`\n1. Format one cpp file:\n   `clang-format --style=Google -i test.cpp`\n1. Format all cpp files:\n   `fd \".cpp$\" | xargs clang-format --style=Google -i`\n\n## git\n\n1. `git-clang-format-12 or git clang-format-12` format staged files\n1. `git-clang-format-12 --diff` 查看要 format 的变化\n\n## Vim Config\n\n1. Find clang-format.py in your system and copy the file path.\n\n```sh\n ~ : find /usr -iname \"clang-format.py\"\n/usr/local/share/clang/clang-format.py\n```\n\n2. Put the following config in your .vimrc. It will automaticly format the c++ file when the file is saved.\n\n```\nfunction! Formatonsave()\n  let l:lines = 'all'\n  let l:formatdiff = 1\n  let g:clang_format_fallback_style = 'Google'\n  \" Remember to repalce the path of clang-format.py\n  pyfile /usr/local/share/clang/clang-format.py\n  \" py3file /usr/local/share/clang/clang-format.py\nendfunction\nautocmd BufWritePre *.h,*.hpp,*.cc,*.cpp call Formatonsave()\n\n```\n\n## Emacs Config\n\n1. use package config\n\n```emacs-lisp\n(use-package clang-format\n  :after (cc-mode)\n  :config\n  (set-default 'clang-format-fallback-style \"Google\")\n  (add-hook 'c-mode-common-hook #'(lambda()\n                                    (add-hook 'before-save-hook\n                                              'clang-format-buffer t t))))\n```\n\n## Config\n\nClang-format understands also special comments that switch formatting in a delimited range. The code between a comment `// clang-format off or /* clang-format off */ `up to a comment `// clang-format on or /* clang-format on */ `will not be formatted. The comments themselves will be formatted (aligned) normally.\n\n```c++\nint formatted_code;\n// clang-format off\n    void    unformatted_code  ;\n// clang-format on\nvoid formatted_code_again;\n```\n","tags":["Emacs","Clang format","Vim"],"categories":["Tools","Clang"]},{"title":"write blog","url":"/2020/06/26/How-to-start-writing-blog/","content":"\n## Set up githubio\n\n1. Follow [github pages guide](https://guides.github.com/features/pages/) to apply a web.\n1. [configuring-a-publishing-source-for-your-github-pages-site](https://docs.github.com/cn/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site)\n\n## Use hexo (or JekyII)\n\n1. [ubuntu16.04 nodejs install](https://github.com/nodesource/distributions/blob/master/README.md)\n   - Need sudo to use npm install packages.\n   - [npm install error solve](https://www.jianshu.com/p/3fd7d90db01a)\n2. on Windows\n   - install node.js on windows and [set env](https://blog.csdn.net/chengyuweng7838/article/details/100996272)\n   - `npm install --force hexo-cli -g --registry=https://registry.npm.taobao.org`\n3. [hexo tutorial](https://hexo.io/zh-cn/)\n4. [hexo theme 3-hexo](https://github.com/yelog/hexo-theme-3-hexo)\n   - `_config.yml` theme: 3-hexo\n5. [3-hexo tutorial](https://yelog.org/2017/03/23/3-hexo-instruction/)\n\n## Write blog\n\n1. [markdown tutorial](https://guides.github.com/features/mastering-markdown/)\n\n## Push blog to githubio\n\n1. [hexo tutorial](https://hexo.io/zh-cn/docs/one-command-deployment)\n\n## Hexo basic comamnd\n\n1. Create a new post\n   `$ hexo new \"My New Post\"`\n1. Generate static files\n   `$ hexo generate`\n1. Run server(local site)\n   `$ hexo server`\n   `$ hexo server -p 5900`\n1. Deploy to remote sites\n   `$ hexo clean && hexo deploy`\n1. shortcuts\n   `alias hs='hexo clean && hexo g && hexo s'`\n   `alias hd='hexo clean && hexo g && hexo d && git add . && git commit -m \"update\" && git push -f'`\n\n## hexo plugins\n\n1. hexo-generator-searchdb\n1. hexo-symbols-count-time\n1. hexo-auto-excerp\n1. hexo-related-popular-posts\n1. hexo-filter-mermaid-diagrams\n1. hexo-generator-topindex\n\n## hexo config\n\n1. [theme-settings/posts](https://theme-next.js.org/docs/theme-settings/posts)\n1. [Hexo 博客 NexT 主题下如何添加分类、标签](https://cs-cshi.github.io/hexo-blog/Hexo-NexT%20%E6%B7%BB%E5%8A%A0%E5%88%86%E7%B1%BB%E3%80%81%E6%A0%87%E7%AD%BE/) [对应的主题配置](https://github.com/cs-cshi/hexo-theme-next)\n1. [mermaid support](http://yelog.org/2019/11/12/3-hexo-support-mermaid/)\n1. [3-hexo theme nav settings](https://github.com/jiaxiyang/blog/commit/e293840421fb4510c5a3499ff4c828b2cca7f51a)\n1. [hexo-theme-next](https://github.com/iissnan/hexo-theme-next)\n1. [theme-next.org](https://theme-next.org/)\n1. [next sample1](https://plantegg.github.io/2021/05/16/CPU_Cache_Line%E5%92%8C%E6%80%A7%E8%83%BD/)\n1. [hexo sample2](https://neuyilan.github.io/)\n\n## next theme\n\n1.  [links 颜色设置](https://vonsdite.github.io/posts/13afd8d4.html)\n1.  `<!-- more -->` It's a common need to show some part of article in home page and then give a link to full article.\n1.  [live2d-widget](https://github.com/stevenjoezhang/live2d-widget)\n1.  [theme-next-docs 可以看各主题效果](https://github.com/next-theme/theme-next-docs)\n    - 参考如何配置\n1.  [good 参考](https://www.idefun.com/)\n1.  [如何添加 google analytics](https://justdong.net/wexin/weixin_03google-analytics)\n1.  [sitemap 让 google 和百度搜索到](https://www.gaotianyang.top/archives/20200814f023bd8e/)\n1.  [valine 评论系统](https://valine.js.org/quickstart.html)\n1.  [显示网站运行时间](https://blog.51cto.com/u_12877374/4977880)\n1.  [显示动画](https://github.com/theme-next/theme-next-three)\n1.  [pdf 支持](https://github.com/next-theme/theme-next-pdf)\n1.  [hexo-filter-mathjax 处理数学公式](https://github.com/next-theme/hexo-filter-mathjax)\n    - `npm install hexo-filter-mathjax`\n    - [mathjax live #demo](https://www.mathjax.org/#demo)\n1.  $$x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$$\n    $$\n    i\\hbar\\frac{\\partial}{\\partial t}\\psi=-\\frac{\\hbar^2}{2m}\\nabla^2\\psi+V\\psi\n    $$\n1.  [(good)hexo-theme-beautify](https://eblog.gitee.io/posts/hexo/hexo-theme-beautify.html)\n1.  [mermaid 支持](https://theme-next.js.org/docs/tag-plugins/mermaid)\n    - `sudo npm install hexo-cli -g` 工程目录下升级 hexo\n1.  [高亮支持的语言](https://github.com/highlightjs/highlight.js/blob/main/SUPPORTED_LANGUAGES.md)\n1.  [高亮预览](https://theme-next.js.org/highlight/)\n\n## [shareX](https://getsharex.com/)\n\n1. 使用 shareX 保存图像，链接到 blog 里, 一篇 blog 一个文件夹\n","tags":["Blog"],"categories":["Tools","Blog"]},{"title":"about","url":"/about/index-1.html"},{"title":"about","url":"/about/index.html","content":"\n## 个人简介\n贾夕阳\n毕业于北京理工大学，目前从事人工智能软件架构工作\n\n## 联系方式\n1. mail: jiaxiyangbit@126.com\n2. qq: 312567150\n"},{"title":"categories","url":"/categories/index.html"},{"title":"tags","url":"/tags/index.html"}]