<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0-rc2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jiaxiyang.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="note 主动设置--cache-control&#x3D;all选项来保证在进行 kernel profiling 的时候清空历史 cache 数据，通过设置--clock-control&#x3D;base选项来锁定 GPU 运行的频率，避免动态调频对性能测量的影响。 Swizzle主要是在到 8192 以后，且 B 矩阵 transpose 的情况下必须要用（不用则效率腰斩，调出来 SWIZZLE 最优是 8）">
<meta property="og:type" content="article">
<meta property="og:title" content="cutlass">
<meta property="og:url" content="https://jiaxiyang.github.io/2024/01/26/cutlass/index.html">
<meta property="og:site_name" content="Xiyang">
<meta property="og:description" content="note 主动设置--cache-control&#x3D;all选项来保证在进行 kernel profiling 的时候清空历史 cache 数据，通过设置--clock-control&#x3D;base选项来锁定 GPU 运行的频率，避免动态调频对性能测量的影响。 Swizzle主要是在到 8192 以后，且 B 矩阵 transpose 的情况下必须要用（不用则效率腰斩，调出来 SWIZZLE 最优是 8）">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://github.com/NVIDIA/cutlass/blob/main/media/images/gemm-structural-components.png">
<meta property="article:published_time" content="2024-01-26T02:20:32.000Z">
<meta property="article:modified_time" content="2025-02-11T06:39:56.783Z">
<meta property="article:author" content="贾夕阳">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/NVIDIA/cutlass/blob/main/media/images/gemm-structural-components.png">

<link rel="canonical" href="https://jiaxiyang.github.io/2024/01/26/cutlass/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>cutlass | Xiyang</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WGS6S6YFJ6"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-WGS6S6YFJ6');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Xiyang</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Think twice, code once!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">197</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">44</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">55</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/jiaxiyang" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2024/01/26/cutlass/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          cutlass
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-01-26 10:20:32" itemprop="dateCreated datePublished" datetime="2024-01-26T10:20:32+08:00">2024-01-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-11 14:39:56" itemprop="dateModified" datetime="2025-02-11T14:39:56+08:00">2025-02-11</time>
              </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/01/26/cutlass/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/01/26/cutlass/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>12k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>10 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="note"><a href="#note" class="headerlink" title="note"></a>note</h2><ol>
<li>主动设置<code>--cache-control=all</code>选项来保证在进行 kernel profiling 的时候清空历史 cache 数据，通过设置<code>--clock-control=base</code>选项来锁定 GPU 运行的频率，避免动态调频对性能测量的影响。</li>
<li><code>Swizzle</code>主要是在到 8192 以后，且 B 矩阵 transpose 的情况下必须要用（不用则效率腰斩，调出来 SWIZZLE 最优是 8）</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cub">cutlass 参考了 cub 的设计</a></li>
<li>cutlass 是对 gemm 实现过程的各种级别的抽象；手写搬数时需要进行大量计算<ul>
<li>block tile 矩阵乘法内积； thread tile 矩阵乘法外积</li>
<li>global memory &lt;-&gt; shared memory &lt;-&gt; register &lt;-&gt; core</li>
<li>double buffering: shared memory, register</li>
<li>bank conflict: permute</li>
</ul>
</li>
<li>排列（英语：Permutation）是将相异对象或符号根据确定的顺序重排。每个顺序都称作一个排列</li>
<li>MMA &#x3D; matrix multiply accumulate</li>
<li>cutlass kernel 融合效果好</li>
<li>可以看一下 cutlass 发版历史，了解一下过程</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/v0.1.1/README.md">v0.1.1&#x2F;README.md</a><ul>
<li>thread block tile, warp tile, thread tile</li>
</ul>
</li>
<li>CUTLASS is a collection of CUDA C++ template abstractions for implementing high-performance <code>matrix-matrix multiplication (GEMM)</code> and <code>related computations</code> at all levels and scales within CUDA<ul>
<li><code>GEMM</code></li>
<li><code>related computations</code> 主要指的是接在 GEMM 后面的 activation 或者一些 pointwise 计算，比如 bias，scales，这些计算可以和 GEMM 融合在一起，从而减少访存压力，由于这类计算一般发生在 GEMM 之后，我们管这类计算叫 epilogue(中文：结语)</li>
</ul>
</li>
<li>大量使用模板以及各种各种手段，把尽可能多的事情放在编译期完成。</li>
<li>对用户暴露大量的实现策略，用户需要自己做出实现策略的选择。这是 CUTLASS 和 CUBLAS,CUDNN 的重要差别所在，CUBLAS 和 CUDNN 用户只需要描述计算问题，所有的策略选择在内部完成。这意味着从软件分层上来说，CUTLASS 可以是 CUBLAS 和 CUDNN 的 backend，换而言之，CUBLAS 和 CUDNN 可以是 CUTLASS 的用户</li>
<li>c++模板，只用包含头文件</li>
<li>CUDA Templates for Linear Algebra Subroutines and Solvers</li>
<li>CUTLASS 和 cuBLAS 都是与 NVIDIA GPUs 相关的库，专门用于高效地执行线性代数运算</li>
<li>大矩阵 GEMM 运算 CuTlass 可以显著提速,是目前 GPU 上最快的 GEMM 库。</li>
<li>它允许开发者使用模板元编程自定义和优化矩阵乘法（GEMM）等线性代数运算，更加灵活。</li>
<li>cuBLAS 提供了一个简单、标准的 BLAS 接口，易于使用，而 CUTLASS 提供了更多的定制性和灵活性，但需要更深入的理解和控制。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/functionality.md">缩写的含义</a><ul>
<li>N: Column Major Matrix</li>
<li>T: Row Major matrix</li>
<li>{N,T} x {N,T}: All combinations, i.e., NN, NT, TN, TT(blas 中约定 Normal(N)矩阵为列优先，T 表示 transpose，即对列优先的矩阵进行转置则为行优先)</li>
<li>f: floating point</li>
<li>s: signed int</li>
<li>b: bit</li>
<li>cf: complex float</li>
<li>bf16: bfloat16</li>
<li>tf32: tfloat32</li>
<li>Simt: Use Simt CUDA Core MMA(使用 cuda core)</li>
<li>TensorOp: Use Tensor Core MMA</li>
<li>SpTensorOp: Use Sparse Tensor Core MMA</li>
<li>WmmaTensorOp: Use WMMA abstraction to use Tensor Core MMA</li>
</ul>
</li>
<li><code>One of CUTLASS&#39;s design patterns is to define gemm argument objects that are constructible in host code and passed to kernels by value.</code> These may include pointers, strides, scalars, and other arguments needed by Gemm and its components.The benefits of this pattern are <code>(1.) a structured, composable strategy for passing host-constructible arguments to kernels and (2.) minimized initialization overhead on kernel entry.</code></li>
<li>group gemm 和 batch gemm 区别： group 是 batch 更通用的形式，允许有不同尺寸的矩阵，batch 则需要多个矩阵尺寸相同</li>
<li>三种 tile<ul>
<li>thread block tile(例如：128x128x32)</li>
<li>warp tile(例如：64x64x32)</li>
<li>mma tile(例如：16x8x16)</li>
</ul>
</li>
</ol>
<h2 id="cute-cuda-tensor-tools"><a href="#cute-cuda-tensor-tools" class="headerlink" title="cute (cuda tensor tools)"></a>cute (cuda tensor tools)</h2><ol>
<li>CUTLASS 对 stream-K GEMM 算法的实现，该算法使用了 CuTe 的许多特性。</li>
<li>One could summarize almost all CuTe use cases as follows:<ul>
<li>create Layouts,</li>
<li>create Tensors with those Layouts</li>
<li>invoke (either CuTe’s, or custom) algorithms on those Tensors.</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=PWWOGrLZtZg">youtube 介绍</a></li>
<li><code>Layout maps from coordinate space(s) to an index space</code>. 从坐标(逻辑存储)映射到索引(物理存储);</li>
<li>需要十分明确 coordinate 和 index 概念， 1D coordinate 和 index 是不同的</li>
<li>The map from a natural coordinate to an index is performed by taking the inner product of the natural coordinate with the Layout’s Stride. coordinate 和 stride 内积来实现到 index 的映射;</li>
<li><code>cute::crd2idx(c, shape, stride)</code>来计算 c(coordinate)的 index</li>
<li>直接根据 coordinate 顺序计算 index， 不要用想象， 用公式； 例如 A &#x3D; (2,2):(4,1); coordinate 顺序为 A(0) &#x3D; A(0, 0) &#x3D; 0; A(1) &#x3D; A(1, 0) &#x3D; 4x1 &#x3D; 4; A(2) &#x3D; A(0, 1) &#x3D; 1; A(3) &#x3D; A(1, 1) &#x3D; 5; 所以 A(c)为 0 4 1 5</li>
<li>Layout 对象调用,返回 index，layout(i)</li>
<li>The core abstraction of CuTe are the hierarchically multidimensional layouts which can be composed with data arrays to represent tensors.</li>
<li>tensor</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor = ptr + layout <span class="comment">## 根据coodinate得到元素value</span></span><br><span class="line">layout = shape + stride <span class="comment">## 根据coodinate得到index</span></span><br></pre></td></tr></table></figure>
<ol>
<li>ptr &lt;–&gt; coodinate &lt;–&gt; index, value</li>
<li><code>从右往左读</code> The map from an input coordinate to a natural coordinate is the application of a colexicographical order (reading right to left, instead of “lexicographical,” which reads left to right) within the Shape</li>
<li>shape and stride 是 IntTupple, A Layout is a tuple of (Shape, Stride).</li>
<li>CuTe 的 Tensor 类表示一个多维数组。数组的元素可以驻留在任何类型的内存中，包括<code>全局内存、共享内存和寄存器内存</code>。</li>
<li>Tensor 的行为完全由它的两个组件决定，这两个组件对应于它的两个模板参数： Engine 和 Layout 。Engine 表示元素的一维数组。当用户对 Tensor 执行数组访问时， Tensor 使用其 Layout 从逻辑坐标映射到一维索引。然后， Tensor 使用其 Engine 将一维索引映射到对该元素的引用。</li>
<li>tensor Ownership of the elements; Tensors can be owning or nonowning.<ul>
<li>Whether a Tensor is “owning” or “nonowning” depends entirely on its Engine.</li>
<li>ViewEngine 和 ConstViewEngine 包装指向各种内存的指针。</li>
<li>make_gmem_ptr(g) when g is a pointer to global memory, or make_smem_ptr(s) when s is a pointer to shared memory.</li>
</ul>
</li>
<li>print 会打印 tensor 存储空间所在的位置和 shape、stride 信息，而 print_tensor 除了以上信息还会打印 Tensor 中具体的每一个数值：</li>
<li>We wrap each MMA’s PTX instruction in an “Operation” struct.</li>
<li>For each Operation struct, we define a “Traits” struct that defines all of the meta-information needed to use the Operation. 对于每个操作结构，我们定义一个“Traits”结构，它定义使用该操作所需的所有信息。</li>
<li>mma ptx -&gt; mma_operation -&gt; mma_trait(封装各种信息) -&gt; mma_atom -&gt; tiled_mma -&gt; thr_mma -&gt; cute::gemm()</li>
<li>copy ptx -&gt; copy_operation -&gt; copy_trait -&gt; copy_atom -&gt; tiled_copy -&gt; thr_copy -&gt; cute::copy()</li>
<li>通过 Tensor 和 Layout 抽象我们可以实现对计算矩阵的分块；基于 Copy 抽象，我们可以完成块状矩阵 A、B 数据从 global 内存到寄存器的加载；通过 MMA 抽象我们可以利用 Tensor Core 完成寄存器上小块矩阵的乘法运算；再次通过 Copy 抽象，我们可以将寄存器上的结果拷贝到 global 内存，完成完整的 GEMM 运算</li>
<li>A has shape (M, K) and strides (1, ldA). Since A has stride 1 in the M mode, we say that A is <code>M major</code>. B has shape (N, K) and strides (1, ldB), so B is “N-major.” Similarly, C has shape (M, N) and strides (1, ldC), so C is “M major.”</li>
<li>layout 两个核心概念<ul>
<li>shape: define the coordinate mappings</li>
<li>stride: defines the index mappings, 在逻辑位置和物理（数据）做映射的时候每一个元素之间的差为 n</li>
</ul>
</li>
<li>thread layout and data layout</li>
<li>mapping correct threads to correct values for you computation</li>
<li>layout is a function</li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/column/c_1696937812497235968">CUDA 高性能编程</a></li>
<li>计算机中的内存是一维的线性地址空间，而数学计算问题所要处理的空间经常是高维的。如 GEMM（General Matrix Multiplication）问题的数学计算体系是二维计算空间，Deep Learning 计算体系是三维以上的计算空间(batch, height, width, channel, etc.)。如何高效的表达高维计算空间，如何高效便捷的将计算所要求的高维空间映射到一维空间变得越来越重要。历史上对该问题的探究可以分为三个阶段：<ul>
<li>第一阶段 BLAS 的 row&#x2F;col-major + leading dimension 描述阶段；</li>
<li>第二阶段 Tensor 的 shape + stride 阶段；</li>
<li>第三阶段为 Hierarchy Tensor 阶段。(组合设计模式实现？)</li>
</ul>
</li>
<li>引入有层次的描述（Layout）代数来表达计算空间和一维地址空间的映射问题。Layout 是一个数据排列的描述体系，其可以实现将<code>逻辑坐标</code>映射到<code>索引(物理)坐标</code>（offset 表示）</li>
<li>Tensor 是数据的表达，其表达一个相对独立且有结构的数据体，而 Tensor 内的数据排布则由 Layout 来表达</li>
<li>二维矩阵的描述和一维类似，shape 表示其逻辑形状，stride 表示具体的某个元素和物理空间的映射时的间隔量。</li>
<li>逻辑空间到物理空间的映射通过点积来完成。如 shape: (3,4); stride: (4, 1); cood: (2, 2), 物理 offset &#x3D; 2x4 + 2x1 &#x3D; 10</li>
<li>Layout 的本质是函数，其可以实现由一种坐标系统变换到一个表示偏移量的标量</li>
<li>Layout 的本质是函数，函数的本质是集合</li>
<li>((2, 4), (3, 5))，同样我们可以得到其 stride: ((3, 6), (1, 24)) <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/662089556">link</a><ul>
<li>(2, 4)表示行方向按 2 个一组，分 4 组</li>
<li>(3, 5)表示列方向按 3 个一组，分 5 组</li>
<li>(3, 6)表示行方向内层元素之间 stride 为 3，外层元素 stride 为 6</li>
<li>(1, 24)表示列方向内存元素之间 stride 为 1，外层元素 stride 为 24</li>
<li>行为：2x4 &#x3D; 8</li>
<li>列为：3x5 &#x3D; 15</li>
<li>(6, 4)表示：((6%2, 6&#x2F;2), (4%3, 4&#x2F;3)) &#x3D; ((0, 3), (1, 1))， offset &#x3D; 0<em>3 + 3</em>6 + 1<em>1 + 24</em>1 &#x3D; 43</li>
<li>((1, 3), (2, 4))位置如何找？在行上，先看 3，后看 1，确定外围 tensor 是第 4 个，内层 tensor 是第 1 个；列同理</li>
</ul>
</li>
<li>Layout 描述了数据的排列和底层存储位置关系，但 Layout 并没有指定存储。Tensor 就是在 Layout 的基础上包含了存储，即<code>Tensor = Layout + storage</code>, 数据存储的具体表现上可以是指针表达的数据或则是栈上数据（GPU 上表现为寄存器）</li>
<li>深度学习框架中的 Tensor 更强调数据的表达实体，通过 Tensor 实体与实体之间的计算产生新的 Tensor 实体，即<code>多份数据实体</code>，cute 中的 Tensor 更多的是对 Tensor 进行分解和组合等操作，而这些操作多是对 Layout 的变换（只是逻辑层面的数据组织形式），<code>底层的数据实体一般不变更</code>。</li>
<li>深度学习框架中的 Tensor 是用来表达数据实体，cute 中的 Tensor 是偏向描述的实体。</li>
<li>使用 Tensor 语义和工具能够更形象化的表达我们的逻辑，方便我们的思考，而 CUDA 的优化思路和技巧并不会因为 Tensor 的引入而变简单或困难。Tensor 只是工具，可以方便我们的表达，至于深层次的优化思路那还是对经验的挑战。</li>
<li>Tensor 表达虽然提供了很多方便，但也只限于表达的高效和便捷，如何对程序进行优化，Tensor 表达的引入并没有提供额外帮助，它仍然需要我们从别的途径来获得。即便如此，<code>表达和抽象依然无比重要</code>，那正如：伽罗瓦如果没有群这一表达工具，就难以解决多项式根的问题；杨振宁没有群这一工具就难以构思举世的杨-米尔斯理论。</li>
<li>抽象和工具让我们可以在更高的维度上思考。</li>
<li>cute 提供了 MMA 能力来完成 D &#x3D; A x B + C 的矩阵乘法运算，其针对指令封装，适配层，原子能力、块状 MMA、线程划分和执行进行了抽象，形成了 MMAOperation、MMA_Traits、MMA_Atom、TiledMMA、ThrMMA、cute::gemm 数据结构和函数，我们通过这些结构能够完成逻辑块状矩阵乘法的划分和执行。这些抽象通过软件分层设计使得各层次独立，我们不必关注底层细节，只需要从提供的模块中组合我们的逻辑即可，同时抽象的解偶设计，使得我们可以专注于顶层逻辑而降低对底层细节的要求。</li>
<li>swizzle 抽象， Layout 的作用是给定坐标返回 offset，而 swizzle 的作用则是给定 offset 返回 bank conflict free 的 offset。 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/671419093">link</a><ul>
<li>shared memory 读写都按逻辑坐标，真实物理坐标都是 bank free 的(太强了)</li>
<li>先从一维映射到逻辑二维，再从逻辑二维找到 bank free 二维，最后再返回一维</li>
<li>ibank &#x3D; irow ^ icol 真是妙啊</li>
<li>在做全局内存到共享内存数据搬运时，<code>思考模型是逻辑空间，而执行时需要考虑存储空间以避免bank conflict</code></li>
</ul>
</li>
<li><code>layout和swizzle都是一种抽象，可以使用逻辑的方式思考，具体存储交给映射关系</code></li>
<li><code>new layout language to describe coordinate and index bookkeepings; coordinate 是思考时用到的逻辑方式，具体的存储由 layout 映射关系来处理，可以将经历放在逻辑上</code></li>
<li><code>Any problem in computer science can be solved by another layer of indirection</code> 计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决;例如虚拟地址<ul>
<li>layout 和 swizzle 都可以看做是中间层或者代理</li>
</ul>
</li>
<li><code>loccal tile</code> 是 Tensor 中用户可以使用到的重要的函数，可以通过 tile 方法对 tensor 进行分块，通过 local_tile 可以实现从大的 tensor 中切取 tile 块，并且通过 coord 进行块的选取<ul>
<li>用于数据分块</li>
</ul>
</li>
<li><code>local partition</code>和 local tile 类似，现将大的 Tensor 按照 tile 大小进行分块，分块后每一块取出 coordinate 指定的元素拼拼成新的块<ul>
<li>用于给线程分数据</li>
</ul>
</li>
</ol>
<h2 id="cutlass-优化手段"><a href="#cutlass-优化手段" class="headerlink" title="cutlass 优化手段"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/677616101">cutlass 优化手段</a></h2><ol>
<li><code>bank conflict free的shared memory layout</code></li>
<li><code>thread block swizzle</code>：这个优化对于中大型矩阵乘法比较明显，更改了发射 block 的顺序，以增加 locality，从而提高 l2cache 的命中率，实现上非常简单，核心代码就是一个取余操作，但有用多级流水线(software pipeline)：2 条可以不要 async.copy 这个指令(sm80 才有的)，大于 2 条流水就需要了，原理上没什么，和 CPU 的多级流水一个道理，主要是指令的应用。</li>
<li><code>predicate iterator</code>：这个是一个软件层组件写法的优化，叫 predicate 的原因是，这个 iterator 会返回一个布尔值，在 gpu 的指令里是一个 special register，用来表示这块内存是不是需要 load，这个在软件层会涉及一些优化手段，比较有趣的是会在 host 侧 precompute 了哪些下标需要 load，用位运算来 mask，计算开销(位运算在 gpu 里开销较小)和存储开销(一个 byte 可以存 8 个 mask 值)都很小。为什么需要让存储开销很小？因为在 gpu 架构里，register 是很贵的，一个 thread 只能使用 255 个 register，如果超出了就会存在 local memory 里，register 读取很快，一个 cycle 就可以完成，而 local memory 就会慢非常多，register 用超了会非常非常影响性能！</li>
<li><code>shared memory重排搬出</code>：mma 指令计算完成之后，结果是存在 register 里的，且 register 中存储的数据是不连续的(32bits 连续)，原因是由于 mma 指令造成的，我们知道 vectorize load&#x2F;store 会提高访存带宽，所以我们可以在 shared memory 里重新排序，一并搬出。但并不是什么情况下重排都是正优化，因为重排还是会增加一次 shared memory store&#x2F;load，比如在小 channel 的 conv2d 中，直接从 register 搬出到 global memory 性能会更好</li>
<li><code>cooperative fetching和vectorize load</code>：这两个是 GPU 的一些基本优化方法，即尽量用更大的 data type 来搬运，以及尽量让一个 warp 里的不同线程是连续地访存同一块内存地址，原理可以参考</li>
<li><code>tiling description</code>: 提供了实例化方法，来调整 block 计算量和 warp 计算量，</li>
</ol>
<h2 id="type"><a href="#type" class="headerlink" title="type"></a>type</h2><ol>
<li><code>cutlass::half_t</code> 不支持 half</li>
<li><code>cutlass::bfloat16</code> Ampere ElementAccumulator 应该为 float, 看手册可以看到只支持 float32 的 accumulate <a target="_blank" rel="noopener" href="https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf">link</a><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu#L68">ampere_gemm_operand_reduction_fusion</a></li>
</ul>
</li>
</ol>
<h2 id="fusion"><a href="#fusion" class="headerlink" title="fusion"></a>fusion</h2><ol>
<li>为了减少核启动(launch)和内存访问的开销</li>
<li>It also removes kernel launch overhead 减少 kernel launch 开销</li>
<li>注意融合分两种：<ul>
<li>简单融合：简单将两个 kernel 合并成一个，并没有减少从主存加载数据次数，只是减少了 kernel 调度的开销</li>
<li>高效融合：利用 shared memory 减少从主存加载数据次数</li>
</ul>
</li>
<li>两个 gemm 融合， 关键在 k 维度， 假设 A@B@C， 分块时 A，B 矩阵滑动取数，结果放到 C 中，只能取一块，结果和 C 相乘的时候需要所有的分块矩阵</li>
</ol>
<h2 id="docs"><a href="#docs" class="headerlink" title="docs"></a>docs</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cutlass_3x_design.md">cutlass_3x_design</a><ul>
<li>CUTLASS 3.0 将其接口层与硬件分离，将它们集中在 GEMM 算法的自然结构周围，不依赖于任何特定的 GPU generation</li>
<li>CUTLASS 2.x 将 GEMM 操作的移动部分分解为一个层次结构，该层次结构密切反映了 GPU 架构的组织。然而，这种设计有时会导致耦合过于紧密，无法扩展到不适合同一架构层次结构的较新 GPU。</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/gemm_api_3x.md">gemm_api_3x</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/gemm_api.md">gemm_api</a><ul>
<li>包含层级结构</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/code_organization.md">代码组织</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/fundamental_types.md">数据类型</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/01_layout.md">CuTe layout</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md">efficient_gemm.md</a></li>
<li>最全流程图<br><img src="https://github.com/NVIDIA/cutlass/blob/main/media/images/gemm-structural-components.png" alt="gemm-structural-components"></li>
</ol>
<h3 id="build"><a href="#build" class="headerlink" title="build"></a>build</h3><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/quickstart.md">quickstart</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/quickstart.md#building-for-multiple-architectures">选择 arch</a></li>
<li>下不下来 google test, 修改 cmakelists.txt, 注释掉 gtest</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libgtest-dev</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/NVIDIA/cutlass.git</span><br><span class="line"><span class="built_in">export</span> CUDACXX=<span class="variable">$&#123;CUDA_INSTALL_PATH&#125;</span>/bin/nvcc</span><br><span class="line"><span class="built_in">export</span> PATH=/usr/local/cuda/bin/:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">mkdir</span> build &amp;&amp; <span class="built_in">cd</span> build</span><br><span class="line">cmake .. -DCUTLASS_NVCC_ARCHS=86</span><br><span class="line">make cutlass_profiler -j12</span><br><span class="line">./tools/profiler/cutlass_profiler --kernels=sgemm --m=4352 --n=4096 --k=4096</span><br></pre></td></tr></table></figure>

<h2 id="samples"><a href="#samples" class="headerlink" title="samples"></a>samples</h2><ol>
<li><p>00 basic gemm</p>
<ul>
<li>创建 cutlass::gemm::device::Gemm，CutlassGemm::Arguments 实例 gemm_operator 和 args</li>
<li>gemm_operator(args) launch kernel</li>
</ul>
</li>
<li><p>01 cutlass utilities</p>
<ul>
<li>HostTensor contributes management for both host and device memory. HostTensor allocates host and device memory upon construction. Basic element-wise operations on host memory synchronize device memory automatically.</li>
<li>cutlass::reference::host::Gemm</li>
<li>cutlass::reference::device::TensorFillRandomGaussian()</li>
<li>cutlass::reference::host::BlockFillSequential()</li>
<li>cutlass::reference::host::TensorEquals()</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/tree/main/examples/02_dump_reg_shmem">02_dump_reg_shmem</a></p>
<ul>
<li>Don’t use cuda-gdb to debug any heavy templated CUDA code. We just insert printf to dump the information to debug. We have some utilities to help you to do it. <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/issues/372#issuecomment-987532556">link</a></li>
</ul>
</li>
<li><p>41_fused_multi_head_attention</p>
</li>
<li><p>cute&#x2F;tutorial&#x2F;sgemm_nt_1.cu</p>
<ul>
<li>thread layout tA, tB 是读 AB 数据时线程 layout, tC 是计算 C 时的线程 layout</li>
<li>tAsA 搬 block tile 数据用， tCsA 计算 block tile 用</li>
<li>tCrC 每个线程算 8x8 数据， shared memory 中 A、B 数据也为 8x8，</li>
<li>使用 cuda core 计算，并没使用 tensor core</li>
<li>tAsA 表示 copy 时 thr_copy A 时 Shared memory 上的 thread 数据</li>
<li>tCsA, tCsB, tCrC 表示 mma 计算 thr_mma 用到的 A，B shared memory thread 数据和 C 寄存器数据; 提供线程号，则获得具体线程的数据划分能力，对给定的数据块进行划分，得到线程级的数据描述。</li>
<li>可以顺序赋值给 A, B, 类型改为 int, 来模仿各 tile layout</li>
<li>axpby:alpha x plus beta y <code>y = alpha*x + beta*y</code>; where x and y are vectors of n elements and alpha and beta are scalars.</li>
<li>axpy here is simply an abbreviation of alpha times x plus y.</li>
</ul>
</li>
</ol>
<h2 id="debug"><a href="#debug" class="headerlink" title="debug"></a>debug</h2><h3 id="cutlass-device-dump"><a href="#cutlass-device-dump" class="headerlink" title="cutlass device dump"></a><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/tools/util/include/cutlass/util/device_dump.h#L131">cutlass device dump</a></h3><ol>
<li>cutlass::debug::dump_shmem()<ul>
<li>每行 128B，bank: 4B x 32 &#x3D; 128B</li>
</ul>
</li>
<li>cutlass::debug::dump_fragment()</li>
</ol>
<h3 id="cute-tensor"><a href="#cute-tensor" class="headerlink" title="cute tensor"></a><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/include/cute/tensor.hpp#L985">cute tensor</a></h3><ol>
<li>cute::print(tensor)</li>
<li>cute::print_tensor(tensor); 可以打印各种维度</li>
<li>cute::print_tensor_os()</li>
<li>operator&lt;&lt;()</li>
<li>print 会打印 tensor 存储空间所在的位置和 shape、stride 信息，而 print_tensor 除了以上信息还会打印 Tensor 中具体的每一个数值：</li>
</ol>
<h3 id="cute-layout"><a href="#cute-layout" class="headerlink" title="cute layout"></a><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/include/cute/layout.hpp#L1694">cute layout</a></h3><ol>
<li>cute::print(layout): <code>print(layout.shape()); print(&quot;:&quot;); print(layout.stride());</code></li>
<li>cute::print_layout(layout)<ul>
<li>NOTE: 只能打印二维 layout; 代码会做检查 <code>CUTE_STATIC_ASSERT_V(rank(layout) == Int&lt;2&gt;&#123;&#125;);</code> 可以 slice 成二维的再打印</li>
</ul>
</li>
<li>cute::print_layout(tensor.layout());</li>
<li>operator&lt;&lt;(): <code> os &lt;&lt; shape(layout) &lt;&lt; &quot;:&quot; &lt;&lt; stride(layout);</code></li>
</ol>
<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=QLdUML5MCfE&t=42s">CUTLASS 3 0 Next Generation Composable and Reusable GPU Linear Algebra Library - TVMCon2023</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/">nvidia cutlass-linear-algebra-cuda</a></li>
<li><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3582016.3582018">Graphene: An IR for Optimized Tensor Computations on GPUs</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/column/c_1696937812497235968">CUDA 高性能编程</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/588953452">cutlass 源码导读（1）——API 与设计理念</a></li>
</ol>

    </div>

    
    
    
      

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>贾夕阳
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://jiaxiyang.github.io/2024/01/26/cutlass/" title="cutlass">https://jiaxiyang.github.io/2024/01/26/cutlass/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/01/25/OpenCL/" rel="prev" title="OpenCL">
      <i class="fa fa-chevron-left"></i> OpenCL
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/02/03/flashattention/" rel="next" title="flashattention">
      flashattention <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#note"><span class="nav-number">1.</span> <span class="nav-text">note</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cute-cuda-tensor-tools"><span class="nav-number">2.</span> <span class="nav-text">cute (cuda tensor tools)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cutlass-%E4%BC%98%E5%8C%96%E6%89%8B%E6%AE%B5"><span class="nav-number">3.</span> <span class="nav-text">cutlass 优化手段</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#type"><span class="nav-number">4.</span> <span class="nav-text">type</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fusion"><span class="nav-number">5.</span> <span class="nav-text">fusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#docs"><span class="nav-number">6.</span> <span class="nav-text">docs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#build"><span class="nav-number">6.1.</span> <span class="nav-text">build</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#samples"><span class="nav-number">7.</span> <span class="nav-text">samples</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#debug"><span class="nav-number">8.</span> <span class="nav-text">debug</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cutlass-device-dump"><span class="nav-number">8.1.</span> <span class="nav-text">cutlass device dump</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cute-tensor"><span class="nav-number">8.2.</span> <span class="nav-text">cute tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cute-layout"><span class="nav-number">8.3.</span> <span class="nav-text">cute layout</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#links"><span class="nav-number">9.</span> <span class="nav-text">links</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="贾夕阳"
      src="/images/coder2.jpg">
  <p class="site-author-name" itemprop="name">贾夕阳</p>
  <div class="site-description" itemprop="description">深度学习/自动驾驶/C++/性能优化</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">197</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jiaxiyang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jiaxiyang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2026/01/06/JIRA/" title="2026&#x2F;01&#x2F;06&#x2F;JIRA&#x2F;">JIRA</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2025/12/29/claude-code/" title="2025&#x2F;12&#x2F;29&#x2F;claude-code&#x2F;">Claude Code</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2025/08/20/AI-coding/" title="2025&#x2F;08&#x2F;20&#x2F;AI-coding&#x2F;">AI coding</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2025/04/28/Architecture/" title="2025&#x2F;04&#x2F;28&#x2F;Architecture&#x2F;">Computer Architecture</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2025/04/18/pytest/" title="2025&#x2F;04&#x2F;18&#x2F;pytest&#x2F;">pytest</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">贾夕阳</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">633k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">9:35</span>
</div>

<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("06/26/2020 14:52:10");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : '[object Object]',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://jiaxiyang.github.io/2024/01/26/cutlass/',]
      });
      });
  </script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'g32ipLmEye1u5l6wBGRJt03S-gzGzoHsz',
      appKey     : 'zHgLkAICsZUl9Mf8LfdoVigP',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

  

  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>





 
</body>
</html>

