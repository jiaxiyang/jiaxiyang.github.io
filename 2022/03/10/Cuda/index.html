<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0-rc2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jiaxiyang.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="Note GPU 有 108 个 SM。每个 SM 可以并行处理多个线程块，具体取决于所使用的内核；为了获得最佳并行化，隐式 GEMM 应包含 108 个图块的整数倍。 fp32, tf32, fp16, bf16, fp8 NVIDIA GPU 的缓存行（cache line）大小一般是 128 字节。 cache line 又分为 4 个 sectors, 1 个 sector 32B sec">
<meta property="og:type" content="article">
<meta property="og:title" content="Cuda">
<meta property="og:url" content="https://jiaxiyang.github.io/2022/03/10/Cuda/index.html">
<meta property="og:site_name" content="Xiyang">
<meta property="og:description" content="Note GPU 有 108 个 SM。每个 SM 可以并行处理多个线程块，具体取决于所使用的内核；为了获得最佳并行化，隐式 GEMM 应包含 108 个图块的整数倍。 fp32, tf32, fp16, bf16, fp8 NVIDIA GPU 的缓存行（cache line）大小一般是 128 字节。 cache line 又分为 4 个 sectors, 1 个 sector 32B sec">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.ibb.co/vdTGHkJ/txy-EFVOs5-P.png">
<meta property="og:image" content="https://i.ibb.co/2qN87rv/QVJcq2r-QQ3.png">
<meta property="article:published_time" content="2022-03-09T18:17:38.000Z">
<meta property="article:modified_time" content="2024-02-26T08:58:59.249Z">
<meta property="article:author" content="贾夕阳">
<meta property="article:tag" content="Cuda">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.ibb.co/vdTGHkJ/txy-EFVOs5-P.png">

<link rel="canonical" href="https://jiaxiyang.github.io/2022/03/10/Cuda/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Cuda | Xiyang</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WGS6S6YFJ6"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-WGS6S6YFJ6');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Xiyang</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Think twice, code once!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">175</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">44</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">55</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/jiaxiyang" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2022/03/10/Cuda/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Cuda
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-10 02:17:38" itemprop="dateCreated datePublished" datetime="2022-03-10T02:17:38+08:00">2022-03-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-02-26 16:58:59" itemprop="dateModified" datetime="2024-02-26T16:58:59+08:00">2024-02-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Program/" itemprop="url" rel="index"><span itemprop="name">Program</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Program/Cuda/" itemprop="url" rel="index"><span itemprop="name">Cuda</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/03/10/Cuda/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/03/10/Cuda/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>24k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>22 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><ol>
<li>GPU 有 108 个 SM。每个 SM 可以并行处理多个线程块，具体取决于所使用的内核；为了获得最佳并行化，隐式 GEMM 应包含 108 个图块的整数倍。</li>
<li>fp32, tf32, fp16, bf16, fp8<br><img src="https://i.ibb.co/vdTGHkJ/txy-EFVOs5-P.png" alt="data type"></li>
<li>NVIDIA GPU 的缓存行（cache line）大小一般是 128 字节。</li>
<li>cache line 又分为 4 个 sectors, 1 个 sector 32B</li>
<li>sector 是 global memory 访问的最小单位，32 个线程一起运行，最少访问 32B</li>
<li>gridDim 划分的是数据，blockDim 划分的是线程：例如: sgemm 分块矩阵乘</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">N=M=<span class="number">1024</span>;</span><br><span class="line">BN=BM=<span class="number">128</span>, TN=TB=<span class="number">8</span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">blockDim</span><span class="params">(BN / TN, BM / TM)</span></span>;  <span class="comment">// block 按线程分为 16x16 thread,threadDim=(16, 16) 每个线程处理8x8数据; x维度是列，y维度是行;</span></span><br><span class="line"><span class="function">dim3 <span class="title">gridDim</span><span class="params">((N + BN - <span class="number">1</span>) / BN, (M + BM - <span class="number">1</span>) / BM)</span></span>; <span class="comment">// grid 按数据分为 8x8 block, blockDim=(8, 8)</span></span><br></pre></td></tr></table></figure>

<ol>
<li><code>GPU通过计算而不是深度缓存层次结构来隐藏访存延迟</code><ul>
<li>当数据访存的时候，就让 warp stall，而后再选一个 warp 进行计算，通过这种方式交错开计算和访存，让访存单元一直忙碌，带宽打满。计算延迟的时候也会让 warp stall</li>
</ul>
</li>
<li>并发的 warp 切换没有开销，因为每个 warp 都分配了硬件资源<ul>
<li>不需要上下文切换</li>
<li>以空间换时间</li>
</ul>
</li>
<li>gpu sram 带宽为什么比主存大很多：<ul>
<li>并行，多个 SM 访问 shared memory， 如有 80 个 sm，每个 sm4 个 partition， 每个 shared memory 有 32 个 bank， 等效位宽为：<code>80*4*32*8 = 81920</code>, hbm 位宽为 8192， 带宽为 1.5TB&#x2F;s, sram 为 19TB&#x2F;s</li>
<li>shared memory 带宽：<code>2(?) * freq * 32(banks) * 8/8 *4(sm partion) * 80(sm num)</code></li>
</ul>
</li>
<li><code>kernel融合主要是使用sram来减少对显存的访问，注意不是device和host数据搬移</code> 也会减少调度开销</li>
<li>kernel 在编译的时候需要明确 block grid size 吗？不需要， 这些参数通常在运行时通过 CUDA 内核启动语法指定，这提供了更高的灵活性和动态调整的可能性。</li>
<li>nvcc 在翻译单元的顶部隐式包含了 cuda_runtime.h 。</li>
<li>核函数 K（kernel function）就是指 K(x, y) &#x3D; ，其中 x 和 y 是 n 维的输入值，f(·) 是从 n 维到 m 维的映射（通常而言，m&gt;&gt;n）。是 x 和 y 的内积（inner product），严格来说 应该叫欧式空间的标准内积，也就是很多人常说的点积（dot product）。</li>
<li><code>sudo update-alternatives --display cuda</code>显示系统 cuda 版本</li>
<li><code>export PATH=/usr/local/cuda/bin/:$&#123;PATH&#125;</code>找不到 nvcc 可能需要 export PATH</li>
<li><code>sudo apt install -y nvidia-cuda-toolkit</code> 安装 cuda 工具链, 不能乱用，需要和系统 cuda 匹配</li>
<li>在 CUDA 中，你会以类似于 C&#x2F;C++函数的形式来表达想要在 GPU 上运行的计算，这个函数被称为 kernel。</li>
<li>GPU 函数耗时统计不能只记录一次的，GPU 可能做一些准备工作，教训： nppiResize_8u_C3R 不管大小第一次运行耗时都很大 The cuda context is lazily initialized</li>
<li><code>autotuning</code> 搜索 kernel grid 划分参数(结果不变)，找性能最优</li>
<li>L1 和 shared memory 共享一块存储 可动态分配比例; 可用比例见<a target="_blank" rel="noopener" href="https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf">link</a><ul>
<li>多用 shared memory 就多分点给 shared memory 多用寄存器就多分点给 L1 cache</li>
</ul>
</li>
<li>warp 调度:延时隐藏 只执行 warp 一部分 当 warp 需要等待时 先执行其他 warp</li>
<li>shared memory 和 register 是 SM 中的稀缺资源.</li>
<li>warp 执行类似 simd</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/2023.3/ProfilingGuide/index.html#metrics-hw-model">Hardware Model</a><ul>
<li>对 sm 介绍比较好</li>
<li>Each SM is partitioned into four processing blocks, called SM sub partitions.</li>
<li>一个 SM 又可以由若干个 SMP（SM Partition）组成</li>
<li>A warp is allocated to a sub partition and resides on the sub partition from launch to completion. warp 被分配给子分区，并且从启动到完成都驻留在子分区上。</li>
</ul>
</li>
<li><code>FMA (Fused Multiply Add)</code>: z&#x3D;a*x+y …z,x,y are vectors or scalars</li>
<li><code>4FMA (Quad FMA)</code>: z&#x3D;A*x+z …A is a FP32 matrix; x,z are vectors</li>
<li><code>WMMA: Warp-level Matrix Mulitply and Accumulate (Tensor Core)</code>: Z&#x3D;AB+C …A,B are FP16 matrices; Z,C are FP32</li>
<li>global memory -&gt; shared memory 时，计算每个线程需要搬移的数据量。假如每个线程要搬移 A，B 矩阵为 4 个 single float point, 4x4 &#x3D; 16B<ul>
<li>A_tile（128,8): 256 个线程， 每个线程搬移 4 个数，每行由 2 个线程处理， 假如线程一维索引为 tid； 行 m &#x3D; tid &#x2F; 2 &#x3D; tid &gt;&gt; 1; 列 k &#x3D; (tid % 2) x 4 &#x3D; (tid &amp; 1) &lt;&lt; 2</li>
<li>B_tile（8,128): 256 个线程， 每个线程搬移 4 个数，每行由 32 个线程处理， 假如线程一维索引为 tid； 行 k &#x3D; tid &#x2F; 32 &#x3D; tid &gt;&gt; 5; 列 n &#x3D; (tid % 32) x 4 &#x3D; (tid &amp; 31) &lt;&lt; 2</li>
</ul>
</li>
<li>load&#x2F;store 关键点在行列索引(分别计算各 tile 维度的 m, n, k)， 通过行列可以找到起始地址。<code>#define OFFSET(row, col, ld) ((row) * (ld) + (col))</code></li>
<li>load&#x2F;store memory 的时候先通过行列坐标找到 global 起始点，然后再算偏移，用宏定义来访问矩阵，先找最外层的 tile，再找里层的 tile。</li>
<li>sgemm 分块 A 矩阵 global 到 shared load 时，shared memory 需要列优先排列，一次 load 四个数都寄存器，然后再分别赋值给列。</li>
<li>shred memory double buffering: 在 for 循环里只用 sync 一次<ul>
<li>load tile[0] 到 buffer[0]; sync</li>
<li>for: load tile[i]; compute tile[i-1]; sync</li>
<li>comute tile[-1]; sync</li>
</ul>
</li>
<li>wave 的概念：wave 表示 GPU 上同时执行的 thread block。例如一个 kernel 中 thread block 为 256 线程，每个线程使用了 128 个寄存器，那么在 GV100 上每个 SM 可同时执行 2 个 thread block，GV100 共 80 个 SM，一个 wave 就是 160 个 thread block。</li>
<li>cuda 遵循 IEEE 754 standard for binary floating-point representation； 由于融合和乘加， 其结果与分别乘加结果略有不同 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#ieee-754-compliance">ieee-754-compliance</a><ul>
<li>One of the key differences is the fused multiply-add (FMA) instruction, which combines multiply-add operations into a single instruction execution. <code>Its result will often differ slightly from results obtained by doing the two operations separately.</code></li>
</ul>
</li>
<li>每个浮点算术运算都涉及一定量的舍入。因此，算术运算的执行顺序很重要。如果 A、B 和 C 是浮点值，则 (A+B)+C 不能保证等于 A+(B+C)</li>
<li>我们可以将 CUDA 内核编写为许多短 <code>__device__</code> 函数的集合，而不是一个大型的整体 <code>__global__</code> 函数；每个设备功能可以在将它们连接在一起之前进行独立测试。</li>
<li>如果大多数函数都定义为 <code>__host__ __device__</code> 而不仅仅是 <code>__device__</code> 函数，那么这些函数就可以在 CPU 和 GPU 上进行测试，</li>
<li><code>local memory</code>之所以如此命名，是因为它的作用域是线程本地的，而不是因为它的物理位置。事实上，本地存储器位于片外。因此，访问本地内存与访问全局内存一样昂贵。换句话说，名称中的“本地”一词并不意味着访问速度更快。本地内存仅用于保存自动变量。当 nvcc 编译器确定没有足够的寄存器空间来保存变量时，就会执行此操作。可能放置在本地内存中的自动变量是大型结构或数组，它们会消耗太多寄存器空间，并且编译器确定可以动态索引的数组。</li>
<li><code>constent memory</code></li>
<li><code>The NVIDIA Management Library (NVML)</code> is a C-based interface that provides direct access to the queries and commands exposed via nvidia-smi intended as a platform for building 3rd-party system management applications.</li>
<li>可以通过 CUDA_VISIBLE_DEVICES 环境变量重新排列已安装的 CUDA 设备的集合; <code>CUDA_VISIBLE_DEVICES=0,2,1,3</code></li>
<li>Starting with CUDA 11.0, devices of compute capability 8.0 and above have the capability to influence persistence of data in the L2 cache, potentially providing higher bandwidth and lower latency accesses to global memory.</li>
<li>从 Hopper 开始，CUTLASS 3.0 将 Warp Specialization 的概念纳入了内核设计的一部分。线程块被划分为两组 warp，生产者 warp 组和消费者 warp 组。生产者 warp 组使用新的张量内存加速器（TMA）将数据从全局内存加载到共享内存缓冲区中。<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#warp-specialization">link</a></li>
<li>TMA 可以异步一次 load 大块数据到 shared memory， ampere 一次最多只能 load 128 bit 数据(指令限制)</li>
<li>想象 shared memory 和 register 是二维的<ul>
<li>shared memory: <code>smem[n][32]</code></li>
<li>register: <code>re[n][4]</code></li>
</ul>
</li>
<li>虽然 block 可以划分为二维 thread, 单调度时按一维调度，每 32 个一个 warp</li>
<li>lanID 是 warp 内第几个线程 threadIdx.x %32</li>
<li>WarpID &#x3D; threadIdx.x &#x2F;32</li>
<li>常量内存位于显存中 片上有缓存 类似于 shared memeory warp 内多个线程读同一个地址最快，不同地址要串行</li>
<li>常量内存对于 kernel 是只读的 对于主机可读写</li>
<li>constant 由主机代码准备 kernel 中直接用</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">__constant__ <span class="type">float</span> coef[<span class="number">100</span>];</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">setup_coef_constant</span><span class="params">(<span class="type">void</span>)</span></span>&#123;</span><br><span class="line">    <span class="built_in">cudaMemorycpyToSymbol</span>(coef, h_coef, size);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li>kernel 第三个参数是动态共享内存的大小</li>
</ol>
<h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><ol>
<li>想办法喂饱硬件, 好好调教它，让它努力打工, 工作饱和，不能偷懒</li>
<li>分析计算部件停顿的原因。</li>
<li>主要由下列因素决定：<ul>
<li>算力(peak)</li>
<li>带宽(peak)</li>
<li>指令和访存延迟</li>
</ul>
</li>
<li>性能 roofline bound 是满流水线时分析，一直忙; 指令和访存延迟会使得处理器空闲</li>
<li>一直忙条件：<code>warp 数量 = 延迟 x 吞吐</code>, 如：延迟 20 cycle; SM 吞吐为 64 fma 每 cycle；SM warp 数量为： 20 x 64 &#x2F; 32 &#x3D; 40</li>
<li>Use peak performance metrics to guide optimization</li>
<li>Optimize your algorithm, then unroll loops</li>
<li>Use template parameters to generate optimal code</li>
<li>bandwidth(带宽)和 throughput(吞吐)区别：<ul>
<li>bandwitdh: 理论最大吞吐</li>
<li>throughput: 实际吞吐</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://developer.download.nvidia.com/GTC/PDF/1083_Wang.pdf">Fundamental Optimizations in CUDA</a></li>
<li>kernels are too small -&gt; kernel launch bound; gpu is idle in many time, reason:kernels are too small <a target="_blank" rel="noopener" href="https://github.com/jiaxiyang/CUDA-PPT/blob/main/GTC2020/s21417-faster-transformer.pdf">link</a><ul>
<li>a simple solution: using tensorflow XLA to fuse kernel automaticlly</li>
</ul>
</li>
<li>在 CUDA 编程中，<code>谓词替换</code>通常指的是一种编程技巧，它通过使用谓词（即条件表达式）来代替显式的分支语句（如 if-else 或 switch 语句）。这种技巧可以帮助<code>减少线程发散</code>，从而提高在 GPU 上的并行执行效率。使用条件运算符（如? :）或逻辑运算符（如&amp;&amp;和||）来替换 if-else 语句。这样可以保证所有线程执行相同数量的指令，尽管这些指令的实际作用可能因条件而异。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">__global__ void traditionalBranch(int *data, int value, int threshold) &#123;</span><br><span class="line">    int index = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    if (data[index] &gt; threshold) &#123;</span><br><span class="line">        data[index] = value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__global__ void predicateReplacement(int *data, int value, int threshold) &#123;</span><br><span class="line">    int index = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    data[index] = (data[index] &gt; threshold) * value + !(data[index] &gt; threshold) * data[index];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li>cache locality 表示程序对缓存中数据的访问 locality 和重用情况。如果同一个 warp 中的线程迭代访问同一缓存线(cache line)上的数据(比如遍历一个数组),那么可以最大化利用 cache,称之为良好的 cache locality。</li>
</ol>
<h3 id="papers"><a href="#papers" class="headerlink" title="papers"></a>papers</h3><ol>
<li><a target="_blank" rel="noopener" href="http://impact.crhc.illinois.edu/shared/papers/optimization2008.pdf">Optimization Principles and Application Performance Evaluation</a></li>
</ol>
<h3 id="cuda-performance-guidelines"><a href="#cuda-performance-guidelines" class="headerlink" title="cuda performance-guidelines"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#performance-guidelines">cuda performance-guidelines</a></h3><ol>
<li><code>整体优化策略</code> Overall Performance Optimization Strategies；Performance optimization revolves around four basic strategies:<ul>
<li>Maximize parallel execution to achieve maximum utilization; 最大化并行执行以达到最大利用率；</li>
<li>Optimize memory usage to achieve maximum memory throughput; 优化存储使用，实现最大存储吞吐量；</li>
<li>Optimize instruction usage to achieve maximum instruction throughput; 优化指令使用，实现最大指令吞吐量；</li>
<li>Minimize memory thrashing. 最大限度地减少内存抖动。(减少内存申请释放频率)</li>
</ul>
</li>
<li>Which strategies will yield the best performance gain for a particular portion of an application depends on the performance limiters for that portion;根据情况使用哪种策略，比如 reduction 时受 memory bound，因此我们应该争取峰值带宽</li>
<li>compute-bound 需要争取达到最大算力，memory bound 需要争取达到最大带宽</li>
</ol>
<h3 id="优化技术"><a href="#优化技术" class="headerlink" title="优化技术"></a>优化技术</h3><ol>
<li>memory coalescing<ul>
<li>让连续的线程访问连续的内存地址。这样就有合并机会，多个线程间会合并访存，可能并不是 32 个线程都合并, 比如一个线程访问 4 个 32bit float 数据，8 个线程可合并，8 x 4 x 32 &#x2F; 8 &#x3D; 128Bx</li>
</ul>
</li>
<li>swizzle</li>
<li>bank conflict</li>
<li>分支优化<ul>
<li>谓词替换</li>
<li>分支预测</li>
<li>循环展开</li>
</ul>
</li>
</ol>
<h3 id="访存优化"><a href="#访存优化" class="headerlink" title="访存优化"></a>访存优化</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/600489819">漫谈高性能计算与性能优化：访存</a></li>
<li>访存优化是第一性原理</li>
<li>当我们在说访存优化的时候，我们具体需要做些什么。总的来说，就是三板斧。<ul>
<li>减少数据搬运<ul>
<li>如何减少数据搬运，最主要的手段就是分块，或者说 tiling。</li>
<li>要尽可能地保证数据连续访问，其中最主要的一个原因就是提高 cache 命中率，从而避免不必要的数据搬运</li>
</ul>
</li>
<li>减少数据访存延时(数据搬运确定)<ul>
<li>减少 bank 冲突</li>
<li>pipeline (double buffer, 预取)</li>
<li>就是切分更多的块，启动更多的 warp 来掩盖访存延时。</li>
</ul>
</li>
<li>保证负载均衡。<ul>
<li>关于负载均衡的话题，主要是在 sparse 里面谈的比较多</li>
</ul>
</li>
</ul>
</li>
<li>如果发现实际带宽比较差，数据搬运效率比较低，这个时候就要去思考，是不是可以有办法，通过分块的一些技巧来减少数据搬运。如果数据搬运不能够再减少了的话，是否可以通过一些方式来提高数据的搬运效率，比如向量化访存、合并访问来提高对 DRAM 的访存性能、避免 bank 冲突来提高对 shared memory 的访存性能、调整分块大小来让更多的 warp 跑起来从而减少访存的延时，如果不是 SIMT 架构，就需要精细地设计各级访存的 pipeline，让访存操作尽可能地 pingpong 起来，从而让访存流水尽可能地连续起来不要被打断。理论大概是这样，但是每一个问题都有着不同的处理方式，每一个问题可能都是不同的瓶颈。总之就是万变不离其宗，准确地评估每一级存储的访存效率然后尽可能地提高每一级的访存效率，尽可能地把访存流水打满，不要有空跑。</li>
<li>其实所谓“加速”或者“性能优化”的本质就是让软件充分利用计算硬件，提升利用率，从而逼近理论性能上限。从这个角度，“通用方法”就是：<code>分析计算部件停顿的原因-选择合理的计算模型减少数据依赖和对流水线的破坏(能兼顾缓解访存墙更好)-通过专用硬件或者结构优化消除剩下的瓶颈，然后不断迭代上述过程，直至各方面因素达到平衡</code>。</li>
<li>cuda gemm 为什么是三级分块，不是四级或者两级。因为 NV 的 GPU 内存结构是三级的，global mem-&gt;shared mem，shared mem-&gt;register。</li>
</ol>
<h3 id="reduction-优化"><a href="#reduction-优化" class="headerlink" title="reduction 优化"></a>reduction 优化</h3><ol>
<li>(great)<a target="_blank" rel="noopener" href="https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf">Optimizing Parallel Reduction in CUDA</a><ul>
<li>Memory coalescing</li>
<li>Divergent branching</li>
<li>Bank conflicts</li>
<li>Latency hiding</li>
</ul>
</li>
<li><code>What is Our Optimization Goal?</code> 先确定最大目标 We should strive to reach GPU peak performance Choose the right metric:<ul>
<li>GFLOP&#x2F;s: for compute-bound kernels</li>
<li>Bandwidth: for memory-bound kernels</li>
</ul>
</li>
<li>通过不断迭代优化，从而达到硬件最优性能。</li>
<li>Reductions have very low arithmetic intensity; Therefore we should strive for <code>peak bandwidth</code></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/426978026">中文解析</a></li>
</ol>
<h3 id="elementwise-优化"><a href="#elementwise-优化" class="headerlink" title="elementwise 优化"></a>elementwise 优化</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/488601925">elementwise 优化</a></li>
</ol>
<h2 id="NVCC"><a href="#NVCC" class="headerlink" title="NVCC"></a>NVCC</h2><ol>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html">NVIDIA CUDA Compiler Driver NVCC</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#supported-input-file-suffixes">编译过程中各类型文件作用说明</a></li>
<li><a target="_blank" rel="noopener" href="https://www.linmao.dev/joy/1165/">CUDA 编译过程</a></li>
</ol>
<h2 id="PTX-Parallel-Thread-Execution"><a href="#PTX-Parallel-Thread-Execution" class="headerlink" title="PTX(Parallel Thread Execution)"></a>PTX(Parallel Thread Execution)</h2><ol>
<li>a low-level parallel thread execution virtual machine and instruction set architecture (ISA)</li>
<li>PTX 是上承 GPU 编程语言 CUDA C++，下启 GPU 硬件 SASS 指令，可以借助 NVRTC 实现运行时优化，某些层面上来说可以称之为 GPU 设备无关代码，因此 PTX 可以理解为<code>CUDA IR</code></li>
<li>PTX 独立于特定 GPU 架构,可以重用相同的代码适用于不同的 GPU 架构,相当于前端</li>
<li>使用虚拟架构生成 PTX 中间文件，虚拟框架由<code>compute_</code>开头。虚拟架构通常是从大的 GPU 代上控制的，真实框架必须大于等于虚拟框架，真实框架对应真正运行的 GPU，即编译阶段就确定要运行的 GPU 是什么。真实框架由<code>sm_</code>开头。</li>
<li><code>nvcc -ptx program.cu -o _program.ptx -arch=sm_86</code></li>
<li><code>cat program.ptx | cu++filt &gt; program_demangle.ptx</code> demangle ptx</li>
</ol>
<h2 id="SASS-Shader-Assembly"><a href="#SASS-Shader-Assembly" class="headerlink" title="SASS(Shader-Assembly)"></a>SASS(Shader-Assembly)</h2><ol>
<li>真正的机器汇编，由 cubin 文件经过 cuobjdump 工具转换而来。目前没有官方的 sass to cubin 的工具。</li>
<li>cuobjdump 可以用来分析 cubin 文件和 host 文件。而 nvdisasm 只能用来分析 cubin 文件，但是可以得到更多的输出信息。我用的比较多的是 nvdisasm。用来看代码的控制流图。</li>
<li>只有官方反汇编器，没有官方汇编器</li>
<li>generate</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ptxas -arch=sm_86 mma_gemm_demangle.ptx -o mma_gemm.cubin</span><br><span class="line">cuobjdump -sass mma_gemm.cubin &gt; mma_gemm.sass</span><br></pre></td></tr></table></figure>

<h2 id="sync"><a href="#sync" class="headerlink" title="sync"></a>sync</h2><ol>
<li><code>block level</code>: <code>__syncthreads()</code></li>
<li><code>grid level</code>: <code>__threadfence()</code></li>
<li><code>stream level</code>: <code>cudaStreamSynchronize(cudaStream_t stream)</code><ul>
<li>这个函数会阻塞 CPU 线程，直到特定的 CUDA stream 中的所有操作完成。</li>
</ul>
</li>
<li><code>device level</code>: <code>cudaDeviceSynchronize()</code></li>
</ol>
<h2 id="bank-conflict"><a href="#bank-conflict" class="headerlink" title="bank conflict"></a>bank conflict</h2><ol>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/3842483/23011500">概念</a></li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Bank    |      1      |      2      |      3      |     ...     |      16     |</span><br><span class="line">Address |  0  1  2  3 |  4  5  6  7 |  8  9 10 11 |     ...     | 60 61 62 63 |</span><br><span class="line">Address | 64 65 66 67 | 68 69 70 71 | 72 73 74 75 |     ...     |     ...     |</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<ol>
<li>Each bank has a bandwidth of 32 bits per clock cycle</li>
<li>shared memory 一次不冲突只能读 32x4B， 看一次读取的数据量，多于 32x4B 需要读取多次 n&#x2F;(32x4)，只要在 n&#x2F;(32x4) 次内读完就是最高效的。</li>
<li>使用 float4 类型访存，用向量化的 LDG.128 和 STG.128 指令一次读 4 个元素，以减少访存指令数， 提高计算访存比</li>
<li>thread tile 时如果 TM 是 8，一个 warp 需要读 32*8 个 TM 的数(shared memory 上)，至少需要 8 次(如果有 32 个 bank)<ul>
<li>如果一个线程一次处理 8 个连续的数，一个 warp 一次只有 4 个线程不 bank 冲突，<ul>
<li>如果一个线程 1 次读 1 个数，一个 warp 一次只读 4 个数， 需要 64 次读完，</li>
<li>如果一个线程一次读 4 个数，一个 warp 一次读 16 个数， 需要读 16 次，</li>
<li>如果一个线程一次读 8 个数，一个 warp 一次读 32 个数，需要读 8 次（最优），但不存在一次读 8 个数的指令。</li>
</ul>
</li>
<li>如果一个线程一次操作 4 个连续的数(处理两个在空间上属于同一 bank 的数)， 那么一个 warp 一次有 8 个线程 bank 不冲突， 一次操作 4*8 个数， 需要 8 次能读完。与 bank 不冲突等效。</li>
</ul>
</li>
<li>这种情况也可以看作是：shared memory 基本单元为 16byte，总 bank 数为 8，冲突与否的分析不在是 32 线程，而变成 4 个 phase 中的不同线程。如果采用 64bit 的访问形式，则相应的基本单元可以看作是 8byte，总 bank 数目为 16，冲突与否的条件变成两个 phase 内的线程是否冲突。</li>
<li>4x32 or 8x16 or 16x8 (16B, 8bank)</li>
</ol>
<h2 id="关键字"><a href="#关键字" class="headerlink" title="关键字"></a>关键字</h2><ol>
<li><code>__restrict__</code> 用于限定指针,表示该指针是唯一访问目标内存的途径。可以避免出现不同指针引用同一内存区域的情况,编译器可以更自由地进行优化。这意味着编译器可以假设这个指针没有别名（alias），即没有其他指针指向相同的内存位置。</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">myKernel</span><span class="params">(<span class="type">float</span>* __restrict__ ptrA, <span class="type">float</span>* __restrict__ ptrB, <span class="type">int</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 内核代码，假设 ptrA 和 ptrB 指向不重叠的内存区域</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="occupancy-设计"><a href="#occupancy-设计" class="headerlink" title="occupancy 设计"></a>occupancy 设计</h2><ol>
<li>(重要) <code>最理想的情况下为 sm max threads 个 thread 都分配资源，占用率 100%， 但受限于 Max warps or max blocks per SM, registers per SM, shared memory per SM, register, 占用率可能不到 100%</code></li>
<li>想得到令人满意的 GPU 性能<code>关键是找到合适的group size与资源的平衡点</code></li>
<li><code>--ptxas-options=-v</code> or <code>-Xptxas -v</code> or <code>--resource-usage</code> 加上编译选项， 显示 register， shared memory 使用<ul>
<li><code>nvcc  -Xptxas=&quot;-v&quot; --ptxas-options=-v -O3 -o my_sgemm my_sgemm.cu -lcublas  2&gt;&amp;1 | c++filt</code> demangle</li>
</ul>
</li>
<li>nishgt compute 里 occupancy 有详细显示</li>
<li><a target="_blank" rel="noopener" href="https://xmartlabs.github.io/cuda-calculator/">cuda-calculator</a> 填入数值，计算 occupancy</li>
<li>注意 shared memory 是动态配置的，可以尝试改变 shared memory 大小来提升性能</li>
</ol>
<h2 id="stream"><a href="#stream" class="headerlink" title="stream"></a>stream</h2><ol>
<li>stream 主要为了隐藏 host device 之间数据搬移的延迟，不是内存和计算</li>
<li>CUDA 流表示一个 GPU 操作队列，该队列中的操作将以添加到流中的先后顺序而依次执行。</li>
<li>stream 作用：在 Stream 的帮助下，CUDA 程序可以有效地将内存读取和数值运算并行，从而提升数据的吞吐量。 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/51402722">link</a><br>由于 GPU 和 CPU 不能直接读取对方的内存，CUDA 程序一般会有一下三个步骤：1）将数据从 CPU 内存转移到 GPU 内存，2）GPU 进行运算并将结果保存在 GPU 内存，3）将结果从 GPU 内存拷贝到 CPU 内存。</li>
<li>cuda7 可以开启每个线程有一个默认 stream, 之前每个设备有一个 stream <a target="_blank" rel="noopener" href="https://developer.nvidia.com/zh-cn/blog/gpu-pro-tip-cuda-7-streams-simplify-concurrency/">gpu-pro-tip-cuda-7-streams-simplify-concurrency&#x2F;</a><ul>
<li><code>nvcc --default-stream per-thread ./pthread_test.cu -o pthreads_per_thread</code>需要加编译选项</li>
</ul>
</li>
</ol>
<h2 id="cuda-grammer"><a href="#cuda-grammer" class="headerlink" title="cuda grammer"></a>cuda grammer</h2><ol>
<li>所有的 kernel 函数返回类型都是 void</li>
<li><code>&lt;&lt;&lt; M , T &gt;&gt;&gt;</code> Which indicate that a kernel launches with a grid of M thread blocks. Each thread block has T parallel threads.</li>
<li><code>vectorAdd&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;</code></li>
<li>可以认为 M, T 对应图片的 H, W; 一个 thread 对应一个像素点；一个 block 对应一行，一个 grid 对应一张图片</li>
<li><code>int tid = blockIdx.x * blockDim.x + threadIdx.x; =&gt; int index = h * W + w;</code></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cuda-samples/blob/master/Common/helper_cuda.h#L595">checkCudaErrors</a> helper_cuda.h</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">check</span><span class="params">(T result, <span class="type">char</span> <span class="type">const</span> *<span class="type">const</span> func, <span class="type">const</span> <span class="type">char</span> *<span class="type">const</span> file,</span></span></span><br><span class="line"><span class="params"><span class="function">           <span class="type">int</span> <span class="type">const</span> line)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (result) &#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(stderr, <span class="string">&quot;CUDA error at %s:%d code=%d(%s) \&quot;%s\&quot; \n&quot;</span>, file, line,</span><br><span class="line">            <span class="built_in">static_cast</span>&lt;<span class="type">unsigned</span> <span class="type">int</span>&gt;(result), _cudaGetErrorEnum(result), func);</span><br><span class="line">    <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// This will output the proper CUDA error strings in the event</span></span><br><span class="line"><span class="comment">// that a CUDA host call returns an error</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> checkCudaErrors(val) check((val), #val, __FILE__, __LINE__)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol>
<li>cutil.h NVIDIA 公司在 CUDA5 之后便不再使用 cutil.h <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cuda-samples/blob/master/Samples/2_Concepts_and_Techniques/EGLSync_CUDAEvent_Interop/EGLSync_CUDAEvent_Interop.cu#L83">CUDA_SAFE_CALL </a></li>
<li><a target="_blank" rel="noopener" href="https://bohipat.wordpress.com/2014/07/11/replacing-cutil-in-cuda-5-0/">replacing-cutil-in-cuda-5-0</a></li>
</ol>
<h2 id="tensor-core"><a href="#tensor-core" class="headerlink" title="tensor core"></a>tensor core</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/620185229">4 种 tensor core 调用方法</a><ul>
<li>WMMA (Warp-level Matrix Multiply Accumulate) API</li>
<li>WMMA PTX (Parallel Thread Execution)</li>
<li>MMA (Matrix Multiply Accumulate) PTX</li>
<li>SASS</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/data-center/tensor-cores/">tensor-cores</a></li>
<li><code>--set roofline</code>能看到 tensor core roofline; 例如运行<code>cuda-samples/Samples/3_CUDA_Features/cudaTensorCoreGemm</code></li>
<li>cuda-samples&#x2F;Samples&#x2F;3_CUDA_Features 包含多个 tensor core 实例</li>
<li><code>Samples/3_CUDA_Features/cudaTensorCoreGemm</code>比<code>Samples/0_Introduction/matrixMul</code> 计算性能高很多<ul>
<li>cudaTensorCoreGemm 测试的是 fp16 性能</li>
</ul>
</li>
<li>cublas 满足特定条件才会使用 tensor core <a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cublas/index.html#tensor-core-usage">link</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc">Tensor Core Requirements</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1893665">FP32 vs TF32</a><ul>
<li>tf32 整数部分精度与 fp32 相同，小数部分与 fp16 相同， 只用了 19 位</li>
<li>包含 Amphere 架构性能</li>
<li>A100、H100 tf32 算力比 fp32 算力高 8 倍左右</li>
<li>3090 上 tf32 算力跟 fp32 算力相同</li>
<li>最新 trtexec 默认是 tf32，注意和 python fp32 计算比较时结果误差可能较大</li>
</ul>
</li>
</ol>
<h2 id="wmma-vs-mma"><a href="#wmma-vs-mma" class="headerlink" title="wmma vs mma"></a>wmma vs mma</h2><ol>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-multiply-accumulate-instructions">warp-level-matrix-multiply-accumulate-instructions</a></li>
</ol>
<h3 id="tenosr-core-和-cuda-core"><a href="#tenosr-core-和-cuda-core" class="headerlink" title="tenosr core 和 cuda core"></a>tenosr core 和 cuda core</h3><ol>
<li>计算层级：CUDA Core 是线程级别，Tensor Core 是 warp 级别</li>
<li>计算维度：CUDA Core 是一维逐点计算，Tensor Core 是二维逐 tile 计算</li>
<li>CUDA Core 是为通用计算设计，而 Tensor Core 是为特定类型的计算（主要是深度学习中的矩阵运算）优化。</li>
<li>在 NVIDIA 的某些 GPU 架构中，例如 Volta、Turing 和 Ampere，CUDA Core 和 Tensor Core 共同存在。它们可以根据计算任务的性质协同工作，提高整体的计算效率。</li>
<li>在执行深度学习任务时，Tensor Core 可以显著加速计算过程，相较于仅使用 CUDA Core，能实现更快的训练和推理速度。</li>
<li>RT core 用于光线追踪</li>
<li>芯片手册中有 cuda core 算力和 tensor core 算力</li>
<li>利用 tensor core 才能达到最大算力</li>
</ol>
<h2 id="build"><a href="#build" class="headerlink" title="build"></a>build</h2><ol>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#application-compatibility">application-compatibility</a></li>
<li><code>-arch=sm_70</code> is a shorthand for <code>-arch=compute_70 -code=compute_70,sm_70</code> (which is the same as <code>-gencode arch=compute_70,code=\&quot;compute_70,sm_70\&quot;</code>)</li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-gpus">Your GPU Compute Capability</a> 包含各种 gpu arch</li>
</ol>
<h2 id="cuda-samples"><a href="#cuda-samples" class="headerlink" title="cuda samples"></a><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cuda-samples">cuda samples</a></h2><ol>
<li>可以全部 sample 一起编译</li>
<li><code>cuobjdump -all  ./matrixMulDrv</code>可以看可执行程序 arch 等信息</li>
<li><code>make SMS=&quot;86&quot;</code>选择 arch</li>
<li><code>make dbg=1</code> debug 编译</li>
<li><code>Samples/1_Utilities/bandwidthTest</code> 可以查看 host &lt;-&gt; memory 之间传输速度</li>
<li>(good)<code>Samples/1_Utilities/deviceQuery</code> 可以查看设备信息, 包含 arch 信息, 多少 sm，每个 sm 多少 cuda core</li>
<li>11.6 之后代码放在 github 上</li>
<li><a target="_blank" rel="noopener" href="https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial01/">hello world</a></li>
<li><code>/usr/local/cuda-11.4/samples</code> tree -L 2</li>
<li><code>/usr/local/cuda-10.2/samples/0_Simple/vectorAdd</code></li>
<li><code>nvprof ./vectorAdd</code> 查看 kenerl 耗时</li>
</ol>
<h2 id="CUDALibrarySamples"><a href="#CUDALibrarySamples" class="headerlink" title="CUDALibrarySamples"></a><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/CUDALibrarySamples">CUDALibrarySamples</a></h2><ol>
<li>cublas</li>
<li>cutlass</li>
<li>npp</li>
</ol>
<h3 id="cuda-info"><a href="#cuda-info" class="headerlink" title="cuda info"></a>cuda info</h3><ol>
<li><a target="_blank" rel="noopener" href="https://linuxconfig.org/how-to-get-cuda-cores-count-on-linux">查看 cuda core</a><br><code>cd /usr/local/cuda-11.4/samples/1_Utilities/deviceQuery &amp;&amp; make &amp;&amp; ./deviceQuery</code></li>
<li>bandwith test<br><code>cd /usr/local/cuda-11.4/samples/1_Utilities/bandwidthTest &amp;&amp; make &amp;&amp; ./bandwidthTest</code></li>
</ol>
<h2 id="NVIDIA-Developer-Tools"><a href="#NVIDIA-Developer-Tools" class="headerlink" title="NVIDIA Developer Tools"></a><a target="_blank" rel="noopener" href="https://developer.nvidia.com/tools-overview">NVIDIA Developer Tools</a></h2><ol>
<li>各工具关系<br><img src="https://i.ibb.co/2qN87rv/QVJcq2r-QQ3.png" alt="tools"></li>
</ol>
<h3 id="nsight-system"><a href="#nsight-system" class="headerlink" title="nsight system"></a>nsight system</h3><ol>
<li>分析系统性能， help you to pinpoint performance issues and identify optimization opportunities; 帮助分析性能问题和识别优化机会<ul>
<li>gpu kernel 运行时间太长</li>
<li>某些 process cpu 处理时间太长</li>
<li>cpu gpu pcie copy 耗时过大</li>
</ul>
</li>
<li>GPU timeline 方便分析</li>
<li>profiling 可以简单地分为粗粒度和细粒度。粗粒度主要是判断瓶颈是不是在 GPU 上，具体又是哪个 kernel，典型代表就是 nsight system 工具，会显示出整个程序的 timeline。可以从 timeline 上直接清晰明了地看到瓶颈是在 CPU 还是 GPU，如果是 GPU，那又是在 GPU 的哪个 kernel 上。</li>
<li>如果是 timeline 中 GPU kernel 的占比很小，CPU 占比很大，那说明瓶颈在 CPU 侧，需要注意是不是数据读取花了太多时间。如果 GPU kernel 的占比很大，说明瓶颈在 GPU 侧，需要重点花精力去优化 GPU kernel 实现。还有一种情况是，如果数据一直放在 GPU 上，但是 kernel 的时间占比不是特别多，那可能是因为 kernel 本身不太耗时，可能只运行了 4us。但 kernel lauch 就花了 6us。这个时间就要想着采用 kernel fusion 的方式，尽可能地在一个 kernel 里面多干点活。</li>
<li>点击时注意竖线上的小三角号, 表示关联</li>
<li>从结果分析看多个 stream 下 kernel 可以同时运行<ul>
<li>all stream 或者 kernels 显示不出所有的 kernel 运行，起始位置被其他 kernel 覆盖的检测不出来</li>
<li>all stream 显示不出的 kernel 可能在被隐藏的 stream 里，鼠标放到 kernel 上能显示出在第几个 stream</li>
</ul>
</li>
<li>打开文件注意生成 log 时的错误</li>
<li>gui 可以远程 profiling， 将 nsys 安装到 target 机器, 类似 compute<ul>
<li><code>~/.local/share/nsight_systems/nsys</code>安装路径</li>
</ul>
</li>
<li>nsys profile 类似 perf record 来记录信息</li>
<li>nsys stats 类似 perf stats 来查看统计信息<ul>
<li><code>nsys stats report1.nsys-rep</code></li>
<li><code>nsys stats --report cuda_gpu_trace report1.nsys-rep</code></li>
</ul>
</li>
<li>gui summary 里可以看 log 具体执行命令</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html#cuda-trace">跟踪 cuda</a><ul>
<li><code>--trace=cuda</code></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html#gpu-metrics">可以收集 gpu-metrics</a><ul>
<li>Is my GPU idle?</li>
<li>Is my GPU full? Enough kernel grids size and streams? Are my SMs and warp slots full?</li>
<li>Am I using TensorCores?</li>
<li>Is my instruction rate high?</li>
<li>Am I possibly blocked on IO, or number of warps, etc</li>
</ul>
</li>
<li>分析系统性能</li>
<li><code>nvprof</code> 旧版本</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html">user guide</a></li>
<li>NsightSystems-2023.2.1.122-3259852.msi 安装包</li>
<li>Nsight_Systems_User_Guide_2023.2.1.122-3259852.pdf 文档</li>
<li>鼠标放到 kernel 上有 kernel 详细信息，<ul>
<li>包括执行时间</li>
<li>latency：launch latency, 与执行时间不一样，latency 是 api 调用到 kernel 开始执行时间<ul>
<li><a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/nsys-timeline-end-start-is-not-the-same-as-latency/238043">NSys Timeline: End - Start is not the same as latency</a></li>
<li>CUDA kernel launch latency could be defined as the time range from the beginning of the launch API call to the beginning of the kernel execution.</li>
<li>执行的是 <code>cudaStreamSynchronize</code>, 没执行这个函数之前，stream 中的 kernel 不会 launch</li>
</ul>
</li>
<li>gird, block 设置;(注意，同一个 stream 下相同函数调用可能设置不一样导致时间不同)</li>
<li>一个线程用多少 register</li>
<li>理论 occupancy, 打开 gpu metrics 可以看对应的实际 occupancy</li>
<li>注意各种颜色</li>
</ul>
</li>
<li>可以在 thread 上看到 <code>cpu call stack</code><ul>
<li>每个线程对应行分层的最后一层 <code>sampling point</code></li>
<li>gpu 空闲时看 CPU 采样点 可以知道 CPU 在干啥</li>
<li>是否可以生成火焰图？</li>
</ul>
</li>
<li>timeline 上点击 stream 下的 tensorrt node， 可以显示对应的 kernel 执行和 cuda api 调用时间点, 注意竖线上的小三角号</li>
<li>tensorrt 多个维度来看<ul>
<li>thread</li>
<li>stream</li>
<li>cuda</li>
</ul>
</li>
<li>左上角 timeline view 可以选择 analysis summary，有各种总结</li>
<li>左侧右键选<code>show in events view</code>， 可以看具体时间, 可以在 all 上操作，看所有 event 运行时间</li>
<li>右键 reset room 显示全部</li>
<li><code>shift + mouseleftdoubleclick</code> timeline 可以找到对应 event 在 event view 位置</li>
<li><code>ctrl + mouseleftdoubleclick</code> timeline 可以 fit to screen</li>
<li><code>backspace</code> timeline 可以 undo room</li>
<li><code>ctrl + mouseleftdoubleclick</code> 可以找到 event view 对应的 timeline 位置</li>
<li><code>sudo nsys profile &lt;app&gt;</code></li>
<li><code>nsys stats report1.nsys-rep</code> 输出各种 report</li>
<li>可以看 cpu 执行情况， tensort 可以看详细算子耗时，也有对应 cuda 执行情况</li>
<li>analysis summary 中有各个线程的 cpu 利用率总结</li>
<li>可以关注 cpu 空闲的地方，为什么会空闲(同步数据？)</li>
<li>可以缩小看颜色占比，关注占比大的模块</li>
<li>cudaMemcpy 会阻塞 cpu 执行， 可以多注意 cudaMemcpy 影响</li>
<li>cudaStreamSynchronize 是 CUDA API 中的一个函数，用于等待指定的 CUDA 流上的所有 CUDA 核函数执行完毕。当 CUDA 核函数被执行时，它们会被添加到一个 CUDA 流中，这些核函数的执行可能是异步的，也可能是同步的，具体取决于如何在代码中调用它们。当我们调用 cudaStreamSynchronize 时，它将会阻塞当前 CPU 线程，直到指定的流上的所有核函数都执行完毕。</li>
</ol>
<h3 id="nsight-compute"><a href="#nsight-compute" class="headerlink" title="nsight compute"></a>nsight compute</h3><ol>
<li><code>balance throughput</code> compute and memory throughput are near; 平衡比较重要，不平衡时说明使用率较高的是瓶颈</li>
<li>Both SM (Compute) and Memory SOL report the categories’ throughput as the achieved percentage of utilization with respect to the theoretical maximum, i.e. the “Speed Of Light”. Both metrics are composed of sub-metrics, with the respective highest contributor defining the resulting value. The Breakdown tables below the chart can be used to identify all such contributors and their values. SM（计算）和内存 SOL 都将类别的吞吐量报告为相对于理论最大值（即“speed of light”）所实现的利用率百分比</li>
<li>SOL(speed of light): 相对于理论最大值的比例</li>
<li>有开销 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/2023.3/ProfilingGuide/index.html#overhead">overhead</a></li>
<li><code>metrics</code> 是指性能指标，这些指标用于衡量 CUDA 应用程序的性能和行为。性能指标可以包括各种硬件级别的统计数据，如内存访问效率、计算操作的执行时间、流处理器（SM）的利用率、寄存器使用情况、分支效率等等。</li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-performance-counters">permission 问题</a><ul>
<li>可以增加临时权限</li>
</ul>
</li>
<li>分析 kernel 性能, 可以选 kernel</li>
<li>NVIDIA Nsight Compute is an interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool. In addition, its baseline feature allows users to compare results within the tool. NVIDIA Nsight Compute provides a customizable and data-driven user interface and metric collection and can be extended with analysis scripts for post-processing results.</li>
<li>需要 windows 客户端通过 connect 将 ncu 送到开发环境<ul>
<li>可以通过 activity 复制命令到板端执行</li>
<li>metrics 可以选择 Sets 和 rules</li>
</ul>
</li>
<li>profile -&gt; metrics details<ul>
<li>点击统计界面某个 metrics 可以看 metrics 详细信息</li>
</ul>
</li>
<li><code>--import-source yes --source-folders ./</code> 导入 source</li>
<li><code>ncu --replay-mode application --set full  ./preprocess</code> 详细信息显示到命令行<ul>
<li>Duration 为 kernel 执行时间</li>
</ul>
</li>
<li><code>ncu -k matrixMul --print-summary per-gpu ./test</code> 查看某个 kernel 信息</li>
<li><code>/tmp/var/target/linux-desktop-glibc_2_11_3-x64/ncu --config-file off --export &quot;/tmp/var/test&quot; --force-overwrite --section-folder /tmp/var/sections --set full ./test</code></li>
<li><code>ncu --set full -f --export nsight_compute ./test</code></li>
<li><code>docker run -itd -v /mnt:/mnt -p 30022:22 --user root --gpus all --name=Ubuntu20.04-CUDA-admin --shm-size 2g --cap-add=SYS_ADMIN nvidia/cuda:11.4.3-cudnn8-devel-ubuntu20.04</code> 尽量使用官方 docker<ul>
<li>需要 <code>--gpus all --cap-add=SYS_ADMIN</code></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/index.html">nsight-compute docs</a><ul>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html">ProfilingGuide</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/NsightCompute/index.html">user manual</a></li>
</ul>
</li>
<li>左上角可以显示：<ul>
<li>session: gpu cpu 等信息</li>
<li>detailed：overview, compute, memory 等</li>
<li>source: 汇编代码及耗时</li>
<li>summary: 耗时，compute and memory throughput; 底下有优化建议</li>
</ul>
</li>
<li>可以添加 baseline，对比多次结果, 分析结果变化</li>
<li><code>ncu --set roofline</code>可以测量详细 roofline</li>
<li><code>ncu --metrics smsp__inst_executed.sum ./matrixMul</code> 打印 metrics</li>
<li><code>--set roofline</code>能看到 tensor core roofline; 例如运行<code>cuda-samples/Samples/3_CUDA_Features/cudaTensorCoreGemm</code></li>
</ol>
<h4 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h4><ol>
<li>The application returned an error code (9).<ul>
<li><code>ncu --replay-mode &lt;application/kernel&gt;</code> 加参数； 见<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/nsight-profiling-crashes-with-error-code-9/230094/5">link</a></li>
<li>可以使用<code>–replay-mode application</code>切换到应用程序重播。 这避免了内存存储需要重放。</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/2023.3/ProfilingGuide/index.html#replay">replay</a><ul>
<li>kernels might need to be replayed one or more times, since not all metrics can be collected in a single pass</li>
<li>the number of metrics originating from hardware (HW) performance counters that the GPU can collect at the same time is limited.</li>
<li>kernal replay 只重跑 kernel, application replay 重跑应用</li>
</ul>
</li>
</ol>
<h4 id="metrics"><a href="#metrics" class="headerlink" title="metrics"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/2023.3/ProfilingGuide/index.html#metrics-guide">metrics</a></h4><ol>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/2023.3/ProfilingGuide/index.html#metrics-reference">metrics-reference</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/2019.5/NsightComputeCli/index.html#nvprof-metric-comparison">nvprof-metric-comparison</a></li>
<li><code>ncu --devices 0 --query-metrics &gt;my_metrics.txt</code> 查看 metrics</li>
</ol>
<h4 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h4><ol>
<li>每项下面会有建议</li>
<li>compute 分析延迟很关键 目标是计算要掩盖延迟; 分析 stall 原因，stall 就是 warp 闲着</li>
<li>bank 冲突能解决指令的延迟</li>
<li>overview(GPU speed of light throughput)<ul>
<li><code>用于查看计算和内存吞吐对理论值的占比以及roofline</code></li>
<li>High-level overview of the <code>throughput for compute and memory</code> resources of the GPU 主要关注计算和内存</li>
<li>Achieved compute throughput and&#x2F;or memory bandwidth below 60.0% of peak typically indicate latency issues.</li>
<li>低于 60% 表明有延迟问题, 例如：指令有延迟，可以看 scheduler statistics 和 warp state statistics 进一步分析;延迟问题可能是由于 occupancy 较低， 也可能是指令执行延迟太大，无法隐藏;</li>
<li>roofline</li>
</ul>
</li>
<li>comupte workload<ul>
<li><code>用于分析 SM 计算资源使用情况</code></li>
<li>IPC</li>
<li>指令执行占比</li>
<li>LSU: load store unit</li>
</ul>
</li>
<li>memory workload<ul>
<li><code>用于分析 GPU memory 使用情况</code></li>
<li>可以选择看 transfer size 和 throughput</li>
<li>要提升 cache 命中率</li>
<li>多用 shared memory(可作为中间结果)</li>
<li>多种 memory<ul>
<li>global</li>
<li>local: 线程私有的。local memory 不是物理空间，而是 global memory 的一部分，所以延时较大。</li>
<li>texture: 只读?</li>
<li>surface</li>
<li>load global stroe shared</li>
<li>shared</li>
</ul>
</li>
</ul>
</li>
<li>scheduler statistics<ul>
<li><code>用于分析每个 scheduler active, eligible, issue warp 情况</code></li>
<li>Issued Warp Per Scheduler: 平均每个 cycle 发射的 warp 数</li>
<li>On cycles with no eligible warps, the issue slot is skipped and no instruction is issued. Having many skipped issue slots indicates poor latency hiding.</li>
<li>Out of the maximum of 12 warps per scheduler(Ampere 1 个 SM 有 4 个 scheduler), this kernel allocates an average of 2.00 active warps per scheduler, but only an average of 0.07 warps were eligible(合格的) per cycle.</li>
<li>可以看出平局每个 sm 有几个 warps; 参考可以看 Theoretical Occupancy 是多少，有可能受 regitster 和 shared memory 等限制</li>
<li>Eligible warps are the subset of active warps that are ready to issue their next instruction.</li>
</ul>
</li>
<li>warp state statistics<ul>
<li><code>用于分析 warp stall 原因</code></li>
<li>Check the Warp Stall Sampling (source counters 中) table for the top stall locations in your source based on sampling data. 查看 stalll source 位置</li>
<li>statll 解决办法： Try to increase the number of active warps to hide the existent latency or try changing the instruction mix to utilize all available pipelines in a more balanced way.</li>
<li>The warp cycles per instruction define the latency between two consecutive instructions. 每条指令的 warp 周期定义两个连续指令之间的延迟</li>
<li>可以查看两个 warp instruction 之间 cyles 组成： stall math pipe throttle, stall mio throttle;</li>
</ul>
</li>
<li>instructions statistics<ul>
<li><code>用于查看指令的类型和执行次数</code></li>
<li>Statistics of the executed low-level assembly instructions (SASS). SASS 指令统计</li>
<li>统计执行的指令数，可以看出哪些指令执行的较多</li>
</ul>
</li>
<li>launch statistics<ul>
<li><code>用于查看 grid, block 设置和 register, shared memory 使用情况</code></li>
</ul>
</li>
<li>occupancy<ul>
<li><code>用于查看occupancy情况及限制原因</code></li>
<li>Occupancy is the ratio of the number of active warps per multiprocessor to the maximum number of possible active warps.</li>
<li>可以看出理论占用率及不能到 100%的原因</li>
<li>一个 warp 中用太多 register 和 shared memory 会影响 Occupancy， Occupancy 会影响 scheduler， 进而会影响延迟隐藏</li>
<li>Occupancy 是 CUDA 编程中一个重要的性能指标,它表示 GPU 中 Streaming Multiprocessor (SM)上的处理单元被运用的比例。</li>
<li>右上角点开 table，可以可视化</li>
</ul>
</li>
<li>source counters<ul>
<li><code>用于分析分支指令是否有影响，是否合并访存</code></li>
<li>Source metrics, including branch efficiency and sampled warp stall reasons.</li>
</ul>
</li>
</ol>
<h3 id="nsgiht-graphics"><a href="#nsgiht-graphics" class="headerlink" title="nsgiht graphics"></a>nsgiht graphics</h3><h2 id="cuda-debug"><a href="#cuda-debug" class="headerlink" title="cuda-debug"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-visual-studio-code-edition/cuda-debugger/index.html">cuda-debug</a></h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/tree/main/examples/02_dump_reg_shmem">02_dump_reg_shmem</a><ul>
<li>dump_shmem</li>
<li>dump_fragment</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#compiling-the-application">compiling 要求</a><ul>
<li><code>nvcc -g -G foo.cu -o foo</code></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://marketplace.visualstudio.com/items?itemName=NVIDIA.nsight-vscode-edition">vs code plugin: nsight-vscode-edition</a></li>
<li>shared memory(L1 cache?) dump<ul>
<li>申请 shared memory 变量，存中间结果， shared memory 对 block 全局可见。</li>
<li>可以申请多于 L1 cache 大小的存储，系统会自动调度</li>
</ul>
</li>
<li>传递主存指针到 kernel，用于 dump</li>
<li>print<ul>
<li><code>if(threadIdx.x == 0) printf...</code> 条件打印</li>
<li>cudaDeviceReset(); 不打印可以加</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/nsight-visual-studio-code-edition">nsight-visual-studio-code-edition</a></li>
<li>cuda-gdb</li>
<li>cuda-memcheck</li>
</ol>
<h2 id="link"><a href="#link" class="headerlink" title="link"></a>link</h2><ol>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-zone">cuda 相关开发</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/gpu-accelerated-libraries">gpu-accelerated-libraries</a></li>
</ol>
<h2 id="build-1"><a href="#build-1" class="headerlink" title="build"></a>build</h2><h3 id="cmake"><a href="#cmake" class="headerlink" title="cmake"></a>cmake</h3><ol>
<li><a target="_blank" rel="noopener" href="https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9444-build-systems-exploring-modern-cmake-cuda-v2.pdf">build-systems-exploring-modern-cmake-cuda-v2.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/building-cuda-applications-cmake/">building-cuda-applications-cmake</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/CUDALibrarySamples/blob/master/cuBLAS/Level-3/gemm/CMakeLists.txt">CUDALibrarySamples</a></li>
<li>basic sample</li>
</ol>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.8</span>)</span><br><span class="line"><span class="comment"># project(my_cuda_project LANGUAGES CXX CUDA)</span></span><br><span class="line"><span class="keyword">project</span>(my_cuda_project)</span><br><span class="line"><span class="keyword">enable_language</span>(CUDA) <span class="comment"># 需要enable language</span></span><br><span class="line"><span class="keyword">add_executable</span>(preprocess kernel.cu preprocess.cpp )</span><br><span class="line"><span class="keyword">find_package</span>(CUDA REQUIRED)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(preprocess <span class="variable">$&#123;CUDA_cudart_LIBRARY&#125;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="compile"><a href="#compile" class="headerlink" title="compile"></a>compile</h3><ol>
<li>混合编译, 注意-lcudart 顺序</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nvcc -O3 -c kernel.cu -o kernel.o</span><br><span class="line">g++ -L /usr/local/cuda/targets/x86_64-linux/lib/ preprocess.cpp kernel.o -o preprocess -lcuda -lcudart &amp;&amp; ./preprocess</span><br></pre></td></tr></table></figure>

<h2 id="unified-memory"><a href="#unified-memory" class="headerlink" title="unified memory"></a>unified memory</h2><ol>
<li>不明白原理时不推荐使用</li>
<li>优缺点<ul>
<li>优点<ul>
<li>简化代码编写和内存管理：cudaMallocManaged 可以简化 CPU 和 GPU 之间数据传递的代码，无需手动管理内存迁移。</li>
</ul>
</li>
<li>缺点<ul>
<li>可能降低性能：在某些情况下，统一内存可能会降低性能，例如在数据访问模式为稀疏的情况下。</li>
<li>可能增加内存占用：统一内存可能会增加内存占用，因为它需要在 CPU 和 GPU 内存中都保留一份数据副本。</li>
</ul>
</li>
</ul>
</li>
<li><code>cudaMalloc -&gt; cudaMallocManaged(&amp;x, N*sizeof(float));</code></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/21990899/23011500">使用 cudaMallocManaged 情况</a><ul>
<li>You are working on a Jetson device.</li>
</ul>
</li>
<li>runtime 负责 copy</li>
<li>unified memory cpu 访问时需要同一个线程。不同线程会 bus error。</li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/unified-memory-cuda-beginners/">Unified Memory for CUDA Beginners</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/">Maximizing Unified Memory Performance in CUDA</a></li>
</ol>
<h2 id="design"><a href="#design" class="headerlink" title="design"></a>design</h2><ol>
<li>lidar prprocess; 三重 for, 最外层作为 x, 一个线程执行一个最里面 for 的内容，</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">TransposeDim</span><span class="params">(<span class="type">int</span> kmax_num_point_pillar,</span></span></span><br><span class="line"><span class="params"><span class="function">                   <span class="type">int</span> kmax_num_point,</span></span></span><br><span class="line"><span class="params"><span class="function">                   <span class="type">int</span> kdim,</span></span></span><br><span class="line"><span class="params"><span class="function">                   <span class="type">int</span> voxel_num, <span class="type">float</span> *voxel_data, <span class="type">int8_t</span> *features_s8)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> kWC = kmax_num_point_pillar * kdim;</span><br><span class="line">  <span class="type">int</span> kHW = kmax_num_point * kmax_num_point_pillar;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> c = <span class="number">0</span>; c &lt; kdim; ++c)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> w = <span class="number">0</span>; w &lt; kmax_num_point_pillar; ++w)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> h = <span class="number">0</span>; h &lt; voxel_num; ++h)</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="type">int</span> old_index = h * kWC + w * kdim + c;</span><br><span class="line">        <span class="type">int</span> new_index = c * kHW + w * kmax_num_point + h;</span><br><span class="line">        <span class="type">float</span> features_tmp = <span class="built_in">round</span>(<span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(voxel_data[old_index]));</span><br><span class="line">        features_tmp = std::<span class="built_in">min</span>(std::<span class="built_in">max</span>(features_tmp, <span class="number">-128.f</span>), <span class="number">127.f</span>);</span><br><span class="line">        features_s8[new_index] = <span class="built_in">static_cast</span>&lt;<span class="type">int8_t</span>&gt;(features_tmp);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">TransposeDim5Kernel</span><span class="params">(<span class="type">int</span> kmax_num_point_pillar,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    <span class="type">int</span> kmax_num_point,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    <span class="type">int</span> kdim,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    <span class="type">int</span> voxel_num,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    <span class="type">float</span> *voxel_data,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    <span class="type">int8_t</span> *features_s8)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> c = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="type">int</span> w = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">  <span class="type">int</span> h = blockIdx.z * blockDim.z + threadIdx.z;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (c &lt; kdim &amp;&amp; w &lt; kmax_num_point_pillar &amp;&amp; h &lt; voxel_num)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="type">int</span> kWC = kmax_num_point_pillar * kdim;</span><br><span class="line">    <span class="type">int</span> kHW = kmax_num_point * kmax_num_point_pillar;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> old_index = h * kWC + w * kdim + c;</span><br><span class="line">    <span class="type">int</span> new_index = c * kHW + w * kmax_num_point + h;</span><br><span class="line">    <span class="type">float</span> features_tmp = <span class="built_in">round</span>(voxel_data[old_index]);</span><br><span class="line"></span><br><span class="line">    features_tmp = <span class="built_in">fmaxf</span>(<span class="built_in">fminf</span>(features_tmp, <span class="number">127.f</span>), <span class="number">-128.f</span>);</span><br><span class="line"></span><br><span class="line">    features_s8[new_index] = <span class="built_in">static_cast</span>&lt;<span class="type">int8_t</span>&gt;(features_tmp);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">4</span>, <span class="number">4</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">gridSize</span><span class="params">((kdim + blockSize.x - <span class="number">1</span>) / blockSize.x,</span></span></span><br><span class="line"><span class="params"><span class="function">                (kmax_num_point_pillar + blockSize.y - <span class="number">1</span>) / blockSize.y,</span></span></span><br><span class="line"><span class="params"><span class="function">                (voxel_num + blockSize.z - <span class="number">1</span>) / blockSize.z)</span></span>;</span><br><span class="line"></span><br><span class="line">TransposeDim5Kernel&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(kmax_num_point_pillar,</span><br><span class="line">                                               kmax_num_point,</span><br><span class="line">                                               kdim,</span><br><span class="line">                                               voxel_num,</span><br><span class="line">                                               d_voxel_data,</span><br><span class="line">                                               d_features_s8);</span><br></pre></td></tr></table></figure>

    </div>

    
    
    
      

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>贾夕阳
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://jiaxiyang.github.io/2022/03/10/Cuda/" title="Cuda">https://jiaxiyang.github.io/2022/03/10/Cuda/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Cuda/" rel="tag"><i class="fa fa-tag"></i> Cuda</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/03/03/Doxygen/" rel="prev" title="Doxygen">
      <i class="fa fa-chevron-left"></i> Doxygen
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/03/18/samba/" rel="next" title="samba">
      samba <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Note"><span class="nav-number">1.</span> <span class="nav-text">Note</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%A7%E8%83%BD"><span class="nav-number">2.</span> <span class="nav-text">性能</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#papers"><span class="nav-number">2.1.</span> <span class="nav-text">papers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cuda-performance-guidelines"><span class="nav-number">2.2.</span> <span class="nav-text">cuda performance-guidelines</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="nav-number">2.3.</span> <span class="nav-text">优化技术</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%BF%E5%AD%98%E4%BC%98%E5%8C%96"><span class="nav-number">2.4.</span> <span class="nav-text">访存优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#reduction-%E4%BC%98%E5%8C%96"><span class="nav-number">2.5.</span> <span class="nav-text">reduction 优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#elementwise-%E4%BC%98%E5%8C%96"><span class="nav-number">2.6.</span> <span class="nav-text">elementwise 优化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NVCC"><span class="nav-number">3.</span> <span class="nav-text">NVCC</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PTX-Parallel-Thread-Execution"><span class="nav-number">4.</span> <span class="nav-text">PTX(Parallel Thread Execution)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SASS-Shader-Assembly"><span class="nav-number">5.</span> <span class="nav-text">SASS(Shader-Assembly)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sync"><span class="nav-number">6.</span> <span class="nav-text">sync</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bank-conflict"><span class="nav-number">7.</span> <span class="nav-text">bank conflict</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%AD%97"><span class="nav-number">8.</span> <span class="nav-text">关键字</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#occupancy-%E8%AE%BE%E8%AE%A1"><span class="nav-number">9.</span> <span class="nav-text">occupancy 设计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#stream"><span class="nav-number">10.</span> <span class="nav-text">stream</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cuda-grammer"><span class="nav-number">11.</span> <span class="nav-text">cuda grammer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tensor-core"><span class="nav-number">12.</span> <span class="nav-text">tensor core</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#wmma-vs-mma"><span class="nav-number">13.</span> <span class="nav-text">wmma vs mma</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tenosr-core-%E5%92%8C-cuda-core"><span class="nav-number">13.1.</span> <span class="nav-text">tenosr core 和 cuda core</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#build"><span class="nav-number">14.</span> <span class="nav-text">build</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cuda-samples"><span class="nav-number">15.</span> <span class="nav-text">cuda samples</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDALibrarySamples"><span class="nav-number">16.</span> <span class="nav-text">CUDALibrarySamples</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cuda-info"><span class="nav-number">16.1.</span> <span class="nav-text">cuda info</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NVIDIA-Developer-Tools"><span class="nav-number">17.</span> <span class="nav-text">NVIDIA Developer Tools</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#nsight-system"><span class="nav-number">17.1.</span> <span class="nav-text">nsight system</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nsight-compute"><span class="nav-number">17.2.</span> <span class="nav-text">nsight compute</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">17.2.1.</span> <span class="nav-text">遇到的问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#metrics"><span class="nav-number">17.2.2.</span> <span class="nav-text">metrics</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90"><span class="nav-number">17.2.3.</span> <span class="nav-text">结果分析</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nsgiht-graphics"><span class="nav-number">17.3.</span> <span class="nav-text">nsgiht graphics</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cuda-debug"><span class="nav-number">18.</span> <span class="nav-text">cuda-debug</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#link"><span class="nav-number">19.</span> <span class="nav-text">link</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#build-1"><span class="nav-number">20.</span> <span class="nav-text">build</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cmake"><span class="nav-number">20.1.</span> <span class="nav-text">cmake</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#compile"><span class="nav-number">20.2.</span> <span class="nav-text">compile</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#unified-memory"><span class="nav-number">21.</span> <span class="nav-text">unified memory</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#design"><span class="nav-number">22.</span> <span class="nav-text">design</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="贾夕阳"
      src="/images/coder2.jpg">
  <p class="site-author-name" itemprop="name">贾夕阳</p>
  <div class="site-description" itemprop="description">深度学习/自动驾驶/C++/性能优化</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">175</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jiaxiyang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jiaxiyang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2024/02/22/pagedattention/" title="2024&#x2F;02&#x2F;22&#x2F;pagedattention&#x2F;">pagedattention</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/02/03/flashattention/" title="2024&#x2F;02&#x2F;03&#x2F;flashattention&#x2F;">flashattention</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/01/26/cutlass/" title="2024&#x2F;01&#x2F;26&#x2F;cutlass&#x2F;">cutlass</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/01/25/OpenCL/" title="2024&#x2F;01&#x2F;25&#x2F;OpenCL&#x2F;">OpenCL</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/01/14/Efficient-LLM/" title="2024&#x2F;01&#x2F;14&#x2F;Efficient-LLM&#x2F;">Efficient-LLM</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">贾夕阳</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">484k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">7:20</span>
</div>

<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("06/26/2020 14:52:10");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : '[object Object]',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://jiaxiyang.github.io/2022/03/10/Cuda/',]
      });
      });
  </script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'g32ipLmEye1u5l6wBGRJt03S-gzGzoHsz',
      appKey     : 'zHgLkAICsZUl9Mf8LfdoVigP',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

  

  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>





 
</body>
</html>

