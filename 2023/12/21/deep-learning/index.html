<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0-rc2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jiaxiyang.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="基础概念 Deep learning is an approach to machine learning characterized by deep stacks of computations. This depth of computation is what has enabled deep learning models to disentangle the kinds of compl">
<meta property="og:type" content="article">
<meta property="og:title" content="deep-learning">
<meta property="og:url" content="https://jiaxiyang.github.io/2023/12/21/deep-learning/index.html">
<meta property="og:site_name" content="Xiyang">
<meta property="og:description" content="基础概念 Deep learning is an approach to machine learning characterized by deep stacks of computations. This depth of computation is what has enabled deep learning models to disentangle the kinds of compl">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.ibb.co/bWthfyQ/o-XLOSNus4-J.png">
<meta property="article:published_time" content="2023-12-21T05:18:24.000Z">
<meta property="article:modified_time" content="2024-01-22T06:39:31.662Z">
<meta property="article:author" content="贾夕阳">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.ibb.co/bWthfyQ/o-XLOSNus4-J.png">

<link rel="canonical" href="https://jiaxiyang.github.io/2023/12/21/deep-learning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>deep-learning | Xiyang</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WGS6S6YFJ6"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-WGS6S6YFJ6');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Xiyang</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Think twice, code once!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">173</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">44</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">55</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/jiaxiyang" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiaxiyang.github.io/2023/12/21/deep-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/coder2.jpg">
      <meta itemprop="name" content="贾夕阳">
      <meta itemprop="description" content="深度学习/自动驾驶/C++/性能优化">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiyang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          deep-learning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-12-21 13:18:24" itemprop="dateCreated datePublished" datetime="2023-12-21T13:18:24+08:00">2023-12-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-01-22 14:39:31" itemprop="dateModified" datetime="2024-01-22T14:39:31+08:00">2024-01-22</time>
              </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2023/12/21/deep-learning/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/12/21/deep-learning/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><ol>
<li><p><code>Deep learning</code> is an approach to machine learning characterized by deep stacks of computations. This depth of computation is what has enabled deep learning models to disentangle the kinds of complex and hierarchical patterns found in the most challenging real-world datasets.</p>
</li>
<li><p><code>SGD</code>: 全称为 Stochastic Gradient Descent 即随机梯度下降,是机器学习中常用的优化算法,用于训练各种模型(如神经网络)寻找最优参数, optimizer</p>
</li>
<li><p><code>neuron</code> : the Linear Unit y &#x3D; wx + b; w: weight, b: bias</p>
</li>
<li><p><code>layers</code>: Neural networks typically organize their neurons into layers. When we collect together linear units having a common set of inputs we get a <code>dense layer</code>.</p>
</li>
<li><p>一个 layer 共享一个 bias: y &#x3D; w1 _ x1 + b1 + w2 _ x2 + b2 &#x3D;&#x3D;&gt; y &#x3D; w1 _ x1 + w2 _ x2 + b</p>
</li>
<li><p><code>ReLU</code>: rectified linear unit</p>
</li>
<li><p><code>Linear Unit + ReLU</code>: y &#x3D; max(0, w * x + b)</p>
</li>
<li><p>Without activation functions, neural networks can only learn linear relationships.</p>
</li>
<li><p>A <code>loss function</code> that measures how good the network’s predictions are.</p>
</li>
<li><p>An <code>optimizer</code> that can tell the network how to change its weights.</p>
</li>
<li><p><code>MAE</code>: mean absolute error; loss function, for regression</p>
</li>
<li><p>Each iteration’s sample of training data is called a <code>minibatch</code> (or often just “batch”), while a complete round of the training data is called an <code>epoch</code>.</p>
</li>
<li><p>The <code>learning rate</code> and the size of the <code>minibatches</code> are the two parameters that have the largest effect on how the SGD training proceeds.</p>
</li>
<li><p><code>Adam</code> is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is “self tuning”, in a sense). Adam is a great general-purpose optimizer.</p>
</li>
<li><p><code>Underfitting the training set</code> is when the loss is not as low as it could be because the model hasn’t learned enough signal.</p>
</li>
<li><p><code>Overfitting the training set</code> is when the loss is not as low as it could be because the model learned too much noise. The trick to training deep learning models is finding the best balance between the two.</p>
</li>
<li><p><code>Early Stopping</code>: stop the training whenever it seems the validation loss isn’t decreasing anymore. Interrupting the training this way is called early stopping. Once we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured.</p>
</li>
<li><p><code>dropout layer</code> we randomly drop out some fraction of a layer’s input units every step of training, making it much harder for the network to learn those spurious patterns in the training data. Instead, it has to search for broad, general patterns, whose weight patterns tend to be more robust. 可以纠正过拟合</p>
</li>
<li><p><code>Batch Normalization layer</code></p>
<ul>
<li>why? Features that tend to produce activations of very different sizes can make for unstable training behavior.</li>
<li>A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters.</li>
<li>做两次 normalize, 先基于输入的 batch 数据做， 后基于训练的均值和方差来做</li>
<li>Models with batchnorm tend to need fewer epochs to complete training. Moreover, batchnorm can also fix various problems that can cause the training to get “stuck”.</li>
<li>get better performance if you standardize your data before using it for training</li>
</ul>
</li>
<li><p>The main difference regression and classification is in the loss function we use and in what kind of outputs we want the final layer to produce. 主要区别是损失函数和最后一层的输出类型</p>
</li>
<li><p><code>Accuracy</code> is one of the many metrics in use for measuring success on a classification problem. Accuracy is the ratio of correct predictions to total predictions: <code>accuracy = number_correct / total</code></p>
</li>
<li><p><code>Cross-Entropy</code> 交叉熵</p>
<ul>
<li>Cross-entropy is a sort of measure for the distance from one probability distribution to another.</li>
<li>SGD needs a loss function that changes smoothly, but accuracy, being a ratio of counts, changes in “jumps”. So, we have to choose a substitute to act as the loss function. This substitute is the cross-entropy function.</li>
<li>With regression, our goal was to minimize the distance between the expected outcome and the predicted outcome. We chose MAE to measure this distance.</li>
<li>For classification, what we want instead is a distance between probabilities, and this is what cross-entropy provides.</li>
</ul>
</li>
<li><p><code>softmax</code> 也是激活函数， layer to layer; not functions of a single fold x; 在 softmax 函数的实现中减去最大值是一种数值稳定性的技巧。从所有输入值中减去同一个常数不会改变函数的输出。如果 x 很大，可能导致 exp(x)溢出</p>
</li>
<li><p><code>relu</code> 是 single x 的激活函数</p>
</li>
<li><p><code>MLP, CNN, RNN, Transformer</code> 四大深度学习架构 Multilayer Perceptron(MLP)</p>
</li>
<li><p>样本和特征, batch 是样本</p>
</li>
<li><p><code>正则化(Regularization)</code> 指的是在训练过程中添加额外信息以防止模型过度拟合的技术。</p>
<ul>
<li>L1 正则化:在损失函数中添加模型权重参数绝对值的和,使权重 decay 到 0,从而使模型更稀疏。</li>
<li>L2 正则化:在损失函数中添加模型权重参数平方和,惩罚大的参数值,使权重较为平均分布,避免个别权重参数过大。也称为权重衰减(weight decay)。</li>
<li>Early Stopping:在模型测试指标不再改善时中止训练,防止过拟合。</li>
<li>Dropout:以一定概率随机置部分节点为 0,增加模型泛化能力</li>
<li>Data Augmentation:人工生成更多训练数据,改善模型泛化能力。</li>
<li>Batch Normalization: 通过调整网络中间层的激活值，使其在训练时保持一个更稳定的分布。虽然其主要目的是加快训练过程，但它也有一定的正则化效果。</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html">Batch Normalization 计算</a></p>
<ul>
<li>全连接层<br>仿射变换和激活函数之间;对 minibatch 整体做 normalization</li>
<li>卷积<br>卷积层之后和非线性激活函数之前; 对每个通道分别做 normalization; NCHW, 固定 C; 对于 RGB， 相当于 R, G, B 单独做 normalization</li>
<li>预测时：均值和方差为整个训练数据集的样本均值和方差(或者学习的均值和方差)</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/LXP-Never/p/11566064.html">各种 normlization 方法， 带图</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.tensorflow.org/2022/11/whats-new-in-tensorflow-211.html">文本 normalization 图示</a></p>
<ul>
<li>layer norm:输入一句话直接对其输出做 norm，不用管其他句子</li>
</ul>
</li>
<li><p><code>SiLU: f(x) = s * sigmoid(x)</code></p>
</li>
<li><p>图神经网络（Graph Neural Networks，GNN)</p>
</li>
</ol>
<h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html">卷积</a></h3><ol>
<li>每个卷积核输出一个 feature map； 代表一种特征</li>
</ol>
<h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/learn/intro-to-deep-learning">kaggle intro-to-deep-learning</a></li>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/code/ryanholbrook/deep-learning-animations-and-illustrations/notebook">sgd 动画</a></li>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/code/ryanholbrook/overfitting-and-underfitting">overfitting-and-underfitting</a></li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Activation_function">激活函数</a></li>
</ol>
<h3 id="术语表"><a href="#术语表" class="headerlink" title="术语表"></a>术语表</h3><ol>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/0evrjcivb5ArZGLQ4tGrmg">深度学习速查词典</a></li>
<li><a target="_blank" rel="noopener" href="https://developers.google.com/machine-learning/glossary?hl=zh-cn">google 机器学习术语表</a></li>
</ol>
<h3 id="全连接层与矩阵计算"><a href="#全连接层与矩阵计算" class="headerlink" title="全连接层与矩阵计算"></a>全连接层与矩阵计算</h3><ol>
<li><a target="_blank" rel="noopener" href="https://excalidraw.com/#json=EUPwP_pkPfoNDDVEC4b71,-89u61cxUzIS_dhKYsdHQQ">图示</a><br><img src="https://i.ibb.co/bWthfyQ/o-XLOSNus4-J.png" alt="图"></li>
<li>输出的每个神经元可以看到所有输入，提取了输入的某种特征</li>
<li>两个相乘的矩阵分别为 m×k 和 k×n 时，计算强度的计算略有不同。在这种情况下，矩阵乘法需要进行大约 m×n×k 次乘法和相同数量的加法操作。<ul>
<li><code>浮点运算次数</code>：每个元素的计算涉及 k 次乘法和 k-1 次加法（对于每行和每列中的每个交叉点）。因此，总的浮点运算次数大约是 2m×n×k 次。</li>
<li><code>内存操作</code>：如果每个矩阵元素是单精度浮点数（4 字节），那么矩阵 A 需要 m×k×4 字节，矩阵 B 需要 k×n×4 字节，矩阵 C 需要 m×n×4 字节的内存。因此，总的内存操作大约是 (m×k + k×n + m×n)×4 字节。</li>
<li><code>计算强度（算术强度)</code>可以表示为： <code>2mnk/(4(mk + kn + mn)) = 0.5/((1/n + 1/m + 1/k))</code>; 计算强度与 m, k, n 成正比, 实际中，由于现代处理器和 GPU 上的内存缓存效应，以及各种数学库和编译器优化技术的应用，真实的计算强度可能会有所不同。</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html">dl-performance-matrix-multiplication</a><ul>
<li>包含 tensor core 计算</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/flame/how-to-optimize-gemm/blob/master/src/MMult_4x4_5.c#L54C2-L78C4">矩阵分块减少访存示例</a></li>
</ol>

    </div>

    
    
    
      

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>贾夕阳
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://jiaxiyang.github.io/2023/12/21/deep-learning/" title="deep-learning">https://jiaxiyang.github.io/2023/12/21/deep-learning/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/12/19/multimodal/" rel="prev" title="multimodal">
      <i class="fa fa-chevron-left"></i> multimodal
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/12/24/llama2-c/" rel="next" title="llama2">
      llama2 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="nav-number">1.</span> <span class="nav-text">基础概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF"><span class="nav-number">1.1.</span> <span class="nav-text">卷积</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#links"><span class="nav-number">2.</span> <span class="nav-text">links</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%AF%E8%AF%AD%E8%A1%A8"><span class="nav-number">2.1.</span> <span class="nav-text">术语表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E4%B8%8E%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97"><span class="nav-number">2.2.</span> <span class="nav-text">全连接层与矩阵计算</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="贾夕阳"
      src="/images/coder2.jpg">
  <p class="site-author-name" itemprop="name">贾夕阳</p>
  <div class="site-description" itemprop="description">深度学习/自动驾驶/C++/性能优化</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">173</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jiaxiyang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jiaxiyang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2024/01/26/cutlass/" title="2024&#x2F;01&#x2F;26&#x2F;cutlass&#x2F;">cutlass</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/01/25/OpenCL/" title="2024&#x2F;01&#x2F;25&#x2F;OpenCL&#x2F;">OpenCL</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/01/14/Efficient-LLM/" title="2024&#x2F;01&#x2F;14&#x2F;Efficient-LLM&#x2F;">Efficient-LLM</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/01/11/blas/" title="2024&#x2F;01&#x2F;11&#x2F;blas&#x2F;">blas</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/01/10/llama-cpp/" title="2024&#x2F;01&#x2F;10&#x2F;llama-cpp&#x2F;">llama.cpp</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">贾夕阳</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">447k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">6:46</span>
</div>

<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("06/26/2020 14:52:10");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : '[object Object]',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://jiaxiyang.github.io/2023/12/21/deep-learning/',]
      });
      });
  </script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'g32ipLmEye1u5l6wBGRJt03S-gzGzoHsz',
      appKey     : 'zHgLkAICsZUl9Mf8LfdoVigP',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

  

  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>





 
</body>
</html>

